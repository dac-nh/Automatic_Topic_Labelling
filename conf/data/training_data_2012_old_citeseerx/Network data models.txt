ID|Title|Summary
1|Data networks|a b s t r a c t In this paper we illustrate the core technologies at the basis of the European SPADnet project (www. spadnet.eu), and present the corresponding first results. SPADnet is aimed at a new generation of MRI-compatible, scalable large area image sensors, based on CMOS technology, that are networked to perform gamma-ray detection and coincidence to be used primarily in (Time-of-Flight) Positron Emission Tomography (PET). The project innovates in several areas of PET systems, from optical coupling to single-photon sensor architectures, from intelligent ring networks to reconstruction algorithms. In addition, SPADnet introduced the first computational model enabling study of the full chain from gamma photons to network coincidence detection through scintillation events, optical coupling, etc. &amp; 2013 Elsevier B.V. All rights reserved. 1.
2|Using Bayesian networks to analyze expression data|DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1.
3|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
4|A Bayesian method for the induction of probabilistic networks from data|This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems.
5|Learning Bayesian networks: The combination of knowledge and statistical data|We describe scoring metrics for learning Bayesian networks from a combination of user knowledge and statistical data. We identify two important properties of metrics, which we call event equivalence and parameter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user’s prior knowledge. In particular, a user can express his knowledge—for the most part—as a single prior Bayesian network for the domain. 1
7|Bayesian Network Classifiers|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.
8|Clustering Gene Expression Patterns|Recent advances in biotechnology allow researchers to measure expression levels for thousands of genes simultaneously, across different conditions and over time. Analysis of data produced by such experiments offers potential insight into gene function and regulatory mechanisms. A key step in the analysis of gene expression data is the detection of groups of genes that manifest similar expression patterns. The corresponding algorithmic problem is to cluster multi-condition gene expression patterns. In this paper we describe a novel clustering algorithm that was developed for analysis of gene expression data. We define an appropriate stochastic error model on the input, and prove that under the conditions of the model, the algorithm recovers the cluster structure with high probability. The running time of the algorithm on an n-gene dataset is O(n 2 (log(n)) c ). We also present a practical heuristic based on the same algorithmic ideas. The heuristic was implemented and its p...
9|Model selection and accounting for model uncertainty in graphical models using Occam&#039;s window|We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P-values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism which averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximising predictive ability. However, this has not been used in practice because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and we propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty byaveraging overamuch smaller set of models. An efficient search algorithm is developed for nding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable
11|Being Bayesian about network structure|Abstract. In many multivariate domains, we are interested in analyzing the dependency structure of the underlying distribution, e.g., whether two variables are in direct interaction. We can represent dependency structures using Bayesian network models. To analyze a given data set, Bayesian model selection attempts to find the most likely (MAP) model, and uses its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed order over network variables. This allows us to compute, for a given order, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orders rather than over network structures. The space of orders is smaller and more regular than the space of structures, and has much a smoother posterior “landscape”. We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach.
12|Learning the structure of dynamic probabilistic networks|Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains. 1
13|A Theory Of Inferred Causation|This paper concerns the empirical basis of causation, and addresses the following issues: 1. the clues that might prompt people to perceive causal relationships in uncontrolled observations. 2. the task of inferring causal models from these clues, and 3. whether the models inferred tell us anything useful about the causal mechanisms that underly the observations. We propose a minimal-model semantics of causation, and show that, contrary to common folklore, genuine causal influences can be distinguished from spurious covariations following standard norms of inductive reasoning. We also establish a sound characterization of the conditions under which such a distinction is possible. We provide an effective algorithm for inferred causation and show that, for a large class of data the algorithm can uncover the direction of causal influences as defined above. Finally, we address the issue of non-temporal causation.  
14|Theory Refinement on Bayesian Networks|Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by &#034;partial theory&#034;, &#034;alternative theory representation &#034;, etc. The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode. 1 Introduction Theory refinement is the task of updating a domain theory in the light of...
15|Minimum complexity density estimation|The minimum complexity or minimum description-length criterion developed by Kolmogorov, Rissanen, Wallace, So&amp;in, and others leads to consistent probability density estimators. These density estimators are defined to achieve the best compromise between likelihood and simplicity. A related issue is the compromise between accuracy of approximations and complexity relative to the sample size. An index of resolvability is studied which is shown to bound the statistical accuracy of the density estimators, as well as the information-theoretic redundancy.  
16|Learning Bayesian network structure from massive datasets: the “sparse candidate” algorithm|Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a sta-tistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of in-stances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the par-ents of each variable to belong to a small sub-set of candidates. We then search for a network that satisfies these constraints. The learned net-work is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. 1
17|Learning Bayesian Networks is NP-Complete|Algorithms for learning Bayesian networks from data havetwo components: a scoring metric and a  search procedure. The scoring metric computes a score reflecting the goodness-of-fit of the structure  to the data. The search procedure tries to identify network structures with high scores. Heckerman  et al. (1995) introduce a Bayesian metric, called the BDe metric, that computes the relative posterior  probabilityofanetwork structure given data. In this paper, we show that the search problem of  identifying a Bayesian network---among those where each node has at most K parents---that has a  relative posterior probability greater than a given constant is NP-complete, when the BDe metric  is used.  12.1 
18|Modeling Regulatory Networks With Weight Matrices|Systematic gene expression analyses provide comprehensive information about the  transcriptional response to different environmental and developmental conditions. With  enough gene expression data points, computational biologists may eventually generate  predictive computer models of transcription regulation. Such models will require  computational methodologies consistent with the behavior of known biological systems that  remain tractable. We represent regulatory relationships between genes as linear coefficients or  weights, with the &#034;net&#034; regulation influence on a gene&#039;s expression being the mathematical  summation of the independent regulatory inputs. Test regulatory networks generated with this  approach display stable and cyclically stable gene expression levels, consistent with known  biological systems. We include variables to model the effect of environmental conditions on  transcription regulation and observed various alterations in gene expression patterns in  response to e...
19|On the Logic of Causal Models|This paper explores the role of Directed Acyclic Graphs (DAGs) as a representation of conditional independence relationships. We show that DAGs offer polynomially sound and complete inference mechanisms for inferring conditional independence relationships from a given causal set of such relationships. As a consequence, d-separation, a graphical criterion for identifying independencies in a DAG, is shown to uncover more valid independencies then any other criterion. In addition, we employ the Armstrong property of conditional independence to show that the dependence relationships displayed by a DAG are inherently consistent, i.e. for every DAG D there exists some probability distribution P that embodies all the conditional independencies displayed in D and none other.  
20|The max-min hill-climbing bayesian network structure learning algorithm|Abstract. We present a new algorithm for Bayesian network structure learning, called Max-Min Hill-Climbing (MMHC). The algorithm combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. In our extensive empirical evaluation MMHC outperforms on average and in terms of various metrics several prototypical and state-of-the-art algorithms, namely the PC, Sparse Candidate, Three Phase Dependency Analysis, Optimal Reinsertion, Greedy Equivalence Search, and Greedy Search. These are the first empirical results simultaneously comparing most of the major Bayesian network algorithms against each other. MMHC offers certain theoretical advantages, specifically over the Sparse Candidate algorithm, corroborated by our experiments. MMHC and detailed results of our study are publicly available at
21|A Transformational Characterization of Equivalent Bayesian Network Structures|We present a simple characterization  of equivalentBayesian network structures  based on local transformations.  The significance of the characterization  is twofold. First, we are able  to easily proveseveral new invariant  properties of theoretical interest  for equivalent structures. Second,  we use the characterization to derive  an efficient algorithm that identifies  all of the compelled edges in a  structure. Compelled edge identification  is of particular importance for  learning Bayesian network structures  from data because these edges indicate  causal relationships when certain  assumptions hold.  1 
22|A Bayesian Approach to Causal Discovery|We examine the Bayesian approach to the discovery of directed acyclic causal models and compare it to the constraint-based approach. Both approaches rely on the Causal Markov assumption, but the two differ significantly in theory and practice. An important difference between the approaches is that the constraint-based approach uses categorical information about conditional-independence constraints in the domain, whereas the Bayesian approach weighs the degree to which such constraints hold. As a result, the Bayesian approach has three distinct advantages over its constraint-based counterpart. One, conclusions derived from the Bayesian approach are not susceptible to incorrect categorical decisions about independence facts that can occur with data sets of finite size. Two, using the Bayesian approach, finer distinctions among model structures---both quantitative and qualitative---can be made. Three, information from several models can be combined to make better inferences and to better ...
23|Causal discovery from a mixture of experimental and observational data|This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (1) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning method was applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were varied systematically. For each of these training datasets, the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in ALARM that are not confounded. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network. 1
25|Data Analysis with Bayesian Networks: A Bootstrap Approach|In recent years there has been significant  progress in algorithms and methods for inducing  Bayesian networks from data. However, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. We need to provide confidence measures  on features of these networks: Is the existence  of an edge between two nodes warranted?  Is the Markov blanket of a given node robust?  Can we say something about the ordering of the  variables? We should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. In this  paper we propose Efron&#039;s Bootstrap as a computationally  efficient approach for answering these  questions. In addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables. 
26|Identifying Gene Regulatory Networks from Experimental Data|this paper, we propose a methodology for making sense of large, multiple time-series data sets arising in expression analysis, and evaluate it both theoretically and through a case study. First, we build a graph representing all putative activation/inhibition relationships by analyzing the expression profiles for all pairs of genes. Second, we prune this graph by solving a combinatorial optimization problem to identify a small set of interesting candidate regulatory elements. We do not assert that we identify &#034;the&#034; regulatory network as a result of this computation. However, we believe that our approach quickly enables biologists to identify and visualize interesting features from raw expression array data sets. We have implemented our methodology and applied it to the analysis of the Saccharomyces cerevisiae data set. In this paper, we report on our implementation and the results of our data analysis. The problem of inducing gene regulation networks has recently come to the computational biology community. Initial attempts at modeling gene expression abd programs to induce regulatory networks from data include [2, 10, 13]. To take fullest advantage of laboratory experiments that can be performed in which a given set of genes can be explicitly expressed or repressed, and the consequences of these genes on expression biologically determined, Akutsu, et.al. [1] considers the problem of designing a minimum-size series of experiments guaranteed to result in the identification of the correct regulatory network. The candidate regulatory network proposed by our system depends upon the specific optimization criteria employed in the second phase of our procedure, although our experiments suggest that the optimal network is surprisingly robust to changes in the objective function...
27|On the Sample Complexity of Learning Bayesian Networks|In recent years there has been an increasing interest in learning Bayesian networks from data. One of the most effective methods for learning such networks is based on the minimum description length (MDL) principle. Previous work has shown that this learning procedure is asymptotically successful: with probability one, it will converge to the target distribution, given a sufficient number of samples. However, the rate of this convergence has been hitherto unknown. In this work we examine the sample complexity of MDL based learning procedures for Bayesian networks. We show that the number of samples needed to learn an ffl-close approximation (in terms of entropy distance) with confidence ffi is O  i  (  1  ffl )  4 3  log  1  ffl log  1  ffi log log  1  ffi  j  . This means that the sample complexity is a low-order polynomial in the error threshold and sublinear in the confidence bound. We also discuss how the constants in this term depend on the complexity of the target distribution. F...
28|Learning Bayesian Networks: A unification for discrete and Gaussian domains|We examine Bayesian methods for learning Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we presented at last year&#039;s conference for discrete and Gaussian domains. We derive a general Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal{Wishart distributions to derive our metrics for discrete and Gaussian domains.  
29|Causal Inference in the Presence of Latent Variables and Selection Bias|This paper uses Bayesian network models for that investigation. Bayesian networks, or directed acyclic graph (DAG) models have proved very useful in representing both causal and statistical hypotheses. The nodes of the graph represent vertices, directed edges represent direct influences, and the topology of the graph encodes statistical constraints. We will consider features of such models that can be determined from data under assumptions that are related to those routinely applied in experimental situations:
30|Learning Mixtures of Bayesian Networks|We describe a heuristic method for learning mixtures of Bayesian Networks (MBNs) from possibly incomplete data. The considered class of models is mixtures in which each mixture component is a Bayesian network encoding a conditional Gaussian distribution over a fixed set of variables. Some variables may be hidden or otherwise have missing observations. A key idea in our approach is to treat expected data as real data. This allows us to interleave structure and parameter search and to take advantage of closed form approximations for marginal likelihood. In addition, by treating expected data as real data, the search criterion factors by variable, making the search processes more efficient. We evaluate our approach on synthetic and real-world data sets.  Keywords : Mixture models, Bayesian networks, structure learning, parameter learning, hidden variables, EM algorithm. 1 Introduction  There is growing interest in a class of models for density estimation known as Bayesian networks. In the...
32|Modeling and Analysis of Heterogeneous Regulation in Biological Networks|In this study we propose a novel model for the representation of biological networks and provide algorithms for learning model parameters from experimental data. Our approach is to build an initial model based on extant biological knowledge, and refine it to increase the consistency between model predictions and experimental data. Our model encompasses networks which contain heterogeneous biological entities (mRNA, proteins, metabolites) and aims to capture diverse regulatory circuitry on several levels (metabolism, transcription, translation, post-translation and feedback loops among them).
33|Latent classification models|Abstract. One of the simplest, and yet most consistently well-performing set of classifiers is the Naïve Bayes models. These models rely on two assumptions: (i) All the attributes used to describe an instance are conditionally independent given the class of that instance, and (ii) all attributes follow a specific parametric family of distributions. In this paper we propose a new set of models for classification in continuous domains, termed latent classification models. The latent classification model can roughly be seen as combining the Naïve Bayes model with a mixture of factor analyzers, thereby relaxing the assumptions of the Naïve Bayes classifier. In the proposed model the continuous attributes are described by a mixture of multivariate Gaussians, where the conditional dependencies among the attributes are encoded using latent variables. We present algorithms for learning both the parameters and the structure of a latent classification model, and we demonstrate empirically that the accuracy of the proposed model is significantly higher than the accuracy of other probabilistic classifiers. Keywords: classification, probabilistic graphical models, Naïve Bayes, correlation
35|Multi-level Modeling and Inference of Transcription Regulation |The understanding of transcription regulation is a major goal of today&#039;s biology. The challenge is to utilize  diverse high-throughput data in order to infer mechanistic models of transcription control. We propose a new  model which integrates transcription factor-gene anity, protein abundance and gene expression pro  les. The  model provides detailed, yet computationally tractable description of the relations between transcription factors,  their binding sites at genes promoters and the combinatorial regulation of transcription. At the core, our model  manipulates dose-anity-response functions that associate transcription factor concentrations and transcription  factor-DNA anities to determine the rate of transcription factor-DNA reactions. We study computational  problems that arise in optimizing such models and develop polynomial algorithms for certain problems. We  show how to assess missing values (notably protein abundance) and describe a novel framework to infer models  from currently available data. On budding yeast carbohydrate metabolism data, our results demonstrate the  sensitivity and speci  city of the approach. They also suggest new active binding sites and a regulation model for  the transcription program of the galactose system.
36|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
37|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
38|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
39|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
40|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
41|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
42|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
43|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
44|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
45|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
46|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
47|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
48|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
49|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
50|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
52|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
53|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
54|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
55|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
56|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
57|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
58|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
59|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
60|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
61|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
62| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
63|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
64|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
65|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
66|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
67|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
68|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
69|Computing iceberg queries efficiently|Many applications compute aggregate functions...
70|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
71|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
72|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
73|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
74|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
75|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
76|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
77|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
78|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
79|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
80|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
81|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
82|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
83|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
84|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
85|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
87|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
89|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
90|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
91|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
92|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
93|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
94|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
96|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
97|Limma: linear models for microarray data|This free open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the appropriate journal articles listed in Section 2.1.Contents
99|Normalization for cDNA microarray data: a robust composite method addressing single and multiple slide systematic variation|There are many sources of systematic variation in cDNA microarray experiments which affect the measured gene expression levels (e.g. differences in labeling efficiency between the two fluorescent dyes). The term normalization refers to the process of removing such variation. A constant adjustment is often used to force the distribution of the intensity log ratios to have a median of zero for each slide. However, such global normalization approaches are not adequate in situations where dye biases can depend on spot overall intensity and/or spatial location within the array. This article proposes normalization methods that are based on robust local regression and account for intensity and spatial dependence in dye biases for different types of cDNA microarray experiments. The selection of appropriate controls for normalization is discussed and a novel set of controls (microarray sample pool, MSP) is introduced to aid in intensity-dependent normalization. Lastly, to allow for comparisons of expression levels across slides, a robust method based on maximum likelihood estimation is proposed to adjust for scale differences among slides.
100|Normalization of cDNA microarray data|Normalization means to adjust microarray data for effects which arise from variation in the technology rather than from biological differences between the RNA samples or between the printed probes. This article describes normalization methods based on the fact that dye balance typically varies with spot intensity and with spatial position on the array. Print-tip loess normalization provides a well-tested general purpose normalization method which has given good results on a wide range of arrays. The method may be refined by using quality weights for individual spots. The method is best combined with diagnostic plots of the data which display the spatial and intensity trends. When diagnostic plots show that biases still remain in the data after normalization, further normalization steps such as plate-order normalization or scalenormalization between the arrays may be undertaken. Composite normalization may be used when control spots are available which are known to be not differentially expressed. Variations on loess normalization include global loess normalization and 2D normalization. Detailed commands are given to implement the normalization techniques using freely available software. 1
101|Use of within-array replicate spots for assessing differential expression in microarray experiments|Motivation. Spotted arrays are often printed with probes in duplicate or triplicate, but current methods for assessing differential expression are not able to make full use of the resulting information. Usual practice is to average the duplicate or triplicate results for each probe before assessing differential expression. This loses valuable information about gene-wise variability. Results. A method is proposed for extracting more information from within-array replicate spots in microarray experiments by estimating the strength of the correlation between them. The method involves fitting separate linear models to the expression data for each gene but with a common value for the between-replicate correlation. The method greatly improves the precision with which the genewise variances are estimated and thereby improves inference methods designed to identify differentially expressed genes. The method may be combined with empirical Bayes methods for moderating the genewise variances between genes. The method is validated using data from a microarray experiment involving calibration and ratio control spots in conjunction with spiked-in RNA. Comparing results for calibration and ratio control spots shows that the common correlation method results in substantially better discrimination of differentially expressed genes from those which are not. The spike-in experiment also confirms that the results may be further improved by empirical Bayes smoothing of the variances when the sample size is small. Availability. The methodology is implemented in the limma software package for R, available from the CRAN repository
102|Identifying differentially expressed genes using false discovery rate controlling procedures|Motivation: DNA microarrays have recently been used for the purpose of monitoring expression levels of thousands of genes simultaneously and identifying those genes that are differentially expressed. The probability that a false identification (type I error) is committed can increase sharply when the number of tested genes gets large. Correlation between the test statistics attributed to gene co-regulation and dependency in the measurement errors of the gene expression levels further complicates the problem. In this paper we address this very large multiplicity problem by adopting the false discovery rate (FDR) controlling approach. In order to address the dependency problem, we present three resampling-based FDR controlling procedures, that account for the test statistics distribution, and compare their performance to that of the naïve application of the linear step-up procedure in Benjamini and Hochberg (1995). The procedures are studied using simulated microarray data, and their performance is examined relative to their ease of implementation. Results: Comparative simulation analysis shows that all four FDR controlling procedures control the FDR at the desired level, and retain substantially more power then the family-wise error rate controlling procedures. In terms of power, using resampling of the marginal distribution of each test statistics substantially improves the performance over the naïve one. The highest power is achieved, at the expense of a more sophisticated algorithm, by the resampling-based procedures that resample the joint distribution of the test statistics and estimate the level of FDR control.
103|Assessing Gene Significance from cDNA Microarray Expression Data via Mixed Models|The determination of a list of differentially expressed genes is a basic objective in many cDNA microarray experiments. We present a statistical approach that allows direct control over the percentage of false positives in such a list and, under certain reasonable assumptions, improves on existing methods with respect to the percentage of false negatives. The method accommodates a wide variety of experimental designs and can simultaneously assess significant differences between multiple types of biological samples. Two interconnected mixed linear models are central to the method and provide a flexible means to properly account for variability both across and within genes. The mixed model also provides a convenient framework for evaluating the statistical power of any particular experimental design and thus enables a researcher to a priori select an appropriate number of replicates. We also suggest some basic graphics for visualizing lists of significant genes. Analyses of published experiments studying human cancer and yeast cells illustrate the results.
104|Statistical Issues in cDNA Microarray Data Analysis|This article summarizes some of the issues involved and provides a brief review of the analysis tools which are available to researchers to deal with them. Any microarray experiment involves a number of distinct stages. Firstly there is the design of the experiment. The researchers must decide which genes are to be printed on the arrays, which sources of RNA are to be hybridized to the arrays and on how many arrays the hybridizations will be replicated. Secondly, after hybridization, there follows a number of data-cleaning steps or `low-level analysis&#039; of the microarray data. The microarray images must be processed to acquire red and green foreground and background intensities for each spot. The acquired red/green ratios must be normalized to adjust for dye-bias and for any systematic variation other than that due to the differences between the RNA samples being studied. Thirdly, the normalized ratios are analyzed by various graphical and numerical means to select differentially expressed genes or to find groups of genes whose expression profiles can reliably classify the different RNA sources into meaningful groups. The sections of this article correspond roughly to the various analysis steps. The following notation will be used throughout the article. The foreground red and green intensities will be written Pp and 9p for each spot. The background intensities will be Pf and 9f . The background-corrected intensities will be P and 9 where usually P Pp Pf 0 # and 9 9p 9f 0 # . The log-differential expression ratio will be vyq # E P 9 0 for each spot. Finally, the log-intensity of the spot will be   vyq 3 P9 0 , a measure of the overall brightness of the spot. (The letter E is a mnemonic for minus as vyq vyq E P 9 0 # while 3 is a mnemonic for add as #vyq vyq #...
105|The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote. Nucleic Acids Res 2013;41:e108 |Read alignment is an ongoing challenge for the analysis of data from sequencing technologies. This article proposes an elegantly simple multi-seed strategy, called seed-and-vote, for mapping reads to a reference genome. The new strategy chooses the mapped genomic location for the read directly from the seeds. It uses a relatively large number of short seeds (called subreads) extracted from each read and allows all the seeds to vote on the optimal location. When the read length is &lt;160bp, overlapping subreads are used. More con-ventional alignment algorithms are then used to fill in detailed mismatch and indel information between the subreads that make up the winning voting block. The strategy is fast because the overall genomic location has already been chosen before the detailed alignment is done. It is sensitive because no individual subread is required to map exactly, nor are individual subreads constrained to map close by other subreads. It is accurate because the final location must be supported by several dif-ferent subreads. The strategy extends easily to find exon junctions, by locating reads that contain sets of subreads mapping to different exons of the same gene. It scales up efficiently for longer reads.
106|CARMAweb: comprehensive R- and Bioconductor-based web service for microarray data analysis|web service for microarray data analysis
107|Optimizing the noise versus bias trade-off for Illumina whole genome expression BeadChips. Nucleic acids research. 2010; 38:e204. [PubMed: 20929874 |Five strategies for pre-processing intensities from Illumina expression BeadChips are assessed from the point of view of precision and bias. The strategies include a popular variance stabilizing transformation and model-based background cor-rections that either use or ignore the control probes. Four calibration data sets are used to evaluate precision, bias and false discovery rate (FDR). The original algorithms are shown to have operating characteristics that are not easily com-parable. Some tend to minimize noise while others minimize bias. Each original algorithm is shown to have an innate intensity offset, by which unlogged intensities are bounded away from zero, and the size of this offset determines its position on the noise–bias spectrum. By adding extra offsets, a continuum of related algorithms with different noise–bias trade-offs is generated, allowing direct comparison of the performance of the strategies on equivalent terms. Adding a positive offset is shown to decrease the FDR of each original algo-rithm. The potential of each strategy to generate an algorithm with an optimal noise–bias trade-off is explored by finding the offset that minimizes its FDR. The use of control probes as part of the back-ground correction and normalization strategy is shown to achieve the lowest FDR for a given bias.
108|Estimating the proportion of microarray probes expressed in an RNA sample. Nucleic acids research 38|A fundamental question in microarray analysis is the estimation of the number of expressed probes in different RNA samples. Negative control probes available in the latest microarray platforms, such as Illumina whole genome expression BeadChips, provide a unique opportunity to estimate the number of expressed probes without setting a threshold. A novel algorithm was proposed in this study to estimate the number of expressed probes in an RNA sample by utilizing these negative controls to measure background noise. The perfor-mance of the algorithm was demonstrated by comparing different generations of Illumina BeadChips, comparing the set of probes targeting well-characterized RefSeq NM transcripts with other probes on the array and comparing pure samples with heterogenous samples. Furthermore, hematopoietic stem cells were found to have a larger transcriptome than progenitor cells. Aire knockout medullary thymic epithelial cells were shown to have significantly less expressed probes than matched wild-type cells.
109|arrayMagic: twocolour cDNA microarray quality control and preprocessing|Summary: arrayMagic is a software package for quality control and preprocessing of two-colour cDNA microarray data. The automated analysis pipeline comprises data import, normalization, replica merging, quality diagnostics and data export. The script-based processing combines reproducibility and flexibility at high-throughput and provides quality-assured and preprocessed microarray data to high-level follow-up analysis. Availability: The R package arrayMagic is available with BSD license at
110|doi:10.1093/nar/gkp366 Pomelo II: finding differentially expressed genes|Pomelo II
111|GK: Individual channel analysis of two-colour microarray data|The traditional approach to the analysis of data from two-colour spotted microarrays is to compute the log-ratio of the expression values for each spot (Chen et al, 1997). The log-ratios are then treated as the responses in any statistical analysis of the data (Yang and Speed, 2003; Smyth, 2004). Relatively few papers have analysed spotted microarrays in terms of the separate red and green log-intensities (Kerr et al, 2000; Jin et al, 2001; Wolfinger et al, 2001). The second and third of these papers popularised a mixed model approach in which each spot is treated as a randomised block of size two. A number of papers starting with Yang et al (2001) have summarised red and green channel intensities in terms of M-values (log-ratios) and A-values (spot log-intensities) for the purposes of graphical displays and normalisation. This paper demonstrates that the usefulness of this partition arises in good part from the fact that the M and A-values for a given spot are approximately independent even though the individual intensities are highly correlated. This paper reformulates the mixed model approach in terms of the M and A-values. This approach not only presents an efficient algorithm for estimating the mixed model but also elucidates the difference between the traditional log-ratio based approach and the analysis of
112|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
113|Approximating discrete probability distributions with dependence trees|A method is presented to approximate optimally an n-dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n-1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.
114|Fusion, Propagation, and Structuring in Belief Networks|Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called &#034;hidden causes. &#034; It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.  
115|Connectionist Learning Procedures|A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks. 
117|An algorithm for fast recovery of sparse causal graphs|An algorithm for fast recovery of sparse causal graphs
118|Decision Theory in Expert Systems and Artificial Intelligence|Despite their different perspectives, artificial intelligence (AI) and the disciplines of decision science have common roots and strive for similar goals. This paper surveys the potential for addressing problems in representation, inference, knowledge engineering, and explanation within the decision-theoretic framework. Recent analyses of the restrictions of several traditional AI reasoning techniques, coupled with the development of more tractable and expressive decisiontheoretic representation and inference strategies, have stimulated renewed interest in decision theory and decision analysis. We describe early experience with simple probabilistic schemes for automated reasoning, review the dominant expert-system paradigm, and survey some recent research at the crossroads of AI and decision science. In particular, we present the belief network and influence diagram representations. Finally, we discuss issues that have not been studied in detail within the expert-systems sett...
119|Constructor: A system for the induction of probabilistic models|The probabilistic network technology is a knowledgebased technique which focuses on reasoning under uncertainty. Because of its well defined semantics and solid theoretical foundations, the technology is finding increasing application in fields such as medical diagnosis, machine vision, military situation assessment, petroleum exploration, and information retrieval. However, like other knowledge-based techniques, acquiring the qualitative and quantitative information needed to build these networks can be highly labor-intensive. CONSTRUCTQR integrates techniques and concepts from probabilistic networks, artificial intelligence, and statistics in order to induce Markov networks (i.e., undirected probabilistic networks). The resulting networks are useful both qualitatively for concept organization and quantitatively for the assessment of new data. The primary goal of CONSTRUCTOR is to find qualitative structure from data. CONSTRUCTOR finds structure by first, modeling each feature in a data set as a node in a Markov network and secondly, by finding the neighbors of each node in the network. In Markov networks, the neighbors of a node have the property of being the smallest set of nodes which “shield ” the node from being affected by other nodes in the graph. This property is used in a heuristic search to identify each node’s neighbors. The traditional x2 test for independence is used to test if a set of nodes “shield ” another node. Cross-validation is used to estimate the quality of alternative structures.
120|Advances in probabilistic reasoning|This paper discuses multiple Bayesian networks representation paradigms for encoding asymmetric independence assertions. We offer three contributions: (1) an inference mechanism that makes explicit use of asymmetric independence to speed up computations, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric independence assertions than do similarity networks. 1
121|Update on the Pathfinder project|This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org. By choosing to view this document, you agree to all provisions of the copyright laws protecting it. Update on the Pathfinder Project *
122|Latent variables, causal models and overidentifying constraints|When is a statistical dependency between two variables best explained by the supposition that one of these variables causes the other, as opposed to the supposition that there is a (possibly unmeasured) common cause acting on both variables? In this paper, we describe an approach towards model specification developed more fully in our book Discovering Cuud Structure, and illustrate its application to the aforementioned question. Briefly, the approach is to determine constraints satisfied by the variance-covariance matrix of a sample, and then to conduct a quasi-automated search for the causal specifications that will best explain those constraints, 1.
123|Causal Structure Among Measured Variables Preserved with Unmeasured Variables|Causal structure among measured variables preserved with unmeasured variables
124|Data mules: Modeling a three-tier architecture for sparse sensor networks|Abstract — This paper presents and analyzes an architecture that exploits the serendipitous movement of mobile agents in an environment to collect sensor data in sparse sensor networks. The mobile entities, called MULEs, pick up data from sensors when in close range, buffer it, and drop off the data to wired access points when in proximity. This leads to substantial power savings at the sensors as they only have to transmit over a short range. Detailed performance analysis is presented based on a simple model of the system incorporating key system variables such as number of MULEs, sensors and access points. The performance metrics observed are the data success rate (the fraction of generated data that reaches the access points) and the required buffer capacities on the sensors and the MULEs. The modeling along with simulation results can be used for further analysis and provide certain guidelines for deployment of such systems. I.
125|An Energy-Efficient MAC Protocol for Wireless Sensor Networks|This paper proposes S-MAC, a medium-access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with individual nodes remaining largely inactive for long periods of time, but then becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in almost every way: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses three novel techniques to reduce energy consumption and support self-configuration. To reduce energy consumption in listening to an idle channel, nodes periodically sleep. Neighboring nodes form virtual clusters to auto-synchronize on sleep schedules. Inspired by PAMAS, S-MAC also sets the radio to sleep during transmissions of other nodes. Unlike PAMAS, it only uses in-channel signaling. Finally, S-MAC applies message passing to reduce contention latency for sensor-network applications that require store-andforward processing as data move through the network. We evaluate our implementation of S-MAC over a sample sensor node, the Mote, developed at University of California, Berkeley. The experiment results show that, on a source node, an 802.11-like MAC consumes 2--6 times more energy than S-MAC for traffic load with messages sent every 1-10s.
126|Mobility increases the capacity of ad-hoc wireless networks|The capacity of ad-hoc wireless networks is constrained by the mutual interference of concurrent transmissions between nodes. We study a model of an ad-hoc network where n nodes communicate in random source-destination pairs. These nodes are assumed to be mobile. We examine the per-session throughput for applications with loose delay constraints, such that the topology changes over the time-scale of packet delivery. Under this assumption, the per-user throughput can increase dramatically when nodes are mobile rather than fixed. This improvement can be achieved by exploiting node mobility as a type of multiuser diversity.  
127|A Survey of Mobility Models for Ad Hoc Network Research|In the performance evaluation of a protocol for an ad hoc network, the protocol should be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer space for the storage of messages, representative data traffic models, and realistic movements of the mobile users (i.e., a mobility model). This paper is a survey of mobility models that are used in the simulations of ad hoc networks. We describe several mobility models that represent mobile nodes whose movements are independent of each other (i.e., entity mobility models) and several mobility models that represent mobile nodes whose movements are dependent on each other (i.e., group mobility models). The goal of this paper is to present a number of mobility models in order to offer researchers more informed choices when they are deciding upon a mobility model to use in their performance evaluations. Lastly, we present simulation results that illustrate the importance of choosing a mobility model in the simulation of an ad hoc network protocol. Specifically, we illustrate how the performance results of an ad hoc network protocol drastically change as a result of changing the mobility model simulated.
128|Energy-Efficient Computing for Wildlife Tracking: Design Tradeoffs and Early Experiences with ZebraNet|Over the past decade, mobile computing and wireless communication have become increasingly important drivers of many new computing applications. The  eld of wireless sensor networks particularly focuses on applications involving autonomous use of compute, sensing, and wireless communication devices for both scienti  c and commercial purposes. This paper examines the research decisions and design tradeos that arise when applying wireless peer-to-peer networking techniques in a mobile sensor network designed to support wildlife tracking for biology research.
129|Next Century Challenges: Mobile Networking for “Smart Dust”|Large-scale networks of wireless sensors are becoming an active topic of research. Advances in hardware technology and engineering design have led to dramatic reductions in size, power consumption and cost for digital circuitry, wire-less communications and Micro ElectroMechanical Systems (MEMS). This has enabled very compact, autonomous and mobile nodes, each containing one or more sensors, computation and communication capabilities, and a power supply. The missing ingredient is the networking and applications layers needed to harness this revolutionary capability into a complete system. We review the key elements of the emergent technology of “Smart Dust ” and outline the research challenges they present to the mobile networking and systems community, which must provide coherent connectivity to large numbers of mobile network nodes co-located within a small volume. 
131|Epidemic routing for partially-connected ad hoc networks|Mobile ad hoc routing protocols allow nodes with wireless adaptors to communicate with one another without any pre-existing network infrastructure. Existing ad hoc routing protocols, while robust to rapidly changing network topology, assume the presence of a connected path from source to destination. Given power limitations, the advent of short-range wireless networks, and the wide physical conditions over which ad hoc networks must be deployed, in some scenarios it is likely that this assumption is invalid. In this work, we develop techniques to deliver messages in the case where there is never a connected path from source to destination or when a network partition exists at the time a message is originated. To this end, we introduce Epidemic Routing, where random pair-wise exchanges of messages among mobile hosts ensure eventual message delivery. The goals of Epidemic Routing are to: i) maximize message delivery rate, ii) minimize message latency, and iii) minimize the total resources consumed in message delivery. Through an implementation in the Monarch simulator, we show that Epidemic Routing achieves eventual delivery of 100 % of messages with reasonable aggregate resource consumption in a number of interesting scenarios. 1
132|Connectivity in Ad-Hoc and Hybrid Networks|We consider a large-scale wireless network, but with a low density of nodes per unit area. Interferences are then less critical, contrary to connectivity. This paper studies the latter property for both a purely ad-hoc network and a hybrid network, where fixed base stations can be reached in multiple hops. We assume here that power constraints are modeled by a maximal distance above which two nodes are not (directly) connected. We find that
133|Smooth is Better than Sharp: A Random Mobility Model for Simulation of Wireless Networks|This paper presents an enhanced random mobility model for simulation-based studies of wireless networks. Our approach makes the movement trace of individual mobile stations more realistic than common approaches for random movement. After giving a survey of mobility models found in the literature, we give a detailed mathematical formulation of our model and outline its advantages. The movement concept is based on random processes for speed and direction control in which the new values are correlated to previous ones. Upon a speed change event, a new target speed is chosen, and an acceleration is set to achieve this target speed. The principles for a direction change are similar. Moreover, we propose two extensions for modeling typical movement patterns of vehicles. Finally, we consider strategies for the nodes&#039; border behavior (i.e., what happens when nodes move out of the simulation area) and point out a pitfall that occurs when using a bounded simulation area.
134|Sending Messages to Mobile Users in Disconnected Ad-Hoc Wireless Networks|An ad-hoc network is formed by a group of mobile hosts upon a wireless network interface. Previous research in this area has concentrated on routing algorithms which are designed for fully connected networks. The usual way to deal with a disconnected ad-hoc network is to let the mobile computer wait for network reconnection passively, which may lead to unacceptable transmission delays. In this paper, we propose an approach that guarantees message transmission in minimal time. In this approach, mobile hosts actively modify their trajectories to transmit messages. We develop algorithms that minimize the trajectory modifications under two different assumptions: (a) the movements of all the nodes in the system are known and (b) the movements of the hosts in the system are not known. 1.
135|Smart-Tag Based Data Dissemination|Monitoring wide, hostile areas requires disseminating data between fixed, disconnected clusters of sensor nodes. It is not always possible to install long-range radios in order to cover the whole area. We propose to leverage the movement of mobile individuals, equipped with smart-tags, to disseminate data across disconnected static nodes spread across a wide area. Static nodes and mobile smart-tags exchange data when they are in the vicinity of each other; smart-tags disseminate data as they move around. In this paper, we propose an algorithm for update propagation and a model for smart-tag based data dissemination. We use simulation to study the characteristics of the model we propose. Finally, we present an implementation based on bluetooth smart-tags.
136|An Efficient Communication Strategy for Ad-hoc Mobile Networks|Abstract. We investigate here the problem of establishing communication in an ad-hoc mobile network, that is, we assume the extreme case of a total absense of any fixed network infrastructure (for example a case of rapid deployment of a set of mobile hosts in an unknown terrain). We propose, in such a case, that a small subset of the deployed hosts (which we call the support) should be used to manage network operations. However, the vast majority of the hosts are moving arbitrarily according to application needs. We then provide a simple, correct and efficient protocol for communication establishment that avoids message flooding. Our protocol manages to establish communication between any pair of mobile hosts in small, apriori guaranteed expected time bounds even in the worst case of arbitrary motions of the hosts that do not belong to the support (provided that they do not deliberately try to avoid the support). These time bounds, interestingly, do not depend, on the number of mobile hosts that do not belong in the support. They depend only on the size of the area of motions. Our protocol can be implemented in very efficient ways by exploiting knowledge of the space of motions or by adding more power to the hosts of the support. Our results exploit and further develop some fundamental properties of random walks in finite graphs. 1
137|Wireless Subscriber Mobility Management using Adaptive Individual Location Areas for PCS Systems|We consider a new mobility management scheme -- the Adaptive Location Area Tracking Scheme, in which each mobile performs a location registration as it crosses the boundary of its current personal location area and is assigned a new location area. The size and shape of the new location area depend on the mobile&#039;s mobility and call characteristics in its previous location area. The objective is to minimize the combined average signaling cost of both paging and registration activities for each individual mobile user.
138|Probability Criterion Based Location Tracking Approach for Mobility Management of Personal Communications Systems|We consider a probability criterion based location tracking scheme for personal communications systems. This scheme seeks to minimize the average signaling cost due to both paging and registration for individual mobile users. A timer-based location update and a single-step dynamic paging procedure are used. Each user-network interaction resets the timer and modifies location record. The user registers when the timer expires. Users are paged over personally constructed  paging areas. The size of these areas is an increasing function of the time since last user-network contact. The shape of the paging area depends on the particular probability distribution of user location. The problem formulation is applicable to arbitrary motion models assuming the associated user location distribution is available or can be estimated. As an illustrative example, we use a Brownian motion process for user motion and assume Poisson call arrivals. We then investigate the influence of user mobility and cal...
139|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
140|Data structure diagrams|Successful communication of ideas has been and will continue to be a limiting factor in man&#039;s endeavors to survive and to better his life. The invention of algebra, essentially a graphic technique for communicating truths with respect to classes of arithmetic statements, broke the bond that slowed the development of mathematics. Whereas &#034;12+ 13=25 &#039; &#039; and &#034;3+7 = 10 &#034; and &#034;14+(-2)  = 12&#034; are arithmetic statements, &#034;a+b=c &#039; &#039; is an algebraic statement. In particular, it is an algebraic statement controlling an entire class of arithmetic statements such as those listed. Data Structure Diagrams The Data Structure Diagram is also a graphic technique. It is based on a type of notation dealing with classes--specifically, with classes of entities and the classes of sets that relate them. For example, individual people and automobiles
141|Data Security|The rising abuse of computers and increasing threat to personal privacy through data banks have stimulated much interest m the techmcal safeguards for data. There are four kinds of safeguards, each related to but distract from the others. Access controls regulate which users may enter the system and subsequently whmh data sets an active user may read or wrote. Flow controls regulate the dissemination of values among the data sets accessible to a user. Inference controls protect statistical databases by preventing questioners from deducing confidential information by posing carefully designed sequences of statistical queries and correlating the responses. Statlstmal data banks are much less secure than most people beheve. Data encryption attempts to prevent unauthorized disclosure of confidential information in transit or m storage. This paper describes the general nature of controls of each type, the kinds of problems they can and cannot solve, and their inherent limitations and weaknesses. The paper is intended for a general audience with little background in the area.
143|New Directions in Cryptography|Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.
144|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
145|A Lattice Model of Secure Information Flow|This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.
146|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
147|Certification of Programs for Secure Information Flow|This paper presents a certification mechanism for verifying the secure flow of information through a program. Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers. Appropriate semantics are presented and proved correct. An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data.
148|Password Security: A Case History|This paper describes the history of the design of the password security scheme on a  remotely accessed time-sharing system. The present design was the result of countering  observed attempts to penetrate the system. The result is a compromise between extreme  security and ease of use.
149|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
150|The tracker: a threat to statistical database security|The query programs of certain databases report raw statistics for query sets, which are groups of records specified implicitly by a characteristic formula. The raw statistics include query set size and sums of powers of values in the query set. Many users and designers believe that the individual records will remain confidential as long as query programs refuse to report the statistics of query sets which are too small. It is shown that the compromise of small query sets can in fact almost always be accomplished with the help of characteristic formulas called trackers. Schlorer’s individual tracker is reviewed, it is derived from known characteristics of a given individual and permits deducing additional characteristics he may have. The general tracker is introduced: It permits calculating statistics for arbitrary query sets, without requiring preknowledge of anything in the database. General trackers always exist if there are enough distinguishable classes of individuals in the database, in which case the trackers have a simple form. Almost all databases have a general tracker, and general trackers are almost always easy to find. Security is not guaranteed by the lack of a general tracker.
151|Operating Systems|Introduction Early operating systems were control programs a few thousand bytes long that scheduled jobs, drove peripheral devices, and kept track of system usage for billing purposes. Modern operating systems are much larger, ranging from hundreds of thousands of bytes for personal computers (e.g., MS/DOS, Xenix) to tens of millions of bytes for mainframes (e.g., Honeywell&#039;s Multics, IBM&#039;s MVS, AT
152|Third generation computer systems|The common features of third generation operating systems are surveyed from a general view, with emphasis on the common abstractions that constitute at least the basis for a &amp;quot;theory &amp;quot; of operating systems. Properties of specific systems are not discussed except where examples are useful. The technical aspects of issues and concepts are stressed, the nontechnical aspects mentioned only briefly. A perfunctory knowledge of third generation systems is presumed. Key words and phrases: multiprogramming systems, operating systems, supervisory systems, time-sharing systems, programming, storage allocation, memory allocation, processes, concurrency, parallelism, resource allocation, protection CR categories: 1.3, 4.0, 4.30, 6.20 It has been the custom to divide the era of electronic computing into &amp;quot;generations&amp;quot; whose approximate dates are:
153|Mining the Network Value of Customers|One of the major applications of data mining is in helping companies determine which potential customers to market to. If the expected pro t from a customer is greater than the cost of marketing to her, the marketing action for that customer is executed. So far, work in this area has considered only the intrinsic value of the customer (i.e, the expected pro t from sales to her). We propose to model also the customer&#039;s network value: the expected pro t from sales to other customers she may inuence to buy, the customers those may inuence, and so on recursively. Instead of viewing a market as a set of independent entities, we view it as a social network and model it as a Markov random eld. We show the advantages of this approach using a social network mined from a collaborative ltering database. Marketing that exploits the network value of customers|also known as viral marketing|can be extremely eective, but is still a black art. Our work can be viewed as a step towards providing a more solid foundation for it, taking advantage of the availability of large relevant databases. Categories and Subject Descriptors H.2.8 [Database Management]: Database Applications| data mining
154|The Anatomy of a Large-Scale Hypertextual Web Search Engine|In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/

To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.

Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
155|Authoritative Sources in a Hyperlinked Environment|The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authoritative ” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages ” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.
156|GroupLens: An Open Architecture for Collaborative Filtering of Netnews|Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.
157|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
158|Learning probabilistic relational models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning — the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1
159|Enhanced Hypertext Categorization Using Hyperlinks|A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain highquality semantic clues that are lost upon a purely termbased classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a  relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented ...
160|Scale-free characteristics of random networks: The topology of the world-wide web|The world-wide web forms a large directed graph, whose vertices are documents and edges are links pointing from one document to another. Here we demonstrate that despite its apparent random character, the topology of this graph has a number of universal scale-free characteristics. We introduce a model that leads to a scale-free network, capturing in a minimal fashion the self-organization processes governing the world-wide web. 
162|Data Mining for Direct Marketing: Problems and Solutions|Direct marketing is a process of identifying likely buyers of certain products and promoting the products accordingly. It is increasingly used by banks, insurance companies, and the retail industry. Data mining can provide an effective tool for direct marketing. During data mining, several specific problems arise. For example, the class distribution is extremely imbalanced (the response rate is about 1~), the predictive accuracy is no longer suitable for evaluating learning methods, and the number of examples can be too large. In this paper, we discuss methods of coping with these problems based on our experience on direct-marketing projects using data mining. 1
163|ReferralWeb: Combining Social Networks and Collaborative Filtering|This paper appears in the Communications of the ACM,
164|Cobot in LambdaMOO: A social statistics agent|We describe our development of Cobot, a software agent who lives in LambdaMOO, a popular virtual world frequented by hundreds of users. We present a detailed discussion of the functionality that has made him one of the objects most frequently interacted with in LambdaMOO, human or artificial.
165|A decision theoretic approach to targeted advertising|A simple advertising strategy that can be used to help increase sales of a product is to mail out special o ers to selected potential customers. Because there is a cost associated with sending each o er, the optimal mailing strategy depends on both the bene t obtained from a purchase and how the o er a ects the buying behavior of the customers. In this paper, we describe two methods for partitioning the potential customers into groups, and show howto perform a simple cost-bene t analysis to decide which, if any, of the groups should be targeted. In particular, weconsidertwodecision-tree learning algorithms. The rst is an \o the shelf &#034; algorithm used to model the probability that groups of customers will buy the product. The second is a new algorithm that is similar to the rst, except that for each group, it explicitly models the probability of purchase under the two mailing scenarios: (1) the mail is sent tomembers of that group and (2) the mail is not sent to members of that group. Using data from a real-world advertising experiment, we compare the algorithms to each other and to a naive mail-to-all strategy. 1
166|Statistical mechanics of complex networks |Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real
167|Books in graphs|A set of q triangles sharing a common edge is called a book of size q. We write ß (n, m) for the the maximal q such that every graph G (n, m) contains a book of size q. In this note 1) we compute ß ( n, cn 2) for infinitely many values of c with 1/4 &lt; c &lt; 1/3, 2) we show that if m = (1/4 - a) n 2 with 0 &lt; a &lt; 17 -3 (), and G has no book of size at least graph G1 of order at least
169|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
170|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
171|A group mobility model for ad hoc wireless networks|In this paper, we present a survey of various mobility models in both cellular networks and multi-hop networks. We show that group motion occurs frequently in ad hoc networks, and introduce a novel group mobility model- Reference Point Group Mobility (RPGM)- to represent the relationship among mobile hosts. RPGM can be readily applied to many existing applications. Moreover, by proper choice of parameters, RPGM can be used to model several mobility models which were previously proposed. One of the main themes of this paper is to investigate the impact of the mobility model on the performance of a specific network protocol or application. To this end, we have applied our RPGM model to two different network protocol scenarios, clustering and routing, and have evaluated network performance under different mobility patterns and for different protocol implementations. As expected, the results indicate that different mobility patterns affect the various protocols in different ways. In particular, the ranking of routing algorithms is influenced by the choice of mobility pattern. 1
172|Multicast Operation of the Ad-hoc On-Demand Distance Vector Routing Protocol|An ad-hoc network is the cooperative engagement of a  collection of (typically wireless) mobile nodes without the required intervention of any centralized access point or existing infrastructure. To provide optimal communication ability, a routing protocol for such a dynamic self-starting network must be capable of unicast, broadcast, and multicast. In this paper we extend Ad-hoc On-Demand Distance Vector Routing (AODV), an algorithm for the operation of such ad-hoc networks, to offer novel multicast capabilities which follow naturally from the way AODV establishes unicast routes. AODV builds  multicast trees as needed (i.e., on-demand) to connect multicast group members. Control of the multicast tree is distributed so that there is no single point of failure. AODV provides loop-free routes for both unicast and multicast, even while repairing broken links. We include an evaluation methodology and simulation results to validate the correct and efficient operation of the AODV algorithm.   
173|User mobility modeling and characterization of mobility patterns|Abstract—A mathematical formulation is developed for systematic tracking of the random movement of a mobile station in a cellular environment. It incorporates mobility parameters under the most generalized conditions, so that the model can be tailored to be applicable in most cellular environments. This mobility model is used to characterize different mobility-related traffic parameters in cellular systems. These include the distribution of the cell residence time of both new and handover calls, channel holding time, and the average number of handovers. It is shown that the cell resistance time can be described by the generalized gamma distribution. It is also shown that the negative exponential distribution is a good approximation for describing the channel holding time. Index Terms—Mobile communication. I.
174|An analysis of the optimum node density for ad hoc mobile networks|An ad hoc mobile network is a collection of nodes, each of which communicates over wireless channels and is capable of movement. Wireless nodes have the unique capability of transmission at different power levels. As the transmission power is varied, a tradeoff exists between the number of hops from source to destination and the overall bandwidth available to individual nodes. Because both battery life and channel bandwidth are limited resources in mobile networks, it is important to ascertain the effects different transmission powers have on the overall performance of the network. This paper explores the nature of this transmission power tradeoff in mobile networks to determine the optimum node density for delivering the maximum number of data packets. It is shown that there does not exist a global optimum density, but rather that, to achieve this maximum, the node density should increase as the rate of node movement increases.
175|Mobile Users: To Update or not to Update?|This paper focuses on three natural strategies in which the mobile users make the decisions when and where to update: the time-based strategy, the number of movements-based strategy, and the distance-based strategy. We consider both memoryless movement patterns and movements with Markovian memory along a topology of cells arranged as a ring. We analyze the performance of each one of the three strategies under such movements, and show the performance differences between the strategies.
176|Predictive Distance-Based Mobility Management for PCS Networks|This paper presents a mobile tracking scheme that exploits the predictability of user mobility patterns in wireless PCS networks. Instead of the constant velocity fluid-flow or the random-walk mobility model, a more realistic Gauss-Markov model is introduced, where a mobile&#039;s velocity is correlated in time to a various degree. Based on the Gauss-Markov model, a mobile&#039;s future location is predicted by the network based on the information gathered from the mobile&#039;s last report of location and velocity. When a call is made, the network pages the destination mobile at and around the predicted location of the mobile and in the order of descending probability until the mobile is found. A mobile shares the same prediction information with the network and reports its new location whenever it reaches some threshold distance away from the predicted location. We describe an analytical framework to evaluate the cost of mobility management for the proposed predictive distance-based scheme. We then...
177|A Multicast Routing Protocol for Ad-Hoc Networks|The Core-Assisted Mesh Protocol (CAMP) is introduced for multicast routing in ad-hoc networks. CAMP generalizes the notion of core-based trees introduced for internet multicasting into multicast meshes that have much richer connectivity than trees. A shared multicast mesh is defined for each multicast group; the main goal of using such meshes is to maintain the connectivity of multicast groups even while network routers move frequently. CAMP consists of the maintenance of multicast meshes and loop-free packet forwarding over such meshes. Within the multicast mesh of a group, packets from any source in the group are forwarded along the reverse shortest path to the source, just as in traditional multicast protocols based on source-based trees. CAMP guarantees that, within a finite time, every receiver of a multicast group has a reverse shortest path to each source of the multicast group. Multicast packets for a group are forwarded along the shortest paths from sources to receivers define...
178|Wireless Hierarchical Routing Protocol with Group Mobility (WHIRL)  (1999) |In this paper we address the problem of routing in a large wireless, mobile network such as found in the automated battle field or in extensive disaster recovery operations. Conventional routing does not scale well to network size. Likewise, conventional hierarchical routing cannot handle mobility efficiently. In this paper, we propose a novel soft state Wireless HIerarchical Routing protocoL (WHIRL). We distinguish between the &#034;physical&#034; routing hierarchy (dictated by geographical relationships between nodes) and &#034;logical&#034; hierarchy of subnets in which the members move as a group (e.g., company, brigade, battalion in the battlefield). WHIRL keeps track of logical subnet movements using Home Agent concepts akin to Mobile IP. A group mobility model is introduced and the performance of the WHIRL is evaluated through a detailed wireless simulation model.
179|Source-Tree Routing in Wireless Networks|We present the source-tree adaptive routing (STAR) protocol and analyze its performance in wireless networks with broadcast radio links. Routers in STAR communicate to its neighbors their source routing trees either incrementally or in atomic updates. Source routing trees are specified by stating the link parameters of each link belonging to the paths used to reach every destination. Hence, a router disseminates link-state updates to its neighbors for only those links along paths used to reach destinations. Simulation results show that STAR is an order of magnitude more efficient than any topology-broadcast protocol, and four times more efficient than ALP, which was the most efficient table-driven routing protocol based on partial link-state information reported to date. The results also show that STAR is even more efficient than the Dynamic Source Routing (DSR) protocol, which has been shown to be one of the best performing on-demand routing protocols.
180|Geographic Routing for Wireless Networks|Distributed shortest-path routing protocols for wired networks either describe the entire topology of a network or provide a digest of the topology to every router. They continu-ally update the state describing the topology at all routers as the topology changes to find correct routes for all destinations. Hence, to find routes robustly, they generate routing pro-tocol message traffic proportional to the product of the number of routers in the network and the rate of topological change in the network. Current ad-hoc routing protocols, de-signed specifically for mobile, wireless networks, exhibit similar scaling properties. It is the reliance of these routing protocols on state concerning all links in the network, or all links on a path between a source and destination, that is responsible for their poor scaling. We present Greedy Perimeter Stateless Routing (GPSR), a novel routing protocol for wireless datagram networks that uses the positions of routers and a packet’s destination to make packet forwarding decisions. GPSR makes greedy forwarding decisions using only information about a router’s immediate neighbors in the network topology. When a packet reaches a region where greedy forwarding is impossible, the algorithm recovers by routing around the perimeter of the region. By keeping state only about the local topology, GPSR
181|Evaluating Mobility Models Within An Ad Hoc Network|With current advances in technology, wireless networks are increasing in popularity. Wireless networks allow users the freedom to travel from one location to another without interruption of their computing services. However, wireless networks require the existence of a wired base station (BS) in order for the wireless user to send/receive messages. Ad hoc networks, a subset of wireless networks, allow the formation of a wireless network without the need for a BS. All participating users in an ad hoc network agree to accept and forward messages, to and from each other. With this flexibility, wireless networks have the ability to form anywhere, at any time, as long as two or more wireless users are willing to communicate. In an ad hoc network, the ability to send a message to a group of users, based solely on their geographic location, is desirable. A geocast protocol serves this purpose. Rescue missions, military scenarios, and even advertising schemes benefit from this type of message delivery service. However, before implementation occurs, an ad hoc network protocol such as a geocast protocol must be tested under realistic conditions including, but not limited to, a sensible transmission range, limited buffer for storage of messages, and realistic movements of the wireless users (i.e., a mobility model). The results presented in this thesis focus on several mobility models in an attempt to compare iv the effects that different mobility models have on an ad hoc network protocol. It is obvious that wireless users will travel from one location to another. However, representing their exact movements is not so simple. The results presented in this thesis illustrate the importance in carefully evaluating and implementing multiple mobility models when evaluating an ad hoc net...
182|On-Demand Multicast in Mobile Wireless Networks|In this paper we propose an &#034;on demand&#034; multicast routing protocol for a wireless, mobile, multihop network. The proposed scheme has two key features: (a) it is based on the forwarding group concept (i.e., a subset of nodes is in charge of forwarding the multicast packets via scoped flooding) rather than on the conventional multicast tree scheme; (b) it dynamically refreshes the forward group members using a procedure akin to on demand routing (hence the name). &#034;On Demand&#034; multicast is well suited to operate in an On Demand routing environment where routes are selectively computed as needed between communicating node pairs instead of being maintained and updated globally by a routing &#034;infrastructure&#034; (like in Distance Vector or Link State, for example). On Demand Multicast is particularly attractive in mobile, rapidly changing networks, where the traffic overhead caused by routing updates and tree reconfigurations may become prohibitive beyond a critical speed; and, in large networks w...
183|Wireless Network Multicasting| Wireless networks provide mobile users with ubiquitous communicating capability and information access regardless of location. Conventional ground radio networks are the &#034;last hop&#034; extension of a wireline network, thus supporting only single hop communications within a &#034;cell&#034;. In this dissertation we address a novel type of wireless networks called &#034;multihop&#034; networks. As a difference from &#034;single hop&#034; (i.e., cellular) networks which require fixed base stations interconnected by a wired backbone, multihop networks have no fixed based stations nor a wired backbone. The main application for mobile wireless multihopping is rapid deployment and dynamic reconfiguration. When the wireline network is not available, as in battlefield communications and search and rescue operations, multihop wireless network...
184|Load Reduction in Ad Hoc Networks Using Mobile Servers|In this thesis, we propose two algorithms for network load reduction in ad hoc networks. With the increasing popularity of mobile computing and Internet based client-server applications, such algorithms should become popular.
185|Max-margin Markov networks|In typical classification tasks, we seek a function which assigns a label to a single object. Kernel-based approaches, such as support vector machines (SVMs), which maximize the margin of confidence of the classifier, are the method of choice for many such tasks. Their popularity stems both from the ability to use high-dimensional feature spaces, and from their strong theoretical guarantees. However, many real-world tasks involve sequential, spatial, or structured data, where multiple labels must be assigned. Existing kernel-based methods ignore structure in the problem, assigning labels independently to each object, losing much useful information. Conversely, probabilistic graphical models, such as Markov networks, can represent correlations between labels, by exploiting problem structure, but cannot handle high-dimensional feature spaces, and lack strong theoretical generalization guarantees. In this paper, we present a new framework that combines the advantages of both approaches: Maximum margin Markov (M 3) networks incorporate both kernels, which efficiently deal with high-dimensional features, and the ability to capture correlations in structured data. We present an efficient algorithm for learning M 3 networks based on a compact quadratic program formulation. We provide a new theoretical bound for generalization in structured domains. Experiments on the task of handwritten character recognition and collective hypertext classification demonstrate very significant gains over previous approaches. 1
186|The Nature of Statistical Learning Theory| Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more
187|Conditional random fields: Probabilistic models for segmenting and labeling sequence data|We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1.
188|On the algorithmic implementation of multi-class kernel-based vector machines |In this paper we describe the algorithmic implementation of multiclass kernel-based vector machines. Our starting point is a generalized notion of the margin to multiclass problems. Using this notion we cast multiclass categorization problems as a constrained optimization problem with a quadratic objective function. Unlike most of previous approaches which typically decompose a multiclass problem into multiple independent binary classification tasks, our notion of margin yields a direct method for training multiclass predictors. By using the dual of the optimization problem we are able to incorporate kernels with a compact set of constraints and decompose the dual problem into multiple optimization problems of reduced size. We describe an efficient fixed-point algorithm for solving the reduced optimization problems and prove its convergence. We then discuss technical details that yield significant running time improvements for large datasets. Finally, we describe various experiments with our approach comparing it to previously studied kernel-based methods. Our experiments indicate that for multiclass problems we attain state-of-the-art accuracy.
189|Belief Propagation|When a pair of nuclear-powered Russian submarines was reported patrolling off the eastern seaboard of the U.S. last summer, Pentagon officials expressed wariness over the Kremlin’s motivations. At the same time, these officials emphasized their confidence in the U.S. Navy’s tracking capabilities: “We’ve known where they were,” a senior Defense Department official told the New York Times, “and we’re not concerned about our ability to track the subs.” While the official did not divulge the methods used by the Navy to track submarines, the Times added that such
190|Discriminative probabilistic models for relational data|In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies. 1
191|Hidden Markov Support Vector Machines|This paper presents a novel discriminative  learning technique for label sequences based  on a combination of the two most successful  learning algorithms, Support Vector Machines  and Hidden Markov Models which  we call Hidden Markov Support Vector Machine.
192|Parameter Estimation for Statistical Parsing Models: Theory and Practice of Distribution-Free Methods|A fundamental problem in statistical parsing is the choice of criteria and algorithms used to estimate the parameters in a model. The predominant approach in computational linguistics has been to use a parametric model with some variant of maximum-likelihood estimation. The assumptions under which maximum-likelihood estimation is justified are arguably quite strong. This paper discusses the statistical theory underlying various parameter-estimation methods, and gives algorithms which depend on alternatives to (smoothed) maximumlikelihood  estimation. We first give an overview of results from statistical learning theory. We then show how important concepts from the classification literature -- specifically, generalization results based on margins on training data -- can be derived for parsing models. Finally, we describe parameter estimation algorithms which are motivated by these generalization bounds.
193|Covering Number Bounds of Certain Regularized Linear Function Classes|Recently, sample complexity bounds have been derived for problems involving linear functions  such as neural networks and support vector machines. In many of these theoretical  studies, the concept of covering numbers played an important role. It is thus useful to  study covering numbers for linear function classes. In this paper, we investigate two closely  related methods to derive upper bounds on these covering numbers. The first method,  already employed in some earlier studies, relies on the so-called Maurey&#039;s lemma; the second  method uses techniques from the mistake bound framework in online learning. We  compare results from these two methods, as well as their consequences in some learning  formulations.
194|Epidemic Spreading in Scale-Free Networks|The Internet, as well as many other networks, has a very complex connectivity  recently modeled by the class of scale-free networks. This feature, which  appears to be very efficient for a communications network, favors at the same  time the spreading of computer viruses. We analyze real data from computer  virus infections and find the average lifetime and prevalence of viral strains on  the Internet. We define a dynamical model for the spreading of infections on  scale-free networks, finding the absence of an epidemic threshold and its associated  critical behavior. This new epidemiological framework rationalize data  of computer viruses and could help in the understanding of other spreading  phenomena on communication and social networks.  PACS numbers: 05.70.Ln, 05.50.+q  Typeset using REVT  E  X 1  Many social, biological, and communication systems can be properly described by complex networks whose nodes represent individuals or organizations, and links mimic the interactions amo...
195|Network information flow|We introduce a new class of problems called network information flow which is inspired by computer network applications. Consider a point-to-point communication network on which a number of information sources are to be mulitcast to certain sets of destinations. We assume that the information sources are mutually independent. The problem is to characterize the admissible coding rate region. This model subsumes all previously studied models along the same line. In this paper, we study the problem with one information source, and we have obtained a simple characterization of the admissible coding rate region. Our result can be regarded as the Max-flow Min-cut Theorem for network information flow. Contrary to one’s intuition, our work reveals that it is in general not optimal to regard the information to be multicast as a “fluid” which can simply be routed or replicated. Rather, by employing coding at the nodes, which we refer to as network coding, bandwidth can in general be saved. This finding may have significant impact on future design of switching systems.  
196|Factor Graphs and the Sum-Product Algorithm|A factor graph is a bipartite graph that expresses how a &#034;global&#034; function of many variables factors into a product of &#034;local&#034; functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute---either exactly or approximately---various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative &#034;turbo&#034; decoding algorithm, Pearl&#039;s belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. 
197|How practical is network coding?|  With network coding, intermediate nodes between the source and the receivers of an end-to-end communication session are not only capable of relaying and replicating data messages, but also of coding incoming messages to produce coded outgoing ones. Recent studies have shown that network coding is beneficial for peer-to-peer content distribution, since it eliminates the need for content reconciliation, and is highly resilient to peer failures. In this paper, we present our recent experiences with a highly optimized and high-performance C++ implementation of randomized network coding at the application layer. We present our observations based on an extensive series of experiments, draw conclusions from a wide range of scenarios, and are more cautious and less optimistic as compared to previous studies.  
198|Application of Phylogenetic Networks in Evolutionary Studies|The evolutionary history of a set of taxa is usually represented by a phylogenetic tree, and this model has greatly facilitated the discussion and testing of hypotheses. However, it is well known that more complex evolutionary scenarios are poorly described by such models. Further, even when evolution proceeds in a tree-like manner, analysis of the data may not be best served by using methods that enforce a tree structure, but rather by a richer visualization of the data to evaluate its properties, at least as an essential first step. Thus, phylogenetic networks should be employed when reticulate events such as hybridization, horizontal gene transfer, recombination, or gene duplication and-loss are believed to be involved, and, even in the absence of such events, phylogenetic networks have a useful role to play. This paper reviews the terminology used for phylogenetic networks and covers both split networks and reticulate networks, how they are defined and how they can be interpreted. Additionally, the paper outlines the beginnings of a comprehensive statistical framework for applying split network methods. We show how split networks can represent confidence sets of trees and introduce a conservative statistical test for whether the conflicting signal in a network is treelike. Finally, this paper describes a new program SplitsTree4, an interactive and comprehensive tool for inferring different types of phylogenetic networks from sequences, distances and trees.
199|A Simple, Fast, and Accurate Algorithm to Estimate Large Phylogenies by Maximum Likelihood| The increase in the number of large data sets and the complexity of current probabilistic sequence evolution models necessitates fast and reliable phylogeny reconstruction methods. We describe a new approach, based on the maximumlikelihood principle, which clearly satisfies these requirements. The core of this method is a simple hill-climbing algorithm that adjusts tree topology and branch lengths simultaneously. This algorithm starts from an initial tree built by a fast distance-based method and modifies this tree to improve its likelihood at each iteration. Due to this simultaneous adjustment of the topology and branch lengths, only a few iterations are sufficient to reach an optimum. We used extensive and realistic computer simulations to show that the topological accuracy of this new method is at least as high as that of the existing maximum-likelihood programs and much higher than the performance of distance-based and parsimony approaches. The reduction of computing time is dramatic in comparison with other maximum-likelihood packages, while the likelihood maximization ability tends to be higher. For example, only 12 min were required on a standard personal computer to analyze a data set consisting of 500 rbcL sequences with 1,428 base pairs from plant plastids, thus reaching a speed of the same order as some popular distance-based and parsimony algorithms. This new method is implemented in the PHYML program, which is freely available on our web page:
200|Probability Distribution of Molecular Evolutionary Trees: A New Method of Phylogenetic Inference|. A new method is presented for inferring evolutionary trees using nucleotide sequence data. The birth--death process is used as a model of speciation and extinction to specify the prior distribution of phylogenies and branching times. Nucleotide substitution is modeled by a continuous-time Markov process. Parameters of the branching model and the substitution model are estimated by maximum likelihood. The posterior probabilities of different phylogenies are calculated and the phylogeny with the highest posterior probability is chosen as the best estimate of the evolutionary relationship among species. We refer to this as the maximum posterior probability (MAP) tree. The posterior probability provides a natural measure of the reliability of the estimated phylogeny. Two example data sets are analyzed to infer the phylogenetic relationship of human, chimpanzee, gorilla, and orangutan. The best trees estimated by the new method are the same as those from the maximum likelihood analysis of...
202|SplitsTree: A Program for Analyzing and Visualizing Evolutionary Data|Motivation. Real evolutionary data often contains a number of different and sometimes conflicting phylogenetic signals and thus does not always clearly support a unique tree. To address this problem, H.-J. Bandelt and A.W.M. Dress developed the method of split decomposition. For ideal data, this method gives rise to a tree, whereas less ideal data is represented by a tree-like network that may indicate evidence for different and conflicting phylogenies.  Results. SplitsTree is an interactive program for analyzing and visualizing evolutionary data, that implements this approach. It also supports a number of distances transformations, the computation of parsimony splits, spectral analysis and bootstrapping.  Availability. There are two versions of SplitsTree, an interactive Macintosh version (shareware) and a command-line unix version (public domain). Both are available from:  ftp://ftp.uni-bielefeld.de/pub/math/splits/splitstree2.  There is a WWW version running at:  http://www.bibiserv...
203|A hybrid micro-macroevolutionary approach to gene tree reconstruction|Gene family evolution is determined by microevolutionary processes (e.g., point mutations) and macroevo-lutionary processes (e.g., gene duplication and loss), yet macroevolutionary considerations are rarely incor-porated into gene phylogeny reconstruction methods. We present a dynamic program to find the most parsi-monious gene family tree with respect to a macroevolutionary optimization criterion, the weighted sum of the number of gene duplications and losses. The existence of a polynomial delay algorithm for duplication/loss phylogeny reconstruction stands in contrast to most formulations of phylogeny reconstruction, which are NP-complete. We next extend this result to obtain a two-phase method for gene tree reconstruction that takes both micro- and macroevolution into account. In the first phase, a gene tree is constructed from sequence data, using any of the previously known algorithms for gene phylogeny construction. In the second phase, the tree is refined by rearranging regions of the tree that do not have strong support in the sequence data to minimize the duplication/lost cost. Components of the tree with strong support are left intact. This hybrid approach incorporates both micro- and macroevolutionary considerations, yet its computational requirements are modest in practice because the two phase approach constrains the search space. Our hybrid algorithm can
204|Reconstructing reticulate evolution in species - theory and practice|We present new methods for reconstructing reticulate evolution of species due to events such as horizontal transfer or hybrid speciation; both methods are based upon extensions of Wayne Maddison’s approach in his seminal 1997 paper. Our first method is a polynomial time algorithm for constructing phylogenetic networks from two gene trees contained inside the network. We allow the network to have an arbitrary number of reticulations, but we limit the reticulation in the network so that the cycles in network are node-disjoint (“galled”). Our second method is a polynomial time algorithm for constructing networks with one reticulation, where we allow for errors in the estimated gene trees. Using simulations, we demonstrate improved performance of this method over both NeighborNet and Maddison’s method. 1
205|New Algorithms for the Duplication-Loss Model|We consider the problem of constructing a species tree given a number of gene trees. In the frameworks introduced by Goodman et al. [3], Page [10], and Guig&#039;o, Muchnik, and Smith [5] this is formulated as an optimization problem; namely, that of finding the species tree requiring the minimumnumber of duplications and/ or losses in order to explain the gene trees.
206|A Framework for Representing Reticulate Evolution|  Acyclic directed graphs (ADGs) are increasingly being viewed as more appropriate for representing certain evolutionary relationships, particularly in biology, than rooted trees. In this paper, we develop a framework for the analysis of these graphs which we call hybrid phylogenies. We are particularly interested in the problem whereby one is given a set of phylogenetic trees and wishes to determine a hybrid phylogeny that ‘embeds’ each of these trees and which requires the smallest number of hybridisation events. We show that this quantity can be greatly reduced if additional species are involved, and investigate other combinatorial aspects of this and related questions.
208|Phylogenetic super-networks from partial trees|Abstract—In practice, one is often faced with incomplete phylogenetic data, such as a collection of partial trees or partial splits. This paper poses the problem of inferring a phylogenetic super-network from such data and provides an efficient algorithm for doing so, called the Z-closure method. Additionally, the questions of assigning lengths to the edges of the network and how to restrict the “dimensionality ” of the network are addressed. Applications to a set of five published partial gene trees relating different fungal species and to six published partial gene trees relating different grasses illustrate the usefulness of the method and an experimental study confirms its potential. The method is implemented as a plug-in for the program SplitsTree4. Index Terms—Molecular evolution, phylogeny, partial trees, networks, closure operations. 1
210|Reconstruction of reticulate networks from gene trees|Abstract. One of the simplest evolutionary models has molecular sequences evolving from a common ancestor down a bifurcating phylogenetic tree, experiencing point-mutations along the way. However, empirical analyses of different genes indicate that the evolution of genomes is often more complex than can be represented by such a model. Thus, the following problem is of significant interest in molecular evolution: Given a set of molecular sequences, compute a reticulate network that explains the data using a minimal number of reticulations. This paper makes four contributions toward solving this problem. First, it shows that there exists a one-to-one correspondence between the tangles in a reticulate network, the connected components of the associated incompatibility graph and the netted components of the associated splits graph. Second, it provides an algorithm that computes a most parsimonious reticulate network in polynomial time, if the reticulations contained in any tangle have a certain overlapping property, and if the number of reticulations contained in any given tangle is bounded by a constant. Third, an algorithm for drawing reticulate networks is described and a robust and flexible implementation of the algorithms is provided. Fourth, the paper presents a statistical test for distinguishing between reticulations due to hybridization, and ones due to other events such as lineage sorting or tree-estimation error. 1
211|Reconstruction Of Biogeographic and Evolutionary Networks Using Reticulograms|A reticulogram is a general network capable of representing a reticulate evolutionary structure.
214| STATISTICAL APPROACH TO TESTS INVOLVING PHYLOGENIES |This chapter reviews statistical testing involving phylogenies. We present both the classical framework with the use of sampling distributions involving the bootstrap and permutation tests and the Bayesian approach using posterior distributions. We give some examples of direct tests for deciding whether the data support a given tree or trees that share a particular property, comparative analyses using tests that condition on the phylogeny being known are also discussed. We introduce a continuous parameter space that enables one to avoid the delicate problem of comparing exponentially many possible models with a finite amount of data. This chapter contains a review of the literature on parametric tests in phylogenetics and some suggestions of non-parametric tests. We also present some open questions that have to be solved by mathematical statisticians to provide the theoretical justification of both current testing strategies and as yet underdeveloped areas of statistical testing in
215|Spectral analysis and a closest tree method for genetic sequences, subbmitted to|Abstract-We describe a new method for estimating the evolutionary tree linking a collection of species from their aligned four-state genetic sequences. This method, which can be adapted to provide a branch-and-bound algorithm, is statistically consistent provided the sequences have evolved according to a standard stochastic model of nucleotide mutation. Our approach exploits a recent grouptheoretic description of this model. 1.
216|Wrappers for Feature Subset Selection|In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach andshow a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. 
217|A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection|We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment -- over half a million runs of C4.5 and a Naive-Bayes algorithm -- to estimate the effects of different parameters on these algorithms on real-world datasets. For cross-validation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 
218|Supervised and unsupervised discretization of continuous features|Many supervised machine learning algorithms require a discrete feature space. In this paper, we review previous work on continuous feature discretization, identify de n-ing characteristics of the methods, and conduct an empirical evaluation of several methods. We compare binning, an unsupervised discretization method, to entropy-based and purity-based methods, which are supervised algorithms. We found that the performance of the Naive-Bayes algorithm signi cantly improved when features were discretized using an entropy-based method. In fact, over the 16 tested datasets, the discretized version of Naive-Bayes slightly outperformed C4.5 on average. We also show that in some cases, the performance of the C4.5 induction algorithm signi cantly improved if features were discretized in advance ? in our experiments, the performance never signi cantly degraded, an interesting phenomenon considering the fact that C4.5 is capable of locally discretizing features. 1
219|Estimating Continuous Distributions in Bayesian Classifiers|When modeling a probability distribution with a Bayesian network, we are faced with the problem of how to handle continuous variables. Most previous work has either solved the problem by discretizing, or assumed that the data are generated by a single Gaussian. In this paper we abandon the normality assumption and instead use statistical methods for nonparametric density estimation. For a naive Bayesian classifier, we present experimental results on a variety of natural and artificial domains, comparing two methods of density estimation: assuming normality and modeling each conditional distribution with a single Gaussian; and using nonparametric kernel density estimation. We observe large reductions in error on several natural and artificial data sets, which suggests that kernel estimation is a useful tool for learning Bayesian models. In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence, Morgan Kaufmann Publishers, San Mateo, 1995 1 Introduction In rec...
220|An analysis of Bayesian classifiers|In this paper we present anaverage-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting
221|A Tutorial on Learning Bayesian Networks|We examine a graphical representation of uncertain knowledge called a Bayesian network. The representation is easy to construct and interpret, yet has formal probabilistic semantics making it suitable for statistical manipulation. We show how we can use the representation to learn new knowledge by combining domain knowledge with statistical data. 1 Introduction  Many techniques for learning rely heavily on data. In contrast, the knowledge encoded in expert systems usually comes solely from an expert. In this paper, we examine a knowledge representation, called a Bayesian network, that lets us have the best of both worlds. Namely, the representation allows us to learn new knowledge by combining expert domain knowledge and statistical data. A Bayesian network is a graphical representation of uncertain knowledge that most people find easy to construct and interpret. In addition, the representation has formal probabilistic semantics, making it suitable for statistical manipulation (Howard,...
222| 	 Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier   |The simple Bayesian classifier (SBC) is commonly thought to assume that attributes are independent given the class, but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences. No explanation for this has been proposed so far. In this paper we show that the SBC does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin. The key to this finding lies in the distinction between classification and probability estimation: correct classification can be achieved even when the probability estimates used contain large errors. We show that the previously-assumed region of optimality of the SBC is a second-order infinitesimal fraction of the actual one. This is followed by the derivation of several necessary and several sufficient conditions for the optimality of the SBC. For example, the SBC is optimal for learning arbitrary conjunctions and disjunctions, even though they violate the independence assumption. The paper also reports empirical evidence of the SBC&#039;s competitive performance in domains containing substantial degrees of attribute dependence.
223|Learning Bayesian Networks With Local Structure|.  We examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability distributions (CPDs) that quantify these networks. This increases the space of possible models, enabling the representation of CPDs with a variable number of parameters. The resulting learning procedure induces models that better emulate the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures and provide an empirical evaluation of the proposed learning procedure. This evaluation indicates that learning curves characterizing this procedure converge faster, in the number of training instances, than those of the standard procedure, which ignores the local structure of the CPDs. Our results also show that networks learned with local structures tend to be more complex (in terms of a...
224|Induction of Selective Bayesian Classifiers|In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that carries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research.
225|Learning Bayesian belief networks: An approach based on the MDL principle|A new approach for learning Bayesian belief networks from raw data is presented. The approach is based on Rissanen&#039;s Minimal Description Length (MDL) principle, which is particularly well suited for this task. Our approach does not require any prior assumptions about the distribution being learned. In particular, our method can learn unrestricted multiply-connected belief networks. Furthermore, unlike other approaches our method allows us to tradeo accuracy and complexity in the learned model. This is important since if the learned model is very complex (highly connected) it can be conceptually and computationally intractable. In such a case it would be preferable to use a simpler model even if it is less accurate. The MDL principle o ers a reasoned method for making this tradeo. We also show that our method generalizes previous approaches based on Kullback cross-entropy. Experiments have been conducted to demonstrate the feasibility of the approach. Keywords: Knowledge Acquisition ? Bayes Nets ? Uncertainty Reasoning. 1
226|On bias, variance, 0/1-loss, and the curse-of-dimensionality|Abstract. The classification problem is considered in which an output variable y assumes discrete values with respective probabilities that depend upon the simultaneous values of a set of input variables x ={x1,...,xn}.At issue is how error in the estimates of these probabilities affects classification error when the estimates are used in a classification rule. These effects are seen to be somewhat counter intuitive in both their strength and nature. In particular the bias and variance components of the estimation error combine to influence classification in a very different way than with squared error on the probabilities themselves. Certain types of (very high) bias can be canceled by low variance to produce accurate classification. This can dramatically mitigate the effect of the bias associated with some simple estimators like “naive ” Bayes, and the bias induced by the curse-of-dimensionality on nearest-neighbor procedures. This helps explain why such simple methods are often competitive with and sometimes superior to more sophisticated ones for classification, and why “bagging/aggregating ” classifiers can often improve accuracy. These results also suggest simple modifications to these procedures that can (sometimes dramatically) further improve their classification performance.
227|A Guide to the Literature on Learning Probabilistic Networks From Data|This literature review discusses different methods under the general rubric of learning Bayesian networks from data, and includes some overlapping work on more general probabilistic networks. Connections are drawn between the statistical, neural network, and uncertainty communities, and between the different methodological communities, such as Bayesian, description length, and classical statistics. Basic concepts for learning and Bayesian networks are introduced and methods are then reviewed. Methods are discussed for learning parameters of a probabilistic network, for learning the structure, and for learning hidden variables. The presentation avoids formal definitions and theorems, as these are plentiful in the literature, and instead illustrates key concepts with simplified examples.  Keywords--- Bayesian networks, graphical models, hidden variables, learning, learning structure, probabilistic networks, knowledge discovery. I. Introduction  Probabilistic networks or probabilistic gra...
228|Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables|We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that (1) the BIC/MDL measure is the least accurate, having a bias in favor of simple models, and (2) the Draper and CS measures are the most accurate. 1
229|Adaptive Probabilistic Networks with Hidden Variables|. Probabilistic networks (also known as Bayesian belief networks) allow a compact description of complex stochastic relationships among several random variables. They are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. In this paper, we investigate the problem of learning probabilistic networks with known structure and hidden variables. This is an important problem, because structure is much easier to elicit from experts than numbers, and the world is rarely fully observable. We present a gradient-based algorithmand show that the gradient can be computed locally, using information that is available as a byproduct of standard probabilistic network inference algorithms. Our experimental results demonstrate that using prior knowledge about the structure, even with hidden variables, can significantly improve the learning rate of probabilistic networks. We extend the method to networks in which the conditional probability tables are described using a ...
230|Learning Belief Networks in the Presence of Missing Values and Hidden Variables|In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from  complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the  Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide...
231|MLC++: a machine learning library in C++|We present MLC++, a library of C++ classes and tools for supervised Machine Learning. While MLC++ provides general learning algorithms that can be used by end users, the main objective is to provide researchers and experts with a wide variety of tools that can accelerate algorithm development, increase software reliability, provide comparison tools, and display information visually. More than just a collection of existing algorithms, MLC++ is an attempt to extract commonalities of algorithms and decompose them for a unified view that is simple, coherent, and extensible. In this paper we discuss the problems MLC++ aims to solve, the design of MLC++, and the current functionality.  
232|Building Classifiers using Bayesian Networks|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approacheswe single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from...
233|Discretizing Continuous Attributes While Learning Bayesian Networks|We introduce a method for learning Bayesian networks that handles the discretization of continuous variables as an integral part of the learning process. The main ingredient in this method is a new metric based on the Minimal Description Length principle for choosing the threshold values for the discretization while learning the Bayesian network structure. This score balances the complexity of the learned discretization and the learned network structure against how well they model the training data. This ensures that the discretization of each variable introduces just enough intervals to capture its interaction with adjacent variables in the network. We formally derive the new metric, study its main properties, and propose an iterative algorithm for learning a discretization policy. Finally, we illustrate its behavior in applications to supervised learning. 1 INTRODUCTION  Bayesian networks provide efficient and effective representation of the joint probability distribution over a set ...
234|Searching for Dependencies in Bayesian Classifiers|Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data. 23.1 Introduction The Bayesian classifier (Duda
235|Efficient Learning of Selective Bayesian Network Classifiers|In this paper, we present a computationally efficient method for inducing  selective Bayesian network classifiers. Our approach is to use informationtheoretic metrics to efficiently select a subset of attributes from which to learn the classifier. We explore three conditional, information-theoretic metrics that are extensions of metrics used extensively in decision tree learning, namely Quinlan&#039;s gain and gain ratio metrics and Mantaras&#039;s distance metric. We experimentally show that the algorithms based on gain ratio and distance metric learn selective Bayesian networks that have predictive accuracies as good as or better than those learned by existing selective Bayesian network induction approaches (K2-AS), but at a significantly lower computational cost. We prove that the subset-selection phase of these information-based algorithms has polynomial complexity as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS. We also compare the performance o...
236|A Comparison of Induction Algorithms for Selective and non-Selective Bayesian Classifiers|In this paper we present a novel induction algorithm for Bayesian networks. This selective Bayesian network classifier selects a subset of attributes that maximizes predictive accuracy prior to the network learning phase, thereby learning Bayesian networks with a bias for small, high-predictive-accuracy networks. We compare the performance of this classifier with selective and non-selective naive Bayesian classifiers. We show that the selective Bayesian network classifier performs significantly better than both versions of the naive Bayesian classifier on almost all databases analyzed, and hence is an enhancement of the naive Bayesian classifier. Relative to the non-selective Bayesian network classifier, our selective Bayesian network classifier generates networks that are computationally simpler to evaluate and that display predictive accuracy comparable to that of Bayesian networks which model all features. 1 INTRODUCTION  Bayesian induction methods have proven to be an important cla...
237|The dynamic behavior of a data dissemination protocol for network programming at scale |To support network programming, we present Deluge, a reliable data dissemination protocol for propagating large data objects from one or more source nodes to many other nodes over a multihop, wireless sensor network. Deluge builds from prior work in density-aware, epidemic maintenance protocols. Using both a real-world deployment and simulation, we show that Deluge can reliably disseminate data to all nodes and characterize its overall performance. On Mica2dot nodes, Deluge can push nearly 90 bytes/second, oneninth the maximum transmission rate of the radio supported under TinyOS. Control messages are limited to 18 % of all transmissions. At scale, the protocol exposes interesting propagation dynamics only hinted at by previous dissemination work. A simple model is also derived which describes the limits of data propagation in wireless networks. Finally, we argue that the rates obtained for dissemination are inherently lower than that for single path propagation. It appears very hard to significantly improve upon the rate obtained by Deluge and we identify establishing a tight lower bound as an open problem.
238|The broadcast storm problem in a mobile ad hoc network|Broadcasting is a common operation in a network to resolve many issues. In a mobile ad hoc network (MANET) in par-ticular, due to host mobility, such operations are expected to be executed more frequently (such as finding a route to a particular host, paging a particular host, and sending an alarm signal). Because radio signals are likely to overlap with others in a geographical area, a straightforward broad-casting by flooding is usually very costly and will result in serious redundancy, contention, and collision, to which we refer as the broadcast storm problem. In this paper, we iden-tify this problem by showing how serious it is through anal-yses and simulations. We propose several schemes to reduce redundant rebroadcasts and differentiate timing of rebroad-casts to alleviate this problem. Simulation results are pre-sented, which show different levels of improvement over the basic flooding approach.
239|TOSSIM: Accurate and Scalable Simulation of Entire TinyOS Applications|Accurate and scalable simulation has historically been a key enabling factor for systems research. We present TOSSIM, a simulator for TinyOS wireless sensor networks. By exploiting the sensor network domain and TinyOS’s design, TOSSIM can capture network behavior at a high fidelity while scaling to thousands of nodes. By using a probabilistic bit error model for the network, TOSSIM remains simple and efficient, but expressive enough to capture a wide range of network interactions. Using TOSSIM, we have discovered several bugs in TinyOS, ranging from network bitlevel MAC interactions to queue overflows in an ad-hoc routing protocol. Through these and other evaluations, we show that detailed, scalable sensor network simulation is possible.
240|Maté: A Tiny Virtual Machine for Sensor Networks|Composed of tens of thousands of tiny devices with very limited resources (&#034;motes&#034;), sensor networks are subject to novel systems problems and constraints. The large number of motes in a sensor network means that there will often be some failing nodes; networks must be easy to repopu-late. Often there is no feasible method to recharge motes, so energy is a precious resource. Once deployed, a network must be reprogrammable although physically unreachable, and this reprogramming can be a significant energy cost. We present Maté, a tiny communication-centric virtual machine designed for sensor networks. Mat~&#039;s high-level in-terface allows complex programs to be very short (under 100 bytes), reducing the energy cost of transmitting new programs. Code is broken up into small capsules of 24 instructions, which can self-replicate through the network. Packet sending and reception capsules enable the deploy-ment of ad-hoc routing and data aggregation algorithms. Maté&#039;s concise, high-level program representation simplifies programming and allows large networks to be frequently re-programmed in an energy-efficient manner; in addition, its safe execution environment suggests a use of virtual ma-chines to provide the user/kernel boundary on motes that have no hardware protection mechanisms. 
241|Trickle: A Self-Regulating Algorithm for Code Propagation and Maintenance in Wireless Sensor Networks|We present Trickle, an algorithm for propagating and maintaining code updates in wireless sensor networks. Borrowing techniques from the epidemic/gossip, scalable multicast, and wireless broadcast literature, Trickle uses a &#034;polite gossip&#034; policy, where motes periodically broadcast a code summary to local neighbors but stay quiet if they have recently heard a summary identical to theirs. When a mote hears an older summary than its own, it broadcasts an update. Instead of flooding a network with packets, the algorithm controls the send rate so each mote hears a small trickle of packets, just enough to stay up to date. We show that with this simple mechanism, Trickle can scale to thousand-fold changes in network density, propagate new code in the order of seconds, and impose a maintenance cost on the order of a few sends an hour.
243|Negotiation-based Protocols for Disseminating Information in Wireless Sensor Networks|Abstract. In this paper, we present a family of adaptive protocols, called SPIN (Sensor Protocols for Information via Negotiation), that efficiently disseminate information among sensors in an energy-constrained wireless sensor network. Nodes running a SPIN communication protocol name their data using high-level data descriptors, called meta-data. They use meta-data negotiations to eliminate the transmission of redundant data throughout the network. In addition, SPIN nodes can base their communication decisions both upon application-specific knowledge of the data and upon knowledge of the resources that are available to them. This allows the sensors to efficiently distribute data given a limited energy supply. We simulate and analyze the performance of four specific SPIN protocols: SPIN-PP and SPIN-EC, which are optimized for a point-to-point network, and SPIN-BC and SPIN-RL, which are optimized for a broadcast network. Comparing the SPIN protocols to other possible approaches, we find that the SPIN protocols can deliver 60 % more data for a given amount of energy than conventional approaches in a point-to-point network and 80 % more data for a given amount of energy in a broadcast network. We also find that, in terms of dissemination rate and energy usage, the SPIN protocols perform close to the theoretical optimum in both point-to-point and broadcast networks.
244|Lessons From A Sensor Network Expedition|Habitat monitoring is an important driving application for wireless sensor networks (WSNs). Although researchers anticipate some challenges arising in the real-world deployments of sensor networks, a number of problems can be discovered only through experience. This paper evaluates a sensor network system described in an earlier work and presents a set of experiences from a four month long deployment on a remote island o# the coast of Maine. We present an in-depth analysis of the environmental and node health data. The close integration of WSNs with their environment provides biological data at densities previous impossible; however, we show that the sensor data is also useful for predicting system operation and network failures. Based on over one million data and health readings, we analyze the node and network design and develop network reliability profiles and failure models.
245|A remote code update mechanism for wireless sensor networks|Wireless sensor networks consist of collections of small, low-power nodes that interface or interact with the physical environment. The ability to add new functionality or perform software maintenance without having to physically reach each individual node is already an essential service, even at the limited scale at which current sensor networks are deployed. TinyOS supports single-hop over-the-air reprogramming today, but the need to reprogram sensors in a multihop network will become particularly critical as sensor networks mature and move toward larger deployment sizes. In this paper we present Multihop Over-the-Air Programming (MOAP), a code distribution mechanism specifically targeted for Mica-2 Motes. We discuss and analyze the design goals, constraints, choices and optimizations focusing in particular on dissemination strategies and retransmission policies. We have implemented MOAP on Mica-2 motes and we evaluate that implementation using both emulation and testbed experiments. We show that our dissemination mechanism obtains a 60–90 % performance improvement in terms of required transmissions compared to flooding. We also show that a very simple windowed retransmission tracking scheme is nearly as effective as arbitrary repairs and yet is much better suited to energy and memory constrained embedded systems. 1
246|Scalable Reliable Multicast Using Multiple Multicast Groups|We examine an approach for providing reliable, scalable multicast communication,  involving the use of multiple multicast groups for reducing receiver processing costs  in a multicast session. In this approach a single multicast group is used for the original  transmission of packets. Retransmissions of packets are done to separate multicast  groups, which receivers dynamically join or leave. We first show that protocols using an  infinite number of multicast groups incur much less processing overhead at the receivers  compared to protocols that use only a single multicast group. This is due to the fact  that receivers do not receive retransmissions of packets they have already received correctly.  Next, we derive the number of unwanted redundant packets at a receiver due to  using only a finite number of multicast groups, for a specific negative acknowledgment  (NAK)-based protocol. We then explore the minimum number of multicast groups  required to keep the cost of processing unwant...
247|Model-Driven Data Acquisition in Sensor Networks|Declarative queries are proving to be an attractive paradigm for interacting with networks of wireless sensors. The metaphor that &#034;the sensornet is a database&#034; is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.
248|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
249|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
250|Query Processing for Sensor Networks|Hardware for sensor nodes that combine physical sensors, actuators, embedded processors, and communication components has advanced significantly over the last decade, and made the large-scale deployment of such sensors a reality. Applications range from monitoring applications such as inventory maintenance over health care to military applications.
251|Approximate aggregation techniques for sensor databases|In the emerging area of sensor-based systems, a significant challenge is to develop scalable, fault-tolerant methods to extract useful information from the data the sensors collect. An approach to this data management problem is the use of sensor database systems, exemplified by TinyDB and Cougar, which allow users to perform aggregation queries such as MIN, COUNT and AVG on a sensor network. Due to power and range constraints, centralized approaches are generally impractical, so most systems use in-network aggregation to reduce network traffic. Also, aggregation strategies must provide fault-tolerance to address the issues of packet loss and node failures inherent in such a system. An unfortunate consequence of standard methods is that they typically introduce duplicate values, which must be accounted for to compute aggregates correctly. Another consequence of loss in the network is that exact aggregation is not possible in general. With this in mind, we investigate the use of approximate in-network aggregation using small sketches. Our contributions are as follows: 1) we generalize well known duplicateinsensitive sketches for approximating COUNT to handle SUM (and by extension, AVG and other aggregates), 2) we present and analyze methods for using sketches to produce accurate results with low communication and computation overhead (even on low-powered CPUs with little storage and no floating point operations), and 3) we present an extensive experimental validation of our methods. 1
252|Evaluating Probabilistic Queries over Imprecise Data|Sensors are often employed to monitor continuously changing entities like locations of moving ob-jects and temperature. The sensor readings are reported to a database system, and are subsequently used to answer queries. Due to continuous changes in these values and limited resources (e.g., net-work bandwidth and battery power), the database may not be able to keep track of the actual values of the entities. Queries that use these old values may produce incorrect answers. However, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. More generally, query answers can be augmented with probabilistic guarantees of the validity of the answers. In this paper, we study probabilistic query evaluation based on uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers, and provide efficient indexing and numeric solutions. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments
253|Scalable information-driven sensor querying and routing for ad hoc heterogeneous sensor networks|This paper describes two novel techniques, informationdriven sensor querying (IDSQ) and constrained anisotropic diffusion routing (CADR), for energy-efficient data querying and routing in ad hoc sensor networks for a range of collaborative signal processing tasks. The key idea is to introduce an information utility measure to select which sensors to query and to dynamically guide data routing. This allows us to maximize information gain while minimizing detection latency and bandwidth consumption for tasks such as localization and tracking. Our simulation results have demonstrated that the information-driven querying and routing techniques are more energy efficient, have lower detection latency, and provide anytime algorithms to mitigate risks of link/node failures. 1
254|Distinct sampling for highly-accurate answers to distinct values queries and event reports |Estimating the number of distinct values is a wellstudied problem, due to its frequent occurrence in queries and its importance in selecting good query plans. Previous work has shown powerful negative results on the quality of distinct-values estimates based on sampling (or other techniques that examine only part of the input data). We present an approach, called distinct sampling, that collects a specially tailored sample over the distinct values in the input, in a single scan of the data. In contrast to the previous negative results, our small Distinct Samples are guaranteed to accurately estimate the number of distinct values. The samples can be incrementally maintained up-to-date in the presence of data insertions and deletions, with minimal time and memory overheads, so that the full scan may be performed only once. Moreover, a stored Distinct Sample can be used to accurately estimate the number of distinct values within any range specified by the query, or within any other subset of the data satisfying a query predicate. We present an extensive experimental study of distinct sampling. Using synthetic and real-world data sets, we show that distinct sampling gives distinct-values estimates to within 0%–10 % relative error, whereas previous methods typically incur 50%–250 % relative error. Next, we show how distinct sampling can provide fast, highlyaccurate approximate answers for “report ” queries in high-volume, session-based event recording environments, such as IP networks, customer service call centers, etc. For a commercial call center environment, we show that a 1 % Distinct Sample
255|Selectivity Estimation using Probabilistic Models|Estimating the result size of complex queries that involve selection on multiple attributes and the join of several relations is a difficult but fundamental task in database query processing. It arises in cost-based query optimization, query profiling, and approximate query answering. In this paper, we show how probabilistic graphical models can be effectively used for this task as an accurate and compact approximation of the joint frequency distribution of multiple attributes across multiple relations. Probabilistic Relational Models (PRMs) are a recent development that extends graphical statistical models such as Bayesian Networks to relational domains. They represent the statistical dependencies between attributes within a table, and between attributes across foreign-key joins. We provide an efficient algorithm for constructing a PRM from a database, and show how a PRM can be used to compute selectivity estimates for a broad class of queries. One of the major contributions of this work is a unified framework for the estimation of queries involving both select and foreign-key join operations. Furthermore, our approach is not limited to answering a small set of predetermined queries; a single model can be used to effectively estimate the sizes of a wide collection of potential queries across multiple tables. We present results for our approach on several real-world databases. For both single-table multi-attribute queries and a general class of select-join queries, our approach produces more accurate estimates than standard approaches to selectivity estimation, using comparable space and time.
256|Independence is Good: Dependency-Based Histogram Synopses for High-Dimensional Data|Approximating the joint data distribution of a multi-dimensional data set through a compact and accurate histogram synopsis is a fundamental problem arising in numerous practical scenarios, including query optimization and approximate query answering. Existing solutions either rely on simplistic independence assumptions or try to directly approximate the full joint data distribution over the complete set of attributes. Unfortunately, both approaches are doomed to fail for high-dimensional data sets with complex correlation patterns between attributes. In this paper, we propose a novel approach to histogram-based synopses that employs the solid foundation of statistical interaction models to explicitly identify and exploit the statistical characteristics of the data. Abstractly, our key idea is to break the synopsis into (1) a statistical interaction model that accurately captures significant correlation and independence patterns in data, and (2) a collection of histograms on low-dimensional marginals that, based on the model, can provide accurate approximations of the overall joint data distribution. Extensive experimental results with several real-life data sets verify the effectiveness of our approach. An important aspect of our general, model-based methodology is that it can be used to enhance the performance of other synopsis techniques that are based on data-space partitioning (e.g., wavelets) by providing an effective tool to deal with the “dimensionality curse”. 1.
258|Monitoring a complex physical system using a hybrid dynamic bayes net|The Reverse Water Gas Shift system (RWGS) is a complex physical system designed to produce oxygen from the carbon dioxide atmosphere on Mars. If sent to Mars, it would operate without human supervision, thus requiring a reliable automated system for monitoring and control. The RWGS presents many challenges typical of real-world systems, including: noisy and biased sensors, nonlinear behavior, effects that are manifested over different time granularities, and unobservability of many important quantities. In this paper we model the RWGS using a hybrid (discrete/continuous) Dynamic Bayesian Network (DBN), where the state at each time slice contains 33 discrete and 184 continuous variables. We show how the system state can be tracked using probabilistic inference over the model. We discuss how to deal with the various challenges presented by the RWGS, providing a suite of techniques that are likely to be useful in a wide range of applications. In particular, we describe a general framework for dealing with nonlinear behavior using numerical integration techniques, extending the successful Unscented Filter. We also show how to use a fixed-point computation to deal with effects that develop at different time scales, specifically rapid changes occurring during slowly changing processes. We test our model using real data collected from the RWGS, demonstrating the feasibility of hybrid DBNs for monitoring complex real-world physical systems. 1
259|Active Learning with Statistical Models|For manytypes of learners one can compute the statistically &#034;optimal&#034; way to select data. We review how  these techniques have been used with feedforward neural networks [MacKay, 1992# Cohn, 1994]. We then  showhow the same principles may be used to select data for two alternative, statistically-based learning  architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural  networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted  regression are both efficient and accurate.
260|Improving generalization with active learning|Abstract. Active learning differs from &amp;quot;learning from examples &amp;quot; in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples. In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers &amp;quot;useful. &amp;quot; We test our implementation, called an SGnetwork, on three domains and observe significant improvement in generalization.
261|Information-Based Objective Functions for Active Data Selection|Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed which measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness. 1 Introduction  Theories for data modelling often assume that the data is provided by a source that we do not control. However, there are two scenarios in which we are able to actively select training data. In the first, data measurements are relatively expensive or slow, and we want to know where to look next so as to learn as much as possible. According to Jaynes (1986), Bayesian reasoning was first applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries...
262|Supervised learning from incomplete data via an EM approach|Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data sets. We use mixture models for the density estimates and make two distinct appeals to the ExpectationMaximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm---EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark---the iris data set---are presented. 1 Introduction  Adaptive systems generally operate in environments that are fraught with imperfections; nonetheless they must cope with these imperfections and learn to extract as much relevant information as needed for their particular goals. One form of imperfection is incompleteness in sensing information. Inc...
263|Neural network exploration using optimal experiment design|We consider the question &#034;How should one act when the only goal is to learn as much as possible?&#034; Building  on the theoretical results of Fedorov [1972] and MacKay [1992], we apply techniques from Optimal  Experiment Design (OED) to guide the query/action selection of a neural network learner. We demonstrate  that these techniques allow the learner to minimize its generalization error by exploring its domain  efficiently and completely.We conclude that, while not a panacea, OED-based query/action has muchto  offer, especially in domains where its high computational costs can be tolerated.
264|Active Exploration in Dynamic Environments|Whenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection. This might be disadvantageous when costs are assigned to &#034;negative experiences&#034;. The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller&#039;s accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors -- exploration and exploitation -- depending on expected costs and knowledge gain. The appropriateness of this method is demonstrated by a simple robot navigation task.   
265|Reinforcement Driven Information Acquisition In Non-Deterministic Environments|For an agent living in a non-deterministic Markov environment (NME), what is, in theory, the fastest way of acquiring information about its statistical properties? The answer is: to design &#034;optimal&#034; sequences of &#034;experiments&#034; by performing action sequences that maximize expected information gain. This notion is implemented by combining concepts from information theory and reinforcement learning. Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration.  
266|Directed Diffusion for Wireless Sensor Networking|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is datacentric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network (e.g., data aggregation). We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network analytically and experimentally. Our evaluation indicates that directed diffusion can achieve significant energy savings and can outperform idealized traditional schemes (e.g., omniscient multicast) under the investigated scenarios.
267|Energy-efficient communication protocol for wireless microsensor networks|Wireless distributed microsensor systems will enable the reliable monitoring of a variety of environments for both civil and military applications. In this paper, we look at communication protocols, which can have significant impact on the overall energy dissipation of these networks. Based on our findings that the conventional protocols of direct transmission, minimum-transmission-energy, multihop routing, and static clustering may not be optimal for sensor networks, we propose LEACH (Low-Energy Adaptive Clustering Hierarchy), a clustering-based protocol that utilizes randomized rotation of local cluster base stations (cluster-heads) to evenly distribute the energy load among the sensors in the network. LEACH uses localized coordination to enable scalability and robustness for dynamic networks, and incorporates data fusion into the routing protocol to reduce the amount of information that must be transmitted to the base station. Simulations show that LEACH can achieve as much as a factor of 8 reduction in energy dissipation compared with conventional routing protocols. In addition, LEACH is able to distribute energy dissipation evenly throughout the sensors, doubling the useful system lifetime for the networks we simulated. 
268|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
269|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
270|Internet time synchronization: The Network Time Protocol|Abstruct- This paper describes the network time protocol (NTP), which is designed to distribute time information in a large, diverse internet system operating at speeds from mundane to lightwave. It uses a symmetric architecture in which a distributed subnet of time servers operating in a self-organizing, hierarchical configuration synchronizes local clocks within the subnet and to national time standards via wire, radio, or calibrated atomic clock. The servers can also redistribute time information within a network via local routing algorithms and time daemons. This paper also discusses the architecture, protocol and algorithms, which were developed over several years of implementation refinement and resulted in the designation of NTP as an Internet Standard protocol. The NTP synchronization system, which has been in regular operation in the Internet for the last several years, is described along with performance data which shows that timekeeping accuracy throughout most portions of the Internet can be ordinarily maintained to within a few milliseconds, even in cases of failure or disruption of clocks, time servers or networks. I.
271| An Architecture for Wide-Area Multicast Routing |Existing multicast routing mechanisms were intended for use within regions where a group is widely represented or bandwidth is universally plentiful. When group members, and senders to those group members, are distributed sparsely across a wide area, these schemes are not efficient; data packets or membership report information are occasionally sent over many links that do not lead to receivers or senders, respectively. Wehave developed a multicast routing architecture that efficiently establishes distribution trees across wide area internets, where many groups will be sparsely represented. Efficiency is measured in terms of the state, control message processing, and data packet processing, required across the entire network in order to deliver data packets to the members of the group. Our Protocol Independent Multicast (PIM) architecture: (a) maintains the traditional IP multicast service model of receiver-initiated membership; (b) can be configured to adapt to different multicast group and network characteristics; (c) is not dependent on a specific unicast routing protocol; and (d) uses soft-state mechanisms to adapt to underlying network conditions and group dynamics. The robustness, flexibility, and scaling properties of this architecture make it well suited to large heterogeneous inter-networks.
272|The design and implementation of an intentional naming system|This paper presents the design and implementation of the Intentional Naming System (INS), a resource discovery and service location system for dynamic and mobile networks of devices and computers. Such environments require a naming system that is (i) expressive, to describe and make requests based on specific properties of services, (ii) responsive, to track changes due to mobility and performance, (iii) robust, to handle failures, and (iv) easily configurable. INS uses a simple language based on attributes and values for its names. Applications use the language to describe what they are looking for (i.e., their intent), not where to find things (i.e., not hostnames). INS implements a late binding mechanism that integrates name resolution and message routing, enabling clients to continue communicating with end-nodes even if the name-to-address mappings change while a session is in progress. INS resolvers self-configure to form an application-level overlay network, which they use to discover new services, perform late binding, and maintain weak consistency of names using soft-state name exchanges and updates. We analyze the performance of the INS algorithms and protocols, present measurements of a Java-based implementation, and describe three applications we have implemented that demonstrate the feasibility and utility of INS.
273|Measuring and Reducing Energy Consumption of Network Interfaces in Hand-Held Devices|Next generation hand-held devices must provide seamless connectivity while obeying stringent power and size constrains. In this paper we examine this issue from the point of view of the Network Interface (NI). We measure the power usage of two PDAs, the Apple Newton Messagepad and Sony Magic Link, and four NIs, the Metricom Ricochet Wireless Modem, the AT&amp;T Wavelan operating at 915 MHz and 2.4 GHz, and the IBM Infrared Wireless LAN Adapter. These measurements clearly indicate that the power drained by the network interface constitutes a large fraction of the total power used by the PDA. We then examine two classes of optimizations that can be used to reduce network interface energy consumption on these devices: transport-level strategies and application-level strategies. Simulation experiments of transportlevel strategies show that the dominant cost comes not from the number of packets sent or received by a particular transport protocol but the amount of time that the NI is in an activ...
274|Impact of network density on Data Aggregation in wireless sensor networks|In-network data aggregation is essential for wireless sensor networks where resources (e.g., bandwidth, energy) are limited. In a previously proposed data dissemination scheme, data is opportunistically aggregated at the intermediate nodes on a low-latency tree which may not necessarily be energy efficient. A more energy-efficient tree is a greedy tree which can be incrementally constructed by connecting each source to the closest point of the existing tree. In this paper, we propose a greedy approach for constructing a greedy aggregation tree to improve path sharing. We evaluated the performance of this greedy approach by comparing it to the prior opportunistic approach. Our preliminary result suggests that although the greedy aggregation and the opportunistic aggregation are roughly equivalent at low-density networks, the greedy aggregation can achieve signficant energy savings at higher densities. In one experiment we found that the greedy aggregation can achieve up to 45 % energy savings over the opportunistic aggregation without an adverse impact on latency or robustness. 
275|Geographical and energy aware routing: A recursive data dissemination protocol for wireless sensor networks|Future sensor networks will be composed of a large number of densely deployed sensors/actuators. A key feature of such networks is that their nodes are untethered and unattended. Consequently, energy efficiency is an important design consideration for these networks. Motivated by the fact that sensor network queries may often be geographical, we design and evaluate an energy efficient routing algorithm that propagates a query to the appropriate geographical region, without flooding. The proposed Geographic and Energy Aware Routing (GEAR) algorithm uses energy aware neighbor selection to route a packet towards the target region and Recursive Geographic Forwarding or Restricted Flooding algorithm to disseminate the packet inside the destination region. We evaluate the GEAR protocol using simulation. We find that, especially for non-uniform traffic distribution, GEAR exhibits noticeably longer network lifetime than non-energyaware geographic routing algorithms. 1
276|Time Synchronization in Wireless Sensor Networks|OF THE DISSERTATION        University of California, Los Angeles, 2003  Professor Deborah L. Estrin, Chair    active research in large-scale networks of small, wireless, low-power sensors and actuators. Time synchronization is a critical piece of infrastructure in any dis- tributed system, but wireless sensor networks make particularly extensive use   physical world. However, while the clock accuracy and precision requirements are often stricter in sensor networks than in traditional distributed systems, energy and channel constraints limit the resources available to meet these goals.
277|Building Efficient Wireless Sensor Networks with Low-Level Naming|In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
278|Compressing TCP/IP headers for low-speed serial links|This RFC is a proposed elective protocol for the Internet community and requests discussion and suggestions for improvement. It describes a method for compressing the headers of TCP/IP datagrams to improve performance over low speed serial links. The motivation, implementation and performance of the method are described. C code for a sample implementation is given for reference. Distribution of this memo is unlimited. NOTE: Both ASCII and Postscript versions of this document are available. The ASCII version, obviously, lacks all the figures and all the information encoded in typographic variation (italics, boldface, etc.). Since this information was, in the author’s opinion, an essential part of the document, the ASCII version is at best incomplete and at worst misleading. Anyone who plans to work with this protocol is strongly encouraged obtain the Postscript version of this RFC.
279|AntNet: A Mobile Agents Approach to Adaptive Routing|This paper introduces AntNet, a new routing algorithm for communications networks. AntNet is an adaptive, distributed, mobile-agents-based algorithm whichwas inspired by recentwork on the ant colony metaphor. We apply AntNet to a datagram network and compare it with both static and adaptive state-of-the-art routing algorithms. We ran experiments for various paradigmatic temporal and spatial traffic distributions. AntNet showed both very good performance and robustness under all the experimental conditions with respect to its competitors.
280|Adaptive web caching: towards a new global caching architecture|An adaptive, highly scalable, and robust web caching system is needed to effectively handle the exponential growth and extreme dynamic environment of the World Wide Web. Our work presented last year sketched out the basic design of such a system. This sequel paper reports our progress over the past year. To assist caches making web query forwarding decisions, we sketch out the basic design of a URL routing framework. To assist fast searching within each cache group, we let neighbor caches share content information. Equipped with the URL routing table and neighbor cache contents, a cache in the revised design can now search the local group, and forward all missing queries quickly and efficiently, thus eliminating both the waiting delay and the overhead associated with multicast queries. The paper also presents a proposal for incremental deployment that provides a smooth transition from the currently deployed cache infrastructure to the new
281|Improving simulation for network research|In recent years, the Internet has grown signicantly in size and scope, and as a result new protocols and algorithms are being developed to meet changing operational
282|Piconet: Embedded Mobile Networking|Piconet is a general-purpose, low-power ad hoc radio network. It provides a base level of connectivity to even the simplest of sensing and  computing objects. It is our intention that a full range of portable and embedded devices may make use of this connectivity. This article outlines  the Piconet system, under development at the Olivetti and Oracle Research Laboratory (ORL). The authors discuss the motivation for providing  this low-level &#034;embedded networking,&#034; and describe their experiences of building such a system. The article concludes with a commentary on  some of the implications that power saving, and other considerations central to Piconet, have on the design of the system.
283|Scalable Timers for Soft State Protocols|Soft state protocols use periodic refresh messages to keep network state alive while adapting to changing network conditions; this has raised concerns regarding the scalability of protocols that use the soft-state approach. In existing soft state protocols, the values of the timers that control the sending of these messages, and the timers for aging out state, are chosen by matching empirical observations with desired recovery and response times. These fixed timer-values fail because they use time as a metric for bandwidth; they adapt neither to (1) the wide range of link speeds that exist in most wide-area internets, nor to (2) fluctuations in the amount of network state over time. We propose and evaluate a new approach in which timervalues adapt dynamically to the volume of control traffic and available bandwidth on the link. The essential mechanisms required to realize this scalable timers approach are: (1) dynamic adjustment of the senders ’ refresh rate so that the bandwidth allocated for control traffic is not exceeded, and (2) estimation of the senders ’ refresh rate at the receiver in order to determine when the state can be timed-out and deleted. The refresh messages are sent in a round robin manner not exceeding the bandwidth allocated to control traffic, and taking into account message priorities. We evaluate two receiver estimation methods for dynamically adjusting network state timeout values: (1) counting of the rounds and (2) exponential weighted moving average. 1
284|A New Proposal for RSVP Refreshes|As a soft-state protocol, RSVP specifies that each RSVP node sends periodic control messages to maintain the state for active RSVP sessions. The protocol overhead due to such periodic messages grows linearly with the number of RSVP sessions. One may reduce the overhead by using a longer refresh period, which unfortunately leads to longer delays in re-synchronizing RSVP state. In this paper we introduce a novel &#034;state-compression&#034; approach to reducing the overhead of periodic refreshes. Instead of per session refresh messages, an RSVP node sends periodically to each of its neighbor node a Digest message that contains a compressed version of the entire RSVP state shared with that particular neighbor. In order to speed up state synchronization in face of message losses we also enhance RSVP with an acknowledgment mechanism. Our mechanisms achieve a constant message transmission overhead and low delay while retaining the soft-state nature of the RSVP protocol. 1. Introduction  RSVP [3] is a...
285|A Survey on Sensor Networks|Recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field.
286|Minimum energy mobile wireless networks| We describe a distributed position-based network protocol optimized for minimum energy consumption in mobile wireless networks that support peer-to-peer communications. Given any number of randomly deployed nodes over an area, we illustrate that a simple local optimization scheme executed at each node guarantees strong connectivity of the entire network and attains the global minimum energy solution for stationary networks. Due to its localized nature, this protocol proves to be self-reconfiguring and stays close to the minimum energy solution when applied to mobile networks. Simulation results are used to verify the performance of the protocol. 
287|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
288|Minimum energy mobile wireless networks revisited|Energy conservation is a critical issue in designing wireless ad hoc networks, as the nodes are powered by batteries only. Given a set of wireless network nodes, the directed weighted transmission graph Gt has an edge uv if and only if node v is in the transmission range of node u and the weight of uv is typically defined as II,,vll + c for a constant 2 &lt;_ t ~ &lt; 5 and c&gt; O. The minimum power topology Gm is the smallest subgraph of Gt that contains the shortest paths between all pairs of nodes, i.e., the union of all shortest paths. In this paper, we described a distributed position-based networking protocol to construct an enclosure graph G~, which is an approximation of Gin. The time complexity of each node u is O(min(dG ~ (u)dG ~ (u), dG ~ (u) log dG ~ (u))), where dc(u) is the degree of node u in a graph G. The space required at each node to compute the minimum power topology is O(dG ~ (u)). This improves the previous result that computes Gm in O(dG, (u) a) time using O(dGt(U) 2) spaces. We also show that the average degree dG,(u) is usually a constant, which is at most 6. Our result is first developed for stationary network and then extended to mobile networks. I.
289|Near ground wideband channel measurement|Abstract- Frequency domain channel propagation measurements in the 800-1000 MHz band have been performed with ground-lying antennas. The range of path-loss exponent and shadowing variance for indoor and outdoor environment were determined. The range of these values roughly agree with those measured for higher elevation antennas. Frequency selectivity of the RF channel was also characterized by means of determining average coherence bandwidth (CBW). It was observed that there is a relationship between CBW and distance between transmitting and receiving antennas. I.
290|Low-power directsequence spread-spectrum modem architecture for distributed wireless sensor networks|Emerging CMOS and MEMS technologies enable the implementation of a large number of wireless distributed microsensors that can be easily and rapidly deployed to form highly redundant, self-configuring, and ad hoc sensor networks. To facilitate ease of deployment, these sensors should operate on battery for extended periods of time. A particular challenge in maintaining extended battery lifetime lies in achieving communications with low power. This paper presents a directsequence spread-spectrum modem architecture that provides robust communications for wireless sensor networks while dissipating very low power. The modem architecture has been verified in an FPGA implementation that dissipates only 33 mW for both transmission and reception. The implementation can be easily mapped to an ASIC technology with an estimated power performance of less than 1 mW.
291| A Stream Enabled Routing (SER) Protocol for Sensor Networks   (2002) |As the number of communication components can be integrated into a single chip increases, the possibility of high volume but low cost sensor nodes is realizable in the near future. Each sensor node can be designed to perform a single or multiple sensing operations, e.g., detecting temperature, seismic activity, object movement, and environmental pollution. As a result, a routing protocol must provide the quality of service (QoS) needed by the sensor nodes. A new routing protocol called Stream Enabled Routing (SER) is proposed to allow the sources choose the routes based on the instruction given by the sinks. It also takes into account the available energy of the sensor nodes. Also, SER allows the sink to give new instruction to the sources without setting up another path. Sources are the sensor nodes in the sensor field that are performing the sensing task. As a result, an interactive user-to-sources communication is achieved. In addition, the routing protocol is shown mathematically to perform well in the sensor network environment.
292|On the complexity of local search|We investigate the complexity of finding locally optimal solutions to NP-hard com-binatorial optimization problems. Local optimality arises in the context of local search algorithms, which try to find improved solutions by considering perturbations of the current solution (“neighbors ” of that solution). If no neighboring solution is better than the current solution, it is locally optimal. Finding locally optimal solutions is presumably easier than finding optimal solutions. Nevertheless, many popular local search algorithms are based on neighborhood structures for which locally optimal solutions are not known to be computable in polynomial time, either by using the local search algorithms themselves or by taking some indirect route. We define a natural class PLS consisting essentially of those local search problems for which local optimality can be verified in polynomial time, and show that there are complete problems for this class. In particular, finding a partition of a graph that is locally optimal with respect to the well-known Kernighan-Lin algorithm for graph partitioning is PLS-complete, and hence can be accomplished in polynomial time only if local optima can be found in polynomial time for all local search problems in PLS. 0 1988 Academic Press, Inc. 1.
293|Reasoning about beliefs and actions under computational resource constraints|Although many investigators arm a desire to build reasoning systems that behave consistently with the axiomatic basis dened by probability theory and utility theory, limited resources for engineering and computation can make a complete normative anal-ysis impossible. We attempt to move discussion beyond the debate over the scope of problems that can be handled eectively to cases where it is clear that there are in-sucient computational resources to perform an analysis deemed as complete. Under these conditions, we stress the importance of considering the expected costs and benets of applying alternative approximation procedures and heuristics for computation and knowledge acquisition. We discuss how knowledge about the structure of user utility can be used to control value tradeos for tailoring inference to alternative contexts. We address the notion of real-time rationality, focusing on the application of knowledge about the expected timewise-renement abilities of reasoning strategies to balance the bene ts of additional computation with the costs of acting with a partial result. We dis-cuss the benets of applying decision theory to control the solution of dicult problems given limitations and uncertainty in reasoning resources. 1
294|The estimation of probabilities|By way of introduction, a classification of kinds of probability is given in the form of a tree which also forms an approximate hierarchy: psychological, subjective, logical, physical, and tautological. Various relationships between these kinds of probability are mentioned. Methods, all more or less Bayesian, for the estimation of physical prob-abilities are then described. Binomial and multinomial probabilities are estimated by means of a three-tiered hierarchical Bayesian method. The method can also be regarded, in some of its aspects, as Bayesian in the ordinary sense, wherein the initial distribution for the physical probabilities is a weighted sum of symmetrical Dirichlet distributions. It can be proved that this is equivalent to the use of a single symmetrical Dirichlet distribution whose parameter is selected after sampling. Thus, in this problem, an ordinary Bayesian method implies an empirical Bayesian method. Next the species-sampling or vocabulary-sampling problem is considered wherein there is a multinomial population having a very large number of categories. The theory derives from a suggestion of Turing&#039;s, which in part anticipates the empirical Bayesian method. Among other things, it leads to estimates of population coverage for an enlarged sample. Finally the estimation of probabilities in multidimensional contingency tables is considered. The main method here is that of maximum entropy, but it is shown that this can be subsumed under a more general method of minimum discriminability for the formulation of hypotheses. Entropy is best regarded as a special case of the older and more obviously Bayesian concept of &#034;expected weight of evidence&#034;. 1.
295|Construction of Bayesian Network Structures From Data: A Brief Survey and an Efficient Algorithm|Previous algorithms for the recovery of Bayesian belief network structures from data have been either highly dependent on conditional independence (CI) tests, or have required on ordering on the nodes to be supplied by the user. We present an algorithm that integrates these two approaches: CI tests are used to generate an ordering on the nodes from the database, which is then used to recover the underlying Bayesian network structure using a non-Cl-test-based method. Results of the evaluation of the algorithm on a number of databases (e.g., ALARM, LED, and SOYBEAN) are presented. We also discuss some algorithm performance issues and open problems.
296|A Bayesian approach to learning causal networks|Whereas acausal Bayesian networks represent probabilistic independence, causal Bayesian networks represent causal relationships. In this paper, we examine Bayesian methods for learning both types of networks. Bayesian methods for learning acausal networks are fairly well developed. These methods often employ assumptions to facilitate the construction of priors, including the assumptions of parameter independence, parameter modularity, and likelihood equivalence. We show that although these assumptions also can be appropriate for learning causal networks, we need additional assumptions in order to learn causal networks. We introduce two sufficient assumptions, called mechanism independence and component independence. We show that these new assumptions, when combined with parameter independence, parameter modularity, and likelihood equivalence, allow us to apply methods for learning acausal networks to learn causal networks. 1
297|Causality in Bayesian Belief Networks|We address the problem of causal interpretation of the graphical structure of Bayesian belief networks (BBNs). We review the concept of causality explicated in the domain of structural equations models and show that it is applicable to BBNs. In this view, which we call mechanism-based, causality is defined within models and causal asymmetries arise when mechanisms are placed in the context of a system. We lay the link between structural equations models and BBNs models and formulate the conditions under which the latter can be given causal interpretation. 1 INTRODUCTION  Although references to causality permeate everyday scientific practice, the notion of causation has been one of the most controversial subjects in the philosophy of science. Hume&#039;s critique that causal connections cannot be observed, and therefore have no empirical basis, strongly influenced the empiricist framework and refocused the concept of causality to scientific models as opposed to reality. A strong attack on ca...
298|Using Causal Information and Local Measures to Learn Bayesian Networks|In previous work we developed a method of learning Bayesian Network models from raw data. This method relies on the well known minimal description length (MDL) principle. The MDL principle is particularly well suited to this task as it allows us to tradeoff, in a principled way, the accuracy of the learned network against its practical usefulness. In this paper we present some new results that have arisen from our work. In particular, we present a new local way of computing the description length. This allows us to make significant improvements in our search algorithm. In addition, we modify our algorithm so that it can take into account partial domain information that might be provided by a domain expert. The local computation of description length also opens the door for local refinement of an existent network. The feasibility of our approach is demonstrated by experiments involving networks of a practical size.
299|A characterization of the Dirichlet distribution with application to learning Bayesian networks|We provide a new characterization of the Dirichlet distribution. This characterization implies that under assumptions made by several previous authors for learning belief networks, a Dirichlet prior on the parameters is inevitable. 1
300|Lectures on Contingency Tables|The present set of lecture notes are prepared for the course “Statistik 2” at the University of Copenhagen. It is a revised version of notes prepared in connection with a series of lectures at the Swedish summerschool in Särö, June 11–17, 1979. The notes do by no means give a complete account of the theory of contingency tables. They are based on the idea that the graph theoretic methods in Darroch, Lauritzen and Speed (1978) can be used directly to develop this theory and, hopefully, with some pedagogical advantages. My thanks are due to the audience at the Swedish summerschool for patiently listening to the first version of these lectures, to Joseph Verducci, Stanford, who read the manuscript and suggested many improvements and corrections, and to Ursula Hansen, who typed the manuscript.
301|A decision-based view of causality|Most traditional models of uncertainty have focused on the associational relationship among variables as captured by conditional dependence. In order to successfully manage intelligent systems for decision making, however, we must be able to predict the effects of actions. In this paper, we attempt to unite two branches of research that address such predictions: causal modeling and decision analysis. First, we provide a definition of causal dependence in decision-analytic terms, which we derive from consequences of causal dependence cited in the literature. Using this definition, we show how causal dependence can be represented within an influence diagram. In particular, we identify two inadequacies of an ordinary influence diagram as a representation for cause. We introduce a special class of influence diagrams, called causal influence diagrams, which corrects one of these problems, and identify situations where the other inadequacy can be eliminated. In addition, we describe the relationships between Howard Canonical Form and existing graphical representations of cause. 1
302|An evaluation of the diagnostic accuracy|This work is an adaptation of Heckerman (1991). All figures and tables are printed with permission from MIT Press. We present an evaluation of the diagnostic accuracy of Pathfinder, an expert system that assists pathologists with the diagnosis of lymph-node diseases. We evaluate two versions of the system using both informal and decision-theoretic metrics of performance. In one version of Pathfinder, we assume incorrectly that all observations are conditionally independent. In the other version, we use a belief network to represent accurately the probabilistic dependencies among the observations. In both versions, we make the assumption—reasonable for this domain—that diseases are mutually exclusive and exhaustive. The results of the study show that (1) it is cost effective to represent probabilistic dependencies among observations in the lymph-node domain, and (2) the diagnostic accuracy of the more complex version of Pathfinder is at least as good as that of the Pathfinder expert. In addition, the study illustrates how informal and decision-theoretic metrics for performance complement one another. 2 1
303|ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING|  We provide an in-depth study of applying wireless sensor networks (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture to meet the requirements of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network reliability profiles and failure models.
304|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
306|Next century challenges: Scalable coordination in sensor networks|Networked sensors-those that coordinate amongst them-selves to achieve a larger sensing task-will revolutionize information gathering and processing both in urban envi-ronments and in inhospitable terrain. The sheer numbers of these sensors and the expected dynamics in these environ-ments present unique challenges in the design of unattended autonomous sensor networks. These challenges lead us to hypothesize that sensor network coordination applications may need to be structured differently from traditional net-work applications. In particular, we believe that localized algorithms (in which simple local node behavior achieves a desired global objective) may be necessary for sensor net-work coordination. In this paper, we describe localized al-gorithms, and then discuss directed diffusion, a simple com-munication model for describing localized algorithms. 1
307|Geography-informed Energy Conservation for Ad Hoc Routing|We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAF suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.   
309|The Impact of Data Aggregation in Wireless Sensor Networks|Sensor networks are distributed event-based systems that differ from traditional communication networks in several ways: sensor networks have severe energy constraints, redundant low-rate data, and many-to-one flows. Datacentric mechanisms that perform in-network aggregation of data are needed in this setting for energy-efficient information flow. In this paper we model data-centric routing and compare its performance with traditional end-toend routing schemes. We examine the impact of sourcedestination placement and communication network density on the energy costs and delay associated with data aggregation. We show that data-centric routing offers significant performance gains across a wide range of operational scenarios. We also examine the complexity of optimal data aggregation, showing that although it is an NP-hard problem in general, there exist useful polynomial-time special cases. 1 
310|Instrumenting the world with wireless sensor networks|Pervasive micro-sensing and actuation may revolutionize the way in which we understand and manage complex physical systems: from airplane wings to complex ecosystems. The capabilities for detailed physical monitoring and manipulation offer enormous opportunities for almost every scientific discipline, and it will alter the feasible granularity of engineering. We identify opportunities and challenges for distributed signal processing in networks of these sensing elements and investigate some of the architectural challenges posed by systems that are massively distributed, physically-coupled, wirelessly networked, and energy limited. 
311|Computing Aggregates for Monitoring Wireless Sensor Networks|Wireless sensor networks involve very large numbers of small, low-power, wireless devices. Given their unattended nature, and their potential applications in harsh environments, we need a monitoring infrastructure that indicates system failures and resource depletion. In this paper, we briefly describe an architecture for sensor network monitoring, then focus on one aspect of this architecture: continuously computing aggregates (sum, average, count) of network properties (loss rates, energylevels  etc., packet counts). Our contributions are two-fold. First, we propose a novel tree construction algorithm that enables energy-efficient computation of some classes of aggregates. Second, we show through actual implementation and experiments that wireless communication artifacts in even relatively benign environments can significantly impact the computation of these aggregate properties. In some cases, without careful attention to detail, the relative error in the computed aggregates can be as much as 50%. However, by carefully discarding links with heavy packet loss and asymmetry, we can improve accuracy by an order of magnitude.
312|Calibration as Parameter Estimation in Sensor Networks|We describe an ad-hoc localization system for sensor networks and explain why traditional calibration methods are inadequate for this system. Building upon previous work, we frame calibration as a parameter estimation problem; we parameterize each device and choose the values of those parameters that optimize the overall system performance. This method reduces our average error from 74.6% without calibration to 10.1%. We propose ways to expand this technique to a method of autocalibration for localization as well as to other sensor network applications.
313|Survey of Quality of Service in Mobile Computing Environments|The specification and management of Quality of Service (QoS) is important in networks and distributed computing systems, particularly to support multimedia applications. The advent of portable lap-top computers, palmtops and Personal Digital Assistants with integrated communication capabilities facilitates mobile computing. This paper is a survey of QoS concepts and techniques for mobile distributed computing environments. The QoS attributes typically specified and negotiated for general communication systems are described as well as some QoS models. A brief overview is given of some practical systems described in the literature. The design issues relating to both mobile and nomadic computing are explained and then the specific QoS issues related to mobile and nomadic systems are discussed. The conclusion summarises the important issues relating to supporting QoS for mobile systems. Keywords Mobile systems, nomadic systems, quality of service, multimedia Quality of Service in Mobile...
314|A Wireless Embedded Sensor Architecture for System-Level Optimization|Emerging low power, embedded, wireless sensor devices are targeting a wide range of applications, yet have very limited processing, storage, and energy resources. An architecture must be developed that can efficiently meet system demands while simultaneously remaining flexible to application specific optimizations. Analysis of past designs identifies core architectural issues and their impact on system performance. This paper outlines these issues and presents a generalized architecture designed to provide efficient communication mechanisms while allowing for system-level optimizations. The importance of providing a tight coupling between application and communication processing is shown and the tradeoffs associated with virtual versus physical parallelism are investigated. We conclude that a shared controller closely integrated with special-purpose hardware accelerators is the preferred building block for a flexible yet efficient node. A subset of the architecture is implemented using commercial building blocks and shows substantial improvements in communication performance and the ability to perform system-level optimizations that obtain tight synchronization and low power channel monitoring.
315|A Collaborative Approach to In-Place Sensor Calibration|Numerous factors contribute to errors in sensor measurements.
316|Design and Implementation of WIreless Sensor Networks for Habitat Monitoring|We provide an in-depth study of applying wireless sensor networks to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the design of the sensor network, and the capabilities for remote data access and management. We propose a system architecture that addresses these requirements for habitat monitoring in general. We present an in-depth discussion of the implementation of the architecture for habitat monitoring. In the summer of 2002, 32 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web using our implementation. Results from the deployment show the profound impact software and hardware power management has on node longevity. The effectiveness of the system architecture is shown through the packet throughput and through the delivery of over 1.2 million readings logged at our database in Berkeley. The system operated for over four months; it provided data for two months after researchers had left the island for the winter due to poor weather conditions. The application-driven design exercise serves to identify important areas of further work in power management, data sampling, communications, network retasking, and health monitoring. We discuss the lessons learned from our deployment and provide a series of solutions that include new hardware, software, and protective enclosures.
317|Preprocessing in a Tiered Sensor Network for Habitat Monitoring|We investigate task-decomposition and collaboration in a two-tiered sensor network for habitat monitoring. The system recognizes and localizes a specified type of birdcalls. The system has a few powerful macro nodes in the first tier, and many less-powerful micro nodes in the second tier. Each macro node combines data collected by multiple micro nodes for target classification and localization. We describe two types of lightweight preprocessing, which significantly reduce data transmission from micro nodes to macro nodes. Micro nodes classify events according to their cross-zero rates and discard irrelevant events. Data about events of interest are reduced and compressed before being transmitted to macro nodes for target localization. Preliminary experiments illustrate the effectiveness of event filtering and data reduction at micro nodes.
318|A Dual-Space Approach to Tracking and Sensor Management in Wireless Sensor Networks|Wireless ad hoc sensor networks have the advantage of spanning a large geographical region and being able to collaboratively detect and track non-local spatio-temporal events. This paper presents a dual-space approach to event tracking and sensor resource management in sensor networks. The dual-space transformation maps a non-local phenomenon, e.g., the edge of a half-plane shadow, to a single point in the dual space, and maps locations of distributed sensor nodes to a set of lines that partitions the dual space. The detection problem becomes finding and tracking the cell that contains the point in the arrangement defined by these lines. This mechanism can be effectively used for power management of the sensor network -- nodes that will not be immediately visited by an event can be turned off to save energy required for sensing, processing, and communication. The approach has been successfully demonstrated on a laboratory testbed built using the UC Berkeley motes sensors. An implemented application of detecting and tracking light shadow edges moving over a sensor field is described.
319|Target Classification and Localization in Habitat Monitoring|We are developing an acoustic habitat-monitoring sensor network that recognizes and locates specific animal calls in real time. In this paper, we investigate the system requirements of such a real-time acoustic monitoring network. We propose a system architecture and a set of lightweight collaborative signal processing algorithms that achieve real-time behavior while minimizing inter-node communication to extend the system lifetime. In particular, the target classification is based on spectrogram pattern matching while the target localization is based on beamforming using Time Difference Of Arrival (TDOA). We describe our preliminary implementation on a Commercial Off The Shelf (COTS) testbed and present its performance based on testbed measurements.
320|Calibration of NASA Turbulent Air Motion Measurement System|. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 1. Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 1 2. Background of Air Motion Measurement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3. Instrumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2 3.1. Nose Boom and Meteorological Instrumentation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 3.2. Inertial Navigation System. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 3.3. Aircraft Data Acquisition System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ...
321|Dynamic Bayesian Networks: Representation, Inference and Learning|Modelling sequential data is important in many areas of science and engineering. Hidden Markov models (HMMs) and Kalman filter models (KFMs) are popular for this because they are simple and flexible. For example, HMMs have been used for speech recognition and bio-sequence analysis, and KFMs have been used for problems ranging from tracking planes and missiles to predicting the economy. However, HMMs
and KFMs are limited in their “expressive power”. Dynamic Bayesian Networks (DBNs) generalize HMMs by allowing the state space to be represented in factored form, instead of as a single discrete random variable. DBNs generalize KFMs by allowing arbitrary probability distributions, not just (unimodal) linear-Gaussian. In this thesis, I will discuss how to represent many different kinds of models as DBNs, how to perform exact and approximate inference in DBNs, and how to learn DBN models from sequential data.
In particular, the main novel technical contributions of this thesis are as follows: a way of representing
Hierarchical HMMs as DBNs, which enables inference to be done in O(T) time instead of O(T 3), where T is the length of the sequence; an exact smoothing algorithm that takes O(log T) space instead of O(T); a simple way of using the junction tree algorithm for online inference in DBNs; new complexity bounds on exact online inference in DBNs; a new deterministic approximate inference algorithm called factored frontier; an analysis of the relationship between the BK algorithm and loopy belief propagation; a way of
applying Rao-Blackwellised particle filtering to DBNs in general, and the SLAM (simultaneous localization
and mapping) problem in particular; a way of extending the structural EM algorithm to DBNs; and a variety of different applications of DBNs. However, perhaps the main value of the thesis is its catholic presentation of the field of sequential data modelling.
322|A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking|Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or “particle”) representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example. 
325|An Empirical Study of Smoothing Techniques for Language Modeling|We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1
327|Kernel independent component analysis|We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms. 1.
328|Complexity of finding embeddings in a k-tree|  A k-tree is a graph that can be reduced to the k-complete graph by a sequence of removals of a degree k vertex with completely connected neighbors. We address the problem of determining whether a graph is a partial graph of a k-tree. This problem is motivated by the existence of polynomial time algorithms for many combinatorial problems on graphs when the graph is constrained to be a partial k-tree for fixed k. These algorithms have practical applications in areas such as reliability, concurrent broadcasting and evaluation of queries in a relational database system. We determine the complexity status of two problems related to finding the smallest number k such that a given graph is a partial k-tree. First, the corresponding decision problem is NP-complete. Second, for a fixed (predetermined) value of k, we present an algorithm with polynomially bounded (but exponential in k) worst case time complexity. Previously, this problem had only been solved for k = 1,2,3.
329|The generalized distributive law |Abstract—In this semitutorial paper we discuss a general message passing algorithm, which we call the generalized dis-tributive law (GDL). The GDL is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. It includes as special cases the Baum–Welch algorithm, the fast Fourier transform (FFT) on any finite Abelian group, the Gal-lager–Tanner–Wiberg decoding algorithm, Viterbi’s algorithm, the BCJR algorithm, Pearl’s “belief propagation ” algorithm, the Shafer–Shenoy probability propagation algorithm, and the turbo decoding algorithm. Although this algorithm is guaranteed to give exact answers only in certain cases (the “junction tree ” condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to. Index Terms—Belief propagation, distributive law, graphical models, junction trees, turbo codes. I.
330|Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks |Particle filters (PFs) are powerful sampling-based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probability distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as “condensation”, “sequential Monte Carlo” and “survival of the fittest”. In this paper, we show how we can exploit the structure of the DBN to increase the efficiency of particle filtering, using a technique known as Rao-Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite dimensional optimal filter. We show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial basis function networks and robot localization and map building. We also discuss other potential application areas and provide references to some Þnite dimensional optimal filters.  
331|Algebraic Decision Diagrams and their Applications|In this paper we present theory and experiments on the Algebraic Decision Diagrams (ADD&#039;s). These diagrams extend BDD&#039;s by allowing values from an arbitrary finite domain to be associated with the terminal nodes. We present a treatment founded in boolean algebras and discuss algorithms and results in applications like matrix multiplication and shortest path algorithms. Furthermore, we outline possible applications of ADD&#039;s to logic synthesis, formal verification, and testing of digital systems.
332|Bucket Elimination: A Unifying Framework for Probabilistic Inference| Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem&#039;s structure.  
333|Tractable inference for complex stochastic processes|The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief state—a probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy. 1
334|Operations for Learning with Graphical Models|This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper conclu...
335|A Variational Bayesian Framework for Graphical Models|This paper presents a novel practical framework for Bayesian model  averaging and model selection in probabilistic graphical models.  Our approach approximates full posterior distributions over model  parameters and structures, as well as latent variables, in an analytical  manner. These posteriors fall out of a free-form optimization  procedure, which naturally incorporates conjugate priors. Unlike  in large sample approximations, the posteriors are generally nonGaussian  and no Hessian needs to be computed. Predictive quantities  are obtained analytically. The resulting algorithm generalizes  the standard Expectation Maximization algorithm, and its convergence  is guaranteed. We demonstrate that this approach can be  applied to a large class of models in several domains, including  mixture models and source separation.  1 Introduction  A standard method to learn a graphical model  1  from data is maximum likelihood (ML). Given a training dataset, ML estimates a single optimal value f...
336|Mixture Kalman filters|In treating dynamic systems,sequential Monte Carlo methods use discrete samples to represent a complicated probability distribution and use rejection sampling, importance sampling and weighted resampling to complete the on-line `filtering&#039; task. We propose a special sequential Monte Carlo method,the mixture Kalman filter, which uses a random mixture of the Gaussian distributions to approximate a target distribution. It is designed for on-line estimation and prediction of conditional and partial conditional dynamic linear models,which are themselves a class of widely used non-linear systems and also serve to approximate many others. Compared with a few available filtering methods including Monte Carlo methods,the gain in efficiency that is provided by the mixture Kalman filter can be very substantial. Another contribution of the paper is the formulation of many non-linear systems into conditional or partial conditional linear form,to which the mixture Kalman filter can be applied. Examples in target tracking and digital communications are given to demonstrate the procedures proposed.
337|Semiring-Based Constraint Satisfaction and Optimization|We introduce a general framework for constraint satisfaction and optimization where classical CSPs, fuzzy CSPs, weighted CSPs, partial constraint satisfaction, and others can be easily cast. The framework is based on a semiring structure, where the set of the semiring specifies the values to be associated with each tuple of values of the variable domain, and the two semiring operations (1 and 3) model constraint projection and combination respectively. Local consistency algorithms, as usually used for classical CSPs, can be exploited in this general framework as well, provided that certain conditions on the semiring operations are satisfied. We then show how this framework can be used to model both old and new constraint solving and optimization schemes, thus allowing one to both formally justify many informally taken choices in existing schemes, and to prove that local consistency techniques can be used also in newly defined schemes.
338|Stochastic Dynamic Programming with Factored Representations|Markov decision processes(MDPs) have proven to be popular models for decision-theoretic planning, but standard dynamic programming algorithms for solving MDPs rely on explicit, state-based specifications and computations. To alleviate the combinatorial problems associated with such methods, we propose new representational and computational techniques for MDPs that exploit certain types of problem structure. We use dynamic Bayesian networks (with decision trees representing the local families of conditional probability distributions) to represent stochastic actions in an MDP, together with a decision-tree representation of rewards. Based on this representation, we develop versions of standard dynamic programming algorithms that directly manipulate decision-tree representations of policies and value functions. This generally obviates the need for state-by-state computation, aggregating states at the leaves of these trees and requiring computations only for each aggregate state. The key to these algorithms is a decision-theoretic generalization of classic regression analysis, in which we determine the features relevant to predicting expected value. We demonstrate the method empirically on several planning problems,
339|Learning Equivalence Classes Of Bayesian Network Structures|Approaches to learning Bayesian networks  from data typically combine a scoring metric  with a heuristic search procedure. Given  aBayesian network structure, many of the  scoring metrics derived in the literature return  a score for the entire equivalence class  to which the structure belongs. When using  such a metric, it is appropriate for the heuristic  search algorithm to searchover equivalence  classes of Bayesian networks as opposed  to individual structures. We present the general  formulation of a search space for which  the states of the search correspond to equivalence  classes of structures. Using this space,  anyoneofanumber of heuristic searchalgorithms  can easily be applied. We compare  greedy search performance in the proposed  search space to greedy search performance in  a search space for which the states correspond  to individual Bayesian network structures.  1 
340|Policy Recognition in the Abstract Hidden Markov Model|In this paper, we present a method for recognising an agent&#039;s behaviour in dynamic, noisy, uncertain domains, and across multiple levels of abstraction. We term this problem on-line plan recognition under uncertainty and view it generally as probabilistic inference on the stochastic process representing the execution of the agent&#039;s plan. Our contributions in this paper are twofold. In terms of probabilistic inference, we introduce the Abstract Hidden Markov Model (AHMM), a novel type of stochastic processes, provide its dynamic Bayesian network (DBN) structure and analyse the properties of this network. We then describe an application of the Rao-Blackwellised Particle Filter to the AHMM which allows us to construct an ecient, hybrid inference method for this model. In terms of plan recognition, we propose a novel plan recognition framework based on the AHMM as the plan execution model. The Rao-Blackwellised hybrid inference for AHMM can take advantage of the independence properties inherent in a model of plan execution, leading to an algorithm for online probabilistic plan recognition that scales well with the number of levels in the plan hierarchy. This illustrates that while stochastic models for plan execution can be complex, they exhibit special structures which, if exploited, can lead to efficient plan recognition algorithms. We demonstrate the usefulness of the AHMM framework via a behaviour recognition system in a complex spatial environment using distributed video surveillance data.
341|Symbolic Dynamic Programming for First-order MDPs|We present a dynamic programming approach for the solution of first-order Markov decisions processes. This technique uses an MDP whose dynamics is represented in a variant of the situation calculus allowing for stochastic actions. It produces a logical description of the optimal value function and policy by constructing a set of first-order formulae that minimally partition state space according to distinctions made by the value function and policy. This is achieved through the use of an operation known as decision-theoretic regression. In effect, our algorithm performs value iteration without explicit enumeration of either the state or action spaces of the MDP. This allows problems involving relational fluents and quantification to be solved without requiring explicit state space enumeration or conversion to propositional form. 1
342|Learning Bayesian Networks from Data: An Information-Theory Based Approach|This paper provides algorithms that use an information-theoretic analysis to learn Bayesian network structures from data. Based on our three-phase learning framework, we develop efficient algorithms that can effectively learn Bayesian networks, requiring only polynomial numbers of conditional independence (CI) tests in typical cases. We provide precise conditions that specify when these algorithms are guaranteed to be correct as well as empirical evidence (from real world applications and simulation tests) that demonstrates that these systems work efficiently and reliably in practice.
343|The graphical models toolkit: An open source software system for speech and time-series processing|This paper describes the Graphical Models Toolkit (GMTK), an open source, publically available toolkit for developing graphical-model based speech recognition and general time series systems. Graphical models are a flexible, concise, and expressive probabilistic modeling framework with which one may rapidly specify a vast collection of statistical models. This paper begins with a brief description of the representational and computational aspects of the framework. Following that is a detailed description of GMTK’s features, including a language for specifying structures and probability distributions, logarithmic space exact training and decoding procedures, the concept of switching parents, and a generalized EM training method which allows arbitrary sub-Gaussian parameter tying. Taken together, these features endow GMTK with a degree of expressiveness and functionality that significantly complements other publically available packages. GMTK was recently used in the 2001 Johns Hopkins Summer Workshop, and experimental results are described in detail both herein and in a companion paper. 1.
344|Markovian Models for Sequential Data|Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many machine learning applications, especially for speech recognition. Furthermore, in the last few years, many new and promising probabilistic models related to HMMs have been proposed. We first summarize the basics of HMMs, and then review several recent related learning algorithms and extensions of HMMs, including in particular hybrids of HMMs with artificial neural networks, Input-Output HMMs (which are conditional HMMs using neural networks to compute probabilities), weighted transducers, variable-length Markov models and Markov switching state-space models. Finally, we discuss some of the challenges of future research in this very active area. 1 Introduction  Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many applications in artificial intelligence, pattern recognition, speech recognition, and modeling of biological ...
345|Input/output hmms for sequence processing|We consider problems of sequence processing and propose a solution based on a discrete state model in order to represent past context. Weintroduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state. The model has a statistical interpretation we call Input/Output Hidden Markov Model (IOHMM). It can be trained by the EM or GEM algorithms, considering state trajectories as missing data, which decouples temporal credit assignment and actual parameter estimation. The model presents similarities to hidden Markov models (HMMs), but allows us to map input se-quences to output sequences, using the same processing style as recurrent neural networks. IOHMMs are trained using a more discriminant learning paradigm than HMMs, while potentially taking advantage of the EM algorithm. We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem. Experimental results are presented for the seven Tomita grammars, showing that these adaptive models can attain excellent generalization.
346|AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential Reasoning in Large Bayesian Networks|Stochastic sampling algorithms, while an attractive alternative to exact algorithms in  very large Bayesian network models, have been observed to perform poorly in evidential  reasoning with extremely unlikely evidence. To address this problem, we propose an adaptive  importance sampling algorithm, AIS-BN, that shows promising convergence rates  even under extreme conditions and seems to outperform the existing sampling algorithms  consistently. Three sources of this performance improvement are (1) two heuristics for  initialization of the importance function that are based on the theoretical properties of importance  sampling in nite-dimensional integrals and the structural advantages of Bayesian  networks, (2) a smooth learning method for the importance function, and (3) a dynamic  weighting function for combining samples from dierent stages of the algorithm.  We tested the performance of the AIS-BN algorithm along with two state of the art  general purpose sampling algorithms, lik...
347|Using Dirichlet Mixture Priors to Derive Hidden Markov Models for Protein Families|A Bayesian method for estimating the amino acid  distributions in the states of a hidden Markov  model (HMM) for a protein family or the columns  of a multiple alignment of that family is introduced.  This method uses Dirichlet mixture densities  as priors over amino acid distributions. These  mixture densities are determined from examination  of previously constructed HMMs or multiple  alignments. It is shown that this Bayesian method  can improve the quality of HMMs produced from  small training sets. Specific experiments on the  EF-hand motif are reported, for which these priors  are shown to produce HMMs with higher likelihood  on unseen data, and fewer false positives  and false negatives in a database search task.  
348|A new approach to analyzing gene expression time series data|1 Introduction Principled methods for estimating unobserved time-points,clustering, and aligning microarray gene expression timeseries are needed to make such data useful for detailed anal-ysis. Datasets measuring temporal behavior of thousands of genes offer rich opportunities for computational biologists. For example, Dynamic Bayesian Networks may be usedto build models and try to understand how genetic responses unfold. However, such modeling frameworks need a suf-ficient quantity of data in the appropriate format. Current gene expression time-series data often do not meet these re-quirements, since they may be missing data points, sampled non-uniformly, and measure biological processes that exhibittemporal variation.
349|Coupled hidden Markov models for modeling interacting processes|We present methods for coupling hidden Markov models (hmms) to model systems of multiple interacting processes. The resulting models have multiple state variables that are temporally coupled via matrices of conditional probabilities. We introduce a deterministic O(T (CN)  2  ) approximation for maximum a posterior (MAP) state estimation which enables fast classification and parameter estimation via expectation maximization. An &#034;N-heads&#034; dynamic programming algorithm samples from the highest probability paths through a compact state trellis, minimizing an upper bound on the cross entropy with the full (combinatoric) dynamic programming problem. The complexity is O(T (CN)  2  ) for C  chains of N states apiece observing T data points, compared with O(TN  2C  ) for naive (Cartesian product), exact (state clustering), and stochastic (Monte Carlo) methods applied to the same inference problem. In several experiments examining training time, model likelihoods, classification accuracy, and ro...
350|Structure Learning in Conditional Probability Models via an Entropic Prior and Parameter Extinction|We introduce an entropic prior for multinomial parameter estimation problems and solve for its maximum...
351|Markov Chain Monte Carlo in Conditionally Gaussian State Space Models|Introduction Linear Gaussian state space models are used extensively, with unknown parameters usually estimated by maximum likelihood: Wecker &amp; Ansley (1983), Harvey (1989). However, many time series and nonparametric regression applications, such as change point problems, outlier detection and switching regression, require the full generality of the conditionally Gaussian model: Harrison &amp; Stevens (1976), Shumway &amp; Stoffer (1991), West &amp; Harrison (1989), Gordon &amp; Smith (1990). The presence of a large number of indicator variables makes it difficult to estimate conditionally Gaussian models using maximum likelihood, and a Bayesian approach using Markov chain Monte Carlo appears more tractable. We propose a new sampler, which is used to estimate an unknown function nonparametrically when there are jumps in the function and outliers in the observations; it is also applied to a time series change point problem previously discussed by Gordon &amp; Smith (1990). For the first example th
352|Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Log-linear Models|this paper, we will only consider undirected graphical models. For details of Bayesian model selection for directed graphical models see Madigan et al (1995). An (undirected) graphical model is determined by a set of conditional independence constraints of the form `fl 1 is independent of fl 2 conditional on all other fl i 2 C&#039;. Graphical models are so called because they can each be represented as a graph with vertex set C and an edge between each pair fl 1 and fl 2 unless fl 1 and fl 2 are conditionally independent as described above. Darroch, Lauritzen and Speed (1980) show that each graphical log-linear model is hierarchical, with generators given by the cliques (complete subgraphs) of the graph. The total number of possible graphical models is clearly given by 2 (
353|A Sufficiently Fast Algorithm for Finding Close to Optimal Junction Trees|An algorithm is developed for finding a close  to optimal junction tree of a given graph G.
354|Graphical models and automatic speech recognition|Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition. This paper first provides a brief overview of graphical models and their uses as statistical models. It is then shown that the statistical assumptions behind many pattern recognition techniques commonly used as part of a speech recognition system can be described by a graph – this includes Gaussian distributions, mixture models, decision trees, factor analysis, principle component analysis, linear discriminant analysis, and hidden Markov models. Moreover, this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph, including many at the acoustic-, pronunciation-, and language-modeling levels. A number of speech recognition techniques born directly out of the graphical-models paradigm are also surveyed. Additionally, this paper includes a novel graphical analysis regarding why derivative (or delta) features improve hidden Markov model-based speech recognition by improving structural discriminability. It also includes an example where a graph can be used to represent language model smoothing constraints. As will be seen, the space of models describable by a graph is quite large. A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model.
355|Dynamic Bayesian Multinets|In this work, dynamic Bayesian multinets are  introduced where a Markov chain state at time  t determines conditional independence patterns  between random variables lying within a local  time window surrounding t. It is shown how  information-theoretic criterion functions can be  used to induce sparse, discriminative, and classconditional  network structures that yield an optimal  approximation to the class posterior probability,  and therefore are useful for the classification  task. Using a new structure learning  heuristic, the resulting models are tested on a  medium-vocabulary isolated-word speech recognition  task. It is demonstrated that these discriminatively  structured dynamic Bayesian multinets,  when trained in a maximum likelihood setting using  EM, can outperform both HMMs and other  dynamic Bayesian networks with a similar number  of parameters.  1 Introduction  While Markov chains are sometimes a useful model for sequences, such simple independence assumptions can lead...
356|Cascaded Markov Models|This paper presents a new approach to  partial parsing of context-free structures. The approach is based
357|Natural Statistical Models for Automatic Speech Recognition|  The performance of state-of-the-art speech recognition systems is still far worse than that of humans. This is partly caused by the use of poor statistical models. In a general statistical pattern classification task, the probabilistic models should represent the statistical structure unique to and distinguishing those objects to be classified. In many cases, however, model families are selected without verification of their ability to represent vital discriminative properties. For example, Hidden Markov Models (HMMs) are frequently used in automatic speech recognition systems even though they possess conditional independence properties that might cause inaccuracies when modeling and classifying speech signals. In this work, a new method for automatic speech recognition is developed where the natural statistical properties of speech are used to determine the probabilistic model. Starting from an HMM, new models are created by adding dependencies only if they are not already well captured by the HMM, and only if they increase the
358|Compositionality, MDL Priors, and Object Recognition|Images are ambiguous at each of many levels of a contextual hierarchy. Nevertheless, the high-level interpretation of most scenes is unambiguous, as evidenced by the superior performance of humans. This observation argues for global vision models, such as deformable templates. Unfortunately, such models are computationally intractable for unconstrained problems. We propose a compositional model in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings. Ambiguity is propagated up the hierarchy in the form of multiple interpretations, which are later resolved by a Bayesian, equivalently minimum-description-length, cost functional. 1 Bayesian decision theory and compositionality  In his Essay on Probability, Laplace (1812) devotes a short chapter---his &#034;Sixth Principle&#034;---to what we call today the Bayesian decision rule. Laplace observes that we interpret a &#034;regular combination,&#034; e.g., an arrangement of objects th...
359|Learning Motion Patterns of Persons for Mobile Service Robots|We propose a method for learning models of people&#039;s motion behaviors in an indoor environment. As people move through their environments, they do not move randomly. Instead, they often engage in typical motion patterns, related to specific locations that they might be interested in approaching and specific trajectories that they might follow in doing so. Knowledge about such patterns may enable a mobile robot to develop improved people following and obstacle avoidance skills. This paper proposes an algorithm that learns collections of typical trajectories that characterize a person&#039;s motion patterns. Data, recorded by mobile robots equipped with laser range finders, is clustered into different types of motion using the popular expectation maximization algorithm, while simultaneously learning multiple motion patterns. Experimental results, obtained using data collected in a domestic residence and in an office building, illustrate that highly predictive models of human motion patterns can be learned.
360|A simple constraint-based algorithm for efficiently mining observational databases for causal relationships|Abstract. This paper presents a simple, efficient computer-based method for discovering causal relationships from databases that contain observational data. Observational data is passively observed, as contrasted with experimental data. Most of the databases available for data mining are observational. There is great potential for mining such databases to discover causal relationships. We illustrate how observational data can constrain the causal relationships among measured variables, sometimes to the point that we can conclude that one variable is causing another variable. The presentation here is based on a constraint-based approach to causal discovery. A primary purpose of this paper is to present the constraint-based causal discovery method in the simplest possible fashion in order to (1) readily convey the basic ideas that underlie more complex constraint-based causal discovery techniques, and (2) permit interested readers to rapidly program and apply the method to their own databases, as a start toward using more elaborate causal discovery algorithms.
361|Tracking and Surveillance in Wide-Area Spatial Environments Using the Abstract Hidden Markov Model|In this paper, we consider the problem of tracking an object and predicting the object future trajectory in a wide-area environment, with complex spatial layout and the use of multiple sensors/cameras. To solve this problem, there is a need for representing the dynamic and noisy data in the tracking tasks, and dealing with them at different levels of detail. We employ the Abstract Hidden Markov Models (AHMM), an extension of the well-known Hidden Markov Model (HMM) and a special type of Dynamic Probabilistic Network (DPN), as our underlying representation framework. The AHMM allows us to explicitly encode the hierarchy of connected spatial locations, making it scalable to the size of the environment being modelled. We describe an application for tracking human movement in a...
362|Approximate Learning of Dynamic Models|Inference is a key component in learning probabilistic models from partially  observable data. When learning temporal models, each of the many  inference phases requires a complete traversal over a potentially very  long sequence; furthermore, the data structures propagated in this procedure  can be extremely large, making the whole process very demanding.  In [2], we describe an approximate inference algorithm for monitoring  stochastic processes, and prove bounds on its approximation error. In this  paper, we apply this algorithm as an approximate forward propagation  step in an EM algorithm for learning temporal Bayesian networks. We  also provide a related approximation for the backward step, and prove  error bounds for the combined algorithm. We show that EM using our  inference algorithm is much faster than EM using exact inference, with  no degradation of the quality of the learned model. We then extend our  analysis to the online learning task, showing a boundon the error resul...
363|Discovering the Hidden Structure of Complex Dynamic Systems|Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no expert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial computational aspects of learning the structure of dynamic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the  Structural Expectation Maximization (SEM) algorithm. The main computational cost of the SEM algorithm is the gathering of expected sufficient statistics. We propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden var...
364|Iterative Algorithms for State Estimation of Jump Markov Linear Systems|Jump Markov linear systems (JMLSs) are linear systems whose parameters evolve with time according to a finite state Markov chain. Given a set of observations, our aim is to estimate the states of the finite state Markov chain and the continuous (in space) states of the linear system.
365|Exploiting the architecture of dynamic systems|Consider the problem of monitoring the state of a complex dynamic system, and predicting its future evolution. Exact algorithms for this task typically maintain a belief state, or distribution over the states at some point in time. Unfortunately, these algorithms fail when applied to complex processes such as those represented as dynamic Bayesian networks (DBNs), as the representation of the belief state grows exponentially with the size of the process. In (Boyen &amp; Koller 1998), we recently proposed an efficient approximate tracking algorithm that maintains an approximate belief state that has a compact representation as a set of independent factors. Its performance depends on the error introduced by approximating a belief state of this process by a factored one. We informally argued that this error is low if the interaction between variables in the processes is “weak”. In this paper, we give formal information-theoretic definitions for notions such as weak interaction and sparse interaction of processes. We use these notions to analyze the conditions under which the error induced by this type of approximation is small. We demonstrate several cases where our results formally support intuitions about strength of interaction.
366|Top-down Construction and Repetitive Structures Representation in Bayesian Networks|Bayesian networks for large and complex domains are  dicult to construct and maintain. For example modifying  a small network fragment in a repetitive structure  might be very time consuming. Top-down modelling  may simplify the construction of large Bayesian  networks, but methods (partly) supporting top-down  modelling have only recently been introduced and tools  do not exist. In this paper, we try to take a topdown  approach to constructing Bayesian networks by  using existing object oriented methods. We change  these where they fail to support top-down modeling.  This provides a new framework that allows topdown  methodologies for the construction of Bayesian  networks, provides an ecient class hierarchy and a  compact way of specifying and representing temporal  Bayesian networks. Furthermore, a conceptual simpli-  cation is achieved.  Introduction  Constructing and maintaining Bayesian networks (BNs) can be a time consuming process. Using current methods and tools, a top-down a...
367|An EM Algorithm for Asynchronous Input/Output Hidden Markov Models|In learning tasks in which input sequences are mapped to output sequences, it is often the case that the input and output sequences are not synchronous. For example, in speech recognition, acoustic sequences are longer than phoneme sequences. Input/Output Hidden Markov Models have already been proposed to represent the distribution of an output sequence given an input sequence of the same length. We extend here this model to the case of asynchronous sequences, and show an Expectation-Maximization algorithm for training such models.
368|Diffusion of context and credit information in Markovian models|This paper studies the problem of ergodicity of transition probabilitymatricesinMarkovian models, such as hidden Markov models (HMMs), and how itmakes very di cult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, weshow that this problem of di usion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approachesbasedon continuous optimization, such asgradient descent and the Baum-Welch algorithm. 1.
369|Data-Driven Extensions To Hmm Statistical Dependencies|... HMM conditional independence assumption in a principled way. Without increasing the number of states, the modeling power of an HMM is increased by including only those additional probabilistic dependencies (to the surrounding observation context) that are believed to be both relevant and discriminative. Conditional mutual information is used to determine both relevance and discriminability. Extended Gaussian-mixture HMMs and new EM update equations are introduced. In an isolated word speech database, results show an average 34% word error improvement over an HMM with the same number of states, and a 15% improvement over an HMM with a comparable number of parameters.
370|Sequential Bayesian Estimation And Model Selection For Dynamic Kernel Machines|In this paper, we address the complex problem of sequential Bayesian estimation and model selection/averaging. This problem does not usually admit any type of closed-form analytical solutions and, as a result, one has to resort to numerical methods. We propose here an original and powerful sequential simulation-based strategy to perform the necessary computations. This strategy is based on Monte Carlo particle methods and model selection/averaging using predictive distributions. It combines sequential importance sampling, Rao-Blackwellisation, a selection procedure and reversible jump MCMC moves. We demonstrate the eectiveness of the method by performing inference and learning on a hybrid model consisting of a dynamic linear model and a dynamic mixture of kernel basis functions. 
371|Causality and graphical models in time series|analysis
372|Structural Learning in Object Oriented Domains|When constructing a Bayesian network, it can be advantageous to employ structural learning algorithms to combine knowledge captured in databases with prior information provided by domain experts. Unfortunately, conventional algorithms do not exploit the occurrence of repetitive structures, which are often found in object oriented domains such as fault prediction in computer networks and large pedigrees.
373|Efficient Inference for Mixed Bayesian Networks|Bayesian network is a compact representation for probabilistic models and inference. They have been used successfully for multisensor fusion and situation assessment. It is well known that, in general, the inference algorithms to compute the exact posterior probability of the target state are either computationally infeasible for dense networks or impossible for mixed discretecontinuous networks. In those cases, one approach is to compute the approximate results using simulation methods. This paper proposes efficient inference methods for those cases. The goal is not to compute the exact or approximate posterior probability of the target state, but to identify the top (most likely) ones in an efficient manner. The approach is to use intelligent simulation techniques where previous samples will be used to guide the future sampling strategy. By focusing the sampling on the &#034;important&#034; space, we are able to sort out the top candidates quickly. Simulation results are included to demonstrate the performances of the algorithms.
374|Structured arc reversal and simulation of dynamic probabilistic networks|We present an algorithm for arc reversal in Bayesian networks with tree-structured conditional probability tables, and consider some of its advantages, especially for the simulation of dynamic probabilistic networks. In particular, the method allows one to produce CPTs for nodes involved in the reversal that exploit regularities in the conditional distributions. We argue that this approach alleviates some of the overhead associated with arc reversal, plays an important role in evidence integration and can be used to restrict sampling of variables in DPNs. We also provide an algorithm that detects the dynamic irrelevance of state variables in forward simulation. This algorithm exploits the structured CPTs in a reversed network to determine, in a timeindependent fashion, the conditions under which a variable does or does not need to be sampled. 1
375|Application of Bayesian controllers to dynamic systems|Bayesian networks for the static as well as for the dynamic case have gained an enormous interest in the research community of machine learning and pattern recognition. Although the parallels between dynamic Bayesian networks and Kalman tikers are well-known since many years, Bayesian networks have not been applied to problems in the area of adaptive control of dynamic systems.
376|Querying Semi-Structured Data|

377|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
378|DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases|In semistructured databases there is no schema fixed  in advance. To provide the benefits of a schema in  such environments, we introduce DataGuides:  concise and accurate structural summaries of  semistructured databases. DataGuides serve as  dynamic schemas, generated from the database; they  are useful for browsing database structure,  formulating queries, storing information such as  statistics and sample values, and enabling query  optimization. This paper presents the theoretical  foundations of DataGuides along with an algorithm  for their creation and an overview of incremental  maintenance. We provide performance results based  on our implementation of DataGuides in the Lore  DBMS for semistructured data. We also describe the  use of DataGuides in Lore, both in the user interface  to enable structure browsing and query formulation,  and as a means of guiding the query processor and  optimizing query execution.
379|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
380|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
381|A Query Language and Optimization Techniques for Unstructured Data|A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility in data representation  What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or &#034;vertical&#034; dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the &#034;horizontal&#034; dimension of UnQL.  
382|Lore: A database management system for semistructured data|Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources. 
383|Maintenance of Materialized Views: Problems, Techniques, and Applications|In this paper we motivate and describe materialized views, their applications, and the problems  and techniques for their maintenance. We present a taxonomy of view maintenanceproblems  basedupon the class of views considered, upon the resources used to maintain the view, upon the types of modi#cations to the base data that areconsidered during maintenance, and whether the technique works for all instances of databases and modi#cations. We describe some of the view maintenancetechniques proposed in the literature in terms of our taxonomy. Finally, we consider new and promising application domains that are likely to drive work in materialized  views and view maintenance.  1 Introduction  What is a view? A view is a derived relation de#ned in terms of base #stored# relations. A view thus de#nes a function from a set of base tables to a derived table; this function is typically recomputed every time the view is referenced.  What is a materialized view? A view can be materialized by storin...
384|Objects and Views|Object-oriented databases have been introduced primarily to ease the development of database applications. However, the difficulties encountered when, for instance, trying to restructure data or integrate databases demonstrate that the models being used still lack flexibility. We claim that the natural way to overcome these shortcomings is to introduce a sophisticated view mechanism. This paper presents such a mechanism, one which allows a programmer to restructure the class hierarchy and modify the behavior and structure of objects. The mechanism allows a programmer to specify attribute values implicitly, rather than storing them. It also allows him to introduce new classes into the class hierarchy. These virtual classes are populated by selecting existing objects from other classes and by creating new objects. Fixing the identify of new objects during database updates introduces subtle issues into view design. Our presentation, mostly informal, leans on a number of illustrative examples meant to emphasize the simplicity of our mechanism. 1
385|Object fusion in mediator systems|One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are &amp;quot;imported &amp;quot; into the mediator.
386|Regular Path Queries with Constraints|The evaluation of path expression queries on semistructured data in a distributed asynchronous environment is considered. The focus is on the use of local information expressed in the form of path constraints in the optimization of path expression queries. In particular, decidability and complexity results on the implication problem for path constraints are established.
387|Supporting Views in Object-Oriented Databases|  Relational database systems provide a powerful abstraction mechanism: any query, since it returns a relation, can be used to define a view, that becomes a derived (or virtual) relation. Views are defined by a statement such as &#034;define view &lt;name&gt; as &lt;query&gt;.&#034; Views can be used to tailor the global database schema.  . Query formulation is simplified, if frequent subexpressions are predefined.  . Application programs are insulated from changes to the underlying schema.  . Information can be restructured to better suit an application programs&#039; requirements.  . Derived information is kept consistent with base data.  . Access restrictions (for authorization) can be enforced by hiding data.  In contrast to base relations, views are typically not stored permanently, but rather computed on demand. Queries to view relations are modified by query substitution so as to operate on the underlying b
388|MultiView: A Methodology for Supporting Multiple Views in Object-Oriented Databases|A view in object-oriented databases (OODB) corresponds to virtual schema graph with possibly restructured generalization and decomposition hierarchies. We propose a methodology, called MultiView, for supporting multiple such view schemata. MultiView represents a simple yet powerful approach achieved by breaking view specification into independent tasks: class derivation, global schema integration, view class selection, and view schema generation. Novel features of MultiView include an object algebra for class customization; an algorithm for the integration of virtual classes into the global schema; a view definition language for view class selection, and the automatic generation of a view class hierarchy. In addition, we present algorithms that verify the closure property of a view and, if found to be incomplete, transform it into a closed, yet minimal, view. Lastly, we introduce the fundamental concept of view independence and show  MultiView to be view independent.  
389|The GMAP: a versatile tool for physical data independence|.&lt;F3.733e+05&gt; Physical data independence is touted as a central feature of modern database systems. It allows users to frame queries in terms of the logical structure of the data, letting a query processor automatically translate them into optimal plans that access physical storage structures. Both relational and object-oriented systems, however, force users to frame their queries in terms of a logical schema that is directly tied to physical structures. We present an approach that eliminates this dependence. All storage structures are defined in a declarative language based on relational algebra as functions of a logical schema. We present an algorithm, integrated with a conventional query optimizer, that translates queries over this logical schema into plans that access the storage structures. We also show how to compile update requests into plans that update all relevant storage structures consistently and optimally. Finally, we report on experiments with a prototype implementation ...
390|Virtual Schemas and Bases|We propose the notions of virtual schemas and virtual bases as a coherent way of integrating various features in OODB views. A virtual schema is defined based on some existing (real) schema. A virtual base is obtained when a (real) base is attached to a virtual schema. We study the consequences of this simple assumption. In particular, we observe the differences between a real schema and a virtual one. We also consider an extension (that we call generic schemas) where it is necessary to specify several real bases to attach data to a virtual schema. We show how the flexibility provided by virtual schemas can be used to cope with various dynamic features of database systems. 1 Introduction Views are intended to increase the flexibility of database systems and their definition in the object-oriented database (OODB) context comes as a natural extension of the original paradigm. The yet relatively young research on this topic has introduced a large variety of indispensable new features. H...
391|Privacy Preserving Data Mining|In this paper we address the issue of privacy preserving data mining. Specifically, we consider a  scenario in which two parties owning confidential databases wish to run a data mining algorithm on  the union of their databases, without revealing any unnecessary information. Our work is motivated  by the need to both protect privileged information and enable its use for research or other purposes. The
392|Public-key cryptosystems based on composite degree residuosity classes| This paper investigates a novel computational problem, namely the Composite Residuosity Class Problem, and its applications to public-key cryptography. We propose a new trapdoor mechanism and derive from this technique three encryption schemes: a trapdoor permutation and two homomorphic probabilistic encryption schemes computationally comparable to RSA. Our cryptosystems, based on usual modular arithmetics, are provably secure under appropriate assumptions in the standard model.  
393|A randomized protocol for signing contracts|Two parties, A and B, want to sign a contract C over a communication network. To do so, they must “simultaneously” exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece by piece manner. During such a protocol, one party or another may have a slight advantage; a “fair” protocol keeps this advantage within acceptable limits. We present a new protocol that is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers, and is proved under very weak cryptographic assumptions. Our protocol has the following additional properties: 4 during the procedure the parties exchange probadilistic options for committing both parties to the contract; the protocol never terminates in an asymmetric situation where party A knows that party B is committed to the contract while he is not; the protocol makes use of a weak form of a third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation. No bookkeeping is required of the judge. 
394|Multiparty unconditionally secure protocols|Under the assumption that each pair of participants em communieatc secretly, we show that any reasonable multiparty protwol can be achieved if at least Q of the Participants am honest. The secrecy achieved is unconditional, It does not rely on any assumption about computational intractability. 1.
395|Security and Composition of Multi-party Cryptographic Protocols|We present general definitions of security for multi-party cryptographic protocols, with focus  on the task of evaluating a probabilistic function of the parties&#039; inputs. We show that, with  respect to these definitions, security is preserved under a natural composition operation.  The definitions follow the general paradigm of known definitions; yet some substantial modifications  and simplifications are introduced. The composition operation is the natural `subroutine  substitution&#039; operation, formalized by Micali and Rogaway.  We consider several standard settings for multi-party protocols, including the cases of eavesdropping,  Byzantine, non-adaptive and adaptive adversaries, as well as the information-theoretic  and the computational models. In particular, in the computational model we provide the first  definition of security of protocols that is shown to be preserved under composition.  
396|Efficient generation of shared RSA keys|We describe efficient techniques for a number of parties to jointly generate an RSA key. At the end of the protocol an RSA modulus N = pq is publicly known. None of the parties know the factorization of N. In addition a public encryption exponent is publicly known and each party holds a share of the private exponent that enables threshold decryption. Our protocols are efficient in computation and communication. All results are presented in the honest but curious settings (passive adversary).
397|Secure Multi-Party Computation|Contents  1 Introduction and Preliminaries 4 1.1 A Tentative Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.1 Overview of the Definitions : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.2 Overview of the Known Results : : : : : : : : : : : : : : : : : : : : : : : : : : 5 1.1.3 Aims and nature of the current manuscript : : : : : : : : : : : : : : : : : : : 6 1.1.4 Organization of this manuscript : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2 Preliminaries (also tentative) : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.1 Computational complexity : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.2 Two-party and multi-party protocols : : : : : : : : : : : : : : : : : : : : : : : 10 1.2.3 Strong Proofs of Knowledge : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2 General Two-Party Computation 13 2.1.1 The semi-honest model : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 
398|Two Party RSA Key Generation|. We present a protocol for two parties to generate an RSA  key in a distributed manner. At the end of the protocol the public key: a  modulus N = PQ, and an encryption exponent e are known to both parties.  Individually, neither party obtains information about the decryption  key d and the prime factors of N : P and Q. However, d is shared among  the parties so that threshold decryption is possible.  1 Introduction  We show how two parties can jointly generate RSA public and private keys. Following the execution of our protocol each party learns the public key: N = PQ and e, but does not know the factorization of N or the decryption exponent d. The exponent d is shared among the two players in such a way that joint decryption of cipher-texts is possible.  Generation of RSA keys in a private, distributed manner figures prominently in several cryptographic protocols. An example is threshold cryptography, see [12] for a survey. In a threshold RSA signature scheme there are k parties who ...
399|Oblivious Polynomial Evaluation|Oblivious polynomial evaluation is a protocol involving two parties, a sender whose input is a polynomial P, and a receiver whose input is a value a. At the end of the protocol the receiver learns P (a) and the sender learns nothing. We describe efficient constructions for this protocol, which are based on new intractability assumptions that are closely related to noisy polynomial reconstruction. Oblivious polynomial evaluation can be used as a primitive in many applications. We describe several such applications, including protocols for private comparison of data, for mutually authenticated key exchange based on (possibly weak) passwords, and for anonymous coupons. 1
400|Adaptive Protocols for Information Dissemination in Wireless Sensor Networks|In this paper, we present a family of adaptive protocols, called SPIN (Sensor Protocols for Information via Negotiation) , that eciently disseminates information among sensors in an energy-constrained wireless sensor network. Nodes running a SPIN communication protocol name their data using high-level data descriptors, called meta-data. They use meta-data negotiations to eliminate the transmission of redundant data throughout the network. In addition, SPIN nodes can base their communication decisions both upon application-specic knowledge of the data and upon knowledge of the resources that are available to them. This allows the sensors to eciently distribute data given a limited energy supply. We simulate and analyze the performance of two specic SPIN protocols, comparing them to other possible approaches and a theoretically optimal protocol. We nd that the SPIN protocols can deliver 60% more data for a given amount of energy than conventional approaches. We also nd that, in terms...
401|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
402|Multicast Routing in Datagram Internetworks and Extended LANs|Multicasting, the transmission of a packet to a group of hosts, is an important service for improving the efficiency and robustness of distributed systems and applications. Although multicast capability is available and widely used in local area networks, when those LANs are interconnected by store-and-forward routers, the multicast service is usually not offered across the resulting internetwork. To address this limitation, we specify extensions to two common internetwork routing algorithms-distance-vector routing and link-state routing-to support low-delay datagram multicasting beyond a single LAN. We also describe modifications to the single-spanning-tree routing algorithm commonly used by link-layer bridges, to reduce the costs of multicasting in large extended LANs. Finally, we discuss how the use of multicast scope control and hierarchical multicast routing allows the multicast service to scale up to large internetworks.
403|Exokernel: An Operating System Architecture for Application-Level Resource Management|We describe an operating system architecture that securely multiplexes machine resources while permitting an unprecedented degree of application-specific customization of traditional operating system abstractions. By abstracting physical hardware resources, traditional operating systems have significantly limited the performance, flexibility, and functionality of applications. The exokernel architecture removes these limitations by allowing untrusted software to implement traditional operating system abstractions entirely at application-level. We have implemented a prototype exokernel-based system that includes Aegis, an exokernel, and ExOS, an untrusted application-level operating system. Aegis defines the low-level interface to machine resources. Applications can allocate and use machine resources, efficiently handle events, and participate in resource revocation. Measurements show that most primitive Aegis operations are 10–100 times faster than Ultrix,a mature monolithic UNIX operating system. ExOS implements processes, virtual memory, and inter-process communication abstractions entirely within a library. Measurements show that ExOS’s application-level virtual memory and IPC primitives are 5–50 times faster than Ultrix’s primitives. These results demonstrate that the exokernel operating system design is practical and offers an excellent combination of performance and flexibility. 1
404|CEDAR: a Core-Extraction Distributed Ad hoc Routing Algorithm|Absiract—CEDARis an algorithm for QoS routing in ad hoc network environments. It has three key components: (a) the establishment and main-tenance of a setf-organizing routing infrastructure catted the core for per-forming route computations, (b) the propagation of the link-state of stable high-bandwidth links in the core through increase/decrease waves, and (c)a QoS ra,ute computation algorithm that is exeeuted at the core nodes using onty locally available state. Our preliminary performance evaluation shows that CEDAR is a robust and adaptive QoS routing algorithm that reacts effectively to the dynamics of the network white stitl approximating link-state performance for stable networks. Keywords—Ad hoc routing, QoS routing I.
405|Videoconferencing on the Internet|This paper describes the INRIA Videoconferencing System (IVS), a low bandwidth tool for real-time video between workstations on the Internet using UDP datagrams and the IP multicast extension. The video coder-decoder (codec) is a software implementation of the UIT-T recommendation H.261 originally developed for the Integrated Services Digital Network (ISDN). Our focus in this paper is on adapting this codec for the Internet environment. We propose a packetization scheme, an error control scheme and an output rate control scheme that adapts the image coding process based on network conditions. This work shows that it is possible to maintain videoconferences with reasonable quality across packet-switched networks without requiring special support from the network such as resource reservation or admission control.
406|Routing in Ad Hoc Networks of Mobile Hosts|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any centralized administration or standard support services. In such an environment, it may be necessary for one mobile host to enlist the aid of others in forwarding a packet to its destination, due to the limitedpropagation range of each mobile host’s wireless transmissions. Some previous attempts have been made to use conventional routing protocols for routing in ad hoc networks, treating each mobile host as a router This position paper points out a number of problems with this design and suggests a new approach based on separate route discovery and route maintenance protocols. 1.
408|Scalable Data Naming for Application Level Framing in Reliable Multicast|The Application Level Framing (ALF) protocol architecture [2] encourages application control over mechanisms that traditionally fall within the &#034;transport layer&#034;, e.g., loss detection and recovery. Traditional ARQ-based reliable protocols for unicast (e.g., TCP) as well as multicast (e.g., Horus [30], RMTP [15], etc.) number data units sequentially to detect losses. Unfortunately, these transport-level sequence numbers do not permit receivers to flexibly tailor their reliability semantics. Achieving receiver-driven reliability is cumbersome in the existing &#034;layered&#034; architecture of the network protocol stack where the receiving application has no knowledge of how application-level objects map onto transport level sequence numbers. In this paper, we propose a new data naming scheme that exposes the structure of application data to the transport layer, thereby enhancing the expressibility of an applications&#039; reliability and ordering semantics. We apply this data naming scheme to a reliab...
409|Epidemic Algorithms for Replicated Databases|We present a family of epidemic algorithms for maintaining replicated database systems. The algorithms are based on the  causal delivery of log records where each record corresponds to one transaction instead of one operation. The first algorithm in this  family is a pessimistic protocol that ensures serializability and guarantees strict executions. Since we expect the epidemic algorithms to  be used in environments with low probability of conflicts among transactions, we develop a variant of the pessimistic algorithm which is  optimistic in that transactions commit as soon as they terminate locally and inconsistencies are detected asynchronously as the effects  of committed transactions propagate through the system. The last member of the family of epidemic algorithms is pessimistic and uses  voting with quorums to resolve conflicts and improve transaction response time. A simulation study evaluates the performance of the  protocols.
410|Biclustering of Expression Data|An efficient node-deletion algorithm is introduced to find submatrices...
411|The NP-completeness column: an ongoing guide|This is the nineteenth edition of a (usually) quarterly column that covers new developments in the theory of NP-completeness. The presentation is modeled on that used by M. R. Garey and myself in our book &#034;Computers and Intractability: A Guide to the Theory of NP-Completeness,&#034; W. H. Freeman &amp; Co., New York, 1979 (hereinafter referred to as &#034;[G&amp;J]&#034;; previous columns will be referred to by their dates). A background equivalent to that provided by [G&amp;J] is assumed, and, when appropriate, cross-references will be given to that book and the list of problems (NP-complete and harder) presented there. Readers who have results they would like mentioned (NP-hardness, PSPACE-hardness, polynomial-time-solvability, etc.) or open problems they would like publicized, should
412|An Algorithm for Clustering cDNAs for Gene Expression Analysis|We have developed a novel algorithm for cluster analysis that is based on graph theoretic techniques. A similarity graph is defined and clusters in that graph correspond to highly connected subgraphs. A polynomial algorithm to compute them efficiently is presented. Our algorithm produces a clustering with some provably good properties.  The application that motivated this study was gene expression analysis, where a collection of cDNAs must be clustered based on their oligonucleotide fingerprints. The algorithm has been tested intensively on simulated libraries and was shown to outperform extant methods. It demonstrated robustness to high noise levels. In a blind test on real cDNA fingerprint data the algorithm obtained very good results. Utilizing the results of the algorithm would have saved over 70% of the cDNA sequencing cost on that data set.  1 Introduction  Cluster analysis seeks grouping of data elements into subsets, so that elements in the same subset are in some sense more cl...
413|Contentment in graph theory: covering graphs with cliques|Fundamental questions posed by Boole in 1868 on the theory of sets have in recent years been translated to problems in graph theory. The major problems that this paper deals with are determining the minimum number of complete subgraphs of graph G which include all of the edges of G, and determining the minimum number of complete bipartite subgraphs which cover G. The two problems are of a very similar nature. Determining whether there is a projective plane of order p is a special case of the former problem. The latter problem has a natural translation into matrix theory which yields tight upper and lower bounds. An elementary proof is given for Graham&#039;s theorem. Two non-obvious classes are given for which the above problems are easily handled; however, this author doubts that these classes can be extended significantly. Two new problems are shown in this
414|Systematic Management and Analysis of Yeast Gene Expression Data|roarrays, Serial Analysis of Gene Expression (SAGE), and other techniques (Velculescu et al. 1995; Lockhart et al. 1996; DeRisi et al. 1997), has led to the rapid accumulation of large expression data sets and the development of the field of functional genomics. Functional genomics has been contrasted as having a systematic, genome-wide approach to the collection and analysis of biological data compared with more traditional methods, which focus in depth on particular genes, proteins, or pathways (Hieter and Boguski 1997). But despite rapid strides, capped by a string of successful studies (see Table 1), functional genomics has yet to develop a highly integrated system of tools and methods such those used in sequence analysis and structural genomics (Hieter and Boguski 1997); for that, we must await development of the three following components: general databases, data standards, and integrated general-purpose analysis tools. A comparison of the status of these fields with respect to t
415|Data Integration: A Theoretical Perspective|Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
416|A Survey of Approaches to Automatic Schema Matching|Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.
417|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
418|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
419|Information integration using logical views|A number of ideas concerning information-integration tools can be thought of as constructing answers to queries using views that represent the capabilities of information sources. We review the formal basis of these techniques, which are closely related to containment algorithms for conjunctive queries and/or Datalog programs. Then we compare the approaches taken by AT&amp;T Labs&#039; &#034;Information Manifold&#034; and the Stanford &#034;Tsimmis&#034; project in these terms.
420|The TSIMMIS Approach to Mediation: Data Models and Languages|TSIMMIS -- The Stanford-IBM Manager of Multiple Information Sources -- is a system for integrating information. It o ers a data model and a common query language that are designed to support the combining of information from many different sources. It also o ers tools for generating automatically the components that are needed to build systems for integrating information. In this paper we shall discuss the principal architectural features and their rationale. 
421|Query Answering in Inconsistent Databases|In this chapter, we summarize the research on querying inconsistent databases we have been conducting over the last five years. The formal framework we have used is based on two concepts: repair and consistent query answer. We describe different approaches to the issue of computing consistent query answers: query transformation, logic programming, inference in annotated logics, and specialized algorithms. We also characterize the computational complexity of this problem. Finally, we discuss related research in artificial intelligence, databases, and logic programming.
422|Index Structures for Path Expressions|In recent years there has been an increased interest in managing data which does not conform to traditional data models, like the relational or object oriented model. The reasons for this non-conformance are diverse. One one hand, data may not conform to such models at the physical level: it may be stored in data exchange formats, fetched from the Internet, or stored as structured les. One the other hand, it may not conform at the logical level: data may have missing attributes, some attributes may be of di erent types in di erent data items, there may be heterogeneous collections, or the data may be simply specified by a schema which is too complex or changes too often to be described easily as a traditional schema. The term semistructured data has been used to refer to such data. The data model proposed for this kind of data consists of an edge-labeled graph, in which nodes correspond to objects and edges to attributes or values. Figure 1 illustrates a semistructured database providing information about a city. Relational databases are traditionally queried with associative queries, retrieving tuples based on the value of some attributes. To answer such queries efciently, database management systems support indexes for translating attribute values into tuple ids (e.g. B-trees or hash tables). In object-oriented databases, path queries replace the simpler associative queries. Several data structures have been proposed for answering path queries e ciently: e.g., access support relations 14] and path indexes 4]. In the case of semistructured data, queries are even more complex, because they may contain generalized path expressions 1, 7, 8, 16]. The additional exibility is needed in order to traverse data whose structure is irregular, or partially unknown to the user.
423|Complexity of answering queries using materialized views|WC study the complexity of the problem of answering queries using materinlized views, This problem has attracted a lot of attention re-cently because of its relevance in data integration. Previous work considered only conjunctive view definitions. We examine the con-sequences of allowing more expressive view definition languages. Tl~olanguagcsweconsiderforviewdefinitionsanduserqueriesare: conjunctive qucrics with inequality, positive queries, datalog, and first-order logic. We show that the complexity of the problem de-pcnds on whether views are assumed to store all the tuples that sat-isfy the view definition, or only a subset of it. Finally, we apply the results to the view consistency and view self-maintainability prob-lems which nrise in data warehousing. 1
424|Semistructured data|In semistructured data, the information that is normally as-sociated with a schema is contained within the data, which is sometimes called “self-describing”. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tu-torial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expres-sive language for querying and transformation, and opti-mizat,ion problems. 1 The motivation The topic of semistructured data (also called unstructured data) is relatively recent, and a tutorial on the topic may well be premature. It represents, if anything, the conver-gence of a number of lines of thinking about new ways to represent and query data that do not completely fit with conventional data models. The purpose of this tutorial is to to describe this motivation and to suggest areas in which further research may be fruitful. For a similar exposition, the reader is referred to Serge Abiteboul’s recent survey pa-per PI. The slides for this tutorial will be made available from a section of the Penn database home page
425| On the Decidability of Query Containment under Constraints |Query containment under constraints is the problem of checking whether for every database satisfying a given set of constraints, the result of one query is a subset of the result of another query. Recent research points out that this is a central problem in several database applications, and we address it within a setting where constraints are specified in the form of special inclusion dependencies over complex expressions, built by using intersection and difference of relations, special forms of quantification, regular expressions over binary relations, and cardinality constraints. These types of constraints capture a great variety of data models, including the relational, the entity-relational, and the object-oriented model. We study the problem of checking whether q is contained in q ' with respect to the constraints specified in a schema S, where q and q ' are nonrecursive Datalog programs whose atoms are complex expressions. We present the following results on query containment. For the case where q does not contain regular expressions, we provide a method for deciding query containment, and analyze its computational complexity. We do the same for the case where neither S nor q, q ' contain number restrictions. To the best of our knowledge, this yields the first decidability result on containment of conjunctive queries with regular expressions. Finally, we prove that the problem is undecidable for the case where we admit inequalities in q'.  
426|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
427|Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity|Most databases contain &#034;name constants&#034; like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user&#039;s query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
428|Context Interchange: New Features and Formalisms for the Intelligent Integration of Information|The Context Interchange strategy presents a novel perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared “ontologies ” can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalism for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing mediated data access to both traditional and web-based information sources. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems—Query processing; H.2.5 [Database Management]: Heterogeneous Databases—Data translation
429|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
430|Catching the Boat with Strudel: Experiences with a Web-Site Management System|The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel&#039;s key idea is separating the management of the site&#039;s data, the creation and management of the site&#039;s structure, and the visual presentation of the site&#039;s pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site&#039;s structure by applying a &#034;site-definition query&#034; to the underlying data. The result of evaluating this query is a &#034;site graph&#034;, which represents both the site&#039;s content and structure. Third, the builder specifies the visual presentation of pages in Strudel&#039;s HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel&#039;s key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing sev...
431|Answering Recursive Queries Using Views|We consider the problem of answering datalog queries using materialized views. The ability to answer queries using views is crucial in the context of information integration. Previous work on answering queries using views restricted queries to being conjunctive. We extend this work to general recursive queries: Given a datalog program P and a set of views, is it possible to find a datalog program that is equivalent to P and only uses views as EDB predicates? In this paper, we show that the problem of whether a datalog program can be rewritten into an equivalent program that only uses views is undecidable. On the other hand, we prove that a datalog program P can be effectively rewritten into a program that only uses views, that is contained in P,  and that contains all programs that only use views and are contained in P. As a consequence, if there exists a program equivalent to P that only uses views, then our construction is guaranteed to yield a program equivalent to P.  1 Introductio...
432|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
433|Managing Semantic Heterogeneity in Databases : A Theoretical Perspective, Tutorial at PODS|A full version of this tutorial appears at
434|Description Logics in Data Management|Description logics and reasoners, which are descendants of the kl-one language, have been studied in depth in Artificial Intelligence. After a brief introduction, we survey in this paper their application to the problems of information management, using the framework of an abstract information server equipped with several operations -- each involving one or more languages. Specifically, we indicate how one can achieve enhanced access to data and knowledge by using descriptions in languages for schema design and integration, queries, answers, updates, rules, and constraints.
435|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
436|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
437|Description logic framework for information integration|Information Integration is one of the core problems in distributed databases, cooperative information systems, and data warehousing, which are key areas in the software development industry. Two critical factors for the design and maintenance of applications requiring Information Integration are conceptual modeling of the domain, and reasoning support over the conceptual representation. We demonstrate that Knowledge Representation and Reasoning techniques can play an important role for both of these factors, by proposing a Description Logic based framework for Information Integration. We show that the development of successful Information Integration solutions requires not only to resort to very expressive Description Logics, but also to significantly extend them. We present a novel approach to conceptual modeling for Information Integration, which allows for suitably modeling the global concepts of the application, the individual information sources, and the constraints among different sources. Moreover, we devise inference procedures for the fundamental reasoning services, namely relation and concept subsumption, and query containment. Finally, we present a methodological framework for Information Integration, which can be applied in several contexts, and highlights the role of reasoning services within the design process. 1
438|Equivalences among relational expressions with the union and difference operators|ABSTRACT Queries in relational databases can be formulated in terms of relational expressions using the relational operations elect, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators elect, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions i proposed This method of query representation can be viewed as a generahzatlon f tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator isnot applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete ven for tableaux that correspond to expressions with only one project and several join operators KEY WORDS AND PHRASES relatmnal database, relational algebra, query optimization, equivalence of queries, conjunctive query, tableau, NP-complete, polynomial-time hierarchy, H 2P-complete CR CATEGORIES 4 33, 5 25
439|Representing and Using Interschema Knowledge in Cooperative Information Systems|Managing interschema knowledge is an essential task when dealing with cooperative information systems. We propose a logical approach to the problem of both expressing interschema knowledge, and reasoning about it. In particular, we set up a structured representation language for expressing semantic interdependencies between classes belonging to different database schemas, and present a method for reasoning over such interdependencies. The language and the associated reasoning technique makes it possible to build a logic-based module that can draw useful inferences whenever the need arises of both comparing and combining the knowledge represented in the various schemas. Notable examples of such inferences include checking the coherence of interschema knowledge, and providing integrated access to a cooperative information system.
440|Navigational Plans For Data Integration|We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...
441|Tableau Techniques For Querying Information Sources Through Global Schemas|. The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al. 1 Introduction Information Integration systems [Ull...
442|What Can Databases Do for Peer-to-Peer?|The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The  grand vision --- a decentralized community of machines pooling their resources to benefit everyone --- is compelling for  many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship.
443|Data Integration under Integrity Constraints|Data integratio n systemspro vide accessto a seto fhetero - geneo us, auto no mo us data so urces thro ugh a so -called glo bal schema. There are basically two appro aches fo r designing a data integratio n system. In the glo bal-centric appro ach,o ne defines the elementso f the glo bal schema as viewso ver the so urces, whereas in the lo cal-centric appro ach, o e characterizes the so rces as viewso ver theglo al schema. It is well kno wn that pro cessing queries in the latter appro ach is similar to query answering with inc o plete infoC atio , and, therefo9 is a c o plex task. On theo ther hand, it is a co mmo no pinio n that query pro cessing is much easier in the fo rmer appro ach. In this paper we sho w the surprising result that, when theglo al schema is expressed in the relatio al mo del with integrity c o straints, eveno f simple types, the pr o lemo f inco6 plete info rmatio n implicitly arises, making querypro cessing di#cult in the glo al-centric approC h as well. We thenfo cuso n glo al schemas with key andfo eign key co straints, which represents a situat io which is veryco#=W in practice, and we illustrate techniques fo e#ectively answering queries po sed to the data integratio n system in this case. 1 
444|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
445|Rewriting of Regular Expressions and Regular Path Queries|Recent work on semi-structured data has revitalized the interest in path queries, i.e., queries that ask for all pairs of objects in the database that are connected by a path conforming to a certain specification, in particular to a regular expression. Also, in semi-structured data, as well as in data integration, data warehousing, and query optimization, the problem of view-based query rewriting is receiving much attention: Given a query and a collection of views, generate a new query which uses the views and provides the answer to the original one. In this paper we address the problem of view-based query rewriting in the context of semi-structured data. We present a method for computing the rewriting of a regular expression E in terms of other regular expressions. The method computes the exact rewriting (the one that defines the same regular language as E) if it exists, or the rewriting that defines the maximal language contained in the one defined by E, otherwise. We present a complexity analysis of both the problem and the method, showing that the latter is essentially optimal. Finally, we illustrate how to exploit the method for view-based rewriting of regular path queries in semi-structured data. The complexity results established for the rewriting of regular expressions apply also to the case of regular path queries. 
446|CARIN: A Representation Language Combining Horn Rules and Description Logics|.  We describe CARIN, a novel family of representation languages, which integrate the expressive power of Horn rules and of description logics. We address the key issue in designing such a language, namely, providing a sound and complete inference procedure. We identify existential entailment as a core problem in reasoning in  CARIN, and describe an existential entailment algorithm for CARIN  languages whose description logic component is ALCNR. This algorithm entails several important results for reasoning in CARIN, most notably: (1) a sound and complete inference procedure for non recursive  CARIN-ALCNR, and (2) an algorithm for determining rule subsumption over ALCNR. 1 Introduction  Horn rule languages have formed the basis for many Artificial Intelligence application languages because their expressive power is sufficient for many applications, and they have good computational properties. One of the significant limitations of Horn rules is that they are not expressive enough to mod...
447|Query optimization in the presence of limited access patterns|We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the di erent conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best- rst search strategy in order to produce a rst complete plan early in the search. We describe experiments to illustrate the performance of our algorithm. 1
448|Quality-driven Integration of Heterogeneous Information Systems|Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scientific and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at different levels to ultimately find a set of high-quality query answering plans.
449|Query Containment for Conjunctive Queries With Regular Expressions|The management of semistructured data has recently received significant attention because of the need of several applications to model and query large volumes of irregular data. This paper considers the problem of query containment for a query language over semistructured data, StruQL0 , that contains the essential feature common to all such languages, namely the ability to specify regular path expressions over the data. We show here that containment of StruQL0 queries is decidable. First, we give a semantic criterion for StruQL0 query containment: we show that it suffices to check containment on only finitely many canonical databases. Second, we give a syntactic criteria for query containment, based on a notion of query mappings, which extends containment mappings for conjunctive queries. Third, we consider a certain fragment of StruQL0 , obtained by imposing restrictions on the regular path expressions, and show that query containment for this fragment of  StruQL0 is NP complete.  1 ...
450|Recursive Plans for Information Gathering|Generating query-answering plans for information gathering agents requires to translate a user query, formulated in terms of a set of virtual relations, to a query that uses relations that are actually stored in information sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle information sources with binding-pattern limitations, and to exploit functional dependencies in the domain model. As a result, these plans were incomplete w.r.t. sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive  information gathering plans, which enables us to settle two open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources in the presence of functional dependencies. Second, we describe an analogous algorithm in the presence of binding-pattern restrictions in the sources...
451|EQUIVALENCES AMONG RELATIONAL EXPRESSIONS|Many database queries can be formulated in terms of expressions whose operands represent tables of information (relations) and whose operators are the relational operations select, project, and join. This paper studies the equivalence problem for these relational expressions, with expression optimization in mind. A matrix, called a tableau, is proposed as a natural representative for the value of an expression. It is shown how tableaux can be made to reflect functional dependencies among attributes. A polynomial time algorithm is presented for the equivalence of tableaux that correspond to an important subset of expressions, although the equivalence problem is shown to be NP-complete under slightly more general circumstances.
452|On the equivalence of recursive and nonrecursive Datalog programs|vardi Abstract: We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Dat-alog programs are equivalent to unions of conjunctive queries, we study also the problem of determining whether a given recursive Datalog program is contained in a union of con-junctive queries. For this problem, we prove doubly exponential upper and lower time bounds. For the equivalence problem, we prove triply exponential upper and lower time bounds. 1
453|Containment of conjunctive regular path queries with inverse |Reasoning on queries is a basic problem both in knowledge representation and databases. A fundamental form of reasoning on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another query. Query containment is crucial in several contexts, such as query optimization, knowledge base verification, information integration, database integrity checking, and cooperative answering. In this paper we address the problem of query containment in the context of semistructured knowledge bases, where the basic querying mechanism, namely regular path queries,
454|Rewriting Aggregate Queries Using Views|We investigate the problem of rewriting queries with aggregate operators using views that mayormay not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum. Our approach is based on syntactic characterizations of the equivalence of aggregate queries. One contribution of this paper are characterizations of the equivalence of disjunctive aggregate queries, which generalize our previous results for the conjunctive case. For each operator &amp;alpha;, we introduce several types of queries using views as candidates for rewritings. We unfold such a candidate by replacing each occurrence of a view predicate with ...
455|An Extensible Framework for Data Cleaning|Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...
456|Answering regular path queries using views|Query answering using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, mobile computing, and maintaining physical data independence. We address query answering using views in a context where queries and views are regular path queries, i.e., regular expressions that denote the pairs of objects in the database connected by a matching path. Regular path queries are the basic query mechanism when the database is conceived as a graph, such as in semistructured data and data on the web. We study algorithms for answering regular path queries using views under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We characterize data, expression, and combined complexity of the problem, showing that the proposed algorithms are essentially optimal. Our results are the first to exhibit decidability in cases where the language for expressing the query and the views allows for recursion. 
457|Query Planning and Optimization in Information Integration|Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
458|Answering queries using views over description logics knowledge bases|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.
459|A Formal Perspective on the View Selection Problem|The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views t into a prespeci ed storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equalityselection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded. 1
460|Query containment for data integration systems|The problem of query containment is fundamental to many aspects of database systems,including query optimization,determining independence of queries from updates,and rewriting queries using views. In the data-integration framework,however,the standard notion of query containment does not suffice. We define relative containment,which formalizes the notion of query containment relative to the sources available to the data-integration system. First,we provide optimal bounds for relative containment for several important classes of datalog queries,including the common case of conjunctive queries. Next,we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly,we show that relative containment for conjunctive queries is still decidable in this case,even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally,we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.
461|Specifying and querying database repairs using logic programs with exceptions|Abstract Databases may be inconsistent with respect to a given set of integrity constraints. Nevertheless, most of the data may be consistent. In this paper we show how to specify consistent data and how to query a relational database in such a way that only consistent data is retrieved. The specification and queries are based on disjunctive extended logic programs with positive and negative exceptions that generalize those previously introduced by Kowalski and Sadri.
462|View-based query processing and constraint satisfaction|View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.  
463|Optimization Properties for Classes of Conjunctive Regular Path Queries |Abstract. We are interested in the theoretical foundations of the optimization of conjunctive regular path queries (CRPQs). The basic problem here is deciding query containment both in the absence and presence of constraints. Containment without constraints for CRPQs is EXPSPACE-complete, as opposed to only NP-complete for relational conjunctive queries. Our past experience with implementing similar algorithms suggests that staying in PSPACE might still be useful. Therefore we investigate the complexity of containment for a hierarchy of fragments of the CRPQ language. The classifying principle of the fragments is the expressivity of the regular path expressions allowed in the query atoms. For most of these fragments, we give matching lower and upper bounds for containment in the absence of constraints. We also introduce for every fragment a naturally corresponding class of constraints in whose presence we show both decidability and undecidability results for containment in various fragments. Finally, we apply our results to give a complete algorithm for rewriting with views in the presence of constraints for a fragment that contains Kleene-star and disjunction. 1
464|Deciding Containment for Queries with Complex Objects and Aggregations|We address the problem of query containment and query equivalence for complex objects. We show that for a certain conjunctive query language for complex objects, query containment and weak query equivalence are decidable. Our results have two consequences. First, when the answers of the two queries are guaranteed not to contain empty sets, then weak equivalence coincides with equivalence, and our result answers partially an open problem about the equivalence of nest; unnest queries for complex objects [GPG90]. Second, we derive an NP-complete algorithm for checking the equivalence of certain conjunctive queries with grouping and aggregates. Our results rely on a translation of the containment and equivalence conditions for complex objects into novel conditions on conjunctive queries, which we call simulation and strong simulation. These conditions are more complex than containment of conjunctive queries, because they involve arbitrary numbers of quantifier alternations. We prove that c...
465|Capability Based Mediation in TSIMMIS|Introduction  The TSIMMIS system [1] integrates data from multiple heterogeneous sources and provides users with seamless integrated views of the data. It translates a user query on the integrated views into a set of source queries and postprocessing steps that compute the answer to the user query from the results of the source queries. TSIMMIS uses a  mediation architecture [11] to accomplish this (Figure 1). User Source 1 Source 2 Source N  Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic [2, 9] and Information Manifold [4] employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM [7]) for dealing with the heterogeneity of the data sources. In particular, it employs source wrappers [3] that provide a uniform OEM interface to the mediator. In SIGMO
466|Query Answering in Information Systems with Integrity Constraints|The specifications of most of the nowadays ubiquitous informations systems include integrity constraints, i.e. conditions rejecting so-called &#034;invalid&#034; or &#034;inconsistent &#034; data. Information system consistency and query answering have been formalized referring to classical logic implicitly assuming that query answering only makes sense with consistent information systems. In practice, however, inconsistent as well as consistent information systems need to be queried. In this paper, it is first argued that classical logic is inappropriate for a formalization of information systems because of its global notion of inconsistency. It is claimed that information systems inconsistency should be understood as a  local notion. Then, it is shown that minimal logic, a constructivistic weakening of classical logic which precludes refutation proofs, provides for local inconsistencies that conveniently reflect a practitioner&#039;s intuition. Further, minimal logic is shown to be a convenient foundation fo...
467|Query Folding with Inclusion Dependencies|Query folding is a technique for determining how a query may be answered using a given set of resources, which may include materialized views, cached results of previous queries, or queries answerable by other databases. The power of query folding can be considerably enhanced by taking into account integrity constraints that are known to hold on base relations. This paper describes an extension of query folding that utilizes inclusion dependencies to find foldings of queries that would otherwise be overlooked. We describe a complete strategy for finding foldings in the presence of inclusion dependencies and present a basic algorithm that implements that strategy. We also describe extensions to this algorithm when both inclusion and functional dependencies are considered.
468|Information Integration: the MOMIS Project Demonstration|ranted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th VLDB Conference,  Cairo, Egypt, 2000.  2  MOMIS is a joint project among the Universit`a di Modena e Reggio Emilia, the Universit`a di Milano, and the Universit`a di Brescia within the national research project INTERDATA, theme n.3 &#034;Integration of Information over the Web&#034;, coordinated by V. De Antonellis, Universit`a di Brescia.  1. a common data model, ODM I 3  , which is defined according to the ODL I 3 language, to describe source schemas for integration purposes. ODM I 3 and ODL I 3&lt;F12.24
469|Query Planning with Limited Source Capabilities|In information-integration systems, sources may have diverse and limited query capabilities. In this paper we show that because sources have restrictions on retrieving their information, sources not mentioned in a query can contribute to the query result by providing useful bindings. In some cases we can access sources repeatedly to retrieve bindings to answer a query, and query planning thus becomes considerably more challenging. We find all the obtainable answers to a query by translating the query and source descriptions to a simple recursive Datalog program, and evaluating the program on the source relations. This program often accesses sources that are not in the query. Some of these accesses are essential, as they provide bindings that let us query sources, which we could not do otherwise. However, some of these accesses can be proven not to add anything to the query&#039;s answer. We show in which cases these off-query accesses are useless, and prove that in these cases we can comput...
470|Querying Aggregate Data|We introduce a first-order language with real polynomial arithmetic and aggregation operators (count, iterated sum and multiply), which is well suited for the definition of aggregate queries involving complex statistical functions. It offers a good trade-off between expressive power and complexity, with a tractable data complexity. Interestingly, some fundamental properties of first-order with real arithmetic are preserved in the presence of aggregates. In particular, there is an effective quantifier elimination for formulae with aggregation. We consider the problem of querying data that has already been aggregated in aggregate views, and focus on queries with an aggregation over a conjunctive query. Our main conceptual contribution is the introduction of a new equivalence relation among conjunctive queries, the isomorphism modulo a product. We prove that the equivalence of aggregate queries such as for instance averages reduces to it. Deciding if two queries are isomorphic modulo a p...
471|Accessing data integration systems through conceptual schemas|Abstract. Data integration systems provide access to a set of heterogeneous, autonomous data sources through a so-called global, or mediated view. There is a general consensus that the best way to describe the global view is through a conceptual data model, and that there are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. It is well known that processing queries in the latter approach is similar to query answering with incomplete information, and, therefore, is a complex task. On the other hand, it is a common opinion that query processing is much easier in the former approach. In this paper we show the surprising result that, when the global schema is expressed in terms of a conceptual data model, even a very simple one, query processing becomes difficult in the global-as-view approach also. We demonstrate that the problem of incomplete information arises in this case too, and we illustrate some basic techniques for effectively answering queries posed to the global schema of the data integration system. 1
472|Answering Queries Using Materialized Views With Disjunctions|We consider the problem of answering datalog queries using  materialized views. More  specifi.
473|On Answering Queries in the Presence of Limited Access Patterns|. In information-integration systems, source relations often  have limitations on access patterns to their data; i.e., when one must  provide values for certain attributes of a relation in order to retrieve its  tuples. In this paper we consider the following fundamental problem: can  we compute the complete answer to a query by accessing the relations  with legal patterns? The complete answer to a query is the answer that  we could compute if we could retrieve all the tuples from the relations.  We give algorithms for solving the problem for various classes of queries,  including conjunctive queries, unions of conjunctive queries, and conjunctive  queries with arithmetic comparisons. We prove the problem is  undecidable for datalog queries. If the complete answer to a query cannot  be computed, we often need to compute its maximal answer. The  second problem we study is, given two conjunctive queries on relations  with limited access patterns, how to test whether the maximal answer to...
474|Answering Queries Using Limited External Query Processors|When answering queries using external information sources, their contents can be described by views. To answer a query, we must rewrite it using the set of views presented by the sources. When the external information sources also have the ability to answer some (perhaps limited) sets of queries that require performing operations on their data, the set of views presented by the source may be infinite (albeit encoded in some finite fashion). Previous work on answering queries using views has only considered the case where the set of views is finite. In order to exploit the ability of information sources to answer more complex queries, we consider the problem of answering conjunctive queries using infinite sets of conjunctive views. Our first result is that an infinite set of conjunctive views can be partitioned into a finite number of equivalence classes, such that picking one view from every nonempty class is sufficient to determine whether the query can be answered using the views. Se...
475|On the Content of Materialized Aggregate Views|We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can b...
476|Lossless Regular Views|If the only information we have on a certain database is through a set of views, the question arises of whether this is sucient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.
477|View-based query answering and query containment over semistructured data|Abstract. The basic querying mechanism over semistructured data, namely regular path queries, asks for all pairs of objects that are connected by a path conforming to a regular expression. We consider conjunctive two-way regular path queries (C2RPQc’s), which extend regular path queries with two features. First, they add the inverse operator, which allows for expressing navigations in the database that traverse the edges both backward and forward. Second, they allow for using conjunctions of atoms, where each atom specifies that a regular path query with inverse holds between two terms, where each term is either a variable or a constant. For such queries we address the problem of view-based query answering, which amounts to computing the result of a query only on the basis of a set of views. More specifically, we present the following results: (1) We exhibit a mutual reduction between query containment and the recognition problem for view-based query answering for C2RPQc’s, i.e., checking whether a given tuple is in the certain answer to a query. Based on such a result, we can show that the problem of view-based query answering for C2RPQc’s is EXPSPACE-complete. (2) By exploiting techniques based on alternating two-way automata we show that for the restricted class of tree two-way regular path queries (in which the links between variables form a tree), query containment and view-based query answering are, rather surprisingly, in PSPACE (and hence, PSPACE-complete). (3) We present a technique to obtain view-based query answering algorithms that compute the whole set of tuples in the certain answer, instead of requiring to check each tuple separately. The technique is parametric wrt the query language, and can be applied both to C2RPQc’s and to tree-queries. 1
478|Models for Information Integration: Turning Local-as-View Into Global-as-View|There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. The goal of this paper is to verify whether we can transform a data integration system built with the local-as-view approach into a system following the global-as-view approach. We study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies, and the queries used in the integration systems (both the queries on the global schema, and the views in the mapping) are expressed in the language of conjunctive queries. The result we present is that such a transformation exists: we can always transform a local-as-view system into a global-as-view system such that, for each query, the set of answers to the query wrt the former is the same as the set of answers wrt the latter.
479|Query Rewriting using Semistructured Views|We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...
480|Intensional Query Answering by Partial Evaluation|. Intensional query answering aims at providing a response to a query addressed to a knowledge base by making use of the intensional knowledge as opposed to extensional. Such a response is an abstract description of the conventional answer that can be of interest in many situations, for example it may increase the cooperativeness of the system, or it may replace the conventional answer in case access to the extensional part of the knowledge base is costly as for Mobile Systems. In this paper we present a general framework to generate intensional answers in knowledge bases adhering to the logic programming paradigm. Such a framework is based on a program transformation technique, namely Partial Evaluation, and allows for generating complete and procedurally complete (wrt SLDNF-resolution) sets of intensional answers, treating both recursion and negation conveniently. Keywords: Knowledge bases, intensional query answering, logic programs, partial evaluation 1. Introduction Intensional an...
481|Atmospheric Modeling, Data Assimilation and Predictability|Numerical weather prediction (NWP) now provides major guidance in our daily weather forecast. The accuracy of NWP models has improved steadily since the first successful experiment made by Charney, Fj!rtoft and von Neuman (1950). During the past 50 years, a large number of technical papers and reports have been devoted to NWP, but the number of textbooks dealing with the subject has been very small, the latest being the 1980 book by Haltiner &amp; Williams, which was dedicated to descriptions of the atmospheric dynamics and numerical methods for atmospheric modeling. However, in the intervening years much impressive progress has been made in all aspects of NWP, including the success in model initialization and ensemble forecasts. Eugenia Kalnay’s recent book covers for the first time in the long history of NWP, not only methods for numerical modeling, but also the important related areas of data assimilation and predictability. It incorporates all aspects of environmental computer modeling including an historical overview of NWP, equations of motion and their approximations, a modern description of the methods to determine the initial conditions using weather observations and a clear discussion of chaos in dynamic systems and how these concepts can be
482|Tinydb: An acquisitional query processing system for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages—Query languages; H.2.4 [Database Management]: Systems—Distributed databases; query processing
483|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
484|The nesC language: A holistic approach to networked embedded systems|We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower “motes, ” each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. nesC’s contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption). nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
485|Timing-Sync Protocol for Sensor Networks|Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are timestamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.
486|TelegraphCQ: Continuous Dataflow Processing for an Uncertan World|Increasingly pervasive networks are leading towards a world where data is constantly in motion. In such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable data streams. In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.
487|The Cougar Approach to In-Network Query Processing in Sensor Networks|The widespread distribution and availability of smallscale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.
488|Routing indices for peer-to-peer systems|Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or ooding the network with queries. In this paper we introduce the concept of Routing Indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by ooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and nd that RIs can improve performance by one or two orders of magnitude vs. a ooding-based system, and by up to 100 % vs. a random forwarding system. We also discuss the tradeo s between the di erent RIschemes and highlight the e ects of key design variables on system performance.
489|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
491|Optimization of nonrecursive queries|State-of-the-art optimization approaches for relational database systems, e.g., those used in systems such as OBE, SQL/DS, and commercial INGRES. when used for queries in non-traditional database applications, suffer from two problems. First, the time complexity of their optimization algorithms, being combinatoric, is exponential in the number of relations to be joined in the query. Their cost is therefore prohibitive in situa-tions such as deductive databases and logic oriented languages for knowledge bases, where hundreds of joins may be required. The second problem with the traditional approaches is that, al-beit effective in their specific domain, it is not clear whether they can be generalized to different scenarios (e.g. parallel evalua-tion) since they lack a formal model to define the assumptions and critical factors on which their valiclity depends. This paper
492|Extensible/Rule Based Query Rewrite Optimization in Starburst|This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general --- and these rewrite rules in particular --- are an essential step in query optimization for modern database systems.  1 Introduction  In traditional database systems, query optimization typically consists of a single phase of processing in which access methods, join orders and join methods are chosen to provide an efficient plan for executing a user&#039;s declarative query. We refer to this phase as plan optimization. In this paper we present a distinct ph...
493|Query Processing, Approximation, and Resource Management In a Data Stream Management System|This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.
494|Querying in highly mobile distributed environments|New challenges to the database field brought about by the rapidly growing area of wireless personal communication are presented. Preliminary solutions to two of the major problems, control of update volume and integration of query processing and data acquisition, are discussed along with the simulation results. 1
495|Adaptive Query Processing: Technology in Evolution|As query engines are scaled and federated, they must cope with highly unpredictable and changeable environments. In the Telegraph project, we are attempting to architect and implement a continuously adaptive query engine suitable for global-area systems, massive parallelism, and sensor networks. To set the stage for our research, we present a survey of prior work on adaptive query processing, focusing on three characterizations of adaptivity: the frequency of adaptivity, the effects of adaptivity, and the extent of adaptivity. Given this survey, we sketch directions for research in the Telegraph project.  
496|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
497|Optimization techniques for queries with expensive methods|Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown &#034; rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
498|Cost-based Query Scrambling for Initial Delays|Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
499|Adaptive Parallel Aggregation Algorithms|Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 1 Introduction  SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPCD benchmark...
500|Bluetooth and Sensor Networks: A Reality Check|The current generation of sensor nodes rely on commodity components. The choice of the radio is particularly important as it impacts not only energy consumption but also software design (e.g., network self-assembly, multihop routing and in-network processing). Bluetooth is one of the most popular commodity radios for wireless devices. As a representative of the frequency hopping spread spectrum radios, it is a natural alternative to broadcast radios in the context of sensor networks. The question is whether Bluetooth can be a viable alternative in practice. In this paper, we report our experience using Bluetooth for the sensor network regime. We describe our tiny Bluetooth stack that allows TinyOS applications to run on Bluetooth-based sensor nodes, we present a multihop network assembly procedure that leverages Bluetooth&#039;s device discovery protocol, and we discuss how Bluetooth favorably impacts in-network query processing. Our results show that despite obvious limitations the Bluetooth sensor nodes we studied exhibit interesting properties, such as a good energy per bit sent ratio. This reality check underlies the limitations and some promises of Bluetooth for the sensor network regime.
501|Aggregation and Relevance in Deductive Databases|In this paper we present a technique to optimize queries on deductive databases that use aggregate operations such as min, max, and “largest Ic values. ” Our approach is based on an extended notion of relevance of facts to queries that takes aggregate operations into account. The approach has two parts: a rewriting part that labels predica.tes with “ag-gregate selections, ” and an evaluat,ion part. t.hat, makes use of “aggregate selections ” to detect that facts are irrelevant and discards them. The rewriting complements standard rewrit-ing algorithms like Magic sets, and the evaluation essentially refines Semi-Naive evaluation. 1
502|Query Optimization In Compressed Database Systems|Over the lastd ecad es, improvements in CPU speed have outpaced improvements in main memory and d isk access rates by ord ers of magnitud , enabling the use ofd ata compression techniques to improve the performance ofd atabase systems. Previous work d scribes the benefits of compression for numerical attributes, whered8 a is stored in compressed  format ond isk. Despite the abund3&amp; e of stringvalued  attributes in relational schemas there is little work on compression for string attributes in ad atabase context. Moreover, none of the previous work suitablyad2 esses the role of the query optimizer: During query execution, dD a is either eagerly d compressed when it is read into main memory, or dD a lazily stays compressed in main memory and is d compressed ond emand only. In this paper, we present an e#ective approach for dD abase compression based on lightweight, attribute-level compression techniques. We propose a Hierarchical ictionary Encod  ing strategy that intelligently selects the most e#ective compression method for string-valued attributes. We show that eager and lazy d compression strategies prod1 e suboptimal plans for queries involving compressed string attributes. We then formalize the problem of compressionaware query optimizationand propose one provably optimal  and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-Hd atad emonstrate the impact of our string compression method s and show the importance of compression-aware query optimization. Our approach results in up to an or d r speed up over existing approaches.  1. 
503|The Design and Implementation of the Ariel Active Database Rule System|This paper describes the design and implementation of the Ariel DBMS and it&#039;s tightlycoupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System Rlike query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual ff-memory nodes which save storage since they contain only the predicate associated with the memory n...
504|DOMINO: Databases fOr MovINg Objects tracking|Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.
505|From ethnography to design in a vineyard, in|This paper summarizes the process from ethnographic study of a vineyard to concept development and interaction design for a ubiquitous computing solution. It provides examples of vineyard interfaces and the lessons learned that could be generally applied to the interaction design of ubiquitous systems. These are: design for multiple perspectives on data, design for multiple access points, and design for varying levels of attention.
506|Query Optimization in Mobile Environments|We consider the issue of optimizing queries for a distributed processing in mobile environment. An interesting characteristic of mobile machines is that they depend on battery as a source of energy which may not be substantial enough. Hence, the appropriate optimization criterion in a mobile environment considers both resource utilization and energy consumption at the mobile client. In this scenario, the optimal plan for a query depends on the residual battery level of the mobile client and the load at the server. We approach this problem by compiling a query into a sequence of candidate plans, such that for any state of the client-server system, the optimal plan is one of the candidate plans. A general solution is proposed by adapting the partial order dynamic programming search algorithm [17, 18] (p.o dp) such that the coverset of the query is the set of candidate plans. We propose two novel algorithms, namely, the linear combinations algorithm and the linearset algorithm (referred t...
507|Online Dynamic Reordering|We present a mechanism for providing dynamic user control during long running, data-intensive operations. Users  can see partial results and dynamically direct the processing to suit their interests, thereby enhancing the interactivity  in several contexts such as online aggregation and large-scale spreadsheets.  In this paper, we develop a pipelining, dynamically tunable reorder operator for providing user control. Users  specify preferences for different data items based on prior results, so that data of interest is prioritized for early  processing. The reordering mechanism is efficient and non-blocking, and can be used over arbitrary data streams  from files and indexes, as well as continuous data feeds. We also investigate several policies for the reordering  based on the performance goals of various typical applications. We present performance results for reordering in the  context of an Online Aggregation implementation in Informix, and in the context of sorting and scrolling in...
508|Loopy Belief Propagation for Approximate Inference: An Empirical Study|Recently, researchers have demonstrated that  &#034;loopy belief propagation&#034; --- the use of  Pearl&#039;s polytree algorithm in a Bayesian  network with loops --- can perform well  in the context of error-correcting codes.  The most dramatic instance of this is the  near Shannon-limit performance of &#034;Turbo  Codes&#034; --- codes whose decoding algorithm  is equivalent to loopy belief propagation in a  chain-structured Bayesian network.  In this paper we ask: is there something special  about the error-correcting code context,  or does loopy propagation work as an approximate  inference scheme in a more general  setting? We compare the marginals computed  using loopy propagation to the exact  ones in four Bayesian network architectures,  including two real-world networks: ALARM  and QMR. We find that the loopy beliefs often  converge and when they do, they give a  good approximation to the correct marginals.  However, on the QMR network, the loopy beliefs  oscillated and had no obvious relationship  ...
509|Learning to Extract Symbolic Knowledge from the World Wide Web|The World Wide Web is a vast source of information  accessible to computers, but understandable  only to humans. The goal of the research described  here is to automatically create a computer  understandable world wide knowledge base whose  content mirrors that of the World Wide Web. Such a 
510|Context-Specific Independence in Bayesian Networks|Bayesiannetworks provide a languagefor qualitatively  representing the conditional independence properties  of a distribution. This allows a natural and compact  representation of the distribution, eases knowledge acquisition,  and supports effective inference algorithms.
511|Correctness of Local Probability Propagation in Graphical Models with Loops|This article analyzes the behavior of local propagation rules in graphical models with a loop.
512|Object-Oriented Bayesian Networks|Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language h...
513|Probabilistic Frame-Based Systems|Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS&#039;s) and Bayesian networks (BNs). FRS&#039;s provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty tha...
514|A Bayesian approach to learning Bayesian networks with local structure|Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability— that is, the Bayesian score—of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function. 1
515|Unsupervised Learning from Dyadic Data|Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.
516|Answering Queries from Context-Sensitive Probabilistic Knowledge Bases|We define a language for representing context-sensitive probabilistic knowledge. A knowledge base consists of a set of universally quantified probability sentences that include context constraints, which allow inference to be focused on only the relevant portions of the probabilistic knowledge. We provide a declarative semantics for our language. We present a query answering procedure which takes a query Q and a set of evidence E and constructs a Bayesian network to compute P (QjE). The posterior probability is then computed using any of a number of Bayesian network inference algorithms. We use the declarative semantics to prove the query procedure sound and complete. We use concepts from logic programming to justify our approach. Keywords: reasoning under uncertainty, Bayesian networks, Probability model construction, logic programming Submitted to Theoretical Computer Science special issue on Uncertainty in Databases and Deductive Systems. This work was partially supported by NSF g...
517|Probabilistic Reasoning for Complex Systems|ii
518|Learning Statistical Models from Relational Data|This workshop is the second in a series of workshops held in conjunction with AAAI and IJCAI. The first workshop was held in July, 2000 at AAAI. Notes from that workshop are available at
519|Learning Probabilities for Noisy First-Order Rules|First-order logic is the traditional basis for knowledge representation languages. However, its applicability to many real-world tasks is limited by its inability to represent uncertainty. Bayesian belief networks, on the other hand, are inadequate for complex KR tasks due to the limited expressivity of the underlying (propositional) language. The need to incorporate uncertainty into an expressive language has led to a resurgence of work on first-order probabilistic logic. This paper addresses one of the main objections to the incorporation of probabilities into the language: &#034;Where do the numbers come from?&#034; We present an approach that takes a knowledge base in an expressive rule-based first-order language, and learns the probabilistic parameters associated with those rules from data cases. Our approach, which is based on algorithms for learning in traditional Bayesian networks, can handle data cases where many of the relevant aspects of the situation are unobserved. It is also capabl...
520|Prospective assessment of ai technologies for fraud detection: A case study|In September 1995, the Congressional O ce of Technology Assessment completed a study of the potential for AI technologies to detect money laundering by screening wire transfers. The study, conducted at the request of the Senate Permanent Subcommittee on Investigations, evaluates the technical and public policy implications of widespread use of AI technologies by the Federal government for fraud detection. Its conclusions are relevant tomanyother uses of AI technologies for fraud detection in both the public and private sectors.
521|PRL: A probabilistic relational language|In this paper, we describe the syntax and semantics for a probabilistic relational language (PRL). PRL is a recasting of recent work in Probabilistic Relational Models (PRMs) into a logic programming framework. We show how to represent varying degrees of complexity in the semantics including attribute uncertainty, structural uncertainty and identity uncertainty. Our approach is similar in spirit to the work in Bayesian Logic Programs (BLPs), and Logical Bayesian Networks (LBNs). However, surprisingly, there are still some important differences in the resulting formalism; for example, we introduce a general notion of aggregates based on the PRM approaches. One of our contributions is that we show how to support richer forms of structural uncertainty in a probabilistic logical language than have been previously described. Our goal in this work is to present a unifying framework that supports all of the types of relational uncertainty yet is based on logic programming formalisms. We also believe that it facilitates understanding the relationship between the frame-based approaches and alternate logic programming approaches, and allows greater
522|A message ferrying approach for data delivery in sparse mobile ad hoc networks|Mobile Ad Hoc Networks (MANETs) provide rapidly deployable and self-configuring network capacity required in many critical applications, e.g., battlefields, disaster relief and wide area sensing. In this paper we study the problem of efficient data delivery in sparse MANETs where network partitions can last for a significant period. Previous approaches rely on the use of either long range communication which leads to rapid draining of nodes ’ limited batteries, or existing node mobility which results in low data delivery rates and large delays. In this paper, we describe a Message Ferrying (MF) approach to address the problem. MF is a mobility-assisted approach which utilizes a set of special mobile nodes called message ferries (or ferries for short) to provide communication service for nodes in the deployment area. The main idea behind the MF approach is to introduce non-randomness in the movement of nodes and exploit such non-randomness to help deliver data. We study two variations of MF, depending on whether ferries or nodes initiate proactive movement. The MF design exploits mobility to improve data delivery performance and reduce energy consumption in nodes. We evaluate the performance of MF via extensive ns simulations which confirm the MF approach is efficient in both data delivery and energy consumption under a variety of network conditions.
523|The capacity of wireless networks| When n identical randomly located nodes, each capable of transmitting at bits per second and using a fixed range, form a wireless network, the throughput @ A obtainable by each node for a randomly chosen destination is 2 bits per second under a noninterference protocol. If the nodes are optimally placed in a disk of unit area, traffic patterns are optimally assigned, and each transmission’s range is optimally chosen, the bit–distance product that can be transported by the network per second is 2 @ A bit-meters per second. Thus even under optimal circumstances, the throughput is only 2 bits per second for each node for a destination nonvanishingly far away. Similar results also hold under an alternate physical model where a required signal-to-interference ratio is specified for successful receptions. Fundamentally, it is the need for every node all over the domain to share whatever portion of the channel it is utilizing with nodes in its local neighborhood that is the reason for the constriction in capacity. Splitting the channel into several subchannels does not change any of the results. Some implications may be worth considering by designers. Since the throughput furnished to each user diminishes to zero as the number of users is increased, perhaps networks connecting smaller numbers of users, or featuring connections mostly with nearby neighbors, may be more likely to be find acceptance. 
524|Ad-hoc On-Demand Distance Vector Routing|An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. In this paper we present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each Mobile Host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic selfstarting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance-vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.
525|GPSR: Greedy perimeter stateless routing for wireless networks| We present Greedy Perimeter Stateless Routing (GPSR), a novel routing protocol for wireless datagram networks that uses the positions of touters and a packer&#039;s destination to make packet forwarding decisions. GPSR makes greedy forwarding decisions using only information about a router&#039;s immediate neighbors in the network topology. When a packet reaches a region where greedy forwarding is impossible, the algorithm recovers by routing around the perimeter of the region. By keeping state only about the local topology, GPSR scales better in per-router state than shortest-path and ad-hoc routing protocols as the number of network destinations increases. Under mobility&#039;s frequent topology changes, GPSR can use local topology information to find correct new routes quickly. We describe the GPSR protocol, and use extensive simulation of mobile wireless networks to compare its performance with that of Dynamic Source Routing. Our simulations demonstrate GPSR&#039;s scalability on densely deployed wireless networks.
526|A Delay-Tolerant Network Architecture for Challenged Internets|The highly successful architecture and protocols of today’s Internet may operate poorly in environments characterized by very long delay paths and frequent network partitions. These problems are exacerbated by end nodes with limited power or memory resources. Often deployed in mobile and extreme environments lacking continuous connectivity, many such networks have their own specialized protocols, and do not utilize IP. To achieve interoperability between them, we propose a network architecture and application interface structured around optionally-reliable asynchronous message forwarding, with limited expectations of end-to-end connectivity and node resources. The architecture operates as an overlay above the transport layers of the networks it interconnects, and provides key services such as in-network data storage and retransmission, interoperable naming, authenticated forwarding and a coarse-grained class of service.
527|Topology Control of Multihop Wireless Networks using Transmit Power Adjustment| We consider the problem of adjusting the transmit powers of nodes in a multihop wireless network (also called an ad hoc network) to create a desired topology. We formulate it as a constrained optimization problem with two constraints- connectivity and biconnectivity, and one optimization objective- maximum power used. We present two centralized algorithms for use in static networks, and prove their optimality. For mobile networks, we present two distributed heuristics that adaptively adjust node transmit powers in response to topological changes and attempt to maintain a connected topology using minimum power. We analyze the throughput, delay, and power consumption of our algorithms using a prototype software implementation, an emulation of a power-controllable radio, and a detailed channel model. Our results show that the performance of multihop wireless networks in practice can be substantially increased with topology control.  
528|Distributed topology control for power efficient operation in multihop wireless ad hoc networks|Abstract — The topology of wireless multihop ad hoc networks can be controlled by varying the transmission power of each node. We propose a simple distributed algorithm where each node makes local decisions about its transmission power and these local decisions collectively guarantee global connectivity. Specifically, based on the directional information, a node grows it transmission power until it finds a neighbor node in every direction. The resulting network topology increases network lifetime by reducing transmission power and reduces traffic interference by having low node degrees. Moreover, we show that the routes in the multihop network are efficient in power consumption. We give an approximation scheme in which the power consumption of each route can be made arbitrarily close to the optimal by carefully choosing the parameters. Simulation results demonstrate significant performance improvements. I.
529|The Performance of Query Control Schemes for the Zone Routing Protocol|In this paper, we study the performance of route query control mechanisms for the Zone Routing Protocol (ZRP) for ad hoc networks. ZRP proactively maintains routing information for a local neighborhood (routing zone), while reactively acquiring routes to destinations beyond the routing zone. This hybrid routing approach can be more efficient than traditional routing schemes. However, without proper query control techniques, the ZRP cannot provide the expected reduction in the control traffic.
530|The Shared Wireless Infostation Model - A New Ad Hoc Networking Paradigm (or Where there is a Whale, there is a Way)  (2003) |In wireless ad hoc networks, capacity can be traded for delay. This tradeoff has been the subject of a number of studies, mainly concentrating on the two extremes: either minimizing the delay or maximizing the capacity. However, in between these extremes, there are schemes that allow instantiations of various degrees of this tradeoff. Infostations, which offer geographically intermittent coverage at high speeds, are one such an example. Indeed, through the use of the Infostation networking paradigm, the capacity of a mobile network can be increased at the expense of delay. We propose to further extend the Infostation concept by integrating it with the ad hoc networking technology. We refer to this networking model as Wireless Infostation Model (SWIM). SWIM allows additional improvement in the capacity-delay tradeoff through a moderate increase in the storage requirements. To demonstrate how SWIM can be applied to solve a practical problem, we use the example of a biological information acquisition system - radio-tagged whales - as nodes in an ad hoc network. We derive an analytical formula for the distribution of end-to-end delays and calculate the storage requirements. We further extend SWIM by allowing multi-tiered operation; which in our biological information acquisition system could be realized through seabirds acting as mobile data collection nodes.
531|Age Matters: Efficient Route Discovery in Mobile Ad Hoc Networks Using Encounter Ages|We propose FResher Encounter SearcH (FRESH), a simple algorithm for efficient route discovery in mobile ad hoc networks. Nodes keep a record of their most recent encounter times with all other nodes. Instead of searching for the destination, the source node searches for any intermediate node that encountered the destination more recently than did the source node itself. The intermediate node then searches for a node that encountered the destination yet more recently, and the procedure iterates until the destination is reached. Therefore, FRESH replaces the single network-wide search of current proposals with a succession of smaller searches, resulting in a cheaper route discovery. Routes obtained are loop-free. The performance of such...
532|Mobility Prediction and Routing in Ad Hoc Wireless Networks|Wireless networks allow a more flexible model of communication than traditional networks since the user is not limited to a fixed physical location. Unlike cellular wireless networks, an ad hoc wireless network does not have any fixed communication infrastructure. For an active connection, the end host as well as the intermediate nodes can be mobile. Therefore routes are subject to frequent disconnections. In such an environment, it is important to minimize disruptions caused by the changing topology for critical application such as voice and video. This presents a difficult challenge for routing protocols, since rapid reconstruction of routes is crucial in the presence of topology changes. By exploiting non-random behaviors for the mobility patterns that mobile users exhibit, we can predict the future state of network topology and perform route reconstruction proactively in a timely manner. Moreover, by using the predicted information on the network topology, we can eliminate transmis...
533|CarNet: A Scalable Ad Hoc Wireless Network System|CarNet is an application for a large ad hoc mobile network system that scales well without requiring a xed network infrastructure to route messages. CarNet places radio nodes in cars, which communicate using Grid, a novel scalable routing system. Grid uses geographic forwarding and a scalable distributed location service to route packets from car to car without ooding the network. CarNet will support IP connectivity as well as applications such as cooperative highway congestion monitoring, eet tracking, and discovery of nearby points of interest. 1 Introduction The Internet has evolved in a way that sacri ces dynamism in favor of scale: it groups nodes into an addressing and routing hierarchy that inhibits movement, and it depends on xed physical infrastructure that inhibits rapid deployment. We are designing a scalable and dynamic network architecture called Grid, which will enable new kinds of applications and will be easier to deploy than existing technology. We desire several...
534|Capacity, Delay and Mobility in Wireless Ad-Hoc Networks|Network throughput and packet delay are two important parameters in the design and the evaluation of routing protocols for ad-hoc networks. While mobility has been shown to increase the capacity of a network, it is not clear whether the delay can be kept low without trading off the throughput. We consider a theoretical framework and propose a routing algorithm which exploits the patterns in the mobility of nodes to provide guarantees on the delay. Moreover, the throughput achieved by the algorithm is only a poly-logarithmic factor off from the optimal. The algorithm itself is fairly simple. In order to analyze its feasibility and the performance guarantee, we used various techniques of probabilistic analysis of algorithms. The approach taken in this paper could be applied to the analyses of some other routing algorithms for mobile ad hoc networks proposed in the literature.
535|Mobility Helps Security in Ad Hoc Networks|Contrary to the common belief that mobility makes security more difficult to achieve, we show that node mobility can, in fact, be useful to provide security in ad hoc networks. We propose a technique in which security associations between nodes are established, when they are in the vicinity of each other, by exchanging appropriate cryptographic material. We show that this technique is generic, by explaining its application to fully self-organized ad hoc networks and to ad hoc networks placed under an (off-line) authority. We also propose an extension of this basic mechanism, in which a security association can be established with the help of a &#034;friend&#034;. We show that our mechanism can work in any network configuration and that the time necessary to set up the security associations is strongly influenced by several factors, including the size of the deployment area, the mobility patterns, and the number of friends; we provide a detailed investigation of this influence.
536|Wearable Computers as Packet Transport Mechanisms in Highly-Partitioned Ad-Hoc Networks|The decreasing size and cost of wearable computers and mobile sensors is presenting new challenges and opportunities for deploying networks. Existing network routing protocols provide reliable communication between nodes and allow for mobility and even ad-hoc deployment. They rely, however, on the assumption of constant end-to-end connectivity in the network. In this paper, we address routing support for ad-hoc, wireless networks under conditions of sporadic connectivity and ever-present network partitions. This work proposes a general framework of agent movement and communication in which mobile computers physically carry packets across network partitions. We then propose algorithms that exploit the relative position of stationary devices and non-randomness in the movement of mobile agents in the network. The learned structure of the network is used to inform an adaptive routing strategy. With a simulation, we evaluate these algorithms and their ability to route packets efficiently through a highly-partitioned network.
537|The minimum latency problem|We are given a set of points pl,...,p. and a symmetric distance matrix (o!ij) giving the distance between pi and pj. We wish to construct a tour that minimizes ~~=1 1(z), where l(i) is
538|Delay Limited Capacity of Ad hoc Networks: Asymptotically Optimal Transmission and Relaying Strategy|The delay limited capacity of an ad hoc wireless network confined to a finite region is investigated. A transmission and relaying strategy making use of the nodes&#039; motion to maximize the throughput is constructed. An approximate expression for the capacity as a function of the maximum allowable delay is obtained. It is found that there exists a critical value of the delay such that: (1) for values of the delay d below critical, the capacity does not benefit appreciably from the motion, (2) for moderate values of the delay d above critical, the capacity that can be achieved by taking advantage of the motion increases   , (3) the dependence of the critical delay on the number of nodes is a very slowly increasing function (n    ) . Finally, asymptotic optimality of the proposed strategy in a certain class is shown.
539|Integrated routing and storage for messaging applications in mobile ad hoc networks|This paper is motivated by the observation that traditional ad hoc routing protocols are not an adequate solution for messaging applications (e.g., e-mail) in mobile ad hoc networks. Routing in ad hoc mobile networks is challenging mainly because of node mobility—the more rapid the rate of movement, the greater the fraction of bad routes and undelivered messages. For applications that can tolerate delays beyond conventional forwarding delays, we advocate a relay-based approach to be used in conjunction with traditional ad hoc routing protocols. This approach takes advantage of node mobility to disseminate messages to mobile nodes. The result is the Mobile Relay Protocol (MRP), which integrates message routing and storage in the network; the basic idea is that if a route to a destination is unavailable, a node performs a controlled local broadcast (a relay) to its immediate neighbors. In a network with sufficient mobility—precisely the situation when conventional routes are likely to be non-existent or broken—it is quite likely that one of the relay nodes to which the packet has been relayed will encounter a node that has a valid, short (conventional) route to the eventual destination, thereby increasing the likelihood that the message will be successfully delivered. Our simulation results under a variety of node movement models demonstrate that this idea can work well for applications that prefer reliability over latency. 1
540|Wireless sensor networks: a survey|This paper describes the concept of sensor networks which has been made viable by the convergence of microelectro-mechanical systems technology, wireless communications and digital electronics. First, the sensing tasks and the potential sensor networks applications are explored, and a review of factors influencing the design of sensor networks is provided. Then, the communication architecture for sensor networks is outlined, and the algorithms and protocols developed for each layer in the literature are explored. Open research issues for the realization of sensor networks are
541|SPINS: Security Protocols for Sensor Networks|As sensor networks edge closer towards wide-spread deployment, security issues become a central concern. So far, the main research focus has been on making sensor networks feasible and useful, and less emphasis was placed on security. We design a suite of security building blocks that are optimized for resource-constrained environments and wireless communication. SPINS has two secure building blocks: SNEP and TESLA. SNEP provides the following important baseline security primitives: Data con£dentiality, two-party data authentication, and data freshness. A particularly hard problem is to provide efficient broad-cast authentication, which is an important mechanism for sensor networks. TESLA is a new protocol which provides authenticated broadcast for severely resource-constrained environments. We implemented the above protocols, and show that they are practical even on minimalistic hardware: The performance of the protocol suite easily matches the data rate of our network. Additionally, we demonstrate that the suite can be used for building higher level protocols. 
542|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
543|I-TCP: Indirect TCP for mobile hosts|Abstract — IP-based solutions to accommodate mobile hosts within existing internetworks do not address the distinctive features of wireless mobile computing. IP-based transport protocols thus suffer from poor performance when a mobile host communicates with a host on the fixed network. This is caused by frequent disruptions in network layer connectivity due to — i) mobility and ii) unreliable nature of the wireless link. We describe the design and implementation of I-TCP, which is an indirect transport layer protocol for mobile hosts. I-TCP utilizes the resources of Mobility Support Routers (MSRs) to provide transport layer communication between mobile hosts and hosts on the fixed network. With I-TCP, the problems related to mobility and the unreliability of wireless link are handled entirely within the wireless link; the TCP/IP software on the fixed hosts is not modified. Using I-TCP on our testbed, the throughput between a fixed host and a mobile host improved substantially in comparison to regular TCP. 1
544|Protocols for self-organization of a wireless sensor network|We present a suite of algorithms for self-organization of wireless sensor networks, in which there is a scalably large number of mainly static nodes with highly constrained energy resources. The protocols further support slow mobility by a subset of the nodes, energy-efficient routing, and formation of ad hoc subnetworks for carrying out cooperative signal processing functions among a set of the nodes.
545|ASCENT: Adaptive self-configuring sensor networks topologies| Advances in microsensor and radio technology will enable small but smart sensors to be deployed for a wide range of environmental monitoring applications. The low per-node cost will allow these wireless networks of sensors and actuators to be densely distributed. The nodes in these dense networks will coordinate to perform the distributed sensing and actuation tasks. Moreover, as described in this paper, the nodes can also coordinate to exploit the redundancy provided by high density so as to extend overall system lifetime. The large number of nodes deployed in these systems will preclude manual configuration, and the environmental dynamics will preclude design-time preconfiguration. Therefore, nodes will have to self-configure to establish a topology that provides communication under stringent energy constraints. ASCENT builds on the notion that, as density increases, only a subset of the nodes are necessary to establish a routing forwarding backbone. In ASCENT, each node assesses its connectivity and adapts its participation in the multihop network topology based on the measured operating region. This paper motivates and describes the ASCENT algorithm and presents analysis, simulation, and experimental measurements. We show that the system achieves linear increase in energy savings as a function of the density and the convergence time required in case of node failures while still providing adequate connectivity. 
546|Smart Dust: Communicating with a Cubic-Millimeter Computer|building virtual keyboards;  . managing inventory control;  . monitoring product quality;  . constructing smart office spaces; and  . providing interfaces for the disabled.  SMART DUST REQUIREMENTS  Smart Dust requires both evolutionary and revolutionary  advances in miniaturization, integration, and  energy management. Designers can use microelectromechanical  systems (MEMS) to build small sensors,  optical communication components, and power supplies,  whereas microelectronics provides increasing  functionality in smaller areas, with lower energy consumption.  Figure 1 shows the conceptual diagram of  a Smart Dust mote. The power system consists of a  thick-film battery, a solar cell with a charge-integrating  capacitor for periods of darkness, or both.  Depending on its objective, the design integrates various  sensors, including light, temperature, vibration,  magnetic field, acoustic, and wind shear, onto the  mote. An integrated circuit provides sensor-signal pr
547|The Simulation and Evaluation of Dynamic Voltage Scaling Algorithms|The reduction of energy consumption in microprocessors can be accomplished without impacting the peak performance through the use of dynamic voltage scaling (DVS). This approach varies the processor voltage under software control to meet dynamically varying performance requirements. This paper presents a foundation for the simulation and analysis of DVS algorithms. These algorithms are applied to a benchmark suite specifically targeted for PDA devices. 2.
548|Comparing Algorithms for Dynamic Speed-Setting of a Low-Power CPU|To take advantage of the full potential of ubiquitous computing, we will need systems which minimize powerconsumption. Weiser et al. and others have suggested that this may be accomplished by a CPU which dynamically changes speed and voltage, thereby saving energy by spreading run cycles into idle time. Here we continue this research, using a simulation to compare a number of policies for dynamic speed-setting. Our work clarifies a fundamental power vs. delay tradeoff, as well as the role of prediction and of smoothing in dynamic speed-setting policies. We conclude that success seemingly depends more on simple smoothing algorithms than on sophisticated prediction techniques, but defer to the replication of these results on future variable-speed systems. 1 Introduction  Recent developments in ubiquitous computing make it likely that the future will see a proliferation of cordless computing devices. Clearly it will be advantageous for such devices to minimize power-consumption. The top p...
549|Power Efficient Organization of Wireless Sensor Networks|Abstract-- Wireless sensor networks have emerged recently as an effective way of monitoring remote or inhospitable physical environments. One of the major challenges in devising such networks lies in the constrained energy and computational resources available to sensor nodes. These constraints must be taken into account at all levels of system hierarchy. The deployment of sensor nodes is the first step in establishing a sensor network. Since sensor networks contain a large number of sensor nodes, the nodes must be deployed in clusters, where the location of each particular node cannot be fully guaranteed a priori. Therefore, the number of nodes that must be deployed in order to completely cover the whole monitored area is often higher than if a deterministic procedure were used. In networks with stochastically placed nodes, activating only the necessary number of sensor nodes at any particular moment can save energy. We introduce a heuristic that selects mutually exclusive sets of sensor nodes, where the members of each of those sets together completely cover the monitored area. The intervals of activity are the same for all sets, and only one of the sets is active at any time. The experimental results demonstrate that by using only a subset of sensor nodes at each moment, we achieve a significant energy savings while fully preserving coverage. I.
550|Achieving MAC Layer Fairness in Wireless Packet Networks|Link-layer fairness models that have been proposed for wireline and packet cellular networks cannot be generalized for shared channel wireless networks because of the unique characteristics of the wireless channel, such as location-dependent contention, inherent conflict between optimizing channel utilization and achieving fairness, and the absence of any centralized control. In this paper, we propose a general analytical framework that captures the unique characteristics of shared wireless channels and allows the modeling of a large class of systemwide fairness models via the specification of per-flow utility functions. We show that system-wide fairness can be achieved without explicit global coordination so long as each node executes a contention resolution algorithm that is designed to optimize its local utility function. We present a general mechanism for translating a given fairness model in our framework into a corresponding contention resolution algorithm. Using this translation...
551|Physical Layer Driven Protocol and Algorithm Design for Energy-Efficient Wireless Sensor Networks|The potential for collaborative, robust networks of microsensors has attracted a great deal of research attention. For the most part, this is due to the compelling applications that will be enabled once wireless microsensor networks are in place
553|Upper Bounds on the Lifetime of Sensor Networks|In this paper, we ask a fundamental question concerning the limits of energy e#ciency of sensor networks - What is the upper bound on the lifetime of a sensor network that collects data from a specified region using a certain number of energy-constrained nodes? The answer to this question is valuable for two main reasons. First, it allows calibration of real world data-gathering protocols and an understanding of factors that prevent these protocols from approaching fundamental limits. Secondly, the dependence of lifetime on factors like the region of observation, the source behavior within that region, basestation location, number of nodes, radio path loss characteristics, e#ciency of node electronics and the energy available on a node, is exposed. This allows architects of sensor networks to focus on factors that have the greatest potential impact on network lifetime. By employing a combination of theory and extensive simulations of constructed networks, we show that in all data gathe...
554|Robust Range Estimation Using Acoustic and Multimodal Sensing|Many applications of robotics and embedded sensor technology can benet from ne-grained localization. Fine-grained localization can simplify multi-robot collaboration, enable energy ecient multi-hop routing for low-power radio networks, and enable automatic calibration of distributed sensing systems. In this work we focus on range estimation, a critical prerequisite for ne-grained localization. While many mechanisms for range estimation exist, any individual mode of sensing can be blocked or confused by the environment. We present and analyze an acoustic ranging system that performs well in the presence of many types of interference, but can return incorrect measurements in non-line-of-sight conditions. We then suggest how evidence from an orthogonal sensory channel might be used to detect and eliminate these measurements. This work illustrates the more general research theme of combining multiple modalities to obtain robust results. 1 
555|Exposure In Wireless Ad-Hoc Sensor Networks|Wireless ad-hoc sensor networks will provide one of the missing connections between the Internet and the physical world. One of the fundamental problems in sensor networks is the calculation of coverage. Exposure is directly related to coverage in that it is a measure of how well an object, moving on an arbitrary path, can be observed by the sensor network over a period of time.  In addition to the informal definition, we formally define exposure and study its properties. We have developed an efficient and effective algorithm for exposure calculation in sensor networks, specifically for finding minimal exposure paths. The minimal exposure path provides valuable information about the worst case exposure-based coverage in sensor networks. The algorithm works for any given distribution of sensors, sensor and intensity models, and characteristics of the network. It provides an unbounded level of accuracy as a function of run time and storage. We provide an extensive collection of experimental results and study the scaling behavior of exposure and the proposed algorithm for its calculation.  I. 
556|Sensor Information Networking Architecture and Applications|This article introduces a sensor information networking architecture, called SINA, that facilitates querying, monitoring, and tasking of sensor networks. SINA plays the role of a middleware that abstracts a network of sensor nodes as a collection of massively distributed objects. The SINA&#039;s execution environment provides a set of configuration and communication primitives that enable scalable and energy-efficient organization of and interactions among sensor objects. On top the execution environment is a programmable substrate that provides mechanisms to create associations and coordinate activities among sensor nodes. Users then access information within a sensor network using declarative queries, or perform tasks using programming scripts.
557|Adaptive Frame Length Control for Improving Wireless Link Throughput, Range, and Energy Efficiency|Wireless network links are characterized by rapidly  time varying channel conditions and battery energy limitations at  the wireless mobile user nodes. Therefore static link control techniques  that make sense in comparatively well behaved wired links  do not necessarily apply to wireless links. New adaptive link layer  control techniques are needed to provide robust and energy efficient  operation even in the presence of orders of magnitude variations  in bit error rates and other radio channel conditions. For  example, recent research has advocated adaptive link layer techniques  such as adaptive error control [Lettieri97], channel state  dependent protocols [Bhagwat96, Fragouli97], and variable  spreading gain [Chien97]. In this paper we explore one such  adaptive technique: dynamic sizing of the MAC layer frame, the  atomic unit that is sent through the radio channel. A trade-off  exists between the desire to reduce header and physical layer  overhead by making frames large, and th...
558|Scalable coordination for wireless sensor networks: self-configuring localization systems|Pervasive networks of micro-sensors and actuators offer to revolutionize the ways in which we understand and construct complex physical systems. Sensor networks must be scalable, long-lived and robust systems, overcoming energy limitations and a lack of pre-installed infrastructure. We explore three themes in the design of self-configuring sensor networks: tuning density to trade operational quality against lifetime; using multiple sensor modalities to obtain robust measurements; and exploiting fixed environmental characteristics. We illustrate these themes through the problem of localization, which is a key building block for sensor systems that itself requires coordination.
559|Error Control and Energy Consumption in Communications for Nomadic Computing|We consider the problem of communications over a wireless channel in support of data transmissions from the  perspective of small portable devices that must rely on limited battery energy. We model the channel outages as statistically  correlated errors. Classic ARQ strategies are found to lead to a considerable waste of energy, due to the large number of  transmissions. The use of finite energy sources in the face of dependent channel errors leads to new protocol design criteria. As an  example, a simple probing scheme, which slows down the transmission rate when the channel is impaired, is shown to be more  energy efficient, with a slight loss in throughput. A modified scheme that yields slightly better performance but requires some  additional complexity is also studied. Some references on the modeling of battery cells are discussed to highlight the fact that  battery charge capacity is strongly influenced by the available &#034;relaxation time&#034; between current pulses. A formal approach ...
560|Intelligent Medium Access for Mobile Ad Hoc Networks with Busy Tones and Power Control|In a mobile ad-hoc networks (MANET), one essential issue is how to increase channel utilization while avoiding the hidden-terminal and the exposed terminal problems. Several MAC protocols, such as RTS/CTS-based and busytone-based schemes, have been proposed to alleviate these problems. In this paper, we explore the possibility of combining the concept of power control with the RTS/CTS-based and busy-tone-based protocols to further increase channel utilization. A sender will use an appropriate power level to transmit its packets so as to increase the possibility of channel reuse. The possibility of using discrete, instead of continuous, power levels is also discussed. Through analyses and simulations, we demonstrate the advantage of our new MAC protocol. This, together with the extra bene ts such as saving battery energy and reducing cochannel interference, does show a promising direction to enhance the performance of MANETs.
561|What is complexity|SFI Working Papers contain accounts of scientific work of the author(s) and do not necessarily represent the views of the Santa Fe Institute. We accept papers intended for publication in peer-reviewed journals or proceedings volumes, but not papers that have already appeared in print. Except for papers by our external faculty, papers must be based on work done at SFI, inspired by an invited visit to or collaboration at SFI, or funded by an SFI grant. ©NOTICE: This working paper is included by permission of the contributing author(s) as a means to ensure timely distribution of the scholarly and technical work on a non-commercial basis. Copyright and all rights therein are maintained by the author(s). It is understood that all persons copying this information will adhere to the terms and constraints invoked by each author&#039;s copyright. These works may be reposted only with the explicit permission of the copyright holder. www.santafe.edu
562|Design Considerations for Distributed Microsensor Systems|Wireless distributed microsensor systems will enable the reliable monitoring and control of a variety of applications that range from medical and home security to machine diagnosis, chemical/biological detection and other military applications. The sensors have to be designed in a highly integrated fashion, optimizing across all levels of system abstraction, with the goal of minimizing energy dissipation. This paper addresses some of the key design considerations for future microsensor systems including the network protocols required for collaborative sensing and information distribution, system partitioning considering computation and communication costs, low energy electronics, power system design and energy harvesting techniques.  1. Introduction  Over the last few years, the design of micropower wireless sensor systems has gained increasing importance for a variety of civil and military applications. The Low Power Wireless Integrated Microsensors (LWIM) project has made major advan...
563|DataSpace: Querying and Monitoring Deeply Networked Collections in Physical Space|In this article we introduce a new conception of three-dimensional DataSpace, which is physical space enhanced by connectivity to the network.
564|Dynamic Voltage Scaling Techniques for Distributed Microsensor Networks|Distributed microsensor networks promise a versatile and robust platform for remote environment monitoring. Crucial to long system lifetimes for these microsensors are algorithms and protocols that provide the option of trading quality for energy savings. Dynamic voltage scaling on the sensor node&#039;s processor enables energy savings from these scalable algorithms. We demonstrate dynamic voltage scaling on the beginnings of a sensor node prototype, which currently consists of a commercial processor, a digitally adjustable DC-DC regulator, and a power-aware operating system.  1. Introduction  Distributed microsensor networks are emerging as a compelling new hardware platform for remote environment monitoring [1]. Researchers are considering a range of applications including remote climate monitoring, battlefield surveillance, and intra-machine monitoring [2]. A distributed microsensor network consists of many small, expendable, battery-powered wireless nodes. Once the nodes are deployed t...
565|Power-Aware Communication for Mobile Computers|Recently, the mobile community has focused on techniques for reducing energy consumption  for mobile hosts. These power management techniques typically target communication  devices such as wireless network interfaces, aiming to reduce usage, and thus energy consumption,  of the particular device itself. We observe that optimization of a single device&#039;s energy  consumption, without considering the effect of the strategy on the rest of the machine, can  have negative consequences. We propose power management techniques addressing mobile host  communications that encompass all components of a mobile host in an effort to optimize total  energy consumption. Specifically, we propose runtime adaptation of communication parameters  in order to minimize the energy consumed during active data transfer. Information about the  network environment is used to drive such adaptations in an effort to compensate for the effect  of dynamic service from wireless communication device on the energy consume...
566|A versatile architecture for the distributed sensor integration problem|Abstract-The computational issues related to information in-tegration in multisensor systems and distributed sensor networks has become an active area of research. From a computational viewpoint, the efficient extraction of information from noisy and faulty signals emanating from many sensors requires the solution of problems related a) to the architecture and fault tolerance of the distributed sensor network, b) to the proper synchronization of sensor signals, and c) to the integration of information to keep the communication and the centralized processing require-ments small. In this paper, we propose a versatile architecture for a distributed sensor network which consists of a multilevel network with the nodes (processing elementlsensor pairs) at each level interconnected as a deBruijn network. We show that this multilevel network has reasonable fault tolerance, admits simple and decentralized routing, and offers easy extensibility. We model information from sensors as real valued intervals and derive an interesting property related to information integration in the presence of faults. Using this property, the search for a fault is narrowed down to two potentially faulty sensors or communication links. In a distributed environment, information has to be integrated from “temporally close ” signals in the presence of imperfect clocks in a distributed environment. We apply the results of past research in this area to state various relationships between the clocks of the processing elements in the network for proper information integration. Index Terms- Abstract estimate, clock synchronization, dis-tributed sensor networks, deBruijn networks, fault tolerance, information integration. I.
567|The Mobile Patient: Wireless Distributed Sensor Networks for Patient Monitoring and Care|In this paper, the concept of a 3 layer distributed sensor network for patient monitoring and care is introduced. The envisioned network has a leaf node layer (consisting of patient sensors), a intermediate node layer (consisting of the supervisory processor residing with each patient) and the root node processor (residing at a central monitoring facility). The introduced paradigm has the capability of dealing with the bandwidth bottleneck at the wireless patient - root node link and the processing bottleneck at the central processor or root node of the network.
568|Energy-efficient link layer for wireless microsensor network |Wireless microsensors are being used to form large, dense networks for the purposes of long-term environmental sensing and data collection. Unfortunately, these networks are typically deployed in remote environments where energy sources are limited. Thus, designing fault-tolerant wire-less microsensor networks with long system lifetimes can be challenging. By applying energy-efficient techniques at all levels of the system hierarchy, system lifetime can be ex-tended. In this paper, energy-efficient techniques that adapt underlying communication parameters will be presented in the context of wireless microsensor networks. In particular, the effect of adapting link and physical layer parameters, such as output transmit power and error control coding, on system energy consumption will be examined. 1.
569|Diagnosis of Sensor Networks|As sensor nodes are embedded into physical environments and becoming integral parts of our daily lives, sensor networks will become the important nerve systems that monitor and actuate our physical environments. We define the process of monitoring the status of a sensor network and figuring out the problematic sensor nodes sensor network diagnosis. However, the high sensor node-to-manager ratio makes it extremely difficult to pay special attention to any individual node. In addition, the response implosion problem, which occurs when a high volume of incoming replies triggered by diagnosis queries cause the central diagnosing node to become a bottleneck, is one major obstacle to be overcome. In this paper, we describe approaches to addressing the response implosion problem in sensor network diagnosis. We will also present simulation experiments on the performance of these approaches, and discuss presentation schemes for diagnostic results.
570|A selforganizing approach to data forwarding in largescale sensor networks|Abstracf- The large number of networked sensors, frequent sensor failures and stringent energy constraints pose unique design challenges for data forwarding in wireless sensor networks. In this paper, we present a new approach to data forwarding in sensor networks that effectively addresses these design issues. Our approach organizes sensors into a dynamic, self-optimizing multicast tree-based forwarding hierarchy, which is data centric and robust to node failures. We demonstrate the effectiveness of our design through simulations. I.
571|All-Digital Impulse Radio For MUI/ISI-Resilient Multi-User Communications Over Frequency-Selective Multipath Channels|Impulse radio (IR) is an ultra-wideband system with attractive features for baseband asynchronous multiple access (MA), multimedia services, and tactical wireless communications. Implemented with analog components, the continuoustime IRMA model utilizes pulse-position modulation (PPM) and random time-hopping codes to alleviate multipath effects and suppress multiuser interference (MUI). We introduce a novel continuous-time Multiple Input Multiple Output (MIMO) PPMIRMA scheme, and derive its discrete-time equivalent model. Relying on a time-division-duplex access protocol and orthogonal user codes, we design composite linear and non-linear receivers for the downlink. The linear step eliminates MUI deterministically and accounts for frequency-selective multipath, while a Maximum Likelihood (ML) receiver performs symbol detection.  1. INTRODU7 ION  The idea of transmitting digital information using ultra-short impulses was first presented in [10] and called Impulse Radio.It  relies on PPM...
572|Overcast: Reliable Multicasting with an Overlay Network|Overcast is an application-level multicasting system that can be incrementally deployed using today&#039;s Internet infrastructure. These properties stem from Overcast&#039;s implementation as an overlay network. An overlay network consists of a collection of nodes placed at strategic locations in an existing network fabric. These nodes implement a network abstraction on top of the network provided by the underlying substrate network. Overcast  provides
573|A Case for End System Multicast|Abstract — The conventional wisdom has been that IP is the natural protocol layer for implementing multicast related functionality. However, more than a decade after its initial proposal, IP Multicast is still plagued with concerns pertaining to scalability, network management, deployment and support for higher layer functionality such as error, flow and congestion control. In this paper, we explore an alternative architecture that we term End System Multicast, where end systems implement all multicast related functionality including membership management and packet replication. This shifting of multicast support from routers to end systems has the potential to address most problems associated with IP Multicast. However, the key concern is the performance penalty associated with such a model. In particular, End System Multicast introduces duplicate packets on physical links and incurs larger end-to-end delays than IP Multicast. In this paper, we study these performance concerns in the context of the Narada protocol. In Narada, end systems selforganize into an overlay structure using a fully distributed protocol. Further, end systems attempt to optimize the efficiency of the overlay by adapting to network dynamics and by considering application level performance. We present details of Narada and evaluate it using both simulation and Internet experiments. Our results indicate that the performance penalties are low both from the application and the network perspectives. We believe the potential benefits of transferring multicast functionality from end systems to routers significantly outweigh the performance penalty incurred. I.
574|Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web|We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.  
575|A Survey of active network Research|Active networks are a novel approach to network architecture in which the switches of the network perform customized computations on the messages flowing through them. This approach is motivated by both lead user applications, which perform user-driven computation at nodes within the network today, and the emergence of mobile code technologies that make dynamic network service innovation attainable. In this paper, we discuss two approaches to the realization of active networks and provide a snapshot of the current research issues and activities. Introduction – What Are Active Networks? In an active network, the routers or switches of the network perform customized computations on the messages flowing through them. For example, a user of an active network could send a “trace ” program to each router and arrange for the program to be executed when their packets are processed. Figure 1 illustrates how the routers of an IP
576|A Hierarchical Internet Object Cache| This paper discusses the design andperformance  of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We believe that the conventional wisdom, that the benefits of hierarchical file caching do not merit the costs, warrants reconsideration in the Internet environment.  The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that the cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape&#039;s Netsite and NCSA 1.4 servers.  Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems.  
577|  Parity-Based Loss Recovery for Reliable Multicast Transmission |We investigate how FEC (Forward Error Correction) can be combined with ARQ (Automatic Repeat Request) to achieve scalable reliable multicast transmission. We consider the two scenarios where FEC is introduced as a transparent layer underneath a reliable multicast layer that uses ARQ, and where FEC and ARQ are both integrated into a single layer that uses the retransmission of parity data to recover from the loss of original data packets. Toevaluate the performance improvements due to FEC, we consider different types of loss behaviors (spatially or temporally correlated loss, homogeneous or heterogeneous loss) and loss rates for up to 10 6 receivers. Our results show that introducing FEC as a layer below ARQ can improve multicast transmission efficiency and scalability and that there are substantial additional improvements when the two are integrated.
578|The Case for Geographical Push-Caching|Most existing wide-area caching schemes are client initiated. Decisions on when and where to cache information are made without the benefit of the server&#039;s global knowledge of the situation. We believe that the server should play a role in making these caching decisions, and we propose  geographical push-caching as a way of bringing the server back into the loop. The World Wide Web is an excellent example of a wide-area system that will benefit from geographical push-caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. 1 Introduction  The World-Wide Web [1] operates for the most part as a cache-less distributed system. When two neighboring clients retrieve a document from the same server, the document is sent twice. This is inefficient, especially considering the ease with which Web browsers allow users to transfer large multimedia documents. To combat this problem, some Web browsers have begun to add local client caches. These prevent ...
579|IP Multicast Channels: Express Support for Large-scale Single-source Applications|In the IP multicast model, a set of hosts can be aggregated into a group of hosts with one address, to which any host can send. However, Internet TV, distance learning, file distribution and other emerging large-scale multicast applications strain the current realization of this model, which lacks a basis for charging, lacks access control, and is difficult to scale.  This paper proposes an extension to IP multicast to support the channel model of multicast and describes a specific realization called EXPlicitly REquested SingleSource (EXPRESS) multicast. In this model, a multicast  channel has exactly one explicitly designated source,  and zero or more channel subscribers. A single protocol supports both channel subscription and efficient collection of channel information such as subscriber count. We argue that EXPRESS addresses the aforementioned problems, justifying this multicast service model in the Internet.   
580|Autonet: A high-speed, self-configuring local area network using point-to-point links|Read it as an adjunct to the lectures on distributed systems, links, and switching. It gives a fairly complete description of a working highly-available switched network providing daily service to about 100 hosts. The techniques used to obtain high reliability and fault-tolerance are characteristic of many distributed systems, not just of networks. The paper also makes clear the essential role of software in modern networks.
581|Detour: a Case for Informed Internet Routing and Transport|Despite its obvious success, robustness, and scalability, the Internet suffers from a number of end-to-end performance and availability problems. In this paper, we attempt to quantify the Internet&#039;s inefficiencies and then we argue that Internet behavior can be improved by spreading intelligent routers at key access and interchange points to actively manage traffic. Our Detour prototype aims to demonstrate practical benefits to end users, without penalizing non-Detour users, by aggregating traffic information across connections and using more efficient routes to improve Internet performance. 1 Introduction  By any metric, the Internet has scaled remarkably; from 4 nodes in 1969 to an estimated 25 million hosts and 100 million users today. This reflects a sustained growth rate over three decades of roughly 80% per year, all while providing nearly continuous service. As a system, the Internet&#039;s growth has been matched only by the major infrastructure projects of the early 1900&#039;s: the ele...
582|Speculative Data Dissemination and Service to Reduce Server Load, Network Traffic and Service Time in . . .|We present two server-initiated protocols to improve the performance of distributed information systems (e.g. WWW). Our first protocol is a hierarchical data dissemination mechanism that allows information to propagate from its producers to servers that are closer to its consumers. This dissemination reduces network traffic and balances load amongst servers by exploiting geographic and temporal locality of reference properties exhibited in client access patterns. Our second protocol relies on &#034;speculative service&#034;, whereby a request for a document is serviced by sending, in addition to the document requested, a number of other documents that the server speculates will be requested inthenear future. This speculation reduces service time by exploiting the spatial locality of reference property. We present results of trace-driven simulations that quantify the attainable performance gains for both protocols.  
583|FLIP: an Internetwork Protocol for Supporting Distributed Systems|Most modern network protocols give adequate support for traditional applications such as file transfer and remote login. Distributed applications, however, have different requirements (e.g., efficient atmost-once remote procedure call even in the face of processor failures). Instead of using ad-hoc protocols to meet each of the new requirements, we have designed a new protocol, called the Fast Local Internet Protocol (FLIP), that provides a clean and simple integrated approach to these new requirements. FLIP is an unreliable message protocol that provides both point-to-point communication and multicast communication, and requires almost no network management. Furthermore, by using FLIP we have simplified higher-level protocols such as remote procedure call and group communication, and enhanced support for process migration and security. A prototype implementation of FLIP has been built as part of the new kernel for the Amoeba distributed operating system, and is in daily use. Measurements of its performance are presented. 1.
584|Seamlessly Selecting the Best Copy from Internet-Wide Replicated Web Servers|. The explosion of the web has led to a situation where a majority of the traffic on the Internet is web related. Today, practically all of the popular web sites are served from single locations. This necessitates frequent long distance network transfers of data (potentially repeatedly) which results in a high response time for users, and is wasteful of the available network bandwidth. Moreover, it commonly creates a single point of failure between the web site and its Internet provider. This paper presents a new approach to web replication, where each of the replicas resides in a different part of the network, and the browser is automatically and  transparently directed to the &#034;best&#034; server. Implementing this architecture for popular web sites will result in a better response-time and a higher availability of these sites. Equally important, this architecture will potentially cut down a significant fraction of the traffic on the Internet, freeing bandwidth for other uses. 1. Introducti...
585|Safe Kernel Extensions Without Run-Time Checking |Abstract This paper describes a mechanism by which an operating system kernel can determine with certainty that it is safe to execute a binary supplied by an untrusted source. The kernel first defines a safety policy and makes it public. Then, using this policy, an application can provide binaries in a special form called proof-carrying code, or simply PCC. Each PCC binary contains, in addition to the native code, a formal proof that the code obeys the safety policy. The kernel can easily validate the proof without using cryptography and without consulting any external trusted entities. If the validation succeeds, the code is guaranteed to respect the safety policy without relying on run-time checks. The main practical difficulty of PCC is in generating the safety proofs. In order to gain some preliminary experience with this, we have written several network packet filters in hand-tuned DEC Alpha assembly language, and then generated PCC binaries for them using a special prototype assembler. The PCC binaries can be executed with no run-time overhead, beyond a one-time cost of 1 to 3 milliseconds for validating the enclosed proofs. The net result is that our packet filters are formally guaranteed to be safe and are faster than packet filters created using Berkeley Packet Filters, Software Fault Isolation, or safe languages such as Modula-3.
586|Improving TCP/IP performance over wireless networks|TCP is a reliable transport protocol tuned to perform well in traditional networks made up of links with low bit-error rates. Networks with higher bit-error rates, such as those with wireless links and mobile hosts, violate many of the assumptions made by TCP, causing degraded end-to-end performance. In tbis paper, we describe the design and implementation of a simple protocol, called the snoop protocol, that improves TCP performance in wireless networks. The protocol modifies network-layer software mainly at a base station and preserves end-to-end TCP semantics. The main idea of the protocol is to cache packets at the base station and perform local retransmissions across the wireless link. We have implemented the snoop protocol on a wireless testbed consisting of IBM ThinkPad laptops and i486 base
587|Making Paths Explicit in the Scout Operating System|This paper makes a case for paths as an explicit abstraction in operating system design. Paths provide a unifying infrastructure for several OS mechanisms that have been introduced in the last several years, including fbufs, integrated layer processing, packet classifiers, code specialization, and migrating threads. This paper articulates the potential advantages of a path-based OS structure, describes the specific path architecture implemented in the Scout OS, and demonstrates the advantages in a particular application domain---receiving, decoding, and displaying MPEG-compressed video. 1 Introduction  Layering is a fundamental structuring technique with a long history in system design. From early work on layered operating systems and network architectures [12, 32], to more recent advances in stackable systems [27, 15, 14, 26], layering has played a central role in managing complexity, isolating failure, and enhancing configurability. This paper describes a complementary, but equally f...
588|An Application Level Video Gateway|The current model for multicast transmission of video over the Internet assumes that a fixed average bandwidth is uniformly present throughout the network. Consequently, sources limit their transmission rates to accommodate the lowest bandwidth links, even though high-bandwidth connectivity might be available to many of the participants. We propose an architecture where a video transmission can be decomposed into multiple sessions with different bandwidth requirements using an application-level gateway. Our video gateway transparently connects pairs of sessions into a single logical conference by manipulating the data and control information of the video streams. In particular, the gateway performs bandwidth adaptation through transcoding and rate-control. We describe an efficient algorithm for transcoding Motion-JPEG to H.261 that runs in real-time on standard workstations. By making the Real-time Transport Protocol (RTP) an integral component of our architecture, the video gateway in...
589|An Architecture for Active Networking|Active networking offers a change in the usual network paradigm: from passive carrier of bits to a more general computation engine. The implementation of such a change is likely to enable radical new applications that cannot be foreseen today. Large-scale deployment, however, involves significant challenges in interoperability, security, and scalability. In this paper we define an active networking architecture in which user control the invocation of pre-defined, network-based functions through control information in packet headers. After defining our active networking architecture, we consider a problem (namely, network congestion) that may benefit in the near-term from active networking, and thus may help justify migration to this new paradigm. Given an architecture allowing applications to exercise some control over network processing, the bandwidth allocated to each application&#039;s packets can be reduced in a manner that is tailored to the application, rather than being applied gener...
590|`C: A Language for High-Level, Efficient, and Machine-independent Dynamic Code Generation|Dynamic code generation allows specialized code sequences to be crafted using runtime information. Since this  information is by definition not available statically, the use of dynamic code generation can achieve performance  inherently beyond that of static code generation. Previous attempts to support dynamic code generation have been  low-level, expensive, or machine-dependent. Despite the growing use of dynamic code generation, no mainstream  language provides flexible, portable, and efficient support for it. We describe

591|Towards Programmable Networks|Intermediate nodes (e.g., routers, switches) of current networks, in contrast with end nodes (e.g., PCs workstations), are vertically integrated closed systems. Their functions, mostly implemented by embedded software, are rigidly built-in by intermediate nodes vendors. Vendors must follow designs dictated by slow and intractable standard committees rather than pursue rapid introduction of innovative costeffective technologies. There is thus a need for new technologies that would enable programming intermediate nodes with the same simplicity of programming end-nodes. This paper describes the NetScript project, pursuing agent-based middleware for programming functions of intermediate network nodes. Delegated agents are used to deploy functions in intermediate nodes. The NetScript programming language provides means to script processing of packet streams; it is particularly suitable to program routing, packet analyzers or signalling functions. This paper describes an architecture for pro...
592|The ACTIVE IP Option|In this paper, we discuss our work on an active network architecture in which passive packets are replaced with active capsules --- encapsulated program fragments that are executed at each switch they traverse. This approach allows application-specific processing to be injected into the network. The accessibility of computation and storage &#034;within&#034; the network provides a substrate that can be tailored to build global applications, including those that invoke customized multicast and merge processing. We describe an extension to the IP options mechanism that supports the embedding of program fragments in datagrams and the evaluation of these fragments as they traverse the Internet. The active option provides a generic approach to the extension of the IP network service.
593|Protocol boosters|This paper describes a new methodology for protocol design, using incremental construction of the protocol from elements called “protocol boosters ” on an as-needed basis. Protocol boosters allow: (1) dynamic protocol customization to heterogeneous environments, and (2) rapid protocol evolution. Unlike alternative adaptation approaches, such as link layer, conversion, and termination protocols, protocol boosters are both robust (end-to-end protocol messages are not modified) and maximize efficiency (does not replicate the functionality of the end-to-end protocol). We give examples of error and congestion control boosters and give initial results from booster implementations. 1
594|Liquid Software: A New Paradigm for Networked Systems|This paper introduces the idea of dynamically moving functionality in a network---between clients and servers, and between hosts at the edge of the network and nodes inside the network. At the heart of moving functionality is the ability to support mobile code---code that is not tied to any single machine, but instead can easily move from one machine to another. Mobile code has been studied mostly for application-level code. This paper explores its use for all facets of the network, and in a much more general way. Issues of efficiency, interface design, security, and resource allocation, among others, are addressed. We use the term liquid software to describe the complete picture---liquidsoftware is an entire infrastructure for dynamically moving functionality throughout a network. We expect liquid software to enble new paradigms, such as active networks that allow users and applications to customize the network by interjecting code into it. Department of Computer Science The Univers...
595|Designing an Academic Firewall: Policy, Practice, . . .|Corporate network firewalls are well-understood and are becoming commonplace. These firewalls establish a security perimeter that aims to block (or heavily restrict) both incoming and outgoing network communication. We argue that these firewalls are neither effective nor appropriate for academic or corporate research environments needing to maintain information security while still supporting the free exchange of ideas. In this paper, we present the Stanford University Research Firewall (SURF), a network firewall design that is suitable for a research environment. While still protecting information and computing resources behind the firewall, this firewall is less restrictive of outward information flow than the traditional model; can be easily deployed; and can give internal users the illusion of unrestricted e-mail, anonymous FTP, and WWW connectivity to the greater Internet. Our experience demonstrates that an adequate firewall for a research environment can be constructed for minim...
596|Error and attack tolerance of complex networks|Many complex systems display a surprising degree of tolerance against errors. For example, relatively simple organisms grow, persist and reproduce despite drastic pharmaceutical or environmental interventions, an error tolerance attributed to the robustness of the underlying metabolic network [1]. Complex communication networks [2] display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these and other complex systems is often attributed to the redundant wiring of the functional web defined by the systems’ components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web (www) [3–5], Internet [6], social networks [7] or a cell [8], display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However,
598|On Power-Law Relationships of the Internet Topology|Despite the apparent randomness of the Internet, we discover some surprisingly simple power-laws of the Internet topology. These power-laws hold for three snapshots of the Internet, between November 1997 and December 1998, despite a 45% growth of its size during that period. We show that our power-laws fit the real data very well resulting in correlation coefficients of 96% or higher. Our observations provide a novel perspective of the structure of the Internet. The power-laws describe concisely skewed distributions of graph properties such as the node outdegree. In addition, these power-laws can be used to estimate important parameters such as the average neighborhood size, and facilitate the design and the performance analysis of protocols. Furthermore, we can use them to generate and select realistic topologies for simulation purposes.  
599|The Large-Scale Organization of Metabolic Networks|In a cell or microorganism the processes that generate mass, energy, information transfer, and cell fate specification are seamlessly integrated through a complex network of various cellular constituents and reactions. However, despite the key role these networks play in sustaining various cellular functions, their large-scale structure is essentially unknown. Here we present the first systematic comparative mathematical analysis of the metabolic networks of 43 organisms representing all three domains of life. We show that, despite significant variances in their individual constituents and pathways, these metabolic networks display the same topologic scaling properties demonstrating striking similarities to the inherent organization of complex non-biological systems. This suggests that the metabolic organization is not only identical for all living organisms, but complies with the design principles of robust and error-tolerant networks, and may represent a common blueprint for the large-scale organization of interactions among all cellular constituents.
601|The Escherichia coli MG1655 in silico metabolic genotype: Its definition, characteristics, and capabilities|The Escherichia coli MG1655 genome has been completely sequenced. The annotated sequence, biochemical information, and other information were used to reconstruct the E. coli metabolic map. The stoichiometric coefficients for each metabolic enzyme in the E. coli metabolic map were assembled to construct a genomespecific stoichiometric matrix. The E. coli stoichiometric matrix was used to define the system’s characteristics and the capabilities of E. coli metabolism. The effects of gene deletions in the central metabolic pathways on the ability of the in silico metabolic network to support growth were assessed, and the in silico predictions were compared with experimental observations. It was shown that based on stoichiometric and capacity constraints the in silico analysis was able to qualitatively predict the growth potential of mutant strains in 86 % of the cases examined. Herein, it is demonstrated that the synthesis of in silico metabolic genotypes based on
602|Noise-based switches and amplifiers for gene expression |02454, U.S.A.ABSTRACT The regulation of cellular function is often controlled at the level of gene transcription. Such genetic regulation usually consists of interacting networks, whereby gene products from a single network can act to control their own expression or the production of protein in another network. Engineered control of cellular function through the design and manipulation of such networks lies within the constraints of current technology. Here we develop a model describing the regulation of gene expression, and elucidate the effects of noise on the formulation. We consider a single network derived from bacteriophage ?, and construct a two-parameter deterministic model describing the temporal evolution of the concentration of ? repressor protein. Bistability in the steadystate protein concentration arises naturally, and we show how the bistable regime is enhanced with the addition of the first operator site in the promotor region. We then show how additive and multiplicative external noise can be used to regulate expression. In the additive case, we demonstrate the utility of such control through the construction of a protein switch, whereby protein production is turned “on ” and “off ” using short noise pulses. In the multiplicative case, we show that small deviations in the transcription rate can lead to large fluctuations in the production of protein, and describe how these fluctuations can be used to amplify protein production significantly. These novel results suggest that an external noise source could be used as a switch and/or amplifier for gene expression. Such a development could have important implications for gene therapy. 1
603|Integrated pathway-genome databases and their role in drug discovery|discovery
604|The Internet&#039;s Achilles&#039; Heel: Error and attack tolerance of complex networks|this paper we demonstrate that such error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We nd that scale-free networks, describing a number of systems, such as the www [3-5], Internet [6], social networks [7] or a cell [8], display an unexpected degree of robustness, the ability of their nodes to communicate being unaected by even unrealistically high failure rates. However, this error tolerance comes at a high price: these networks are extremely vulnerable to attacks, i.e. to the selection and removal of a few nodes that play the most 1
605|Towards an Active Network Architecture|Active networks allow their users to inject customized programs into the nodes of the network. An extreme case, in which we are most interested, replaces packets with &#034;capsules&#034; -- program fragments that are executed at each network router/switch they traverse. Active architectures permit a massive increase in the sophistication of the computation that is performed within the network. They will enable new applications, especially those based on application-specific multicast, information fusion, and other services that leverage network-based computation and storage. Furthermore, they will accelerate the pace of innovation by decoupling network services from the underlying hardware and allowing new services to be loaded into the infrastructure on demand. In this paper, we describe our vision of an active network architecture, outline our approach to its design, and survey the technologies that can be brought to bear on its implementation. We propose that the research community mount a j...
606|A logic of authentication|Questions of belief are essential in analyzing protocols for the authentication of principals in distributed computing systems. In this paper we motivate, set out, and exemplify a logic specifically designed for this analysis; we show how various protocols differ subtly with respect to the required initial assumptions of the participants and their final beliefs. Our formalism has enabled us to isolate and express these differences with a precision that was not previously possible. It has drawn attention to features of protocols of which we and their authors were previously unaware, and allowed us to suggest improvements to the protocols. The reasoning about some protocols has been mechanically verified. This paper starts with an informal account of the problem, goes on to explain the formalism to be used, and gives examples of its application to protocols from the literature, both with shared-key cryptography and with public-key cryptography. Some of the examples are chosen because of their practical importance, while others serve to illustrate subtle points of the logic and to explain how we use it. We discuss extensions of the logic motivated by actual practice -- for example, in order to account for the use of hash functions in signatures. The final sections contain a formal semantics of the logic and some conclusions.  
607|Active Messages: a Mechanism for Integrated Communication and Computation|The design challenge for large-scale multiprocessors is (1) to minimize communication overhead, (2) allow communication to overlap computation, and (3) coordinate the two without sacrificing processor cost/performance. We show that existing message passing multiprocessors have unnecessarily high communication costs. Research prototypes of message driven machines demonstrate low communication overhead, but poor processor cost/performance. We introduce a simple communication mechanism, Active Messages, show that it is intrinsic to both architectures, allows cost effective use of the hardware, and offers tremendous flexibility. Implementations on nCUBE/2 and CM-5 are described and evaluated using a split-phase shared-memory extension to C, Split-C. We further show that active messages are sufficient to implement the dynamically scheduled languages for which message driven machines were designed. With this mechanism, latency tolerance becomes a programming/compiling concern. Hardware support for active messages is desirable and we outline a range of enhancements to mainstream processors.  
608|Efficient Software-Based Fault Isolation|One way to provide fault isolation among cooperating software modules is to place each in its own address space. However, for tightly-coupled modules, this solution incurs prohibitive context switch overhead. In this paper, we present a software approach to implementing fault isolation within a single address space. Our approach has two parts. First, we load the code and data for a distrusted module into its own fault domain, a logically separate portion of the application&#039;s address space. Second, we modify the object code of a distrusted module to prevent it from writing or jumping to an address outside its fault domain. Both these software operations are portable and programming language independent. Our approach poses a tradeo relative to hardware fault isolation: substantially faster communication between fault domains, at a cost of slightly increased execution time for distrusted modules. We demonstrate that for frequently communicating modules, implementing fault isolation in software rather than hardware can substantially improve end-to-end application performance.
609|A dynamic network architecture|Network software is a critical component of any distributed system. Because of its complexity, network software is commonly layered into a hierarchy of protocols, or more generally, into a protocol graph Typical protocol graphs-including those standardized in the IS0 and TCP/IP network architectures-share three important properties: the protocol graph is simple, the nodes of the graph (protocols) encapsulate complex functionality, and the topology of the graph is relatively static. This paper describes a new way to organize network software that differs from conventional architectures in all three of these properties In our approach, the protocol graph is complex, individual protocols encapsulate a single function. and the topology of the graph is dynamic. The main contribution of this paper is to describe the ideas behind our new architec-ture, illustrate the advantages of using the architecture, and demonstrate that the architecture results in efficient network software.
610|Interposition Agents: Transparently Interposing User Code at the System Interface|1.1. Terminology Many contemporary operating systems utilize a system Many contemporary operating systems provide an call interface between the operating system and its clients. interface between user code and the operating system Increasing numbers of systems are providing low-level services based on special &#034;system calls&#034;. One can view mechanisms for intercepting and handling system calls in the system interface as simply a special form of structured user code. Nonetheless, they typically provide no higher- communication channel on which messages are sent, level tools or abstractions for effectively utilizing these allowing such operations as interposing programs that mechanisms. Using them has typically required record or modify the communications that take place on reimplementation of a substantial portion of the system this channel. In this paper, such a program that both uses interface from scratch, making the use of such facilities and provides the system interface will be refe...
611|Scout: A Communications-Oriented Operating System|This white paper describes Scout, a new operating system being designed for systems connected  to the National Information Infrastructure (NII). Scout provides a communication-oriented  software architecture for building operating system code that is specialized for the different  systems that we expect to be available on the NII. It includes an explicit path abstraction that  both facilitates effective resource management and permits optimizations of the critical path  that I/O data follows. These path-enabled optimizations, along with the application of advanced  compiler techniques, result in a system that has both predictable and scalable performance. 
612|Distributed Object Management in Thor|Thor is a new object-oriented database management system (OODBMS), intended to be used in heterogeneous distributed systems to allow programs written in different programming languages to share objects in a convenient manner. Thor objects are persistent in spite of failures, are highly likely to be accessible whenever they are needed, and can be structured to reflect the kinds of information of interest to users. Thor combines the advantages of the object-oriented approach with those of database systems: users can store and manipulate objects that capture the semantics of their applications, and can also access objects via queries. Thor is an ongoing project, and this paper is a snapshot: we describe our first design and a partial implementation of that design. This design is primarily concerned with issues related to the implementation of an OODBMS as a distributed system. 1 INTRODUCTION Distributed systems contain different kinds of computers, and users write programs in different...
613|Filter Propagation in Dissemination Trees: Trading off Bandwidth for Processing|We describe the concept of the relocatable continuous media filter. The novelty of these filters is how they can propagate over a dissemination tree in a network. We describe the filter propagation protocol to achieve this. Execution of filters inside a network allows the network to be viewed in a novel way, as a &#039;+processor &#034; with its &#034;&#039;instruction set &#034; being the various types of available filters. Since filters generally modify the data rate of the continuous media stream, usually (but not necessarily) reducing it, filters allow the trading off of bandwidth and processing in a network. 1.
614|Omniware: A Universal Substrate for Mobile Code|In this paper we describe Omniware, a system for producing and executing mobile code. Mobile code will be used in next generation Web applications to specify dynamic behavior in Web pages, implement new Web protocols and data formats, and dynamically distribute computation between servers and browsers. Like all mobile code systems, Omniware provides portability and safety. The same compiled Omniware module can be executed transparently on different machines, and a module&#039;s access to host resources can be precisely controlled. In addition to portability and safety, Omniware has two unique features. First, Omniware is open. The Omniware virtual machine, OmniVM, supports standard programming languages, enabling Web developers to leverage the vast store of existing software and programming expertise. OmniVM was designed to be a straightforward compilation target for a large variety of source languages. Second, Omniware is fast. We evaluate Omniware under the Solaris 2.4 operating system on...
615|Cognitive networks|Abstract — This paper presents a definition and framework for a novel type of adaptive data network: the cognitive network. In a cognitive network, the collection of elements that make up the network observes network conditions and then, using prior knowledge gained from previous interactions with the network, plans, decides and acts on this information. Cognitive networks are different from other “intelligent ” communication technologies because these actions are taken with respect to the end-to-end goals of a data flow. In addition to the cognitive aspects of the network, a specification language is needed to translate the user’s end-to-end goals into a form understandable by the cognitive process. The cognitive network also depends on a Software Adaptable Network that has both an external interface accessible to the cognitive network and network status sensors. These devices are used to provide control and feedback. The paper concludes by presenting a simple case study to illustrate a cognitive network and its framework. I.
616|Cognitive Radio: Brain-Empowered Wireless Communications| Cognitive radio is viewed as a novel approach for improving the utilization of a precious natural resource: the radio electromagnetic spectrum. The cognitive radio, built on a software-defined radio, is defined as an intelligent wireless communication system that is aware of its environment and uses the methodology of understanding-by-building to learn from the environment and adapt to statistical variations in the input stimuli, with two primary objectives in mind: • highly reliable communication whenever and wherever needed; • efficient utilization of the radio spectrum. Following the discussion of interference temperature as a new metric for the quantification and management of interference, the paper addresses three fundamental cognitive tasks. 1) Radio-scene analysis. 2) Channel-state estimation and predictive modeling. 3) Transmit-power control and dynamic spectrum management. This paper also discusses the emergent behavior of cognitive radio. 
617|STATEMATE: A Working Environment for the Development of Complex Reactive Systems|This paper provides an overview of the STATEMATE  system, constructed over the past several years by the authors and their colleagues at Ad Cad Ltd., the R&amp;D subsidiary of i-Logix, Inc. STATEMATE is a set of tools, with a heavy graphical orientation, in- tended for the specification, analysis, design, and documentation of large and complex reactive systems, such as real-time embedded sys- tems, control and communication systems, and interactive software or hardware. It enables a user to prepare, analyze, and debug diagram- matic, yet precise, descriptions of the system under development from three interrelated points of view, capturing structure, functionality, and behavior. These views are represented by three graphical languages, the most intricate of which is the language of statecharts [4], used to depict reactive behavior over time. In addition to the use of statecharts, the main novelty of STATEMATE is in the fact that it &#034;understands &#034; the entire descriptions perfectly, to the point of being able to analyze them for crucial dynamic properties, to carry out rigorous ex- ecutions and simulations of the described system, and to create run- ning code automatically. These features are invaluable when it comes to the quality and reliability of the final outcome.
619|Smart Packets for Active Networks|Smart Packets is a DARPA-funded Active Networks project focusing on applying active networks technology to network management and monitoring without placing undue burden on the nodes in the network. Messages in active networks are programs that are executed at nodes on the path to one or more target hosts. Smart Packets programs are written in a tightly-encoded, safe language specifically designed to support network management and avoid dangerous constructs and accesses. Smart Packets improves the management of large complex networks by (1) moving management decision points closer to the node being managed, (2) targeting specific aspects of the node for information rather than exhaustive collection via polling, and (3) abstracting the management concepts to language constructs, allowing nimble network control. This paper introduces Smart Packets and describes the Smart Packet architecture, the packet formats, the language and its design goals, and security considerations. Keywords--...
620|Hardware/Software Co-Design|... This paper introduces the reader to various aspects of co-design. We highlight the commonalities and point out the differences in various co-design problems in some application areas. Co-design issues and their relationship to classical system implementation tasks are discussed to help the reader develop a perspective on modern digital system design that relies on computer-aided design (CAD) tools and methods.
621|Hardware-Software Co-Synthesis of Distributed Embedded System|This article describes a new hardware-software cosynthesis algorithm that takes advantage of the structure inherent in an object-oriented specification. The algorithm creates a distributed system implementation with arbitrary topology, using the object-oriented structure to parti-tion functionality in addition to scheduling and allocating processes. Process partitioning is an especially important optimization for such systems because the specification will not, in general, take into account the process structure required for efficient execution on the distributed engine. The object-oriented specification naturally provides both coarse-grained and fine-grained partitions of the system. Our algorithm uses that multilevel structure to guide synthesis. Experimental results show that our algorithm takes advantage of the object-oriented specification to quickly converge on high-quality implementations.
622|System-level Synthesis using Re-programmable Components|We formulate the synthesis problem of complex behavioral descriptions with performance constraints as a hardware-software co-design problem. The target system architecture consists of a software component as a program running on a re-programmable processor assisted by application-specific hardware components.
623|Integrating Communication Protocol Selection with Partitioning in Hardware/Software Codesign|This paper presents a codesign approach which incorporates communication protocol selection as a design parameter within hardware/software partitioning. The presented approach takes into account data transfer rates depending on communication protocol types and configurations, and different operating frequencies of system components, i.e. CPUs, ASICs, and busses. It also takes into account the timing and area influences of drivers and driver calls needed to perform the communication. The approach is illustrated by a number of design space exploration experiments which use models of the PCI and USB communication protocols. 1. Introduction  This paper presents an approach to hardware/software codesign which integrates communication protocol selection with hardware/software partitioning. The approach has been implemented as an extension to the LYCOS[6] cosynthesis system. Most current approaches to co-synthesis consider communication synthesis to be a final step in the co-synthesis traject...
624|Next-generation wireless communications concepts and technologies|Next-generation wireless (NextG) involves the concept that the next generation of wireless communications will be a major move toward ubiquitous wireless communications systems and seamless high-quality wireless services. This article presents the concepts and technologies involved, including possible innovations in architectures, spectrum allocation, and utilization, in radio communications, networks, and services and applications. These include dynamic and adaptive systems and technologies that provide a new paradigm for spectrum assignment and management, smart resource management, dynamic and fast adaptive multilayer approaches, smart radio, and adaptive networking. Technologies involving adaptive and highly efficient modulation, coding, multiple access, media access, network organization, and networking that can provide ultraconnectivity at high data rates with effective QoS for Next Gare are also described.
625|Adaptive downlink scheduling and rate selection: a cross layer design|  In this paper we discuss a cross layer design for joint user scheduling and adaptive rate control for downlink wireless transmission. We take a stochastic learning based approach to achieve this. The scheduling is performed at the medium access control (MAC) layer whereas the rate selection takes place at the physical/link (PHY/LINK) layer. These two components residing in the two layers exchange information to ensure that user defined rate requests are satisfied by the right combination of transmission schedules and rate selections. The method is highly efficient for low mobility applications with mobile speeds in the order of a few km/h. While simple to implement, this technique requires no explicit channel estimation phase. The only feedback used are the single bit ACK/NACK signal indicating the correct reception/failure of the packet. As shown in the convergence theorems, the algorithm achieves optimal performance in “stationary” channels. With slowly varying channels, the rate selection algorithm sees a “quasi-stationary” channel and adaptively converges to an optimal solution. Simulations performed using a 3G wireless system namely high speed downlink packet access (HSDPA) validate the theoretical results. 
626|Hardware/software partitioning for multi-function systems|Abstract—We are interested in optimizing the design of multifunction embedded systems such as multistandard audio/video codecs and multisystem phones. Such systems run a prespecified set of applications, and any “one ” of the applications is selected at a run time, depending on system parameters. Our goal is to develop a methodology for the efficient design of such systems. A key observation underlying our method is that it may not be efficient to design for each application separately. This is attributed to two factors. First, considering each application in isolation can lead to application-specific decisions that do not necessarily lead to the best overall system solution. Second, these applications typically tend to have several commonalities among them, and considering applications independently may lead to inconsistent mappings of common tasks in different applications. Our approach is to optimize jointly across the set
627|Hardware Software Partitioning with Integrated Hardware Design Space Exploration|This paper presents an integrated approach to hardware software partitioning and hardware design space exploration. We propose a genetic algorithm which performs hardware software partitioning on a task graph while simultaneously contemplating various design alternatives for tasks mapped to hardware. We primarily deal with data dominated designs typically found in digital signal processing and image processing applications. A detailed description of various genetic operators is presented. We provide results to illustrate the effectiveness of our integrated methodology.  
628|High-Level Estimation Techniques for Usage in Hardware/Software Co-Design|Abstract- High-level estimation techniques are of paramount importance for design decisions like hardwarekoftware partition-ing or design space explorations. In both cases an appropriate compromise between accuracy and computation time determines about the feasibility of those estimation techniques. In this paper we present high--level estimation techniques for hardware effort and hardwarekoftware communication time. Our techniques deliver fast results at sufficient accuracy. Furthermore, it is shown in which way these techniques are applied in order to cope with contradictory design goals like performance constraints and hardware effort constraints. As a solution, we present a cost function for the purpose of hardwarekoftware partitioning that offers a dynamic weighting of its components. The conducted experiments show that the usage of our estimation techniques in conjunction with their efficient combination leads to reasonable hardwarekoftware implementations as opposed to approaches that consider single constraints only. I.
629|Modifying Min-Cut for Hardware and Software Functional Partitioning|The Kernighan/Lin heuristic, also known as min-cut, has been extended very successfully for circuit partitioning over several decades. Those extensions customized the heuristic and its associated data structure to rapidly compute the minimum-cut metric required during circuit partitioning; thus, those extensions are not applicable to problems requiring other metrics. In this paper, we extend the heuristic for functional partitioning in a manner applicable to the codesign problem of hardware/software partitioning as well as to hardware/hardware partitioning. The extension customizes the heuristic and data structure to rapidly compute execution-time and communication metrics, crucial to hardware and software partitioning, and leads to nearlinear time-complexity and excellent results. Our experiments demonstrate extremely fast execution times (just a few seconds) with results matched only by the much slower simulated annealing heuristic, meaning that the extended Kernighan/Lin heuristic w...
630|Hardware resource allocation for hardware/software partitioning in the lycos system|This paper presents a novel hardware resource alloca-tion technique for hardware/software partitioning. It al-locates hardware resources to the hardware data-path us-ing information such as data-dependencies between op-erations in the application, and profiling information. The algorithm is useful as a designer’s/designtool’s aid to generate good hardware allocations for use in hard-ware/software partitioning. The algorithm has been imple-mented in a tool under the LYCOS system [9]. The results show that the allocations produced by the algorithm come close to the best allocations obtained by exhaustive search. 1
631|Bayesian Belief Networks: Odds and Ends|In artificial intelligence research, the belief network framework for automated reasoning  with uncertainty is rapidly gaining in popularity. The framework provides a powerful  formalism for representing a joint probability distribution on a set of statistical variables.
632|Improving the Capacity in Wireless Ad Hoc Networks through Multiple Channel Operation: Design Principles and Protocols|Despite recent advances in wireless local area network (WLAN) technologies, today’s WLANs still cannot offer the same data rates as their wired counterparts. The throughput problem is further aggravated in multi-hop wireless environments due to collisions and in-terference caused by multi-hop routing. Because all current IEEE 802.11 physical (PHY) standards divide the available frequency into several orthogonal channels, which can be used simultaneously within a neighborhood, increasing capacity by exploiting multiple channels becomes particularly appealing. To improve the capacity of wireless ad hoc networks by exploiting multiple available channels, I propose three principles that facilitate the design of efficient distributed channel assignment protocols. Distributed channel assignment problems have been proven to be N P-complete and, thus, computationally intractable [32, 6]. Though being a subject of many years of research, distributed channel assignment remains a challenging problem. There exist only a few heuristic solutions, none of which is efficient, especially for the mobile ad hoc environment [6, 23, 32]. However, protocols that implement the proposed design principles are shown to require fewer channels and exhibit significantly lower communication,
633|Adaptive clustering for mobile wireless networks|This paper describes a self-organizing, multihop, mobile radio network, which relies on a code division access scheme for multimedia support. In the proposed network architecture, nodes are organized into nonoverlapping clusters. The clusters are independently controlled and are dynamically reconfigured as nodes move. This network architecture has three main advantages. First, it provides spatial reuse of the bandwidth due to node clustering. Secondly, bandwidth can be shared or reserved in a controlled fashion in each cluster. Finally, the cluster algorithm is robust in the face of topological changes caused by node motion, node failure and node insertion/removal. Simulation shows that this architecture provides an efficient, stable infrastructure for the integration of different types of traffic in a dynamic radio network. 1.
634|Maisie: A Language for the Design of Efficient Discrete-event Simulations|Maisie is a C-based discrete-event simulation language that was designed to cleanly separate a simulation model from the underlying algorithm (sequential or parallel) used for the execution of the model. With few modifications, a Maisie program may be executed using a sequential simulation algorithm, a parallel conservative algorithm or a parallel optimistic algorithm. The language constructs allow the runtime system to implement optimizations that reduce recomputation and state saving overheads for optimistic simulations and synchronization overheads for conservative implementations. This paper presents the Maisie simulation language, describes a set of optimizations and illustrates the use of the language in the design of efficient parallel simulations. 1 Introduction Distributed (or parallel) simulation refers to the execution of a simulation program on parallel computers. A number of algorithms[25, 10, 11, 21, 20] have been suggested for distributed simulation and many experimental...
635|Radio Link Admission Algorithms for Wireless Networks with Power Control and Active Link Quality Protection|In this paper we present a distributed power control scheme, which maintains the SIRs of operational (active) links above their required thresholds at all time (link quality protection), while new users are being admitted; furthermore, when new users cannot be successfully admitted, existing ones do not suffer fluctuations of their SIRs below their required thresholds values. We also present two admission /rejection control algorithms, which exercise voluntary drop-out of links inadmissible to the network, so as to reduce interference and possibly facilitate the admission of other links.
636|Asynchronous Multimedia Multihop Wireless Networks+|Personal communications and mobile computing will require a wireless network infrastructure which is fast deployable, possibly multihop, and capable of multimedia service support. The first infrastructure of this type was the Packet Radio Network (PRNET), developed in the 70&#039;s to address the battlefield and disaster recovery communication requirements. PRNET was totally asynchronous and was based on a completely distributed architecture. It handled datagram traffic reasonably well, but did not offer efficient multimedia support. Recently, under the WAMIS and Glomo ARPA programs several mobile, multimedia, multihop (M  3  ) wireless network architectures have been developed, which assume some form of synchronous, time division infrastructure. The synchronous time frame leads to efficient multimedia support implementations. However, it introduces more complexity and is less robust in the face of mobility and channel fading. In this paper, we examine the impact of synchronization on wirel...
637|Multicluster, Mobile, Multimedia Radio Network|A multi-cluster, multi-hop packet radio network architecture for wireless adaptive mobile information systems is presented...
638|Providing Connection-oriented Network Services to Mobile Hosts|Mobile computers using wireless networks, along with multimedia applications, are two emerging trends in computer systems. This new mobile multimedia computing environment presents many challenges, due to the requirements of multimedia applications and the mobile nature of hosts. We present several alternative schemes for maintaining network connections used to provide multimedia service, as hosts move through a nano-cellular radio network. These algorithms modify existing connections by partially reestablishing them to perform handoffs. Using a simple analytical model, we compare the schemes on the basis of the service disruption caused by handoffs, required buffering, and excess resources required to perform the handoffs. Two technological trends of the 1990s are the emerging use of wireless computers and network support for multimedia services. The products of these trends hold forth the promise of being combined in innovative ways to provide applications such as mobile digital video and audio conferencing. The new computing environment presented by wireless multimedia personal communication systems such as discussed in
639|A distributed architecture for multimedia in dynamic wireless networks|The paper presents a self-organizing, wireless mobile radio net-work for multimedia support. The proposed architecture is distri-buted and it has the capability of rapid deployment and dynamic reconfiguration. Without the need of base stations, this architec-ture can operate in areas without a wired backbone infrastruc-ture. This architecture provides an instant infrastructure for real-time traffic transmission. Based on the instant infrastruc-ture, a stable and loop-free routing protocol is implemented. 1.
640|Networks versus Markets in International Trade|I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative
641|CONTINENTAL TRADING BLOCS: ARE THEY NATURAL, OR SUPER-NATURAL?|Using the gravity model, we find evidence of three continental trading blocs: the Americas, Europe and Pacific Asia. Intra-regional trade exceeds what can be explained by the proximity of a pair of countries, their sizes and GNP/capitas, and whether they share a common border or language. We then turn from the econometrics to the economic welfare implications. Krugman has supplied an argument against a three-bloc world, assuming no transport costs, and another argument in favor, assuming prohibitively high transportation costs between continents. We complete the model for the realistic case where intercontinental transport costs are neither prohibitive nor zero. If transport costs are low, continental Free Trade Areas can reduce welfare. We call such blocs super-natural. Partial liberalization is better than full liberalization within regional Preferential Trading Arrangements, despite the GATT&#039;s Article 24. The super-natural zone occurs when the regionalization of trade policy exceeds what is justified by natural factors.
642|Predicting Internet Network Distance with Coordinates-Based Approaches|In this paper, we propose to use coordinates-based mechanisms in a peer-to-peer architecture to predict Internet network distance (i.e. round-trip propagation and transmission delay) . We study two mechanisms. The first is a previously proposed scheme, called the triangulated heuristic, which is based on relative coordinates that are simply the distances from a host to some special network nodes. We propose the second mechanism, called Global Network Positioning (GNP), which is based on absolute coordinates computed from modeling the Internet as a geometric space. Since end hosts maintain their own coordinates, these approaches allow end hosts to compute their inter-host distances as soon as they discover each other. Moreover coordinates are very efficient in summarizing inter-host distances, making these approaches very scalable. By performing experiments using measured Internet distance data, we show that both coordinates-based schemes are more accurate than the existing state of the art system IDMaps, and the GNP approach achieves the highest accuracy and robustness among them.
643|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
644|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
645|Topologically-aware overlay construction and server selection|  A number of large-scale distributed Internet applications could potentially benefit from some level of knowledge about the relative proximity between its participating host nodes. For example, the performance of large overlay networks could be improved if the application-level connectivity between the nodes in these networks is congruent with the underlying IP-level topology. Similarly, in the case of replicated web content, client nodes could use topological information in selecting one of multiple available servers. For such applications, one need not find the optimal solution in order to achieve significant practical benefits. Thus, these applications, and presumably others like them, do not require exact topological information and can instead use sufficiently informative hints about the relative positions of Internet hosts. In this paper, we present a binning scheme whereby nodes partition themselves into bins such that nodes that fall within a given bin are relatively close to one another in terms of network latency. Our binning strategy is simple (requiring minimal support from any measurement infrastructure), scalable (requiring no form of global knowledge, each node only needs knowledge of a small number of well-known landmark nodes) and completely distributed (requiring no communication or cooperation between the nodes being binned). We apply this binning strategy to the two applications mentioned above: overlay network construction and server selection. We test our binning strategy and its application using simulation and Internet measurement traces. Our results indicate that the performance of these applications can be significantly improved by even the rather coarse-grained knowledge of topology offered by our binning scheme.
646|On the constancy of Internet path properties|Abstract — Many Internet protocols and operational procedures use measurements to guide future actions. This is an effective strategy if the quantities being measured exhibit a degree of constancy: that is, in some fundamental sense, they are not changing. In this paper we explore three different notions of constancy: mathematical, operational, and predictive. Using a large measurement dataset gathered from the NIMI infrastructure, we then apply these notions to three Internet path properties: loss, delay, and throughput. Our aim is to provide guidance as to when assumptions of various forms of constancy are sound, versus when they might prove misleading. I.
647|Application-layer multicast with Delaunay triangulations| Application-layer multicast supports group applications without the need for a network-layer multicast protocol. Here, applications arrange themselves in a logical overlay network and transfer data within the overlay. In this paper, we present an application-layer multicast solution that uses a Delaunay triangulation as an overlay network topology. An advantage of using a Delaunay triangulation is that it allows each application to locally derive next-hop routing information without requiring a routing protocol in the overlay. A disadvantage of using a Delaunay triangulation is that the mapping of the overlay to the network topology at the network and data link layer may be suboptimal. We present a protocol, called Delaunay triangulation (DT protocol), which constructs Delaunay triangulation overlay networks. We present measurement experiments of the DT protocol for overlay networks with up to 10 000 members, that are running on a local PC cluster with 100 Linux PCs. The results show that the protocol stabilizes quickly, e.g., an overlay network with 10 000 nodes can be built in just over 30 s. The traffic measurements indicate that the average overhead of a node is only a few kilobits per second if the overlay network is in a steady state. Results of throughput experiments of multicast transmissions (using TCP unicast connections between neighbors in the overlay network) show an achievable throughput of approximately 15 Mb/s in an overlay with 100 nodes and 2 Mb/s in an overlay with 1000 nodes. 
648|Locating Nearby Copies of Replicated Internet Servers|In this paper we consider the problem of choosing among a collection of replicated servers, focusing on the question of how to make choices that segregate client/server traffic according to network topology.We explore the cost and effectiveness of a variety of approaches, ranging  from those requiring routing layer support (e.g., anycast) to those that build location databases using application-level probe tools like traceroute. Weuncover a number of tradeoffs between effectiveness, network cost, ease of deployment, and portability across differenttypes of networks. We performed our experiments using a simulation parameterized by a topology collected from  7 survey sites across the United States, exploring a global collection of Network Time Protocol  servers. 
649|An Architecture for a Global Internet Host Distance Estimation Service|There is an increasing need for Internet hosts to be able to quickly and efficiently learn the distance, in terms of metrics such as latency or bandwidth, between Internet hosts. For example, to select the nearest of multiple equal-content web servers. This paper explores technical issues related to the creation of a public infrastructure service to provide such information. In so doing, we suggests an architecture, called IDMaps, whereby Internet distance information is distributed over the Internet, using IP multicast groups, in the form of a virtual distance map. Systems listening to the groups can estimate the distance between any pair of IP addresses by running a spanning tree algorithm over the received distance map. We also present the results of experiments that give preliminary evidence supporting the architecture. This work thus lays the initial foundation for future work in this new area.
650|The Stationarity of Internet Path Properties: Routing, Loss, and Throughput|There is much interest in using network measurements for both modeling and operational purposes. In this paper we focus on the fundamental question of the stationarity of such measurements. That is, to what extent are past measurements a good predictor of the future? We used the NIMI infrastructure and a set of public traceroute servers to capture large measurement datasets of three quantities: routing, packet loss, and TCP throughput. We apply statistical tests to attempt to develop sound characterizations of the stationarity of these data sets, and discuss several types of nonstationarity. 1 Introduction In recent years there has been a surge of interest in network measurements. These measurements have deepened our understanding of network behavior and led to more accurate and qualitatively different models of network traffic. Network measurements are also used operationally by various protocols to guide network usage. For instance, RLM [MJV96] and equation-based congestion control...
651|Routing in a delay tolerant network|We formulate the delay-tolerant networking routing problem, where messages are to be moved end-to-end across a connectivity graph that is time-varying but whose dynamics may be known in advance. The problem has the added constraints of finite buffers at each node and the general property that no contemporaneous end-to-end path may ever exist. This situation limits the applicability of traditional routing approaches that tend to treat outages as failures and seek to find an existing end-to-end path. We propose a framework for evaluating routing algorithms in such environments. We then develop several algorithms and use simulations to compare their performance with respect to the amount of knowledge they require about network topology. We find that, as expected, the algorithms using the least knowledge tend to perform poorly. We also find that with limited additional knowledge, far less than complete global knowledge, efficient algorithms can be constructed for routing in such environments. To the best of our knowledge this is the first such investigation of routing issues in DTNs.
652|On-demand Multipath Distance Vector Routing in Ad Hoc Networks|We develop an on-demand, multipath distance vector protocol for mobile ad hoc networks. Specifically, we propose multipath extensions to a well-studied single path routing protocol known as Ad hoc On-demand Distance Vector (AODV). The resulting protocol is referred to as Ad hoc Ondemand Multipath Distance Vector (AOMDV). The protocol computes multiple loop-free and link-disjoint paths. Loopfreedom is guaranteed by using a notion of &#034;advertised hopcount.&#034; Link-disjointness of multiple paths is achieved by using a particular property of flooding. Performance comparison of AOMDV with AODV using ns-2 simulations shows that AOMDV is able to achieve a remarkable improvement in the end-to-end delay --- often more than a factor of two, and is also able to reduce routing overheads by about 20%.  1 
653|An Analysis of Internet Content Delivery Systems|In the span of only a few years, the Internet has experienced an astronomical increase in the use of specialized content delivery systems, such as content delivery networks and peer-to-peer file sharing systems. Therefore, an understanding of content delivery on the Internet now requires a detailed understanding of how these systems are used in practice. This paper examines content delivery from the point of view of four content delivery systems: HTTP web traffic, the Akamai content delivery network, and Kazaa and Gnutella peer-to-peer file sharing traffic. We collected a trace of all incoming and outgoing network traffic at the University of Washington, a large university with over 60,000 students, faculty, and staff. From this trace, we isolated and characterized traffic belonging to each of these four delivery classes. Our results (1) quantify the rapidly increasing importance of new content delivery systems, particularly peerto-peer networks, (2) characterize the behavior of these systems from the perspectives of clients, objects, and servers, and (3) derive implications for caching in these systems. 1
654|Shortest-path and minimumdelay algorithms in networks with time-dependent edge-length|We consider in this paper the shortest-path problem in networks in which the delay (or weight) of the edges changes with time according to arbitrary functions. We present algorithms for finding the shortest-path and minimum-delay under various waiting constraints and investigate the properties of the derived path. We show that if departure time from the source node is unrestricted then a shortest path can be found that is simple and achieves a delay as short as the most unrestricted path. In the case of restricted transit, it is shown that there exist cases where the minimum delay is finite but the path that achieves it is infinite.
655|Trajectory Based Forwarding and Its Applications|Trajectory based forwarding (TBF) is a novel method to forward packets in a dense ad hoc network that  makes it possible to route a packet along a predefined curve. It is a generalization of source based routing  and Cartesian forwarding in that the trajectory is set by the source, but the forwarding decision is based on  the relationship to the trajectory rather than the final destination. The fundamental aspect of TBF is that it  decouples path naming from the actual path, thereby providing a common framework for applications such as:  flooding, unicast, multicast and multipath routing, and discovery in ad hoc networks. TBF requires that nodes  know their position relative to a coordinate system. While a global coordinate system a#orded by a system  such as GPS would be ideal, in this paper we propose Local Positioning System (LPS), a method that only  positions the nodes along the trajectory, by making use of other node capabilities, such as angle of arrival or  range estimations, compasses and accelerometers. We explore several forwarding strategies that are appropriate  for these node capabilities.
656|The Quickest Transshipment Problem|A dynamic network consists of a graph with capacities and transit times on its edges. The  quickest transshipment problem is defined by a dynamic network with several sources and sinks;  each source has a specified supply and each sink has a specified demand. The problem is to send  exactly the right amount of flow out of each source and into each sink in the minimum overall  time. Variations of 
657|Enabling disconnected transitive communication in mobile ad hoc networks|Recently, mobile ad hoc computing has attracted much attention in the research community. Different from most current wireless networks, mobile ad-hoc networks have no fixed infrastructure, all hosts are capable of movement, and the network is continuously reconstructed into multiple disconnected clusters. In such environments, connection is unstable, and communication is unpredictable. Current research mainly focuses on providing the same models of communication in this environment as in fixed networks, focusing on routing protocols for message delivery within connected subsets of hosts, also referred to as clusters. Although this kind of work is crucial, it does not address the possibility of communication across clusters, taking advantage of the movement of mobile hosts which themselves are able to carry messages from one cluster to another. In this paper, we propose a new model of communication model, Disconnected Transitive Communication (DTC), which focuses on cross-cluster communication, and provide the details of a routing protocol to enable it.
658|A Linear Programming Formulation of Flows over Time with Piecewise Constant Capacity and Transit Times| We present an algorithm to solve a deterministic form of a routing problem in delay tolerant networking, in which contact possibilities are known in advance. The algorithm starts with a finite set of contacts with time-varying capacities and transit delays. The output is an optimal schedule assigning messages to edges and times, that respects message priority and minimizes the overall delivery delay. The algorithm consists of two main ingredients: a discretization step in which the raw data provided by the contacts is used to obtain appropriate subdivisions of the relevant time intervals, and a linear program, a dynamic version of the classical multicommodity flow problem, in which transit times are piecewise constant, and where both edges and nodes are capacitated (in the case of edges, with piecewise constant capacities). In fact, we present two equivalent LP formulations, of which one is smaller and runs faster in CPLEX, a general purpose linear solver.  
659|An annotated overview of dynamic network flows| The need for more realistic network models led to the development of the dynamic network flow theory. In dynamic flow models it takes time for the flow to pass an arc, the flow can be delayed at nodes, and the network parameters, e.g., the arc capacities, can change in time. Surprisingly perhaps, despite being closer to reality, dynamic flow models have been overshadowed by the classical, static model. This is largely due to the fact that while very efficient solution methods exist for static flow problems, dynamic flow problems have proved to be more difficult to solve. Our purpose with this overview is to compensate for this eclipse and introduce dynamic flows to the interested reader. To this end, we present the main flow problems that can appear in a dynamic network, and review the literature for existing results about them. Our approach is solution oriented, as opposed to dealing with modelling issues. We intend to provide a survey that can be a first step for readers wondering whether a given dynamic network flow problem has been solved or not. Besides restating the problems, we also describe the main proposed solution methods. An additional feature of this paper is an annotated list of the most important references about the subject.
660|Centrality in social networks conceptual clarification|The intuitive background for measures of structural centrality in social networks is reviewed aPzd existing measures are evaluated in terms of their consistency with intuitions and their interpretability. Three distinct intuitive conceptions of centrality are uncovered and existing measures are refined to embody these conceptions. Three measures are developed for each concept, one absolute and one relative measure of the ~entra~~t~ ~ of ~os~tio~ls in a network, and one relenting the degree of centralization of the entire network. The implications of these measures for the experimental study of small groups is examined. The problem of centrality The idea of centrality as applied to human communication was introduced by Bavelas in 1948. He was specifically concerned with communication in small groups and he hypothesized a relationship between structural centrality and influence in group processes.
661|Imagenet classification with deep convolutional neural networks |We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0 % which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called “dropout” that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2 % achieved by the second-best entry. 1
662|Random forests|Abstract. Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund &amp; R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148–156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression.
663|Imagenet: A large-scale hierarchical image database|The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond. 1.
664|Learning generative visual models from few training examples: an incremental Bayesian approach tested on 101 object categories|Abstract — Current computational approaches to learning visual object categories require thousands of training images, are slow, cannot learn in an incremental manner and cannot incorporate prior information into the learning process. In addition, no algorithm presented in the literature has been tested on more than a handful of object categories. We present an method for learning object categories from just a few training images. It is quick and it uses prior information in a principled way. We test it on a dataset composed of images of objects belonging to 101 widely varied categories. Our proposed method is based on making use of prior information, assembled from (unrelated) object categories which were previously learnt. A generative probabilistic model is used, which represents the shape and appearance of a constellation of features belonging to the object. The parameters of the model are learnt incrementally in a Bayesian manner. Our incremental algorithm is compared experimentally to an earlier batch Bayesian algorithm, as well as to one based on maximum-likelihood. The incremental and batch versions have comparable classification performance on small training sets, but incremental learning is significantly faster, making real-time learning feasible. Both Bayesian methods outperform maximum likelihood on small training sets. I.
665|LabelMe: A Database and Web-Based Tool for Image Annotation|  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.
667|Learning multiple layers of features from tiny images|April 8, 2009Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it di cult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signi cantly
668|Learning methods for generic object recognition with invariance to pose and lighting|We assess the applicability of several popular learning methods for the problem of recognizing generic visual categories with invariance to pose, lighting, and surrounding clutter. A large dataset comprising stereo image pairs of 50 uniform-colored toys under 36 angles, 9 azimuths, and 6 lighting conditions was collected (for a total of 194,400 individual images). The objects were 10 instances of 5 generic categories: four-legged animals, human figures, airplanes, trucks, and cars. Five instances of each category were used for training, and the other five for testing. Low-resolution grayscale images of the objects with various amounts of variability and surrounding clutter were used for training and testing. Nearest Neighbor methods, Support Vector Machines, and Convolutional Networks, operating on raw pixels or on PCA-derived features were tested. Test error rates for unseen object instances placed on uniform backgrounds were around 13 % for SVM and 7 % for Convolutional Nets. On a segmentation/recognition task with highly cluttered images, SVM proved impractical, while Convolutional nets yielded 14 % error. A real-time version of the system was implemented that can detect and classify objects in natural scenes at around 10 frames per second. 1
669|What is the Best Multi-Stage Architecture for Object Recognition? |In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63 % recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 (&gt; 65%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53%). 1.
670|J.C.: Best practices for convolutional neural networks applied to visual document analysis|Neural networks are a powerful technology for classification of visual inputs arising from documents. However, there is a confusing plethora of different neural network methods that are used in the literature and in industry. This paper describes a set of concrete best practices that document analysis researchers can use to get good results with neural networks. The most important practice is getting a training set as large as possible: we expand the training set by adding a new form of distorted data. The next most important practice is that convolutional neural networks are better suited for visual document tasks than fully connected networks. We propose that a simple “do-it-yourself ” implementation of convolution with a flexible architecture is suitable for many visual document problems. This simple convolutional neural network does not require complex methods, such as momentum, weight decay, structuredependent learning rates, averaging layers, tangent prop, or even finely-tuning the architecture. The end result is a very simple yet general architecture which can yield state-of-the-art performance for document analysis. We illustrate our claims on the MNIST set of English digit images. 1.
671|Rectified Linear Units Improve Restricted Boltzmann Machines Vinod Nair |Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these “Stepped Sigmoid Units ” are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors. 1.
672|Multi-column deep neural networks for image classification|Traditional methods of computer vision and machine learning cannot match human performance on tasks such as the recognition of handwritten digits or traffic signs. Our biologically plausible deep artificial neural network architectures can. Small (often minimal) receptive fields of convolutional winnertake-all neurons yield large network depth, resulting in roughly as many sparsely connected neural layers as found in mammals between retina and visual cortex. Only winner neurons are trained. Several deep neural columns become experts on inputs preprocessed in different ways; their predictions are averaged. Graphics cards allow for fast training. On the very competitive MNIST handwriting benchmark, our method is the first to achieve near-human performance. On a traffic sign recognition benchmark it outperforms humans by a factor of two. We also improve the state-of-the-art on a plethora of common image classification benchmarks.  
673|Why is Real-World Visual Object Recognition Hard |Progress in understanding the brain mechanisms underlying vision requires the construction of computational models that not only emulate the brain’s anatomy and physiology, but ultimately match its performance on visual tasks. In recent years, ‘‘natural’ ’ images have become popular in the study of vision and have been used to show apparently impressive progress in building such models. Here, we challenge the use of uncontrolled ‘‘natural’ ’ images in guiding that progress. In particular, we show that a simple V1-like model—a neuroscientist’s ‘‘null’ ’ model, which should perform poorly at real-world visual object recognition tasks—outperforms state-of-the-art object recognition systems (biologically inspired and otherwise) on a standard, ostensibly natural image recognition test. As a counterpoint, we designed a ‘‘simpler’ ’ recognition test to better span the real-world variation in object pose, position, and scale, and we show that this test correctly exposes the inadequacy of the V1-like model. Taken together, these results demonstrate that tests based on uncontrolled natural images can be seriously misleading, potentially guiding progress in the wrong direction. Instead, we reexamine what it means for images to be natural and argue for a renewed focus on the core problem of object recognition—real-world image variation.
674|D.D.: A high-throughput screening approach to discovering good forms of biologically inspired visual representation. PLoS Comput Biol|While many models of biological object recognition share a common set of ‘‘broad-stroke’ ’ properties, the performance of any one model depends strongly on the choice of parameters in a particular instantiation of that model—e.g., the number of units per layer, the size of pooling kernels, exponents in normalization operations, etc. Since the number of such parameters (explicit or implicit) is typically large and the computational cost of evaluating one particular parameter set is high, the space of possible model instantiations goes largely unexplored. Thus, when a model fails to approach the abilities of biological visual systems, we are left uncertain whether this failure is because we are missing a fundamental idea or because the correct ‘‘parts’ ’ have not been tuned correctly, assembled at sufficient scale, or provided with enough training. Here, we present a high-throughput approach to the exploration of such parameter sets, leveraging recent advances in stream processing hardware (high-end NVIDIA graphic cards and the PlayStation 3’s IBM Cell Processor). In analogy to high-throughput screening approaches in molecular biology and genetics, we explored thousands of potential network architectures and parameter instantiations, screening those that show promising object recognition performance for further analysis. We show that this approach can yield significant, reproducible gains in performance across an array of basic object recognition tasks, consistently outperforming a variety of state-of-the-art purpose-built vision systems from the literature. As the scale of available computational power continues to expand, we argue that this approach has the potential to greatly
675|Convolutional Networks and Applications in Vision |Abstract — Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or ”features”), which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologicallyinspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some non-linearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described.
676|Convolutional Networks Can Learn to Generate Affinity Graphs for Image Segmentation|Many image segmentation algorithms first generate an affinity graph and then partition it. We present a machine learning approach to computing  an affinity graph using a convolutional network (CN) trained using ground truth provided by human experts. The CN affinity graph can be paired with any standard partitioning algorithm and improves segmentation accuracy significantly compared to standard hand-designed affinity functions. We apply our algorithm to the challenging 3D segmentation problem of reconstructing neuronal processes from volumetric electron microscopy (EM) and show that we are able to learn a good affinity graph directly from the raw EM images. Further, we show that our affinity graph improves the segmentation accuracy of both simple and sophisticated graph partitioning algorithms. In contrast to previous work, we do not rely on prior knowledge in the form of hand-designed image features or image preprocessing. Thus, we expect our algorithm to generalize effectively to arbitrary image types. 
677|Convolutional deep belief networks on cifar-10|We describe how to train a two-layer convolutional Deep Belief Network (DBN) on the 1.6 million tiny images dataset. When training a convolutional DBN, one must decide what to do with the edge pixels of teh images. As the pixels near the edge of an image contribute to the fewest convolutional lter outputs, the model may
678|High-performance neural networks for visual object classification|We present a fast, fully parameterizable GPU implementation of Convolutional Neural Network variants. Our feature extractors are neither carefully designed nor pre-wired, but rather learned in a supervised way. Our deep hierarchical architectures achieve the best published results on benchmarks for object classification (NORB, CIFAR10) and handwritten digit recognition (MNIST), with error rates of 2.53%, 19.51%, 0.35%, respectively. Deep nets trained by simple back-propagation perform better than more shallow ones. Learning is surprisingly rapid. NORB is completely trained within five epochs. Test error rates on MNIST drop to 2.42%, 0.97 % and 0.48 % after 1, 3 and 17 epochs, respectively. 
679|Metric learning for large scale image classification: Generalizing to new classes at near-zero cost|Abstract. We are interested in large-scale image classification and especially in the setting where images corresponding to new or existing classes are con-tinuously added to the training set. Our goal is to devise classifiers which can incorporate such images and classes on-the-fly at (near) zero cost. We cast this problem into one of learning a metric which is shared across all classes and ex-plore k-nearest neighbor (k-NN) and nearest class mean (NCM) classifiers. We learn metrics on the ImageNet 2010 challenge data set, which contains more than 1.2M training images of 1K classes. Surprisingly, the NCM classifier compares favorably to the more flexible k-NN classifier, and has comparable performance to linear SVMs. We also study the generalization performance, among others by using the learned metric on the ImageNet-10K dataset, and we obtain competi-tive performance. Finally, we explore zero-shot classification, and show how the zero-shot model can be combined very effectively with small training datasets. 1
680|Using Very Deep Autoencoders for Content-Based Image Retrieval |We show how to learn many layers of features on color images and we use these features to initialize deep autoencoders. We then use the autoencoders to map images to short binary codes. Using semantic hashing [6], 28-bit codes can be used to retrieve images that are similar to a query image in a time that is independent of the size of the database. This extremely fast retrieval makes it possible to search using multiple different transformations of the query image. 256-bit binary codes allow much more accurate matching and can be used to prune the set of images found using the 28-bit codes.
681|Design and Implementation or the Sun Network Filesystem|this paper we discuss the design and implementation of the/&#039;fiesystem interface in the kernel and the NF$ virtual/&#039;fiesystem. We describe some interesting design issues and how they were resolved, and point out some of the shortcomings of the current implementation. We conclude with some ideas for future enhancements
682|Energy Conserving Routing in Wireless Ad-hoc Networks| An ad-hoc network of wireless static nodes is considered as it arises in a rapidly deployed, sensor based, monitoring system. Information is generated in certain nodes and needs to reach a set of designated gateway nodes. Each node may adjust its power within a certain range that determines the set of possible one hop away neighbors. Traffic forwarding through multiple hops is employed when the intended destination is not within immediate reach. The nodes have limited initial amounts of energy that is consumed in different rates depending on the power level and the intended receiver. We propose algorithms to select the routes and the corresponding power levels such that the time until the batteries of the nodes drain-out is maximized. The algorithms are local and amenable to distributed implementation. When there is a single power level, the problem is reduced to a maximum flow problem with node capacities and the algorithms converge to the optimal solution. When there are multiple power levels then the achievable lifetime is close to the optimal (that is computed by linear programming) most of the time. It turns out that in order to maximize the lifetime, the traffic should be routed such that the energy consumption is balanced among the nodes in proportion to their energy reserves, instead of routing to minimize the absolute consumed power.
683|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
684|Performance comparison of two on-demand routing protocols for ad hoc networks|Abstract — Ad hoc networks are characterized by multihop wireless connectivity, frequently changing network topology and the need for efficient dynamic routing protocols. We compare the performance of two prominent ondemand routing protocols for mobile ad hoc networks — Dynamic Source Routing (DSR) and Ad Hoc On-Demand Distance Vector Routing (AODV). A detailed simulation model with MAC and physical layer models is used to study interlayer interactions and their performance implications. We demonstrate that even though DSR and AODV share a similar on-demand behavior, the differences in the protocol mechanics can lead to significant performance differentials. The performance differentials are analyzed using varying network load, mobility and network size. Based on the observations, we make recommendations about how the performance of either protocol can be improved.
685|The state of the art in distributed query processing|Distributed data processing is fast becoming a reality. Businesses want to have it for many reasons, and they often must have it in order to stay competitive. While much of the infrastructure for distributed data processing is already in place (e.g., modern network technology), there are a number of issues which still make distributed data processing a complex undertaking: (1) distributed systems can become very large involving thousands of heterogeneous sites including PCs and mainframe server machines ? (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system? (3) legacy systems need to be integrated|such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. This paper presents the state of the art of query processing for distributed database and information systems. The paper presents the \textbook &amp;quot; architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intra-query parallelism, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses di erent kinds of distributed systems such as client-server, middleware (multi-tier), and heterogeneous database systems and shows how query processing works in these systems. Categories and subject descriptors: E.5 [Data]:Files ? H.2.4 [Database Management Systems]: distributed databases, query processing ? H.2.5 [Heterogeneous Databases]: data translation General terms: algorithms ? performance Additional key words and phrases: query optimization ? query execution ? client-server databases ? middleware ? multi-tier architectures ? database application systems ? wrappers? replication ? caching ? economic models for query processing ? dissemination-based information systems 1
686| Ripple Joins for Online Aggregation |We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (dbms). Such queries arise naturally in interactive exploratory decision-support applications. Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of online aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing dbms using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join &#034;aspect-ratio&#034; parameters. In experiments with an initial implementation of our algorithms in the postgres dbms, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers. 
688|Multimedia Database Systems|Though there are now numerous examples of multimedia systems in the commercial market, these systems have been developed primarily on a case-by-case basis. The largescale development of such systems requires a principled characterization of multimedia systems which is independent of any single application. It requires a unified query language framework to access these different structures in a variety of ways. It requires algorithms that are provably correct in processing such queries and whose efficiency can be appropriately evaluated. In this paper, we develop a framework for characterizing multimedia information systems which builds on top of the implementations of individual media, and provides a logical query language that integrates such diverse media. We develop indexing structures and algorithms to process such queries and show that these algorithms are sound and complete and relatively efficient (polynomial-time). We show that the generation of media-events (i.e. generating di...
689|Adaptive Selectivity Estimation Using Query Feedback|In this paper, we propose a novel approach for estimating the record selectivities of database queries. The real attribute value distribution is adaptively approximated by a curve-fitting function using a query feedback mechanism. This approach has the advantages of requiring no extra database access overhead for gathering statistics and of being able to continuously adapt the value distribution through queries and updates. Experimental results show that the estimation accuracy of this approach is comparable to traditional methods based on statistics gathering. 1 Introduction  In most database systems, the task of query optimization is to choose an efficient execution plan. Best plan selection requires accurate estimates of the costs of alternative plans. One of the most important factors that affects plan cost is selectivity, which is the number of tuples satisfying a given predicate. Therefore, in most cases, the accuracy of selectivity estimates directly affects the choice of best p...
690|Parallel Query Processing|With relations growing larger and queries becoming more complex, parallel query processing is an increasingly attractive option for improving the performance of database systems. The objective of this paper is to examine the various issues encountered in parallel query processing and the techniques available for addressing these issues. The focus of the paper is on the join operation with both sort-merge join and hash joins being considered. Three types of parallelism can be exploited, namely intra-operator, inter-operator, and inter-query parallelism. In intra-operator parallelism the major issue is task creation, and the objective is to split a join operation into tasks in a manner such that the load can be spread evenly across a given number of processors. This is a challenge when the values on the join attribute are not uniformly distributed. Inter-operator parallelism can be achieved either through parallel execution of independent operations or through pipelining. In either case,...
691|GADT: A Probability Space ADT for Representing and Querying the Physical World|Large sensor networks are being widely deployed for measurement, detection, and monitoring applications. Many of these applications involve database systems to store and process data from the physical world. This data has inherent measurement uncertainties that are properly represented by continuous probability distribution functions (pdf&#039;s). We introduce a new object-relational data type, the Gaussian ADT GADT, that models physical data as gaussian pdf&#039;s, and we show that existing index structures can be used as fast access methods for GADT data. We also present a measure-theoretic model of probabilistic data and evaluate GADT in its light.
692|Dynamic Query Optimization on a Distributed Object Management Platform|A Distributed Object Management (DOM) architecture, when used as the infrastructure of a multidatabase system, not only enables easy and flexible interoperation of DBMSs, but also facilitates interoperation of the multidatabase system with other repositories that do not have DBMS capabilities. This is an important advantage, since most of data still resides on repositories that do not have DBMS capabilities. In this paper, we describe a dynamic query optimization technique for a multidatabase system, namely MIND, implemented on a DOM environment. Dynamic query optimization, which schedules intersite operations at runtime, fits better to such an environment since it benefits from location transparency provided by the DOM framework. In this way, the dynamic changes in the configuration of system resources such as a relocated DBMS or a new mirror to an existing DBMS, do not affect the optimized query execution in the system. Furthermore, the uncertainty in estimating the appearance times ...
693|Memory Management during Run Generation in External Sorting|If replacement selection is used in an external mergesort to generate initial runs, individual records are deleted and in-  serted in the sort operation&#039;s workspace. Variable-length records introduce the need for possibly complex memory management and extra copying of records. As a result, few systems employ replacement selection, even though it produces longer runs than commonly used algorithms. We experimentally compared several algorithms and variants for managing this workspace. We found that the simple best fit algorithm achieves memory utilization of 90% or better and run lengths over 1.8 times workspace size, with no extra copying of records and very little other overhead, for widely varying record sizes and for a wide range of memory sizes. Thus, replacement selection is a viable algorithm for commercial database systems, even for variable-length records.  Efficient memory management also enables an external sort algorithm that degrades gracefully when its input is only slightl...
694|Linked Data -- The story so far  |The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions- the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.
695|DBpedia: A Nucleus for a Web of Open Data|Abstract DBpedia is a community effort to extract structured informa-tion from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data. 1
696|Duplicate record detection: A survey|Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area.  
697|Sindice.com: A document-oriented lookup index for open linked data |Developers of Semantic Web applications face a challenge with respect to the decentralised publication model: how and where to find statements about encountered resources. The “linked data” approach mandates that resource URIs should be de-referenced to return resource metadata. But for data discovery linkage itself is not enough, and crawling and indexing of data is necessary. Existing Semantic Web search engines are focused on database-like functionality, compromising on index size, query performance and live updates. We present Sindice, a lookup index over resources crawled on the Semantic Web. Our index allows applications to automatically locate documents containing information about a given resource. In addition, we allow resource retrieval through uniquely identifying inverse-functional properties, offer a full-text search and index SPARQL endpoints. Finally we introduce an extension to the sitemap protocol which allows us to efficiently index large Semantic Web datasets with minimal impact on the data providers.
698|Querying Distributed RDF Data Sources with SPARQL|Abstract. Integrated access to multiple distributed and autonomous RDF data sources is a key challenge for many semantic web applications. As a reaction to this challenge, SPARQL, the W3C Recommendation for an RDF query language, supports querying of multiple RDF graphs. However, the current standard does not provide transparent query federation, which makes query formulation hard and lengthy. Furthermore, current implementations of SPARQL load all RDF graphs mentioned in a query to the local machine. This usually incurs a large overhead in network traffic, and sometimes is simply impossible for technical or legal reasons. To overcome these problems we present DARQ, an engine for federated SPARQL queries. DARQ provides transparent query access to multiple SPARQL services, i.e., it gives the user the impression to query one single RDF graph despite the real data being distributed on the web. A service description language enables the query engine to decompose a query into sub-queries, each of which can be answered by an individual service. DARQ also uses query rewriting and cost-based query optimization to speed-up query execution. Experiments show that these optimizations significantly improve query performance even when only a very limited amount of statistical information is available. DARQ is available under GPL License at
699|Principles of dataspace systems|The most acute information management challenges today stem from organizations relying on a large number of diverse, interrelated data sources, but having no means of managing them in a convenient, integrated, or principled fashion. These challenges arise in enterprise and government data management, digital libraries, “smart ” homes and personal information management. We have proposed dataspaces as a data management abstraction for these diverse applications and DataSpace Support Platforms (DSSPs) as systems that should be built to provide the required services over dataspaces. Unlike data integration systems, DSSPs do not require full semantic integration of the sources in order to provide useful services. This paper lays out specific technical challenges to realizing DSSPs and ties them to existing work in our field. We focus on query answering in DSSPs, the DSSP’s ability to introspect on its content, and the use of human attention to enhance the semantic relationships in a dataspace.  
701|Triplify -- Light-Weight Linked Data Publication from Relational Databases|In this paper we present Triplify – a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.
702|Named Graphs|The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such named graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. Named graphs are given an abstract syntax, a formal semantics, an XML syntax, and a syntax based on N3. SPARQL is a query language applicable to named graphs. A specific application area discussed in detail is that of describing provenance information. This paper provides a formally defined framework suited to being a foundation for the Semantic Web trust layer.
703|Bootstrapping pay-as-you-go data integration systems|Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary. This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.
704|Provenance Information in the Web of Data|The openness of the Web and the ease to combine linked data from different sources creates new challenges. Systems that consume linked data must evaluate quality and trustworthiness of the data. A common approach for data quality assessment is the analysis of provenance information. For this reason, this paper discusses provenance of data on the Web and proposes a suitable provenance model. While traditional provenance research usually addresses the creation of data, our provenance model also represents data access, a dimension of provenance that is particularly relevant in the context of Web data. Based on our model we identify options to obtain provenance information and we raise open questions concerning the publication of provenance-related metadata for linked data on the Web.
705|Automatic Interlinking of Music Datasets on the Semantic Web |In this paper, we describe current efforts towards interlinking music-related datasets on the Web. We first explain some initial interlinking experiences, and the poor results obtained by taking a naïve approach. We then detail a particular interlinking algorithm, taking into account both the similarities of web resources and of their neighbours. We detail the application of this algorithm in two contexts: to link a Creative Commons music dataset to an editorial one, and to link a personal music collection to corresponding web identifiers. The latter provides a user with personally meaningful entry points for exploring the web of data, and we conclude by describing some concrete tools built to generate and use such links.
706|The Open Provenance Model|The Open Provenance Model (OPM) is a community-driven data model for Provenance that is designed to support inter-operability of provenance technology. Underpinning OPM, is a notion of directed acyclic graph, used to represent data products and processes involved in past computations, and causal dependencies between these. The Open Provenance Model was derived following two “Provenance Challenges”, international, multidisciplinary activities trying to investigate how to exchange information between multiple systems supporting provenance and how to query it. The OPM design was mostly driven by practical and pragmatic considerations, and is being tested in a third Provenance Challenge, which has just started. The purpose of this paper is to investigate the theoretical foundations of this data model. The formalisation consists of a set-theoretic definition of the data model, a definition of the inferences by transitive closure that are permitted, a formal description of how the model can be used to express dependencies in past computations, and finally, a description of the kind of time-based inferences that are supported. A novel element that OPM introduces is the concept of an account, by which multiple descriptions of a same execution are allowed to co-exist in a same graph. Our formalisation gives a precise meaning to such accounts and associated notions of alternate and refinement. Warning It was decided that this paper should be released as early as possible since it brings useful clarifications on the Open Provenance Model, and therefore can benefit the Provenance Challenge 3 community. The reader should recognise that this paper is however an early draft, and several sections are incomplete. Additionally, figures rely on colours but these may be difficult to read when printed in a black and white. It is advisable to print the paper in colour. 1 1
707|Which Semantic Web?|Through scenarios in the popular press and technical papers in the research literature, the promise of the Semantic Web has raised a number of different expectations. These expectations can be traced to three different perspectives on the Semantic Web. The Semantic Web is portrayed as: (1) a universal library, to be readily accessed and used by humans in a variety of information use contexts; (2) the backdrop for the work of computational agents completing sophisticated activities on behalf of their human counterparts; and (3) a method for federating particular knowledge bases and databases to perform anticipated tasks for humans and their agents. Each of these perspectives has both theoretical and pragmatic entailments, and a wealth of past experiences to guide and temper our expectations. In this paper, we examine all three perspectives from rhetorical, theoretical, and pragmatic viewpoints with an eye toward possible outcomes as Semantic Web efforts move forward.
708|M.: Linked movie data base|The Linked Movie Database (LinkedMDB) project provides a demonstration of the first open linked dataset connecting several major existing (and highly popular) movie web resources. The database exposed by LinkedMDB contains millions of RDF triples with hundreds of thousands of RDF links to existing web data sources that are part of the growing Linking Open Data cloud, as well as to popular movierelated web pages such as IMDb. LinkedMDB uses a novel way of creating and maintaining large quantities of high quality links by employing state-of-the-art approximate join techniques for finding links, and providing additional RDF metadata about the quality of the links and the techniques used for deriving them.
709|A Framework for Semantic Link Discovery over Relational Data |In this paper, we present a framework for online discovery of semantic links from relational data. Our framework is based on declarative specification of the linkage requirements by the user, that allows matching data items in many real-world scenarios. These requirements are translated to queries that can run over the relational data source, potentially using the semantic knowledge to enhance the accuracy of link discovery. Our framework lets data publishers to easily find and publish high-quality links to other data sources, and therefore could significantly enhance the value of the data in the next generation of web.
710|How will we interact with the web of data|The Semantic Web is a global information space of linked data, designed not for human use but for consumption by machines. Right? Well, yes and no. It&#039;s true to say that machine-readable data,  given explicit semantics and published online,  coupled with the ability to link data in distributed data sets are the key selling points of the Semantic Web. Together, these features allow aggregation and integration of heterogeneous data on an unprecedented scale,  and machines will do the grunt work for us. However, without a human being somewhere in this process, to reap the rewards of these new capabilities, the endeavour is meaningless. Far from removing human beings from the equation, a Web of machine-readable data creates significant challenges and significant opportunities for human-computer interaction. To date,  the Semantic Web community has mostly been busy developing the technical infrastructure to make the Web of Data feasible in principle and on publishing linked data sets in order to make it a reality. If we are to fully exploit the challenges and opportunities of a Web of Data from a human perspective,  we need to move beyond the initial phase and work to understand how this changes the user interaction paradigm of the Web.
711|What is the Size of the Semantic Web|Abstract: When attempting to build a scaleable Semantic Web application, one has to know about the size of the Semantic Web. In order to be able to understand the characteristics of the Semantic Web, we examined an interlinked dataset acting as a representative proxy for the Semantic Web at large. Our main finding was that regarding the size of the Semantic Web, there is more than the sheer number of triples; the number and type of links is an equally crucial measure.
712|Tabulator Redux: Browsing and Writing Linked Data |second frame shows information within that source expanded, the third frame shows another source within that source expanded, and finally, the last frame shows that the label of that source has been edited from “Music and artist data interlinked ” to “Music and artist data linked on the Semantic Web” A first category of Semantic Web browsers was designed to present a given dataset (an RDF graph) for perusal in various forms. These include mSpace, Exhibit, and to a certain extent
713|Integration of semantically annotated data by the knofuss architecture|Abstract. Most of the existing work on information integration in the Semantic Web concentrates on resolving schema-level problems. Specific issues of data-level integration (instance coreferencing, conflict resolu-tion, handling uncertainty) are usually tackled by applying the same techniques as for ontology schema matching or by reusing the solutions produced in the database domain. However, data structured according to OWL ontologies has its specific features: e.g., the classes are organized into a hierarchy, the properties are inherited, data constraints differ from those defined by database schema. This paper describes how these fea-tures are exploited in our architecture KnoFuss, designed to support data-level integration of semantic annotations. 1
714|DBpedia Mobile - A Location-Aware Semantic Web Client|Abstract. DBpedia Mobile is a location-aware client for the Semantic Web that can be used on an iPhone and other mobile devices. Based on the current GPS position of a mobile device, DBpedia Mobile renders a map indicating nearby locations from the DBpedia dataset. Starting from this map, the user can explore background information about his surroundings by navigating along data links into other Web data sources. DBpedia Mobile has been designed for the use case of a tourist exploring a city. As the application is not restricted to a fixed set of data sources but can retrieve and display data from arbitrary Web data sources, DBpedia Mobile can also be employed within other use cases, including ones un-foreseen by its developers. Besides accessing Web data, DBpedia Mobile also enables users to publish their current location, pictures and reviews to the Semantic Web so that they can be used by other Semantic Web applications. Instead of simply being tagged with geographical coordi-nates, published content is interlinked with a nearby DBpedia resource and thus contributes to the overall richness of the Geospatial Semantic Web.
715|Information-seeking on the Web with Trusted Social Networks – from Theory to Systems|This research investigates how synergies between the Web and social networks can enhance the process of obtaining relevant and trustworthy information. A review of literature on personalised search, social search, recommender systems, social networks and trust propagation reveals limitations of existing technology in areas such as relevance, collaboration, task-adaptivity and trust. In response to these limitations I present a Web-based approach to information-seeking using social networks. This approach takes a source-centric perspective on the information-seeking process, aiming to identify trustworthy sources of relevant information from within the user&#039;s social network. An empirical study of source-selection decisions in information- and recommendationseeking identified five factors that influence the choice of source, and its perceived trustworthiness. The priority given to each of these factors was found to vary according to the criticality and subjectivity of the task. A series of algorithms have been developed that operationalise three of these factors (expertise, experience, affinity) and generate from various data sources a number of trust metrics for use in social network-based information seeking. The most significant of these data sources is Revyu.com, a reviewing and rating Web site implemented as part of this research, that takes input from regular users and makes it available on the Semantic Web for easy re-use by the implemented algorithms. Output of the algorithms is used in Hoonoh.com, a Semantic Web-based system that has been developed to support users in identifying relevant and trustworthy information   sources within their social networks. Evaluation of this system&#039;s ability to predict source selections showed more promising results for the experience factor than for expertise or affinity. This may be attributed to the greater demands these two factors place in terms of input data. Limitations of the work and opportunities for future research are discussed.  
716|An Efficient Routing Protocol for Wireless Networks|We present
717|Power-law distributions in empirical data|Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the empirical detection and characterization of power laws is made difficult by the large fluctuations that occur in the tail of the distribution. In particular, standard methods such as least-squares fitting are known to produce systematically biased estimates of parameters for power-law distributions and should not be used in most circumstances. Here we describe statistical techniques for making accurate parameter estimates for power-law data, based on maximum likelihood methods and the Kolmogorov-Smirnov statistic. We also show how to tell whether the data follow a power-law distribution at all, defining quantitative measures that indicate when the power law is a reasonable fit to the data and when it is not. We demonstrate these methods by applying them to twentyfour real-world data sets from a range of different disciplines. Each of the data sets has been conjectured previously to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.
718|A Brief History of Generative Models for Power Law and Lognormal Distributions |Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a a lognormal distribution. In trying
719|A Random Graph Model for Massive Graphs|  We propose a random graph model which is a special case of sparse random graphs with given degree sequences. This model involves only a small number of parameters, called logsize and log-log growth rate. These parameters capture some universal characteristics of massive graphs. Furthermore, from these parameters, various properties of the graph can be derived. For example, for certain ranges of the parameters, we will compute the expected distribu-tion of the sizes of the connected components which almost surely occur with high probability. We will illustrate the consistency of our model with the behavior of some massive graphs derived from data in telecommunications. We will also discuss the threshold function, the giant component, and the evolution of random graphs in this model.  
720|Collective entity resolution in relational data|Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.
721|Where mathematics meets the Internet|The Internet has experienced a fascinating evolution in the recent past, especially since the early days of the Web, a fact well-documented not only in the trade journals, but also in the popular press. Unprecedented in its growth, unparalleled in its heterogeneity, and unpredictable or even chaotic in the behavior of its tra c, \the Internet is its own revolution&#034;, as Anthony-Michael Rutkowski, former Executive Director of the Internet Society, likes to put it.
722|A Functional Approach to External Graph Algorithms|. We present a new approach for designing external graph algorithms  and use it to design simple external algorithms for computing connected components,  minimum spanning trees, bottleneck minimum spanning trees, and maximal  matchings in undirected graphs and multi-graphs. Our I/O bounds compete  with those of previous approaches. Unlike previous approaches, ours is purely  functional---without side effects---and is thus amenable to standard checkpointing  and programming language optimization techniques. This is an important  practical consideration for applications that may take hours to run.  1 Introduction  We present a divide-and-conquer approach for designing external graph algorithms, i.e., algorithms on graphs that are too large to fit in main memory. Our approach is simple to describe and implement: it builds a succession of graph transformations that reduce to sorting, selection, and a recursive bucketing technique. No sophisticated data structures are needed. We apply our t...
723|Problems with fitting to the powerlaw distribution |Abstract. This short communication uses a simple experiment to show that fitting to a power law distribution by using graphical methods based on linear fit on the log-log scale is biased and inaccurate. It shows that using maximum likelihood estimation (MLE) is far more robust. Finally, it presents a new
724|Functional and topological characterization of protein interaction networks|The elucidation of the cell’s large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network’s generic large-scale properties and the impact of the proteins ’ function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network’s structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies. Keywords: Bioinformatics / Protein interaction networks / Scale-free networks 1
725|On the bias of traceroute sampling: or, power-law degree distributions in regular graphs|Understanding the graph structure of the Internet is a crucial step for building accurate network models and designing efficient algorithms for Internet applications. Yet, obtaining this graph structure can be a surprisingly difficult task, as edges cannot be explicitly queried. For instance, empirical studies of the network of Internet Protocol (IP) addresses typically rely on indirect methods like traceroute to build what are approximately single-source, all-destinations, shortest-path trees. These trees only sample a fraction of the network’s edges, and a recent paper by Lakhina et al. found empirically that the resulting sample is intrinsically biased. Further, in simulations, they observed that the degree distribution under traceroute sampling exhibits a power law even when the underlying degree distribution is Poisson. In this paper, we study the bias of traceroute sampling mathematically and, for a very general class of underlying degree distributions, explicitly calculate the distribution that will be observed. As example applications of our machinery, we prove that traceroute sampling finds power-law degree distributions in both d-regular and Poisson-distributed random graphs. Thus, our work puts the observations of Lakhina et al. on a rigorous footing, and extends them to nearly arbitrary degree distributions.
726|Currency and commodity metabolites: Their identification and relation to the modularity of metabolic networks|The large-scale shape and function of metabolic networks are intriguing topics of systems biology. Such networks are on one hand commonly regarded as modular (i.e. built by a number of relatively independent subsystems), but on the other hand they are robust in a way not expected of a purely modular system. To address this question we carefully discuss the partition of metabolic networks into subnetworks. The practice of preprocessing such networks by removing the most abundant substrates, “currency metabolites,” is formalized into a network-based algorithm. We study partitions for metabolic networks of many organisms and find cores of currency metabolites and modular peripheries of what we call “commodity metabolites.” The networks are found to be more modular than random networks but far from perfectly divisible into modules. We argue that cross-modular edges are the key for the robustness of metabolism. 
727|Likelihood-Based Inference for Stochastic Models of Sexual Network Formation|Sexually-Transmitted Diseases (STDs) constitute a major public health concern. Mathematical models for the transmission dynamics of STDs indicate that heterogeneity in sexual activity level allow them to persist even when the typical behavior of the population would not support endemicity. This insight focuses attention on the distribution of sexual activity level in a population. In this paper, we develop several stochastic process models for the f&#039;ormation of sexual partnership networks. Using likelihood-based model selection procedures, we assess the fit of the different models to three large distributions of sexual partner counts: (1) Rakai, Uganda, (2) Sweden, and (3) the USA. Five of&#039; the six single-sex networks were fit best by the negative binomial model. The American women&#039;s network was best fit by a power-law model, the Yule. For most networks, several competing models fit approximately equally well. These results sug- gest three conclusions: (1) no single unitary process clearly underlies the formation of these sexual networks, (2) behavioral heterogeneity plays an essential role in network structure, (3) substantial model uncertainty exists for sexual network degree distributions. Behavioral research focused on the mechanisms of partnership f&#039;ormation will play an essential role in specifying the best model for empirical degree distributions. We discuss the limitations of inferences f&#039;rom such data, and the utility of degree-based epidemiological models more generally.
728|Editorial: The future of power law research |Abstract. I argue that power law research must move from focusing on observation, interpretation, and modeling of power law behavior to instead considering the challenging problems of validation of models and control of systems. 1. The Problem with Power Law Research To begin, I would like to recall a humorous insight from the paper of Fabrikant, Koutsoupias, and Papadimitriou [Fabrikant et al. 01], consisting of this quote and the following footnote. “Power laws... have been termed ‘the signature of human activity’... ” 1 The study of power laws, especially in networks, has clearly exploded over the last decade, with seemingly innumerable papers and even popular books, such as Barabási’s Linked [Barabási 02] and Watts ’ Six Degrees [Watts 03]. Power laws are, indeed, everywhere. Despite this remarkable success, I believe that research into power laws in computer networks (and networks more generally) suffers from glaring deficiencies that need to be addressed by the community. Coping with these deficiencies should lead to another great burst of exciting and compelling research. To explain the problem, I would like to make an analogy to the area of string theory. String theory is incredibly rich and beautiful mathematically, with a simple and compelling basic starting assumption: the universe’s building blocks do not really correspond to (zero-dimensional) points, but to small 1 “They are certainly the product of one particular kind of human activity: looking for power laws... ” [Fabrikant et al. 01]
729|DYNAMICS OF BAYESIAN UPDATING WITH DEPENDENT DATA AND MISSPECIFIED MODELS|Recent work on the convergence of posterior distributions under Bayesian updating has established conditions under which the posterior will concentrate on the truth, if the latter has a perfect representation within the support of the prior, and under various dynamical assumptions, such as the data being independent and identically distributed or Markovian. Here I establish sufficient conditions for the convergence of the posterior distribution in non-parametric problems even when all of the hypotheses are wrong, and the data-generating process has a complicated dependence structure. The main dynamical assumption is the generalized asymptotic equipartition (or “Shannon-McMillan-Breiman”) property of information theory. I derive a kind of large deviations principle for the posterior measure, and discuss the advantages of predicting using a combination of models known to be wrong. An appendix sketches connections between the present results and the “replicator dynamics” of evolutionary theory.  
730|The QQ-Estimator And Heavy Tails|. A common visual technique for assessing goodness of fit and estimating location and scale is the qq--plot. We apply this technique to data from a Pareto distribution and more generally to data generated by a distribution with a heavy tail. A procedure for assessing the presence of heavy tails and for estimating the parameter of regular variation is discussed which can supplement other standard techniques such as the Hill plot. 1. Introduction.  A graphical technique called the qq-plot is a commonly used method of visually assessing goodness of fit and of estimating location and scale parameters. The method is standard and ubiquitious in various forms. See for example Rice (1988) and Castillo (1988). The method is based on the following simple observation: If  U 1;n  U 2;n  : : : U n;n  are the order statistics from n iid observations which are uniformly distributed on [0; 1], then by symmetry E(U i+1;n \Gamma U i;n ) = 1  n + 1 and hence  EU i;n =  i n + 1  :  Thus since U i;n should...
731|On the frequency of severe terrorist events|The online version of this article can be found at:
732|Radial structure of the internet|The structure of the Internet at the autonomous system (AS) level has been studied by the
733|Estimating heavy–tail exponents through max self–similarity|2 Heavy tailed data • A random variable X is said to be heavy–tailed if P{|X |  = x}  ~ L(x)x -a, as x ? 8, for some a&gt; 0 and a slowly varying function L. ? Here we focus on the simpler but important context: X = 0, a.s. and P{X&gt; x}  ~ Cx -a, as x ? 8. ? X (infinite moments) For p&gt; 0, EX p &lt; 8 if and only if p &lt; a. In particular, and 0 &lt; a = 2 ? Var(X)  = 8 0 &lt; a = 1 ? E|X |  = 8. • The estimation of the heavy–tail exponent a is an important problem with rich history. • Why do we need heavy–tail models? Every finite sample X1,..., Xn has finite sample mean, variance and all sample moments! Why consider heavy tailed models in practice?! 3 Why use heavy–tailed models? “All models are wrong, but some are useful.” George Box Let F and G be any two distributions with positive densities on (0, 8). Let ?&gt; 0 and x1,..., xn ? (0, 8) be arbitrary, then both: and PF {Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 are positive! PG{Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 • For a given sample, very many models apply. • The ones that continue to work as the sample grows are most suitable. We next present real data sets of Financial, Insurance and Internet data. They can be very heavy tailed. 4 Traded volumes on the Intel stock
734|Empirical distributions of logreturns: Between the stretched exponential and the power law? Quantitative Finance |A large consensus now seems to take for granted that the distributions of empirical returns of financial time series are regularly varying, with a tail exponent b close to 3. First, we show by synthetic tests performed on time series with time dependence in the volatility with both Pareto and Stretched-Exponential distributions that for sample of moderate size, the standard generalized extreme value (GEV) estimator is quite inefficient due to the possibly slow convergence toward the asymptotic theoretical distribution and the existence of biases in presence of dependence between data. Thus it cannot distinguish reliably between rapidly and regularly varying classes of distributions. The Generalized Pareto distribution (GPD) estimator works better, but still lacks power in the presence of strong dependence. Then, we use a parametric representation of the tail of the distributions of returns of 100 years of daily return of the Dow Jones Industrial Average and over 1 years of 5-minutes returns of the Nasdaq Composite index, encompassing both a regularly varying distribution in one limit of the parameters and rapidly varying distributions of the class of the Stretched-Exponential (SE) and Log-Weibull distributions in other limits. Using the method of nested hypothesis testing (Wilks ’ theorem),
735|A Protocol for Packet Network Intercommunication|A protocol that supports the sharing of resources that exists in different packet switching networks is presented. The protocol provides for variation in individual network packet sizes, transmission failures, sequencing, flow control, end-to-end error checking, and the creation and destruction of logical process-to-process connections. Some implementation issues are considered, and problems such as internetwork routing, accounting, and timeouts are exposed.
736|Low power distributed MAC for ad hoc sensor radio networks|Abstract- Targeting at multi-hop wireless sensor networks, a set of low power MAC design principles have been proposed, and a novel ultra-low power MAC is designed to be distributed in nature to support scalability, survivability and adaptability requirements. Simple CSMA and spread spectrum technique are combined to trade off bandwidth and power efficiency. A distributed algorithm is used to do dynamic channel assignment. A novel wake-up radio scheme is incorporated to take advantage of new radio technologies. The notion of mobility awareness is introduced into an adaptive protocol to reduce network maintenance overhead. The resulted protocol shows much higher power efficiency for typical sensor network applications.
738|Latency-Aware Information Access with User-Directed Fetch Behaviour for Weakly-Connected Mobile Wireless Clients”, BBN|Abstract – Mobile wireless clients have highly variable network connectivity and available bandwidth. There are times when they may be completely disconnected from the larger Internet. This dynamism of connectivity poses unique constraints for the problem of information access in general, and Web access in particular. These and other factors (such as loaded servers and congested networks) contribute to unpredictably high response times in content retrieval. We examine the problem of improving the utility of information access applications for these imperfectly connected devices. In particular, we are concerned with three related problems: 1. Providing information to the user on the estimated response time to retrieve content, the freshness of cached content, and the status on the strength of connection to the
739|Verbal reports as data|The central proposal of this article is that verbal reports are data. Accounting for verbal reports, as for other kinds of data, requires explication of the mech-anisms by which the reports are generated, and the ways in which they are sensitive to experimental factors (instructions, tasks, etc.). Within the theoret-ical framework of human information processing, we discuss different types of processes underlying verbalization and present a model of how subjects, in re-sponse to an instruction to think aloud, verbalize information that they are attending to in short-term memory (STM). Verbalizing information is shown to affect cognitive processes only if the instructions require verbalization of information that would not otherwise be attended to. From an analysis of what would be in STM at the time of report, the model predicts what can reliably be reported. The inaccurate reports found by other research are shown to result from requesting information that was never directly heeded, thus forcing subjects to infer rather than remember their mental processes. After a long period of time during which stimulus-response relations were at the focus of attention, research in psychology is now seeking to understand in detail the mecha-nisms and internal structure of cognitive pro-cesses that produce these relations. In the limiting case, we would like to have process models so explicit that they could actually produce the predicted behavior from the in-formation in the stimulus.
740|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
741|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
742|Investigating the Energy Consumption of a Wireless Network Interface in an Ad Hoc Networking Environment|Energy-aware design and evaluation of network protocols requires knowledge of the energy consumption behavior of actual wireless interfaces. But little practical information is available about the energy consumption behavior of well-known wireless network interfaces and device specifications do not provide information in a form that is helpful to protocol developers. This paper describes a series of experiments which obtained detailed measurements of the energy consumption of an IEEE 802.11 wireless network interface operating in an ad hoc networking environment. The data is presented as a collection of linear equations for calculating the energy consumed in sending, receiving and discarding broadcast and pointto -point data packets of various sizes. Some implications for protocol design and evaluation in ad hoc networks are discussed. Keywords---energy consumption, IEEE 802.11, ad hoc networks I.
743|A Cluster-based Approach for Routing in Dynamic Networks|The design and analysis of routing protocols is an important issue in dynamic networks such as packet radio and ad-hoc wireless networks. Most conventional protocols exhibit their least desirable behavior for highly dynamic interconnection topologies. We propose a new methodology for routing and topology information maintenance in dynamic networks. The basic idea behind the protocol is to divide the graph into a number of overlapping clusters. A change in the network topology corresponds to a change in cluster membership. We present algorithms for creation of clusters, as well as algorithms to maintain them in the presence of various network events. Compared to existing and conventional routing protocols, the proposed cluster-based approach incurs lower overhead during topology updates and also has quicker reconvergence. The effectiveness of this approach also lies in the fact that existing routing protocols can be directly applied to the network -- replacing the nodes by clusters.  1 ...
744|Power Management Techniques for Mobile Communication|In mobile computing, power is a limited resource. Like other devices, communication devices need to be properly managed to conserve energy. In this paper, we present the design and implementation of an innovative transport level protocol capable of significantly reduc- ing the power usage of the communication device. The protocol achieves power savings by selectively choosing short periods of time to suspend communications and shut down the communication device. It manages the important task of queuing data for future delivery during periods of communication suspension, and decides when to restart communication. We also address the tradeoff between reducing power consumption and reducing delay for incoming data.
745|Quantifying the Energy Consumption of a Pocket Computer and a Java Virtual Machine|In tiffs paper, we examine the energy consumption era stateof -the-art pocket computer. UsLug a data acquisltion system, we measure the energy consumption of the ]tsy Pocket Computer, developed by Compaq Computer Corporation&#039;s Palp Alto Research Labs. We begin by showing that the energy usage characteristics of the Itsy differ markedly from that of a notebook computer. Then, since we expect that flexible software environments will become increasingly pre- valent on pocket computers, we consider applications running iu a Java environment. In particular, we explain some of the Java design tradeoffs applicable to pocket computers, and quantify their energy costs. For the design options we considered and the three workloads we studied, we find a maximum change in energy use of 25%.
746|A Taxonomy for Routing Protocols in Mobile Ad Hoc Networks|A Mobile Ad hoc NETwork (manet) is a mobile, multi-hop wireless network which is capable of autonomous operation. It is characterized by energy-constrained nodes, bandwidth-constrained, variable-capacity wireless links and dynamic topology, leading to frequent and unpredictable connectivity changes. In the absence of a fixed infrastructure, manet nodes cooperate to provide routing services, relying on each other to forward packets to their destination. Routing protocols designed for the fixed network are not effective in the dynamic and resource-constrained manet environment; many alternative routing protocols have been suggested. This report provides an overview of a number of manet routing protocols. More importantly, it defines a taxonomy that is suitable for examining a wide variety of protocols in a structured way and exploring tradeoffs associated with various design choices. The emphasis is on practical design and implementation issues rather than complexity analysis.
747|An Energy-consumption Model for Performance Analysis of Routing Protocols for Mobile Ad Hoc Networks|This report presents a model for evaluating the energy consumption behavior of a mobile ad hoc network. The model was used to examine the energy consumption of two wellknown manet routing protocols. Energy-aware performance analysis is shown to provide new insights into costly protocol behaviors and suggests opportunities for improvement at the protocol and link layers
748|Reducing Power Consumption for the Next Generation of PDAs: It&#039;s in the Network Interface!|The current trends in the computer industryseem to indicate that low-cost, light-weight personal digital assistants (PDAs) will emerge as a major player in the information access and mobile computing markets. An important issue which must be addressed for PDAs, however, is battery longevity. In this paper we examine this issue from the point of view of the possible policies employed by the PDA in controlling the sleep/active states of its network interface (NI) in order to most efficiently utilize its power resources without unduly compromising the timeliness of its user&#039;s ability to access information. In particular, we measured the power usage of two PDAs, the Apple Newton Messagepad and Sony Magic Link, and three NIs, the Metricom Ricochet Wireless Modem, and the AT&amp;T Wavelan operating at 915 MHz and 2.4 GHz. These measurements clearly indicate that the power drained by the network interface consititutes a large fraction of the total power used by the PDA. Based on these measurement...
749|Investigating the Energy Consumption of an IEEE 802.11 Network Interface|This report describes a series of simple experiments which measure the per-packet energy consumption of an IEEE 802.11 wireless network interface. The goal of this work is to develop a solid experimental basis for assumptions that can (or cannot) be made in the design and analysis of network protocols operating in the ad hoc wireless environment.  Keywords: wireless network interface, IEEE 802.11, energy consumption, energy-aware network protocols, ad hoc networks, measurement  1 Motivation  Energy consumption at the network interface is an issue for all mobile computing devices, whether they operate within a base-station infrastructure or in a free-standing mobile ad hoc network (manet).  In a wireless ad hoc network, a host communicates directly with hosts within wireless range and indirectly with all other hosts using a dynamically-computed, multi-hop route via the other hosts of the manet. There has been a great deal of interest in the design and analysis of network protocols for t...
750|The x-Kernel: An Architecture for Implementing Network Protocols|This paper describes a new operating system kernel, called the x-kernel, that provides an  explicit architecture for constructing and composing network protocols. Our experience  implementing and evaluating several protocols in the x-kernel shows that this architecture  is both general enough to accommodate a wide range of protocols, yet efficient enough to  perform competitively with less structured operating systems.  1 Introduction  Network software is at the heart of any distributed system. It manages the communication hardware that connects the processors in the system and it defines abstractions through which processes running on those processors exchange messages. Network software is extremely complex: it must hide the details of the underlying hardware, recover from transmission failures, ensure that messages are delivered to the application processes in the appropriate order, and manage the encoding and decoding of data. To help manage this complexity, network software is divi...
751|The v distributed system|The V distributed System was developed at Stanford University as part of a research project to explore issues in distributed systems. Aspects of the design suggest important directions for the design of future operating systems and communication systems. 
752|The sprite network operating system|Sprite is a new operating system for networked uniprocessor and multiprocessor workstations with large physical memories. It implements a set of kernel calls much like those of 4.3 BSD UNIX, with extensions to allow processes on the same workstation to share memory and to allow processes to migrate between workstations. The implementation of the Sprite kernel contains several interesting features, including a remote procedure call facility for communication between kernels, the use of prefix tables to implement a single file name space and to provide flexibility in administering the network file system, and large variable-size file caches on both client and server machines, which provide high performance even for diskless workstations.
753|A Stream Input-Output System|In a new version of the Unix operating system, a flexible coroutine-based design replaces the traditional rigid connection between processes and terminals or networks. Processing modules may be inserted dynamically into the stream that connects a user&#039;s program to a device. Programs may also connect directly to programs, providing interprocess communication. Introduction  The part of the Unix operating system that deals with terminals and other character devices has always been complicated. In recent versions of the system it has become even more so, for two reasons. 1) Network connections require protocols more ornate than are easily accommodated in the existing structure. A notion of &#034;line disciplines&#034; was only partially successful, mostly because in the traditional system only one line discipline can be active at a time. 2) The fundamental data structure of the traditional character I/O system, a queue of individual characters (the &#034;clist&#034;), is costly because it accepts and dispense...
754|Preserving and Using Context Information in Interprocess Communication|ion  Psync is based on a conversation abstraction that provides a shared message space through which a collection of processes exchange messages. The general form of this message space is defined by a directed acyclic graph that preserves the partial order of the exchanged messages. For the purpose of this section, we view a conversation as an abstract data type that is implemented in shared memory; Section 3 gives an algorithm for implementing a conversation in an unreliable network.  A conversation behaves much like any connection-oriented IPC abstraction: A well-defined set of processes---called participants---explicitly open a conversation, exchange messages through it, and close the conversation. Only processes that have been identified as participants may exchange message through the conversation, and this set is fixed for the duration of the conversation. Processes begin a conversation with the operations:  conv = active open(participant set)  conv = passive open(pid)  The first...
755|Performance of Firefly RPC|In this paper, we report on the performance of the remote procedure call implementation for the Firefly multiprocessor and analyze the implemen-tation to account precisely for all measured latency. From the analysis and measurements, we estimate how much faster RPC could be if certain improve-ments were made. The elapsed time for an inter-machine call to a remote procedure that accepts no arguments and produces no results is 2.66 milliseconds. The elapsed time for an RPC that has a single 1440-byte result (the maximum result that will fit in a single packet) is 6.35 milliseconds. Maximum inter-machine throughput using RPC is 4.65 megabits/second, achieved with 4 threads making parallel RPCs that return the maximum sized single packet result. CPU utilization at maximum throughput is about 1.2 on the calling machine and a little less on the server. These measurements are for RPCs from user space on one machine to user space on another, using the installed system and a 10 megabit/second E...
756|Distributed Process Groups in the V Kernel|The V kernel supports an abstraction of processes, with operations for interprocess communication, process management, and memory management. This abstraction is used as a software base for constructing distributed systems. As a distributed kernel, the V kernel makes intermachine bound-aries largely transparent. In this environment of many cooperating processes on different machines, there are many logical groups of processes. Examples include the group of tile servers, a group of processes executing a particular job, and a group of processes executing a distributed parallel computation. In this paper we describe the extension of the V kernel to support process groups. Operations on groups include group interprocess communication, which provides an application-level abstraction of network multicast. Aspects of the implementation and performance, and initial experience with applications are discussed.
757|An overview of the SR language and implementation|SR is a language for programming distributed systems ranging from operating systems to application programs. On the basis of our experience with the initial version, the language has evolved consider-ably. In this paper we describe the current version of SR and give an overview of its implementation. The main language constructs are still resources and operations. Resources encapsulate processes and variables that they share; operations provide the primary mechanism for process interaction. One way in which SR has changed is that both resources and processes are now created dynamically. Another change is that inheritance is supported. A third change is that the mechanisms for operation invocation-call and send-and operation implementation-proc and in-have been extended and integrated. Consequently, all of local and remote procedure call, rendezvous, dynamic process creation, asynchronous message passing, multicast, and semaphores are supported. We have found this flexibility to be very useful for distributed programming. Moreover, by basing SR on a small number of well-integrated concepts, the language has proved easy to learn and use, and it has a reasonably efficient implementation.
758|Performance of the World&#039;s Fastest Distributed Operating System|Distributed operating systems have been in the experimental stage for a  number of years now, but few have progressed to the point of actually being  used in a production environment. It is our belief that the reason lies primarily  with the performance of these systems---they tend to be fairly slow  compared to traditional single computer systems. The Amoeba system has  been designed with high performance in mind. In this paper some performance  measurements of Amoeba are presented and comparisons are made  with UNIX on the SUN, as well as with some other interesting systems. In  particular, short remote procedure calls take 1.4 msec and long data transfers  achieve a user-to-user bandwidth of 677 kbytes/sec. Furthermore, the file  server is so fast that it is limited by the communication bandwidth to 677  kbytes/sec. The real speed of the file server is too high to measure. To the  best of our knowledge, these are the best figures yet reported in the literature  for the class of hard...
759|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
760|Berkeley UNIX+ on 1000 Workstations: Athena Changes to 4.3BSD |4.3BSD UNIX as shipped is designed for use on individually-managed, networked  timesharing systems. A large network of individual workstations and server machines,  all managed centrally, has many important differences from such a model. This paper  discusses some of the changes necessary for 4.3 in this new world, including the file system  layout, configuration files, and software. The integration with Athena&#039;s authentication  system, name service, and service management system are also discussed.  1. Overview  &#034;By 1988, create a new educational computing environment environment at MIT built around high-performance graphics workstations, high-speed networking, and servers of various types.&#034; This one-sentence statement is a highlevel description of the technical goals of Project Athena. While the primary goals are to enhance education, attaining them has required a significant effort to engineer a software environment for use in a large network of workstations and servers.  The Athena...
761|Agile Application-Aware Adaptation for Mobility|In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a benchmark of three applications concurrently using remote services over a network with highly variable bandwidth.  
762|Ants: A toolkit for building and dynamically deploying network protocols|We present a novel approach to building and deploying network protocols. The approach is based on mobile code, demand loading, and caching techniques. The architecture of our system allows new protocols to be dynamically deployed at both routers and end systems, without the need forcoordination and without unwanted interaction between co-existing protocols. In this paper, we describe our architecture and its realization in a prototype implementation. To demonstrate how to exploit our architecture, we present two simple protocols that operate within our prototype to introduce multicast and mobility services into a network that initially lacks them. 1
763|Disco: Running commodity operating systems on scalable multiprocessors|In this paper we examine the problem of extending modern operating systems to run efficiently on large-scale shared memory multiprocessors without a large implementation effort. Our approach brings back an idea popular in the 1970s, virtual machine monitors. We use virtual machines to run multiple commodity operating systems on a scalable multiprocessor. This solution addresses many of the challenges facing the system software for these machines. We demonstrate our approach with a prototype called Disco that can run multiple copies of Silicon Graphics ’ IRIX operating system on a multiprocessor. Our experience shows that the overheads of the monitor are small and that the approach provides scalability as well as the ability to deal with the non-uniform memory access time of these systems. To reduce the memory overheads associated with running multiple operating systems, we have developed techniques where the virtual machines transparently share major data structures such as the program code and the file system buffer cache. We use the distributed system support of modern operating systems to export a partial single system image to the users. The overall solution achieves most of the benefits of operating systems customized for scalable multiprocessors yet it can be achieved with a significantly smaller implementation effort. 1
764|The J-Machine Multicomputer: An Architectural Evaluation|The MIT J-Machine multicomputer has been constructed to study the role of a set of primitive mechanisms in providing efficient support for parallel computing. Each J-Machine node consists of an integrated multicomputer component, the Message-Driven Processor (MDP), and 1 MByte of DRAM. The MDP provides mechanisms to support efficient communication, synchronization, and naming. A 512 node J-Machine is operational and is due to be expanded to 1024 nodes in March 1993. In this paper we discuss the design of the J-Machine and evaluate the effectiveness of the mechanisms incorporated into the MDP. We measure the performance of the communication and synchronization mechanisms directly and investigate the behavior of four complete applications.  1 Introduction  Over the past 40 years, sequential von Neumann processors have evolved a set of mechanisms appropriate for supporting most sequential programming models. It is clear, however, from efforts to build concurrent machines by connecting man...
765|Scylla: A Smart Virtual Machine for Mobile Embedded Systems|With the proliferation of wireless devices with embedded processors, there is an increasing desire to deploy applications that run transparently over the varied architectures of these devices. Virtual machines are one solution for code mobility, providing a virtualized processor architecture that is implemented over the individual node architectures. Proposed virtual machines for embedded systems are generally slow and consume significant energy, making them unsuitable for devices with limited processing power and energy resources.  Presented is a novel virtual machine architecture, Scylla, specially designed for mobile embedded systems, that is simple, fast and robust. In addition to a basic instruction set, Scylla supports inter-device communication, power management and error recovery. To make on-the-fly compilation extremely efficient, the instruction set closely matches popular processor architectures that can be found in embedded systems today. This paper describes Scylla, along ...
767|Expressing Meaningful Processing Requirements among Heterogeneous Nodes in an Active Network|Active Network technology envisions deployment of virtual execution environments within network elements, such as switches and routers. As a result, nonhomogeneous processing can be applied to network traffic associated with services, flows, or even individual packets. To use such a technology safely and efficiently, individual nodes must provide mechanisms to enforce resource limits. To provide effective enforcement mechanisms, each node must have a meaningful understanding of the resource requirements for specific network traffic. In Active Network nodes, resource requirements typically come in three categories: bandwidth, memory, and processing. Well-accepted metrics exist for expressing bandwidth (bits per second) and memory (bytes) in units independent of the capabilities of particular nodes. Unfortunately, no well-accepted metric exists for expressing processing (i.e., CPU time) requirements in a platformindependent form. This paper investigates a method to express the CPU time r...
768|A Strategic Model of Social and Economic Networks|We study the stability and efficiency of social and economic networks, when selfinterested individuals can form or sever links. First, for two stylized models, we characterize the stable and efficient networks. There does not always exist a stable network that is efficient. Next, we show that this tension persists generally: to assure that there exists a stable network that is efficient, one is forced to allocate resources to nodes that are not responsible for any of the production. We characterize one such allocation rule: the equal split rule, and another rule that arises naturally from bargaining of the players. Journal Economic Literature Classification
769|Systems Competition and Network Effects|Many products have little or no value in isolation, but generate value when combined with others. Examples include: nuts and bolts, which together provide fastening services; home audio or video components and programming, which together provide entertainment services; automobiles, repair parts and service, which together provide transportation services; facsimile machines and their associated communications protocols, which together provide fax services; automatic teller machines and ATM cards, which together provide transaction services; camera bodies and lenses, which together provide photographic services. These are all examples of products that are strongly complementary, although they need not be consumed in fixed proportions. We describe them as forming systems, which refers to collections of two or more components together with an interface that allows the components to work together. This paper and the others in this symposium explore the economics of such systems. Market competition between systems, as opposed to market competition between individual products, highlights at least three important issues: expectations, coordination, and compatibility. A recent wave of research has focused on the behavior and performance of the variety of private and public institutions that arise in systems markets to influence expectations, facilitate coordination, and achieve compatibility. In many cases, the components purchased for a single system are spread over time, which means that rational buyers must form expectations about
770|Social Networks and Labor-Market Outcomes: Towards an Economic Analysis|prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
771|R&amp;D networks|We develop a model of strategic networks that captures two distinctive features of interfirm collaboration: bilateral agreements and nonexclusive relationships. Our analysis highlights the relationship between market competition, firms ’ incentives to invest in R&amp;D, and the architecture of collaboration networks. In the absence of firm rivalry, the complete network, where each firm collaborates with all others, is uniquely stable, industry-profit maximizing, and efficient. By contrast, under strong market rivalry the complete network is stable, but intermediate levels of collaboration and asymmetric networks are more attractive from a collective viewpoint. This suggests that competing firms may have excessive incentives to form collaborative links. 1.
772|Organizational design and technology choice with nonbinding contracts|We present a new methodology for studying the problem of labor contracting within a firm&#039;s boundaries where contracts provide only a minimal commitment to wages and employment. Given the peculiar contractual incompleteness of labor contracts, the resulting wages and pro ts under an interesting class of complete information bargaining games distort the technological and organizational decisions facing the owner of the firm&#039;s capital. We show that in such settings where labor contracts are nonbinding, these decisions are distorted in an economically distinct way compared to the standard neoclassical firm. Among other things, we demonstrate that a firm with a nonbinding contractual basis will, relative to a neoclassical firm, (i) overemploy labor, (ii) underemploy capital, (iii) choose inefficient &#034;frontloaded &#034; technologies, (iv) de-emphasize scale and scope economies, and (v) inefficiently allocate labor across productive assets. We apply our analysis to product market competition, unionization, hierarchical management, and horizontal mergers.
773|Routing Techniques in Wireless Sensor Networks: A Survey|Wireless Sensor Networks (WSNs) consist of small nodes with sensing, computation, and wireless communications capabilities. Many routing, power management, and data dissemination protocols have been specifically designed for WSNs where energy awareness is an essential design issue. The focus, however, has been given to the routing protocols which might differ depending on the application and network architecture. In this paper, we present a survey of the state-of-the-art routing techniques in WSNs. We first outline the design challenges for routing protocols in WSNs followed by a comprehensive survey of different routing techniques. Overall, the routing techniques are classified into three categories based on the underlying network structure: flat, hierarchical, and location-based routing. Furthermore, these protocols can be classified into multipath-based, query-based, negotiation-based, QoS-based, and coherent-based depending on the protocol operation. We study the design tradeoffs between energy and communication overhead savings in every routing paradigm. We also highlight the advantages and performance issues of each routing technique. The paper concludes with possible future research areas. 1
774|Secure Routing in Wireless Sensor Networks: Attacks and Countermeasures|We  consider routing  security in wireless sensor networks. Many sensor  network routing  protocols have been proposed, but none of them have been designed with security as agq1( We propose  securitygcur forrouting  in sensor networks, show how attacks agacks ad-hoc and peer-to-peer networks can be adapted into powerful attacks agacks  sensor networks, introduce two classes of novel attacks agacks sensor networks----sinkholes and HELLO floods, and analyze the security of all the major sensor  networkrouting  protocols. We describe crippling  attacks against all of them and sug@(5 countermeasures anddesig considerations. This is the first such analysis of  secure routing  in sensor networks.
775|Energy Aware Routing for Low Energy Ad Hoc Sensor Networks|The recent interest in sensor networks has led to a number of routing schemes that use the limited resources available at sensor nodes more efficiently. These schemes typically try to find the minimum energy path to optimize energy usage at a node. In this paper we take the view that always using lowest energy paths may not be optimal from the point of view of network lifetime and long-term connectivity. To optimize these measures, we propose a new scheme called energy aware routing that uses sub-optimal paths occasionally to provide substantial gains. Simulation results are also presented that show increase in network lifetimes of up to 40% over comparable schemes like directed diffusion routing. Nodes also burn energy in a more equitable way across the network ensuring a more graceful degradation of service with time.
776|Rumor Routing Algorithm for Sensor Networks|Advances in micro-sensor and radio technology will enable small but smart sensors to be deployed for a wide range of environmental monitoring applications. In order to constrain communication overhead, dense sensor networks call for new and highly efficient methods for distributing queries to nodes that have observed interesting events in the network. A highly efficient data-centric routing mechanism will offer significant power cost reductions [17], and improve network longevity. Moreover, because of the large amount of system and data redundancy possible, data becomes disassociated from specific node and resides in regions of the network [10][7][8]. This paper describes and evaluates through simulation a scheme we call Rumor Routing, which allows for queries to be delivered to events in the network. Rumor Routing is tunable, and allows for tradeoffs between setup overhead and delivery reliability. It&#039;s intended for contexts in which geographic routing criteria are not applicable because a coordinate system is not available or the phenomenon of interest is not geographically correlated.
777|An Energy Efficient Hierarchical Clustering Algorithm for Wireless Sensor Networks|A wireless network consisting of a large number of small sensors with low-power transceivers can be an effective tool for gathering data in a variety of environments. The data collected by each sensor is communicated through the network to a single processing center that uses all reported data to determine characteristics of the environment or detect an event. The communication or message passing process must be designed to conserve the Hmited energy resources of the sensors. Clustering sensors into groups, so that sensors communicate information only to clusterheads and then the clusterheads communicate the aggregated information to the processing center, may save energy. In this paper, we propose a distributed, randomized clustering algorithm to organize the sensors in a wireless sensor network into clusters. We then extend this algorithm to generate a hierarchy of clusterheads and observe that the energy savings increase with the number of levels in the hierarchy. Results in stochastic geometry are used to derive solutions for the values of parameters of our algorithm that minimize the total energy spent in the network when all sensors report data through the clusterheads to the processing center.
778|SPEED: A Stateless Protocol for Real-Time Communication In Sensor Networks|In this paper, we present a real-time communication protocol for sensor networks, called SPEED. The protocol provides three types of real-time communication services, namely, real-time unicast, real-time area-multicast and real-time area-anycast. SPEED is specifically tailored to be a stateless, localized algorithm with minimal control overhead End-to-end soft real-time communication is achieved by maintaining a desired delivery speed across the sensor network through a novel combination of feedback control and non-deterministic geographic forwarding. SPEED is a highly efficient and scalable protocol for sensor networks where the resources of each node are scarce. Theoretical analysis, simulation experiments and a real implementation on Berkeley motes are provided to validate our claims.
779|Maximum Lifetime Routing In Wireless Sensor Networks|Routing in power-controlled wireless sensor networks is formulated as an optimization problem with the goal of maximizing the system lifetime. Considering that the information is delivered in the form of packets, we identified the problem as an integer programming problem. It is known that the system lifetime can be significantly extended by using a link metric that utilizes the information about the residual energy of the sensor nodes as well as the energy expenditure in transmission of a unit information over the wireless links. In this paper, some of the routing algorithms are proposed and examined in order to find the best link cost function and the method of shortest path calculation. The performance comparison is made through simulation in a typical battlefield scenario where sensors detecting a moving target vehicle periodically send a reporting packet to one of the gateway nodes. The results are also compared with the optimal solution obtained by the linear programming relaxation of the problem, which showed close-to-optimal performance.
780|A Taxonomy of Wireless Micro-Sensor Network Models|... This paper examines this emerging field to classify wireless micro-sensor networks according to different communication functions, data delivery models, and network dynamics. This taxonomy will aid in defining appropriate communication infrastructures for different sensor network application sub-spaces, allowing network designers to choose the protocol architecture that best matches the goals of their application. In addition, this taxonomy will enable new sensor network models to be defined for use in further research in this area.
781|Worst-Case Optimal and Average-Case Efficient Geometric Ad-Hoc Routing|In this paper we present GOAFR, a new geometric ad-hoc routing algorithm combining greedy and face routing. We evaluate this algorithm by both rigorous analysis and comprehensive simulation. GOAFR is the first ad-hoc algorithm to be both asymptotically optimal and average-case e#cient. For our simulations we identify a network density range critical for any routing algorithm. We study a dozen of routing algorithms and show that GOAFR outperforms other prominent algorithms, such as GPSR or AFR.
782|APTEEN: A hybrid protocol for efficient routing and comprehensive information retrieval in wireless sensor networks|Wireless sensor networks with thousands of tiny sensor nodes, are expected to find wide applicability and increas-ing deployment in coming years, as they enable reliable monitoring and analysis of the environment. In this paper, we propose a hybrid routing protocol (APTEEN) which al-lows for comprehensive information retrieval. The nodes in such a network not only react to time-critical situations, but also give an overall picture of the network at periodic in-tervals in a very energy efficient manner. Such a network enables the user to request past, present and future data from the network in the form of historical, one-time and per-sistent queries respectively. We evaluated the performance of these protocols and observe that these protocols are ob-served to outperform existing protocols in terms of energy consumption and longevity of the network. 1.
783|A Scalable Solution to Minimum Cost Forwarding in Large Sensor|Wireless sensor networks offer a wide range of challenges to networking research, including unconstrained network scale, limited computing, memory and energy resources, and wireless channel errors. In this paper, we study the problem of delivering messages from any sensor to an interested client user along the minimum-cost path in a large sensor network. We propose a new cost field based approach to minimum cost forwarding. In the design, we present a novel backoff-based cost field setup algorithm that finds the optimal costs of all nodes to the sink with one single message overhead at each node. Once the field is established, the message, carrying dynamic cost information, flows along the minimum cost path in the cost field. Each intermediate node forwards the message only if it finds itself to be on the optimal path, based on dynamic cost states. Our design does not require an intermediate node to maintain explicit &#034;forwarding path&#034; states. It requires a few simple operations and scales to any network size. We show the correctness and effectiveness of the design by both simulations and analysis.
784|Constrained Random Walks on Random Graphs: Routing Algorithms for Large Scale Wireless Sensor Networks|We consider a routing problem in the context of large scale networks with uncontrolled dynamics. A case of uncontrolled dynamics that has been studied extensively is that of mobile nodes, as this is typically the case in cellular and mobile ad-hoc networks. In this paper however we study routing in the presence of a different type of dynamics: nodes do not move, but instead switch between active and inactive states at random times. Our interest in this case is motivated by the behavior of sensor nodes powered by renewable sources, such as solar cells or ambient vibrations. In this paper we formalize the corresponding routing problem as a problem of constructing suitably constrained random walks on random dynamic graphs. We argue that these random walks should be designed so that their resulting invariant distribution achieves a certain load balancing property, and we give simple distributed algorithms to compute the local parameters for the random walks that achieve the sought behavior. A truly novel feature of our formulation is that the algorithms we obtain are able to route messages along all possible routes between a source and a destination node, without performing explicit route discovery/repair computations, and without maintaining explicit state information about available routes at the nodes. To the best of our knowledge, these are the first algorithms that achieve true multipath routing (in a statistical sense), at the complexity of simple stateless operations.
785|The ACQUIRE Mechanism for Efficient Querying in Sensor Networks|We propose a novel and efficient mechanism for obtaining information in sensor networks which we refer to as ACQUIRE. In ACQUIRE an active query is forwarded through the network, and intermediate nodes use cached local information (within a look-ahead of d hops) in order to partially resolve the query. When the query is fully resolved, a completed response is sent directly back to the querying node. We take a...
786|Lightweight sensing and communication protocols for target enumeration and aggregation|The development of lightweight sensing and communication protocols is a key requirement for designing resource constrained sensor networks. This paper introduces a set of efficient protocols and algorithms, DAM, EBAM, and EMLAM, for constructing and maintaining sensor aggregates that collectively monitor target activity in the environment. A sensor aggregate comprises those nodes in a network that satisfy a grouping predicate for a collaborative processing task. The parameters of the predicate depend on the task and its resource requirements. Since the foremost purpose of a sensor network is to selectively gather information about the environment, the formation of appropriate sensor aggregates is crucial for optimally allocating resources to sensing and communication tasks. This paper makes minimal assumptions about node onboard processing and communication capabilities so as to allow possible implementations on resource-constrained hardware. Factors affecting protocol performance are discussed. The paper presents simulation results showing how the protocol performance varies as key network and task parameters are varied. It also provides probabilistic analyses of network behavior consistent with the simulation results. The protocols have been experimentally validated on a sensor network testbed comprising 25 Berkeley MICA sensor motes.
787|Trade-Off between Traffic Overhead and Reliability in Multipath Routing for Wireless Sensor Networks|In wireless sensor networks (WSN) data produced by one or more sources usually has to be routed through several intermediate nodes to reach the destination. Problems arise when intermediate nodes fail to forward the incoming messages. The reliability of the system can be increased by providing several paths from source to destination and sending the same packet through each of them (the algorithm is known as multipath routing). Using this technique, the traffic increases significantly. In this paper, we analyze a new mechanism that enables the tradeoff between the amount of traffic and the reliability. The data packet is split in k subpackets (k = number of disjoined paths from source to destination). If only Ek subpackets (Ek &lt;k)are necessary to rebuild the original data packet (condition obtained by adding redundancy to each subpacket), then the trade-off between traffic and reliability can be controlled.
788|Hierarchical Power-aware Routing in Sensor Networks |This paper discusses online power-aware routing in large sensor networks. We seek to optimize the lifetime of the network. We develop an approximation algorithm called max-min zPmin that has a good empirical competitive ratio. To ensure scalability, we introduce a hierarchical algorithm, which is called zone-based routing.  
789|Parallel Networks that Learn to Pronounce English Text|This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (;i) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations. 
790|Transfer of Cognitive Skill|A framework for skill acquisition is proposed that includes two major stages in the development of a cognitive skill: a declarative stage in which facts about the skill domain are interpreted and a procedural stage in which the domain knowledge is directly embodied in procedures for performing the skill. This general framework has been instantiated in the ACT system in which facts are encoded in a propositional network and procedures are encoded as productions. Knowledge compilation is the process by which the skill transits from the declarative stage to the procedural stage. It consists of the subprocesses of composition, which collapses sequences of productions into single productions, and proceduralization, which embeds factual knowledge into productions. Once proceduralized, further learning processes operate on the skill to make the productions more selective in their range of applications. These processes include generalization, discrimination, and strengthening of productions. Comparisons are made to similar concepts from past learning theories. How these learning mechanisms apply to produce the power law speedup in processing time with practice is discussed. It requires at least 100 hours of learning and practice to acquire any significant cognitive skill to a reasonable degree of proficiency. For instance, after 100 hours a student learning to program a computer has achieved only a very modest facility in the skill. Learning one&#039;s primary language takes tens of thousands of hours. The psychology of human learning has been very thin in ideas about what happens to skills under the impact of this amount of learning—and for obvious reasons. This article presents a theory about the changes in the nature of a skill over such large time scales and about the basic learning processes that are responsible.
791|A learning algorithm for Boltzmann machines|The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the con-straints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a gen-eral learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure. 1.
792|Optimal perceptual inference|When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth&#039;value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:
794|Learning symmetry groups with hidden units: Beyond the perceptron|Learning to recognize mirror, rotational and translational symmetries is a difficult problem for massively-parallel network models. These symmetries cannot be learned by first-order perceptrons or Hopfield networks, which have no means for incorporating additional adaptive units that are hidden from the input and output layers. We demonstrate that the Boltzmann learning algorithm is capable of finding sets of weights which turn hidden units into useful higher-order feature detectors capable of solving symmetry problems. 1.
795|The spacing effect on NETtalk, a massively-parallel network|NETtalk is a massively-parallel network that learns to convert English text to phonemes. In NETtalk, the memory representations are shared among many processing units, and these representations are learned by practice. In humans, distributed practice is more effective for longterm retention than massed practice, and we wondered whether learning in NETtalk had similar properties. NETtalk was tested on cued paired-associate recall using nonwords as stimuli. Retention of these target items was measured as a function of spacing, or the number of interspersed items between successive repetitions of the target. A significant advantage for spaced or distributed items was found for spacings of up to forty intervening items when tested at a retention interval of 64 items. Conversely, a significant advantage for massed items was found if testing immediately followed study. These results&#039;are strikingly similar to the results of many experiments using human subjects and suggest an explanation based on distributed representations
796|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
797|Optimal Clock Synchronization|We present a simple, efficient, and unified solution to the problems of synchronizing, initializing, and integrating clocks for systems with different types of failures: crash, omission, and arbitrary failures with and without message authentication. This is the ft known solution that achieves optimal accuracy -- the accuracy of synchronized clocks (with respect to real time) is as good as that specified for the underlying hardware clocks. The solution is also optimal with respect to the number of faulty processes that can be tolerated to achieve this accuracy.
798|Dynamic Fault-Tolerant Clock Synchronization|This paper gives two simple efficient distributed algorithms: one for keeping clocks in a network synchronized and one for allowing new processors to join the network with their clocks synchronized. Assuming a fault tolerant authentication protocol, the algorithms tolerate both link and processor failures of any type. The algorithm for maintaining synchronization works for arbitrary networks (rather than just completely connected networks) and tolerates any number of processor or communication link faults as long as the correct processors remain connected by fault-free paths. It thus represents an improvement over other clock synchronization algorithms such as [LM,WL], although, unlike them, it does require an authentication protocol to handle Byzantine faults. Our algorithm for allowing new processors to join requires that more than half the processors be correct, a requirement that is provably necessary. 1 Introduction  In a distributed system it is often necessary for processors to ...
799|A New Fault-Tolerant Algorithm for Clock Synchronization|We describe a new fault-tolerant algorithm for solving a variant of Lamport&#039;s clock synchronization problem. The algorithm is designed for a system of distributed processes that communicate by sending messages. Each process has its own read-only physical clock whose drift rate from real time is very small. By adding a value to its physical clock time, the process obtains its local time. The algorithm solves the problem of .maintaining closely synchronized local times, assuming that processes&#039; local times are clo. sely synchronized initially. The algorithm is able to tolerate the failure of just under a third of the participating processes. It maintains synchronization to within a small constant, whose magnitude depends upon the rate of clock drift, the message delivery time, and the initial closeness of synchronization. We also give a characterization of how far the clocks drift from real time. Reintegration of a repaired process can be accomplished using a slight modification of the &#039;basic algorithm. A similar style algorithm can also be used to achieve synchronization initially.
800|A Paradigm for Reliable Clock Synchronization|Existing fault-tolerant clock synchronization protocols are shown to result from refining a  single clock synchronization paradigm. In that paradigm, a reliable time source  periodically issues messages that cause processors to resynchronize their clocks. The  reliable time source is approximated by reading all clocks in the system and using a  convergence function to compute a fault-tolerant average of the values read. The  performance of a clock synchronization algorithm based on the paradigm can be quantified  in terms of the two parameters that characterize the behavior of the convergence function  used: accuracy and precision.
801|The Fuzzball|The Fuzzball is an operating system and applications library designed for the PDP11 family of  computers. It was intended as a development platform and research pipewrench for the DARPA/NSF  Internet, but has occasionally escaped to earn revenue in commercial service. It was designed,  implemented and evolved over a seventeen-year era spanning the development of the ARPANET  and TCP/IP protocol suites and can today be found at Internet outposts from Hawaii to Italy standing  watch for adventurous applications and enduring experiments. This paper describes the Fuzzball and  its applications, including a description of its novel congestion avoidance/control and timekeeping  mechanisms.&lt;E-110&gt; Keywords: protocol testing, network testing, performance evaluation, Internet architecture, TCP/IP protocols, congestion control, internetwork time synchronization.&lt;E-132&gt; 1. Introduction&lt;E-128&gt; The Fuzzball is a software package consisting of a fast, compact operating system, support for the DARPA/...
802|Measured performance of the Network Time Protocol in the Internet system. DARPA Network Working Group Report RFC-1128|This paper describes a series of experiments involving over 100,000 hosts of the Internet system and located in the U.S., Europe and the Pacific. The experiments are designed to evaluate the availability, accuracy and reliability of international standard time distribution using the DARPA/NSF Internet and the Network Time Protocol (NTP), which is specified as an Internet Standard in RFC-1119. NTP is designed specifically for use in a large, diverse internet system operating at speeds from mundane to lightwave. In NTP a distributed subnet of time servers operating in a self-organizing, hierarchical, master-slave configuration exchange precision timestamps in order to synchronize subnet clocks to each other and national time standards via wire or radio. The experiments are designed to locate Internet hosts and gateways that provide time by one of three time distribution protocols and evaluate the accuracy of their indications. For those hosts that support NTP, the experiments determine the distribution of errors and other statistics over paths spanning major portions of the globe. Finally, the experiments evaluate the accuracy and reliability of precision timekeeping using NTP and typical Internet paths involving DARPA, NSFNET and other agency networks. The experiments demonstrate that timekeeping accuracy throughout most
803|The National Bureau of Standards atomic time scale: Generation, stability, accuracy and accessibility|Absrruct-The independent atomic time scale at the National Bureau of Standards AT(NBS), is based upon an ensemble of continuously operating cesium clocks calibrated occasionally by an NBS primary frequency standard. The data of frequency calibrations and interclock comparisons are statistically processed to provide nearly optimum time stability and frequency accuracy. The long-term random fluctuation of AT(NBS) due to nondeterministic perturbations is estimated to be a few parts in and the present accuracy is inferred to be 1 part in A small coordinate rate is added to the rate of AT(NBS) to generate UTC(NBS): this small addition is for the purpose of maintaining syn-chronization within a few microseconds of other international timing centers. IJTQNBS) is readily operationally available over a large part of the world via WWV. WWVH, WWVB, and telephone; also via some passwe time transfer systems, e.g., Loran € and the TV line-10 system; and also experimentaliy via satellite and WWVL. The precision and ac-
805|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.
806|AN n 5/2 ALGORITHM FOR MAXIMUM MATCHINGS IN BIPARTITE GRAPHS|The present paper shows how to construct a maximum matching in a bipartite graph with n vertices and m edges in a number of computation steps proportional to (m + n)x/.
807|E-Cell : Software environment for whole cell simulation|We present E-CELL, a generic computer software environment for modeling a cell and conducting experiments in silico. The E-CELL system allows a user to de ne functions of proteins, protein-protein interactions, protein-DNA interactions, regulation of gene expression and other features of cellular metabolism, in terms of a set of reaction rules. The system then executes those reactions iteratively, and the user can observe, through a computer display, dynamic changes in concentrations of proteins, protein complexes and other chemical compounds in the cell. Using this software, we constructed a model of a hypothetical cell with only 127 genes su cient for transcription, translation, energy production and phospholipid synthesis. Most of the genes are taken from Mycoplasma genitalium, the organism having the smallest known chromosome, whose complete 580kb genome sequence was determined at TIGR in 1995. We discuss future applications of the E-CELL system with special respect to genome engineering.
808|Conservation analysis in biochemical networks: computational issues for software writers|Large scale genomic studies are generating significant amounts of data on the structure of cellular networks. This is in contrast to kinetic data which is frequently absent, unreliable or fragmentary. There is therefore a desire by many in the community to investigate the potential rewards of analyzing the more readily available topological data. This brief review is concerned with a particular property of biological networks, namely structural conservations (e.g. moiety conserved cycles). There has been much discussion in the literature on these cycles but a review on the computational issues related to conserved cycles has been missing 1. This review is concerned with the detection and characterization of conservation relations in arbitrary networks and related issues which impinge on simulation simulation software writers. This review will not address flux balance constraints or small-world type analyses in any significant detail. Contact:
809|Link-Sharing and Resource Management Models for Packet Networks| This paper discusses the use of link-sharing mechanisms in packet networks and presents algorithms for hierarchical link-sharing. Hierarchical link-sharing allows multiple agencies, protocol families, or traflic types to share the bandwidth on a tink in a controlled fashion. Link-sharing and real-time services both require resource management mechanisms at the gateway. Rather than requiring a gateway to implement separate mechanisms for link-sharing and real-time services, the approach in this paper is to view link-sharing and real-time service requirements as simultaneous, and in some respect complementary, constraints at a gateway that can be implemented with a unified set of mechanisms. White it is not possible to completely predict the requirements that might evolve in the Internet over the next decade, we argue that controlled link-sharing is an essential component that can provide gateways with the flexibility to
810|Random Early Detection Gateways for Congestion Avoidance|This paper presents Random Early Detection (RED) gate-ways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by com-puting the average queue size. The gateway could notify connections of congestion either by dropping packets ar-riving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold,the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a func-tion of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection&#039;s share of the bandwidth throughthe gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP.The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connectionsdecreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways. 
812|Analysis, Modeling and Generation of Self-Similar VBR Video Traffic|We present a detailed statistical analysis of a 2-hour long empirical sample of VBR video. The sample was obtained by applying a simple intraframe video compression code to an action movie. The main findings of our analysis are (1) the tail behavior of the marginal bandwidth distribution can be accurately described using &#034;heavy-tailed&#034; distributions (e.g., Pareto); (2) the autocorrelation of the VBR video sequence decays hyperbolically (equivalent to long-range dependence) and can be modeled using self-similar processes. We combine our findings in a new (non-Markovian) source model for VBR video and present an algorithm for generating synthetic traffic. Trace-driven simulations show that statistical multiplexing results in significant bandwidth efficiency even when long-range dependence is present. Simulations of our source model show long-range dependence and heavy-tailed marginals to be important components which are not accounted for in currently used VBR video traffic models. 1 I...
813|Scalable Feedback Control for Multicast Video Distribution in the Internet|We describe a mechanism for scalable control of multicast continuous media streams. The mechanism uses a novel probing mechanism to solicit feedback information in a scalable manner and to estimate the number of receivers. In addition, it separates the congestion signal from the congestion control algorithm, so as to cope with heterogeneous networks. This mechanism has been implemented in the IVS videoconference system using options within RTP to elicit information about the quality of the video delivered to the receivers. The H.261 coder of IVS then uses this information to adjust its output rate, the goal being to maximize the perceptual quality of the image received at the destinations while minimizing the bandwidth used by the video transmission. We find that our prototype control mechanism is well suited to the Internet environment. Furthermore, it prevents video sources from creating congestion in the Internet. Experiments are underway to investigate how the scalable probing mech...
814|Connections with Multiple Congested Gateways in Packet-Switched Networks Part 1: One-way Traffic|In this paper we explore the bias in TCP/IP networks against connections with multiple congested gateways. We consider the interaction between the bias against connections with multiple congested gateways, the bias of the TCP window modification algorithm against connections with longer roundtrip times, and the bias of Drop Tail and Random Drop gateways against bursty traffic. Using simulations and a heuristic analysis, we show that in a network with the window modification algorithm in 4.3 tahoe BSD TCP and with Random Drop or Drop Tail gateways, a longer connection with multiple congested gateways can receive unacceptably low throughput. We show that in a network with no bias against connections with longer roundtrip times and with no bias against bursty traffic, a connection with multiple congested gateways can receive an acceptable level of throughput. We discuss the application of several current measures of fairness to networks with multiple congested gateways, and show that diff...
815|Network Support For Multimedia: A Discussion of the Tenet Approach|Multimedia communication can be supported in an integrated-services network in the general framework of realtime communication. The Tenet Group has devised an approach that provides some initial solutions to the realtime communication problem. This paper attempts to identify the principles behind these solutions. We also describe a suite of protocols, and their implementations in several environments, that embody these principles, and work in progress that will lead towards more complete solutions.   This research was supported by the National Science Foundation and the Defense Advanced Research Projects Agency (DARPA) under CooperativeAgreement NCR-8919038 with the Corporation for National Research Initiatives, by AT&amp;T Bell Laboratories, Digital Equipment Corporation, Hitachi, Ltd., Hitachi America, Ltd., Pacific Bell, the University of California under a MICRO grant, and the International Computer Science Institute. The views and conclusions contained in this document are those of th...
816|A Scheduling Service Model and a Scheduling Architecture for an Integrated Services Packet Network|Integrated Services Packet Networks (ISPN) are designed to integrate the network service requirements of a wide variety of computer-based applications. Some of these services are delivered primarily through the packet scheduling algorithms used in the network switches. This paper addresses two questions related to these scheduling algorithms. The first question is: what scheduling services should an ISPN offer? In answer, we propose a scheduling service model for ISPN&#039;s which is based on our projections about future application and institutional service requirements. Our service model includes both a delay-related component designed to meet the ergonomic requirements of individual applications, and also a hierarchical link-sharing component designed to meet the economic needs of resource sharing between different entities. The second question we address is: what implications does this service model have for the packet scheduling algorithms? We answer this question by construc...
817|Implementing Real Time Packet Forwarding Policies using Streams|This paper describes an implementation of the class based queueing (CBQ) mechanisms proposed by Sally Floyd and Van Jacobson [1] [2] to provide real time policies for packet forwarding. CBQ allows the traffic flows sharing a data link to be guaranteed a share of the bandwidth when the link is congested, yet allows flexible sharing of the unused bandwidth when the link is unloaded. In addition, CBQ provides mechanisms which give flows requiring low delay priority over other flows. In this way, links can be shared by multiple flows yet still meet the policy and Quality of Service (QoS) requirements of the flows.  We present a brief description of the implementation and some preliminary performance measurements. The problems of packet classification are addressed in a flexible and extensible, yet efficient manner, and whilst the Streams implementation cannot cope with very high speed interfaces, it can cope with the serial link speeds that are likely to be loaded. 
818|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
819|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
820|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
821|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
822|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
823|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
824|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
825|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
826|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
827|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
828|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
829|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
830|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
831|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
832|An Investigation of the Therac-25 Accidents|A thorough account of the Therac-25 medical electron accelerator accidents reveals previously unknown details and suggests ways to reduce risk in the future. omputers are increasingly being introduced into safety-critical systems and, as a consequence, have been involved in accidents. Some of the most widely cited software-related accidents in safety-critical systems involved a computerized radiation therapy machine called the Therac-25. Between June 1985 and January 1987, six known accidents involved massive overdoses by the Therac-25-with resultant deaths and serious injuries. They have been described as the worst series of radiation accidents in the 35year history of medical acceler-
833|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
834|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
835|Knit: Component Composition for Systems Software|Knit is a new component definition and linking language for systems code. Knit helps make C code more understandable and reusable by third parties, helps eliminate much of the performance overhead of componentization, detects subtle errors in component composition that cannot be caught with normal component type systems, and provides a foundation for developing future analyses over C-based components, such as cross-component optimization. The language is especially designed for use with component kits, where standard linking tools provide inadequate support for component configuration. In particular, we developed Knit for use with the OSKit, a large collection of components for building low-level systems. However, Knit is not OSKit-specific, and we have implemented parts of the Click modular router in terms of Knit components to illustrate the expressiveness and flexibility of our language. This paper provides an overview of the Knit language and its applications.  
836|A Statically Allocated Parallel Functional Language|We describe SAFL, a call-by-value first-order functional language which is syntactically restricted so that storage may be statically allocated to fixed locations. Evaluation of independent sub-expressions happens in parallel - we use locking techniques to protect shared-use function definitions (i.e. to prevent unrestricted parallel accesses to their storage locations for argument and return values). SAFL programs have a well defined notion of total (program and data) size which we refer to as `area&#039;; similarly we can talk about execution `time&#039;. Fold/unfold transformations on SAFL provide mappings between different points on the area-time spectrum. The space of functions expressible in SAFL is incomparable with the space of primitive recursive functions, in particular interpreters are expressible. The motivation behind SAFL is hardware description and synthesis|we have built an optimising compiler for translating SAFL to silicon.
837|Event-Driven FRP|Functional Reactive Programming (FRP) is a high-level declarative language for programming reactive systems. Previous work on FRP has demonstrated its utility in a wide range of application domains, including animation, graphical user interfaces, and robotics. FRP has an elegant continuous-time denotational semantics. However, it guarantees no bounds on execution time or space, thus making it unsuitable for many embedded real-time applications. To alleviate this problem, we recently developed Real-Time FRP (RT-FRP), whose operational semantics permits us to formally guarantee bounds on both execution time and space. In this paper we present a formally verifiable compilation strategy from a new language based on RT-FRP into imperative code. The new language, called Event-Driven FRP (E-FRP), is more tuned to the paradigm of having multiple external events. While it is smaller than RT-FRP, it features a key construct that allows us to compile the language into efficient code. We have used this language and its compiler to generate code for a small robot controller that runs on a PIC16C66 micro-controller. Because the formal specification of compilation was crafted more for clarity and for technical convenience, we describe an implementation that produces more efficient code.
838|Programming Language Techniques for Modular Router Configurations|This paper applies programming language techniques to a high-level system  description, both to optimize the system and to prove useful properties about  it. The system in question is Click, a modular software router framework [13]. Click routers
839|MACAW: Media access protocol for wireless lans|In recent years, a wide variety of mobile computing devices has emerged, including portables, palmtops, and personal digit al assistants. Providing adequate network connectivity y for these devices will require a new generation of wireless LAN technology. In this paper we study media access protocols for a single channel wireless LAN being developed at Xerox Corporation’s Palo Alto Research Center. We start with the MACA media access protocol first proposed by Karn [9] and later refined by Biba [3] which uses an RTS-CTS-DATA packet exchange and binary exponential backoff. Using packet-level simulations, we examine various performance and design issues in such protocols, Our analysis leads to a new protocol, MACAW, which uses an RTS-CTS-DS-DATA-ACK message exchange and includes a significantly different backoff algorithm. 1
840|PAMAS -- Power Aware Multi-Access protocol with Signalling for Ad Hoc Networks|  In this paper we develop a new multiaccess protocol for ad hoc radio networks. The protocol is based on the original MACA protocol with the adition of a separate signalling channel. The unique feature of our protocol is that it conserves battery power at nodes by intelligently powering off nodes that are not actively transmitting or receiving packets. The manner in which nodes power themselves off does not influence the delay or throughput characteristics of our protocol. We illustrate the power conserving behavior of PAMAS via extensive simulations performed over ad hoc networks containing 10-20 nodes. Our results indicate that power savings of between 10 % and 70 % are attainable in most systems. Finally, we discuss how the idea of power awareness can be built into other multiaccess protocols as well.  
841|The Bluetooth radio system|A few years ago it was recognized that the vision of a truly low-cost, low-power radio-based cable replacement was feasible. Such a ubiquitous  link would provide the basis for portable devices to communicate together in an ad hoc fashion by creating personal area networks which have  similar advantages to their office environment counterpart, the local area network. Bluetooth is an effort by a consortium of companies to  design a royalty-free technology specification enabling this vision. This article describes the radio system behind the Bluetooth concept. Designing an ad hoc radio system for worldwide usage poses several challenges. The article describes the critical system characteristics and motivates the  design choices that have been made.
842|Performance of a novel self-organization protocol for wireless ad hoc sensor networks|Abstract- The paper presents an ad-hoc architecture for wireless sensor networks and other wireless sys-tems similar to them. In this class of wireless system the physical resource at premium is energy. Band-width available to the system is in excess of system requirements. The approach to solve the problem of ad-hoc network formation here is to use available bandwidth in order to save energy. The method introduced solves the problem of connecting an ad-hoc network. This algorithm gives procedures for the joint formation of a time schedule (similar to a TDMA schedule) and activation of links therein for random network topologies.This self-organization method is energy-sensitive, distributed, scalable, and able to form a connected network rapidly. I.
843|The structure and function of complex networks|Inspired by empirical studies of networked systems such as the Internet, social networks, and biological networks, researchers have in recent years developed a variety of techniques and models to help us understand or predict the behavior of these systems. Here we review developments in this field, including such concepts as the small-world effect, degree distributions, clustering, network correlations, random graph models, models of network growth and preferential attachment, and dynamical processes taking place on networks.
846|The Average Distance in a Random Graph with Given Expected Degrees | Random graph theory is used to examine the “small-world phenomenon”– any two strangers are connected through a short chain of mutual acquaintances. We will show that for certain families of random graphs with given expected degrees, the average distance is almost surely of order log n / log ˜ d where ˜ d is the weighted average of the sum of squares of the expected degrees. Of particular interest are power law random graphs in which the number of vertices of degree k is proportional to 1/k ß for some fixed exponent ß. For the case of ß&gt; 3, we prove that the average distance of the power law graphs is almost surely of order log n / log ˜ d. However, many Internet, social, and citation networks are power law graphs with exponents in the range 2 &lt; ß &lt; 3 for which the power law random graphs have average distance almost surely of order log log n, but have diameter of order log n (provided having some mild constraints for the average distance and maximum degree). In particular, these graphs contain a dense subgraph, that we call the core, having n c / log log n vertices. Almost all vertices are within distance log log n of the core although there are vertices at distance log n from the core.
847|Topology of evolving networks: local events and universality |Networks grow and evolve by local events, such as the addition of new nodes and links, or rewiring of links from one node to another. We show that depending on the frequency of these processes two topologically different networks can emerge, the connectivity distribution following either a generalized power-law or an exponential. We propose a continuum theory that predicts these two regimes as well as the scaling function and the exponents, in good agreement with the numerical results. Finally, we use the obtained predictions to fit the connectivity distribution of the network describing the professional links between movie actors. 1 The complexity of numerous social, biological or communication systems is rooted in the rather interwoven web defined by the system’s components and their interactions. For example, cell functioning is guaranteed by a complex metabolic network, whose nodes are substrates and enzymes, and edges represent chemical interactions [1]. Similarly, the society is characterized by a huge social network whose nodes are individuals or organizations,
849|Resilience of the internet to random breakdowns|A common property of many large networks, including the Internet, is that the connectivity of the various nodes follows a scale-free power-law distribution, P(k)  = ck -a. We study the stability of such networks with respect to crashes. Our approach, based on percolation theory, leads to a general condition for the critical fraction of nodes, pc, that need to be removed before the network disintegrates. We show that for a = 3 the transition never takes place, unless the network is finite. In the special case of the Internet (a ˜ 2.5), we find that it is impressively robust, with pc ˜ 0.999. Recently there has been increasing interest in the formation of random networks and in the connectivity of these networks, especially in the context of the Internet [1–7]. When such networks are subject to random breakdowns- a fraction p of the nodes and their connections are removed randomly- their integrity might be compromised: when p exceeds a certain threshold, p&gt; pc, the network disintegrates into smaller, disconnected parts. Below that critical threshold, there still exists a connected cluster that spans the entire system (its size
850|The Small World Web|I show that the World Wide Web is a small world, in the sense that sites are highly clustered yet the path length between them is small. I also demonstrate the advantages of a search engine which makes use of the fact that pages corresponding to a particular search query can form small world networks. In a further application, the search engine uses the small-worldness of its search results to measure the connectedness between communities on the Web.
851|Chains of affection: The structure of adolescent romantic and sexual networks|This article describes the structure of the adolescent romantic and sexual network in a population of over 800 adolescents residing in a midsized town in the midwestern United States. Precise images and measures of network structure are derived from reports of relationships that occurred over a period of 18 months between 1993 and 1995. The study offers a comparison of the structural characteristics of the observed network to simulated networks conditioned on the distribution of ties; the observed structure reveals networks characterized by longer contact chains and fewer cycles than expected. This article identifies the micromechanisms that generate networks with structural features similar to the observed network. Implications for disease transmission dynamics and social policy are explored.
852|Competition and multiscaling in evolving networks |The rate at which nodes in a network increase their connectivity depends on their fitness to compete for links. For example, in social networks some individuals acquire more social links than others, or on the www some webpages attract considerably more links than others. We find that this competition for links translates into multiscaling, i.e. a fitness dependent dynamic exponent, allowing fitter nodes to overcome the more connected but less fit ones. Uncovering this fitter-gets-richer phenomena can help us understand in quantitative terms the evolution of many competitive systems in nature and society. PACS numbers: 5.65+b, 89.75-k, 89.75Fb, 89.75Hc. Typeset using REVTEXThe complexity of many systems can be attributed to the interwoven web in which their constituents interact with each other. For example, the society is organized in a social web, whose nodes are individuals and links represent various social interactions, or the www forms a complex web whose nodes are documents and links are URLs. While for a long time these networks have been modeled as completely random [1, 2], recently there is increasing
853|The Origin of Power Laws in Internet Topologies Revisited|In a recent paper, Faloutsos et al. [1] found that the inter Autonomous System (AS) topology exhibits a power-law vertex degree distribution. This result was quite unexpected in the networking community and stirred significant interest in exploring the possible causes of this phenomenon. The work of Barabasi and Albert [2] and its application to network topology generation in the work of Medina et al. [3] have explored a promising class of models that yield strict power-law vertex degree distributions. In this paper, we re-examine the BGP measurements that form the basis for the results reported in [1]. We find that by their very nature (i.e., being strictly BGP-based), the data provides a very incomplete picture of Internet connectivity at the AS level. The AS connectivity maps constructed from this data (the original maps) typically miss 20--50% or even more of the physical links in AS maps constructed using additional sources (the extended maps). Subsequently, we find that while the vertex degree distributions resulting from the extended maps are heavy-tailed, they deviate significantly from a strict power law. Finally, we show that available historical data does not support the connectivity-based dynamics assumed in [2]. Together, our results suggest that the Internet topology at the AS level may well have developed over time following a very different set of growth processes than those proposed in [2].
854|Random Evolution in Massive Graphs|Many massive graphs (such as WWW graphs and Call graphs) share certain universal characteristics which can be described by socalled the &#034;power law&#034;. In this paper, we will first briefly survey the history and previous work on power law graphs. Then we will give four evolution models for generating power law graphs by adding one node/edge at a time. We will show that for any given edge density and desired distributions for in-degrees and out-degrees (not necessarily the same, but adhered to certain general conditions), the resulting graph will almost surely satisfy the power law and the in/out-degree conditions. We will show that our most general directed and undirected models include nearly all known models as special cases. In addition, we consider another crucial aspects of massive graphs that is called &#034;scale-free&#034; in the sense that the f requency of sampling (w.r.t. the growth rate) is independent of the parameter of the resulting power law graphs. We will show that our evolution models generate scale-free power law graphs. 1 
855|Internet topology: connectivity of IP graphs|In this paper we introduce a framework for analyzing local properties of Internet connectivity. We compare BGP and probed topology data, finding that currently probed topology data yields much denser coverage of AS-level connectivity. We describe data acquisition and construction of several IP-level graphs derived from a collection of 220M skitter traceroutes. We find that a graph consisting of IP nodes and links contains 90.5% of its 629K nodes in the acyclic subgraph. In particular, 55% of the IP nodes are in trees. Full bidirectional connectivity is observed for a giant component containing 8.3% of IP nodes.
856|Duplication models for biological networks|Are biological networks different from other large complex networks? Both large biological and nonbiological networks exhibit power-law graphs (number of nodes with degree k, N.k /  ? k ¡ ¯), yet the exponents, ¯, fall into different ranges. This may be because duplication of the information in the genome is a dominant evolutionary force in shaping biological networks (like gene regulatory networks and protein–protein interaction networks) and is fundamentally different from the mechanisms thought to dominate the growth of most nonbiological networks (such as the Internet). The preferential choice models used for nonbiological networks like web graphs can only produce power-law graphs with exponents greater than 2. We use combinatorial probabilistic methods to examine the evolution of graphs by node duplication processes and derive exact analytical relationships between the exponent of the power law and the parameters of the model. Both full duplication of nodes (with all their connections) as well as partial duplication (with only some connections) are analyzed. We demonstrate that partial duplication can produce power-law graphs with exponents less than 2, consistent with current data on biological networks. The power-law exponent for large graphs depends only on the growth process, not on the starting graph.
857|A p* primer: logit models for social networks|A major criticism of the statistical models for analyzing social networks developed by Holland, Leinhardt, and others wHolland, P.W., Leinhardt, S., 1977. Notes on the statistical analysis of social network data; Holland, P.W., Leinhardt, S., 1981. An exponential family of probability distributions for directed graphs. Journal of the American Statistical Association. 76, pp. 33–65 Ž with discussion.; Fienberg, S.E., Wasserman,
858|Bose-Einstein condensation in complex networks |The evolution of many complex systems, including the world wide web, business and citation networks is encoded in the dynamic web describing the interactions between the system’s constituents. Despite their irreversible and non-equilibrium nature these networks follow Bose statistics and can undergo Bose-Einstein condensation. Addressing the dynamical properties of these non-equilibrium systems within the framework of equilibrium quantum gases predicts that the ’first-mover-advantage’, ’fit-get-rich ’ and ’winner-takes-all’ phenomena observed in competitive systems are thermodynamically distinct phases of the underlying evolving networks.
859|Models for network evolution|Abstract: This paper describes mathematical models for network evolution when ties (edges) are directed and the node set is xed. Each of these models implies a speci c type of departure from the standard null binomial model. We provide statistical tests that, in keeping with these models, are sensitive to particular types of departures from the null. Each model (and associated test) discussed follows directly from one or more socio-cognitive theories about how individuals alter the colleagues with whom they are likely to interact. The models include triad completion models, degree variance models, polarization and balkanization models, the Holland-Leinhardt models, metric models, and the constructural model. We nd that many of these models, in their basic form, tend asymptotically towards an equilibrium distribution centered at the completely connected network (i.e., all individuals are equally likely to interact with all other individuals) ? a fact that can inhibit the development of satisfactory tests. Keywords: triad completion, Holland-Leinhardt model, polarization, degree variance, network evolution, constructuralism
860|Local search in unstructured networks|Recently, studies of networks in a wide variety of fields, from biology to social science to computer science, have revealed some commonalities [4]. It has become clear that the simplest classical model of random networks, the Erdos-Renyi model [8], is inadequate for describing the topology of many naturally occurring networks. These diverse networks are more
861|Topological Properties of Citation and Metabolic Networks|Topological properties of &#034;scale-free&#034; networks are investigated by determining their spectral dimensions d S , which reflect a diffusion process in the corresponding graphs. Data bases for citation networks and metabolic networks together with simulation results from the growing network model [1] are probed. For completeness and comparisons lattice, random, small-world models are also investigated. We find that d S is around 3 for citation and metabolic networks, which is significantly different from the growing network model, for which d S is approximately 7.5. This signals a substantial difference in network topology despite the observed similarities in vertex order distributions. In addition, the diffusion analysis indicates that whereas the citation networks are tree-like in structure, the metabolic networks contain many loops.
862|Some Analyses of Erdos Collaboration Graph|Patrick Ion (Mathematical Reviews) and Jerry Grossman (Oakland University) maintain a collection of data on Paul Erdos, his co-authors and their co-authors. These data can be represented by a graph, also called the Erdos collaboration graph. In the paper some techniques for analysis of large networks (different approaches to identify ’interesting ’ individuals and groups, analysis of internal structure of the main core using pre-specified blockmodeling and hierarchical clustering) and visualizations of their parts, are presented on the case of Erdos collaboration graph, using program Pajek.
863|Famous trails to Paul Erdos|The notion of Erdos number has floated around the mathematical research  community for more than thirty years, as a way to quantify the common knowledge  that mathematical and scientific research has become a very collaborative  process in the twentieth century, not an activity engaged in solely by isolated  individuals. In this paper we explore some (fairly short) collaboration paths  that one can follow from Paul Erdos to researchers inside and outside of  mathematics. In particular, we find that all the Fields Medalists up through  1998 have Erdos numbers less than 6, and that over 60 Nobel Prize winners in  physics, chemistry, economics, and medicine have Erdos numbers less than 9.  
864|Epidemic thresholds on scale-free graphs: the interplay between exponent and preferential choice |ar
865|RESEARCH Applying Network Theory to Epidemics: Control Measures for Mycoplasma pneumoniae Outbreaks |We introduce a novel mathematical approach to investigating the spread and control of communicable infections in closed communities. Mycoplasma pneumoniae is a major cause of bacterial pneumonia in the United States. Outbreaks of illness attributable to mycoplasma commonly occur in closed or semi-closed communities. These outbreaks are difficult to contain because of delays in outbreak detection, the long incubation period of the bacterium, and an incomplete understanding of the effectiveness of infection control strategies. Our model explicitly captures the patterns of interactions among patients and caregivers in an institution with multiple wards. Analysis of this contact network predicts that, despite the relatively low prevalence of mycoplasma pneumonia found among caregivers, the patterns of caregiver activity and the extent to which they are protected against infection may be fundamental to the control and prevention of mycoplasma outbreaks. In particular, the most effective interventions are those that reduce the diversity of interactions between caregivers and patients.
866|Ferromagnetic phase transition in Barabási-Albert networks|Abstract: Ising spins put onto a Barabási-Albert scale-free network show an effective phase transition from ferromagnetism to paramagnetism upon heating, with an effective critical temperature increasing as the logarithm of the system size. Starting with all spins up and upon equilibration pinning the few most-connected spins down nucleates the phase with most of the spins down.
867|Self-organized criticality on small world networks|We study the BTW-height model of self-organized criticality on a square lattice with some long range connections giving to the lattice the character of small world network. We find that as function of the fraction p of long ranged bonds the power law of the avalanche size and lifetime distribution changes following a crossover scaling law with crossover exponents 2/3 and 1 for size and lifetime respectively. Small world networks have recently been observed to exist in many physical, biological and social cases [1, 2, 3, 4, 5]. A simple minded example is the structure of the neo-cortex and the network of acquaintances in certain societies. Another ubiquitous phenomenon (SOC) [6, 7, 8] in nature and society is self-organized criticality, i.e. the appearance of avalanches of all sizes without characteristic scale over a certain range. Easily one can imagine cases in which both of these phenomena occur simultaneously, that means that we find an avalanche type spreading on a sparsely long-range connected network. As
868|Search and congestion in complex networks|In recent years, the study of static and dynamical properties of complex networks has received a lot of attention [1–5]. Complex networks appear in such diverse disciplines as sociology, biology, chemistry, physics or computer science. In particular, great effort has been exerted to understand the behavior of technologically
869|Reversible Boolean Networks I: Distribution of Cycle Lengths|We consider a class of models describing the dynamics of N Boolean variables, where the time evolution of each depends on the values of K of the other variables. Previous work has considered models with dissipative dynamics. Here we consider time-reversible models, which necessarily have the property that every possible point in the state-space is an element of one and only one cycle. As in the dissipative case, when K is large, typical orbit lengths grow exponentially with N, whereas for small enough K, typical orbit lengths grow much more slowly with N. The numerical data are consistent with the existence of a phase transition at which the average orbit length grows as a power of N at a value of K between 1.4 and 1.7. However, in the reversible models the interplay between the discrete symmetry and quenched randomness can lead to enormous fluctuations of orbit lengths and other interesting features that are unique to the reversible case. The orbits can be classified by their behavior under time reversal. The orbits that transform into themselves under time reversal have properties quite different from those that do not; in particular, a significant fraction of latter-type orbits have lengths enormously longer than orbits that are time reversal-symmetric. For large K and moderate N, the vast majority of points in the state-space are on one of the time reversal singlet orbits, and a random hopping model gives an accurate description of orbit lengths. However, for any finite K, the random hopping approximation fails qualitatively when N is large enough
870|Route Packets, Not Wires: On-Chip Interconnection Networks|Using on-chip interconnection networks in place of ad-hoc global wiring structures the top level wires on a chip and facilitates modular design. With this approach, system modules (processors, memories, peripherals, etc...) communicate by sending packets to one another over the network. The structured network wiring gives well-controlled electrical parameters that eliminate timing iterations and enable the use of high-performance circuits to reduce latency and increase bandwidth. The area overhead required to implement an on-chip network is modest, we estimate 6.6%. This paper introduces the concept of on-chip networks, sketches a simple network, and discusses some challenges in the architecture and design of these networks.  1
871|Token flow control |As companies move towards many-core chips, an efficient onchip communication fabric to connect these cores assumes critical importance. To address limitations to wire delay scalability and increasing bandwidth demands, state-of-the-art on-chip networks use a modular packet-switched design with routers at every hop which allow sharing of network channels over multiple packet flows. This, however, leads to packets going through a complex router pipeline at every hop, resulting in the overall communication energy/delay being dominated by the router overhead, as opposed to just wire energy/delay. In this work, we propose token flow control (TFC), a flow control mechanism in which nodes in the network send out tokens in their local neighborhood to communicate information about their available resources. These tokens are then used in both routing and flow control: to choose less congested paths in the network and to bypass the router pipeline along those paths. These bypass paths are formed dynamically, can be arbitrarily long and, are highly flexible with the ability to match to a packetâ??s exact route. Hence, this allows packets to potentially skip all routers along their path from source to destination, approaching the communication energy-delaythroughput of dedicated wires. Our detailed implementation analysis shows TFC to be highly scalable and realizable at an aggressive target clock cycle delay of 21FO4 for large networks while requiring low hardware complexity. Evaluations of TFC using both synthetic traffic and traces from the SPLASH-2 benchmark suite show reduction in packet latency by up to 77.1 % with upto 39.6 % reduction in average router energy consumption as compared to a state-of-theart baseline packet-switched design. For the same saturation throughput as the baseline network, TFC is able to reduce the amount of buffering by 65 % leading to a 48.8 % reduction in leakage energy and a 55.4 % lower total router energy.  
872|A Delay Model and Speculative Architecture for Pipelined Routers|This paper introduces a router delay model that accurately models key aspects of modern routers. The model accounts for the pipelined nature of contemporary routers, the specific flow control method employed, the delay of the flowcontrol credit path, and the sharing of crossbar ports across virtual channels. Motivated by this model, we introduce a microarchitecture for a speculative virtual-channel router that significantly reduces its router latency to that of a wormhole router. Simulations using our pipelined model give results that differ considerably from the commonly- assumed `unit-latency&#039; model which is unreasonably optimistic. Using realistic pipeline models, we compare wormhole [6] and virtual-channel flow control [4]. Our results show that a speculative virtual-channel router has the same per-hop router latency as a wormhole router, while improving throughput by up to 40%.  1. Introduction  Interconnection networks are used to connect processors to memories in multicomputers ...
873|Mitigating routing misbehavior in mobile ad hoc networks|This paper describes two techniques that improve through-put in an ad hoc network in the presence of nodes that agree to forward packets but fail to do so. To mitigate this prob-lem, we propose categorizing nodes based upon their dynam-ically measured behavior. We use a watchdog that identifies misbehaving nodes and a patl~rater that helps routing pro-tocols avoid these nodes. Through simulation we evaluate watchdog and pathrater using packet throughput, percent-age of overhead (routing) transmissions, and the accuracy of misbehaving node detection. When used together in a net-work with moderate mobility, the two techniques increase throughput by 17 % in the presence of 40 % misbehaving nodes, while increasing the percentage ofoverhead transmis-sions from the standard routing protocol&#039;s 9 % to 17%. Dur-ing extreme mobility, watchdog and pathrater can increase network throughput by 27%, while increasing the overhead transmissions from the standard routing protocol&#039;s 12 % to 24%. 1.
874|Wireless Ad Hoc Networks|A mobile ad hoc network is a relatively new term for an old technology - a network that does not rely on pre-existing infrastructure. Roots of this technology could be traced back to the early 1970s with the DARPA PRNet and the SURAN projects. The new twitch is the application of this technology in the non-military communication environments. Additionally, the research community has also recently addressed some extended features of this technology, such as multicasting and security. Also numerous new solutions to the &#034;old&#034; problems of routing and medium access control have been proposed. This survey attempts to summarize the state-ofthe -art of the ad hoc networking technology in four areas: routing, medium access control, multicasting, and security. Where possible, comparison between the proposed protocols is also discussed.
875|Ariadne: A secure on-demand routing protocol for ad hoc networks|An ad hoc network is a group of wireless mobile computers (or nodes), in which individual nodes cooperate by forwarding packets for each other to allow nodes to communicate beyond direct wireless transmission range. Prior research in ad hoc networking has generally studied the routing problem in a non-adversarial setting, assuming a trusted environment. In this paper, we present attacks against routing in ad hoc networks, and we present the design and performance evaluation of a new secure on-demand ad hoc network routing protocol, called Ariadne. Ariadne prevents attackers or compromised nodes from tampering with uncompromised routes consisting of uncompromised nodes, and also prevents a large number of types of Denial-of-Service attacks. In addition, Ariadne is efficient, using only highly efficient symmetric cryptographic primitives.
876|Packet Leashes: A Defense against Wormhole Attacks in Wireless Ad Hoc Networks|Abstract — As mobile ad hoc network applications are deployed, security emerges as a central requirement. In this paper, we introduce the wormhole attack, a severe attack in ad hoc networks that is particularly challenging to defend against. The wormhole attack is possible even if the attacker has not compromised any hosts, and even if all communication provides authenticity and confidentiality. In the wormhole attack, an attacker records packets (or bits) at one location in the network, tunnels them (possibly selectively) to another location, and retransmits them there into the network. The wormhole attack can form a serious threat in wireless networks, especially against many ad hoc network routing protocols and location-based wireless security systems. For example, most existing ad hoc network routing protocols, without some mechanism to defend against the wormhole attack, would be unable to find routes longer than one or two hops, severely disrupting communication. We present a new, general mechanism, called packet leashes, for detecting and thus defending against wormhole attacks, and we present a specific protocol, called TIK, that implements leashes. I.
877|Practical Byzantine Fault Tolerance |This paper describes a new replication algorithm that is able to tolerate Byzantine faults. We believe that Byzantinefault-tolerant algorithms will be increasingly important in the future because malicious attacks and software errors are increasingly common and can cause faulty nodes to exhibit arbitrary behavior. Whereas previous algorithms assumed a synchronous system or were too slow to be used in practice, the algorithm described in this paper is practical: it works in asynchronous environments like the Internet and incorporates several important optimizations that improve the response time of previous algorithms by more than an order of magnitude. We implemented a Byzantine-fault-tolerant NFS service using our algorithm and measured its performance. The results show that our service is only 3 % slower than a standard unreplicated NFS.  
878|Secure Routing for Mobile Ad hoc Networks|The emergence of the Mobile Ad Hoc Networking (MANET)technology advocates self-organized wireless interconnection of communication devices that would either extend or operate in concert with the wired networking infrastructure or, possibly, evolve to autonomous networks. In either case, the proliferation of MANET-based applications depends on a multitude of factors, with trustworthiness being one of the primary challenges to be met. Despite the existence of well-known security mechanisms, additional vulnerabilities and features pertinent to this new networking paradigm might render such traditional solutions inapplicable. In particular, the absence of a central authorization facility in an open and distributed communication environment is a major challenge, especially due to the need for cooperative network operation. In particular, in MANET, any node may compromise the routing protocol functionality by disrupting the route discovery process. In this paper, we present a route discovery protocol that mitigates the detrimental effects of such malicious behavior, as to provide correct connectivity information. Our protocol guarantees that fabricated, compromised, or replayed route replies would either be rejected or never reach back the querying node. Furthermore, the protocol responsiveness is safeguarded under different types of attacks that exploit the routing protocol itself. The sole requirement of the proposed scheme is the existence of a security association between the node initiating the query and the sought destination. Specifically, no assumption is made regarding the intermediate nodes, which may exhibit arbitrary and malicious behavior. The scheme is robust in the presence of a number of non-colluding nodes, and provides accurate routing information in a timely manner.
879|The Resurrecting Duckling:  Security Issues for Ad-hoc Wireless Networks|In the near future, many personal electronic devices will be  able to communicate with each other over a short range wireless channel. We investigate
880|SEAD: Secure Efficient Distance Vector Routing for Mobile Wireless Ad Hoc Networks|An ad hoc network is a collection of wireless computers (nodes), communicating among themselves over possibly multihop paths, without the help of any infrastructure such as base stations or access points. Although many previous ad hoc network routing protocols have been based in part on distance vector approaches, they have generally assumed a trusted environment. In this paper, we design and evaluate the Secure Efficient Ad hoc Distance vector routing protocol (SEAD), a secure ad hoc network routing protocol based on the design of the Destination-Sequenced Distance-Vector routing protocol. In order to support use with nodes of limited CPU processing capability, and to guard against Denial-of-Service attacks in which an attacker attempts to cause other nodes to consume excess network bandwidth or processing time, we use efficient one-way hash functions and do not use asymmetric cryptographic operations in the protocol. SEAD performs well over the range of scenarios we tested, and is robust against multiple uncoordinated attackers creating incorrect routing state in any other node, even in spite of any active attackers or compromised nodes in the network.
881|A Secure Routing Protocol for Ad Hoc Networks|Most recent ad hoc network research has focused on providing routing services without considering security. In this paper, we detail security threats against ad hoc routing protocols, specifically examining AODV and DSR. In light of these threats, we identify three different environments with distinct security requirements. We propose a solution to one, the managed-open scenario where no network infrastructure is pre-deployed, but a small amount of prior security coordination is expected. Our protocol, ARAN, is based on certificates and successfully defeats all identified attacks.
882|Providing robust and ubiquitous security support for mobile ad-hoc networks|Providing security support for mobile ad-hoc networks is challenging for several reasons: (a) wireless networks are susceptible to attacks ranging from passive eavesdropping to active interfering, occasional break-ins by adversaries may be inevitable in a large time window; (b) mobile users demand “anywhere, anytime ” services; (c) a scalable solution is needed for a large-scale mobile network. In this paper, we describe a solution that supports ubiquitous security services for mobile hosts, scales to network size, and is robust against break-ins. In our design, we distribute the certification authority functions through a threshold secret sharing mechanism, in which each entity holds a secret share and multiple entities in a local neighborhood jointly provide complete services. We employ localized certification schemes to enable ubiquitous services. We also update the secret shares to further enhance robustness against break-ins. Both simulations and implementation confirm the effectiveness of our design.
883|Self-securing ad hoc wireless networks|Mobile ad hoc networking offers convenient infrastructure-free communication over the shared wireless channel. However, the nature of ad hoc networks makes them vulnerable to security attacks. Examples of such attacks include passive eavesdropping over the wireless channel, denial of service attacks by malicious nodes as well as attacks from compromised nodes or stolen devices. Unlike their wired counterpart, infrastructureless ad hoc networks do not have a clear line of defense, and every node must be prepared for encounters with an adversary. Therefore, a centralized or hierarchical network security solution does not work well. This work provides scalable, distributed authentication services in ad hoc networks. Our design takes a self-securing approach, in which multiple nodes (say, k) collaboratively provide authentication services for any node in the network. This paper follows the design guidelines of [7] and makes several new contributions. We first formalize a localized trust model that lays the foundation for the design, and then expand the adversary model that the system should handle. We further propose refined localized certification services, and develop a new scalable solution of share updates to resist more powerful adversaries. Finally, the new solution is evaluated through simulations. 1
884|Theory of spread spectrum communications—a tutorial|AbstracrSpread-spectrum communications, with its inherent terference attenuation capability, has over the years become an illincreasingly popular technique for use in many different systeml:. Applications range from antijam systems, to code division multiple access systems, to systems designed to combat multipath. It is the intention of this paper to provide a tutorial treatment of the theory c% spread-spectrum communications, including a discussion on the applications referred to above, on the properties of commom spreading sequences, and quisition and tracking. on techniques that can he used for a(-S
885|An Empirical Study of Epidemic Algorithms in Large Scale Multihop Wireless Networks|A new class of networked systems is emerging that involve very large numbers of small, low-power, wireless devices. We present findings from a large scale empirical study involving over 150 such nodes operated at various transmission power settings. The study reveals that even a simple epidemic protocol, flooding, can exhibit surprising complexity at scale. The instrumentation in our experiments permits us to separate effects at the various layers of the protocol stack. At the physical/link layer, we present statistics on packet loss, effective communication range and link asymmetry; at the MAC layer, we measure contention, collision and latency; and at the net- work/application layer, we analyze the structure of trees constructed using flooding. The data and analysis lay a foundation for a much wider set of algorithmic studies in this space.
886|Adaptive Security for Multi-layer Ad-hoc Networks|Secure communication is critical in military environments where the network infrastructure is vulnerable to various attacks and compromises. A conventional centralized solution breaks down when the security servers are destroyed by the enemies. In this paper we design and evaluate a security framework for multi-layer ad-hoc wireless networks with unmanned aerial vehicles (UAVs). In battlefields, the framework adapts to the contingent damages on the network infrastructure. Depending
887|Authenticated Ad Hoc Routing at the Link Layer for Mobile Systems|Introduction  New networks are being created based on nodes communicating via wireless technologies such as infrared, cellular radio, and radio LAN. LAN radio devices, such as Lucent&#039;s WaveLAN  TM  , a 2 Mbps spread-spectrum radio LAN technology, are capable of broadcast and multicast similar to Ethernet.  1 Networks based on Ethernet-like radio broadcast bring certain topological and security challenges with them not previously encountered with wired technologies. Unlike a wired network, a wireless network is accessible to anyone in the vicinity. Predicting what machines can communicate with each other in a wired network is easy (any machine on a wire can reach any other machine on the same wire), but in a wireless network, reachability depends on the location of the computers and extrinsic sources of interference.  1  See http://www.WaveLAN.com/.  Mobile-IP [12] is a network-layer routing protocol that speci
889|Energy-aware routing in cluster-based sensor networks|Recently there has been a growing interest in the applications of sensor networks. Since sensors are generally constrained in on-board energy supply, efficient management of the network is crucial in extending the life of the sensor. In this paper, we present a novel approach for energy-aware and context-aware routing of sensor data. The approach calls for network clustering and assigns a less-energy-constrained gateway node that acts as a centralized network manager. Based on energy usage at every sensor node and changes in the mission and the environment, the gateway sets routes for sensor data, monitors latency throughout the cluster, and arbitrates medium access among sensors. Simulation results demonstrate that our approach can achieve substantial energy saving. 1.
890|Data Gathering in Sensor Networks using the Energy*Delay Metric|In this paper we consider the problem of data collection from a sensor web consisting of N nodes, where nodes have packets of data in each round of communication that need to be gathered and fused with other nodes&#039; packets into one packet and transmitted to a distant base station. Nodes have power control in their wireless communications and can transmit directly to any node in the network or to the base station. With unit delay cost for each packet transmission, if all nodes transmit data directly to the base station, then both high energy and high delay per round will occur. In our prior work [6], we developed an algorithm to minimize the energy cost per round, where a linear chain of all the nodes are formed to gather data, and nodes took turns to transmit to the base station. If the goal is to minimize the delay cost, then a binary combining scheme can be used to accomplish this task in about log N units of delay with parallel communications and incurring a slight increase in energy cost. The goal is to find data gathering schemes that balance the energy and delay cost, as measured by energy*delay. We conducted extensive simulation experiments with a number of schemes for this problem with 100 nodes in playing fields of 50m x 50m and 100m x 100m and the base station located at least 100 meters and 200 meters, respectively, from any node. With CDMA capable sensor nodes, a chain-based binary scheme performs best in terms of energy*delay. If the sensor nodes are not CDMA capable, then parallel communications are possible only among spatially separated nodes, and a chain-based 3 level hierarchy scheme performs well. These schemes perform 60 to 100 times better than direct scheme and also outperform a cluster based scheme, called LEACH [3].
891|Routing on a Curve|Relentless progress in hardware technology and recent advances in sensor technology, and wireless networking have made it feasible to deploy large scale, dense ad-hoc networks. These networks together with sensor technology can be considered as the enablers of emerging models of computing such as embedded computing, ubiquitous computing, or pervasive computing. In this paper, we propose a new paradigm called trajectory based forwarding (or TBF), which is a generalization of source based routing and Cartesian routing. We argue that TBF is an ideal technique for routing in dense ad-hoc networks. Trajectories are a natural namespace for describing route paths when the topology of the network matches the topography of the physical surroundings in which it is deployed which by very definition is embedded computing. We show how simple trajectories can be used in implementing important networking protocols such as flooding, discovery, and network management. Trajectory routing is very effective in implementing many networking functions in a quick and approximate way, as it needs very few support services. We discuss several research challenges in the design of network protocols that use specific trajectories for forwarding packets.
892|A constrained shortest-path energy-aware routing algorithm for wireless sensor networks|Abstract-While traditional routing protocols try to minimize the end-to-end delay or maximize the throughput, most energyaware routing protocols for wireless sensor networks try to extend the life time of the network by minimizing the energy consumption sacrificing other performance metrics. In this paper, we introduce a new energy-aware routing protocol that tries to minimize the energy consumption and, at the same time, maintain good end-to-end delay and throughput performance. The new algorithm is based on a constrained shortest-path algorithm. We compare the new algorithm with some traditional routing and energy-aware routing algorithms. The results show that the new algorithm performance is acceptable under all performance metrics and presents a performance balance between the traditional routing algorithms and the energy-aware routing algorithms. The constraint value can be chosen to achieve different performance objectives for different sensor network missions. I.
893|Energy and QoS aware routing in wireless sensor networks |Abstract. Many new routing protocols have been proposed for wireless sensor networks in recent years. Almost all of the routing protocols considered energy efficiency as the ultimate objective since energy is a very scarce resource for sensor nodes. However, the introduction imaging sensors has posed additional challenges. Transmission of imaging data requires both energy and QoS aware routing in order to ensure efficient usage of the sensors and effective access to the gathered measurements. In this paper, we propose an energy-aware QoS routing protocol for sensor networks which can also run efficiently with best-effort traffic. The protocol finds a least-cost, delay-constrained path for real-time data in terms of link cost that captures nodes ’ energy reserve, transmission energy, error rate and other communication parameters. Moreover, the throughput for non-real-time data is maximized by adjusting the service rate for both real-time and non-real-time data at the sensor nodes. Such adjustment of service rate is done by using two different mechanisms. Simulation results have demonstrated the effectiveness of our approach for different metrics with respect to the baseline approach where same link cost function is used without any service differentiation mechanism.
894|Max-Min D-Cluster Formation in Wireless Ad Hoc Networks|An ad hoc network may be logically represented as a set of clusters. The clusterheads form a d-hop dominating set. Each node is at most d hops from a clusterhead. Clusterheads form a virtual backbone and may be used to route packets for nodes in their cluster. Previous heuristics restricted themselves to 1-hop clusters. We show that the minimum d-hop dominating set problem is NP-complete. Then we present a heuristic to form d-clusters in a wireless ad hoc network. Nodes are assumed to have non-deterministic mobility pattern. Clusters are formed by diffusing node identities along the wireless links. When the heuristic terminates, a node either becomes a clusterhead, or is at most d wireless hops away from its clusterhead. The value of d is a parameter of the heuristic. The heuristic can be run either at regular intervals, or whenever the network configuration changes. One of the features of the heuristic is that it tends to re-elect existing clusterheads even when the network configurat...
895|Scalable routing strategies for ad hoc wireless networks| In this paper, we consider a large population of mobile stations that are interconnected by a multihop wireless network. The applications of this wireless infrastructure range from ad hoc networking (e.g., collaborative, distributed computing) to disaster recovery (e.g., fire, flood, earthquake), law enforcement (e.g., crowd control, search-and-rescue), and military (automated battlefield). Key characteristics of this system are the large number of users, their mobility, and the need to operate without the support of a fixed (wired or wireless) infrastructure. The last feature sets this system apart from existing cellular systems and in fact makes its design much more challenging. In this environment, we investigate routing strategies that scale well to large populations and can handle mobility. In addition, we address the need to support multimedia communications, with low latency requirements for interactive traffic and quality-of-service (QoS) support for real-time streams (voice/video). In the wireless routing area, several schemes have already been proposed and implemented (e.g., hierarchical routing, on-demand routing, etc.). We introduce two new schemes—fisheye state routing (FSR) and hierarchical state routing (HSR)—which offer some competitive advantages over the existing schemes. We compare the performance of existing and proposed schemes via simulation. 
896|Soft Handoffs in CDMA Mobile Systems|This article presents an overview of soft handoff, an idea which is becoming quite important because of its use in the IS-95 code-division multiple access (CDMA) cellular phone standard. The benefits and disadvantages of using soft handoff over hard handoff are discussed, with most results  drawn from the available literature. The two most well-known benefits are fade margin improvement and higher uplink capacity, while disadvantages  include increased downlink interference and more complex implementation. Handoff parameter optimization is extremely important, so  various studies on the trade-offs to be considered when selecting these parameters are surveyed, from both the link quality and resource  allocation perspectives. Finally, research directions and future trends are discussed.
897|Mobility Management in Hierarchical Multihop Mobile Wireless Networks |In this paper, we consider the mobility management in large, hierarchically organized multihop wireless networks. The examples of such networks range from battlefield networks, emergency disaster relief and law enforcement etc. We present a novel network addressing architecture to accommodate mobility using a “Home Agent ” concept akin to mobile IP. The performance of the mobility management scheme is investigated through simulations. I.
898|Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network|We present a new part-of-speech tagger that  demonstrates the following ideas: (i) explicit  use of both preceding and following tag contexts  via a dependency network representation,  (ii) broad use of lexical features, including  jointly conditioning on multiple consecutive  words, (iii) effective use of priors in conditional  loglinear models, and (iv) fine-grained  modeling of unknown word features. Using  these ideas together, the resulting tagger gives  a 97.24% accuracy on the Penn Treebank WSJ,  an error reduction of 4.4% on the best previous  single automatically learned tagging result.
899|Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging|this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging
900|A Maximum Entropy Model for Part-Of-Speech Tagging|This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual &#034;features&#034; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
901|Dependency networks for inference, collaborative filtering, and data visualization |We describe a graphical model for probabilistic relationships|an alternative tothe Bayesian network|called a dependency network. The graph of a dependency network, unlike aBayesian network, is potentially cyclic. The probability component of a dependency network, like aBayesian network, is a set of conditional distributions, one for each nodegiven its parents. We identify several basic properties of this representation and describe a computationally e cient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative ltering (the task of predicting preferences), and the visualization of acausal predictive relationships.
902|Estimators for Stochastic &#034;Unification-Based&#034; Grammars*|Log-linear models provide a statistically sound framework for Stochastic &#034;Unification-Based&#034; Grammars (SUBGs) and stochastic versions of  other kinds of grammars. We describe two  computationally-tractable ways of estimating  the parameters of such grammars from a training  corpus of syntactic analyses, and apply  these to estimate a stochastic version of LexicalFunctional  Grammar.
903|Equations for Part-of-Speech Tagging|We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.  Introduction  The last few years have seen a fair number of papers on part-of-speech tagging --- assigning the correct part of speech to each word in a text [1,2,4,5,7,8,9,10]. Most of these systems view the text as having been produced by a hidden Mar...
904|Classifier Combination for Improved Lexical Disambiguation|One of the most exciting recent directions in  machine learning is the discovery that the  combination of multiple classifiers often  results in significantly better performance  than what can be achieved with a single  classifier. In this paper, we first show that  the errors made from three different state of  the art part of speech taggers are strongly  complementary. Next, we show how this  complementary behavior can be used to our  advantage. By using contextual cues to  guide tagger combination, we are able to  derive a new tagger that achieves  performance significantly greater than any  of the individual taggers.
905|A second-order hidden markov model for part-of-speech tagging|This paper describes an extension to the hidden Markov model for part-of-speech tagging using second-order approximations for both contex-tual and lexical probabilities. This model in-creases the accuracy of the tagger to state of the art levels. These approximations make use of more contextual information than standard statistical systems. New methods of smoothing the estimated probabilities are also introduced to address the sparse data problem. 1
906|Conditional Structure versus Conditional Estimation in NLP Models|This paper separates conditional parameter estimation,  which consistently raises test set accuracy on  statistical NLP tasks, from conditional model structures,  such as the conditional Markov model used  for maximum-entropy tagging, which tend to lower  accuracy. Error analysis on part-of-speech tagging  shows that the actual tagging errors made by the  conditionally structured model derive not only from  label bias, but also from other ways in which the independence  assumptions of the conditional model  structure are unsuited to linguistic sequences. The  paper presents new word-sense disambiguation and  POS tagging experiments, and integrates apparently  conflicting reports from other recent work.
907|Finding community structure in networks using the eigenvectors of matrices|We consider the problem of detecting communities or modules in networks, groups of vertices with a higher-than-average density of edges connecting them. Previous work indicates that a robust approach to this problem is the maximization of the benefit function known as “modularity ” over possible divisions of a network. Here we show that this maximization process can be written in terms of the eigenspectrum of a matrix we call the modularity matrix, which plays a role in community detection similar to that played by the graph Laplacian in graph partitioning calculations. This result leads us to a number of possible algorithms for detecting community structure, as well as several other results, including a spectral measure of bipartite structure in networks and a new centrality measure that identifies those vertices that occupy central positions within the communities to which they belong. The algorithms and measures proposed are illustrated with applications to a variety of real-world complex networks.  
908|Complex networks: Structure and dynamics| Coupled biological and chemical systems, neural networks, social interacting species, the Internet and the World Wide Web, are only a few examples of systems composed by a large number of highly interconnected dynamical units. The first approach to capture the global properties of such systems is to model them as graphs whose nodes represent the dynamical units, and whose links stand for the interactions between them. On the one hand, scientists have to cope with structural issues, such as characterizing the topology of a complex wiring architecture, revealing the unifying principles that are at the basis of real networks, and developing models to mimic the growth of a network and reproduce its structural properties. On the other hand, many relevant questions arise when studying complex networks ’ dynamics, such as learning how a large ensemble of dynamical systems that interact through a complex wiring topology can behave collectively. We review the major concepts and results recently achieved in the study of the structure and dynamics of complex networks, and summarize the relevant applications of these ideas in many different disciplines,
909|Nunes Amaral. Functional cartography of complex metabolic networks|High-throughput techniques are leading to an explosive growth in the size of biological databases and creating the opportunity to revolutionize our understanding of life and disease. Interpretation of these data remains, however, a major scientific challenge. Here, we propose a methodology that enables us to extract and display information contained in complex networks 1,2,3. Specifically, we demonstrate that one can (i) find functional modules 4,5 in complex networks, and (ii) classify nodes into universal roles according to their pattern of intra- and inter-module connections. The method thus yields a “cartographic representation ” of complex networks. Metabolic networks 6,7,8 are among the most challenging biological networks and, arguably, the ones with more potential for immediate applicability 9. We use our method to analyze the metabolic networks of twelve organisms from three different super-kingdoms. We find that, typically, 80 % of the nodes are only connected to other nodes within their respective modules, and that nodes with different roles are affected by different evolutionary constraints and pressures. Remarkably, we
910|Self-Organization and Identification of Web Communities|Despite the decentralized and unorganized nature of the web, we show that the web self-organizes such that communities of highly related pages can be efficiently identified based purely on connectivity.
911|Preferential attachment of communities: the same principle, but a higher level|but a higher level
912|Packet Switching In Radio Channels: Part III -- Polling and . . .| Here we continue the analytic study of packet switching in radio channels which we reported upon in our two previous papers [ 11, [2]. Again we consider a population of terminals communicating with a central station over a packet-switched radio channel. The allocation of bandwidth among the contending terminals can befixed [e.g., time-division multiple access (TDMA) or frequency-division multiple access (FDMA)]-,rundom [e.&amp;, ALOHA or carrier sen? multiple access (CSMA)] or centrally controlled (e.g., polling or reservation). In this paper we show that with a large population of bursty users, (as expected) random access is superior to both futed assignment and polling. We also introduce and analyze a dynamic reservation technique which we call split-channel reservation multiple access (SRMA) which is interesting in that it is both simple and efficient over a large range of system parameters.  
913|Comparative Performance Evaluation of Routing Protocols for Mobile, Ad hoc Networks|We evaluate several routing protocols for mobile, wireless, ad hoc networks via packet level simulations. The protocol suite includes routing protocols specifically designed for ad hoc routing, as well as more traditional protocols, such as link state and distance vector, used for dynamic networks. Performance is evaluated with respect to fraction of packets delivered, end-to-end delay and routing load for a given traffic and mobility model. It is observed that the new generation of on-demand routing protocols use much lower routing load. However, the traditional link state and distance vector protocols provide, in general, better packet delivery and delay performance. 1. Introduction  A mobile, ad hoc network [4] is an autonomous system of mobile hosts connected by wireless links. There is no static infrastructure such as base stations. If two hosts are not within radio range, all message communication between them must pass through one or more intermediate hosts that double as router...
914|Measurement and Analysis of the Error Characteristics of an In-Building Wireless Network|There is general belief that networks based on wireless technologies have much higher error rates than those based on more traditional technologies such as optical fiber, coaxial cable, or twisted pair wiring. This difference has motivated research on new protocol suites specifically for wireless networks. While the error characteristics of wired networks have been well documented, less experimental data is available for wireless LANs.  In this
915|The Effects of On-Demand Behavior in Routing Protocols for Multi-Hop Wireless Ad Hoc Networks|Abstract—A number of different routing protocols proposed for use in multi-hop wireless ad hoc networks are based in whole or in part on what can be described as on-demand behavior. By ondemand behavior, we mean approaches based only on reaction to the offered traffic being handled by the routing protocol. In this paper, we analyze the use of on-demand behavior in such protocols, focusing on its effect on the routing protocol’s forwarding latency, overhead cost, and route caching correctness, drawing examples from detailed simulation of the dynamic source routing (DSR) protocol. We study the protocol’s behavior and the changes introduced by variations on some of the mechanisms that make up the protocol, examining which mechanisms have the greatest impact and exploring the tradeoffs that exist between them. Index Terms—Communication system routing, computer network performance, dynamic source routing (DSR) protocol, wireless ad hoc networks. I.
916|From data mining to knowledge discovery in databases|¦ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are
918|The Merge/Purge Problem for Large Databases|Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Clos...
919|Unexpectedness as a Measure of Interestingness in Knowledge Discovery|Organizations are taking advantage of &#034;data-mining&#034; techniques to leverage the vast amounts of data captured as they process routine transactions. Data-mining is the process of discovering hidden structure or patterns in data. However several of the pattern discovery methods in datamining systems have the drawbacks that they discover too many obvious or irrelevant patterns and that they do not leverage to a full extent valuable prior domain knowledge that managers have. This research addresses these drawbacks by developing ways to generate interesting patterns by incorporating managers&#039; prior knowledge in the process of searching for patterns in data. Specifically we focus on providing methods that generate unexpected patterns with respect to managerial intuition by eliciting managers&#039; beliefs about the domain and using these beliefs to seed the search for unexpected patterns in data. Our approach should lead to the development of decision support systems that provide managers with mor...
920|The interestingness of deviations|gps~gte.com, matheus~gte.com One of the moet promising areas in Knowledge Discovery in Databases is the automatic analysis of changes and deviations. Several systems have recently been developed for this task. Suc ~ of these systems hinges on their ability to identify s few important and relevant deviations among the multitude of potentially interesting events:  ~ In this paper we argue that related deviations should be grouped togetherin a finding and that the interestingness of a finding is the estimated benefit from a poesible ~tion connected to it. We discuss methods for determining the estimated benefit from the impact of the deviations and the success probability of an action. Our analysis is done in the context of the Key Findings Reporter (KEFIIt), a system for discovering and explaining ~key findings &#034; in large relational databases, currently being applied to the analysis of healthcare information.
921|The World Wide Web: quagmire or gold mine?|This article considers the question: is effective Web mining possible?
922|Discovering Informative Patterns and Data Cleaning|We present a method for discovering informative patterns from data. With this method, large databases can be reduced to only a few representative data entries. Our framework also encompasses methods for cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposed and experimentally checked on databases of handwritten images. The generality of the framework makes it an attractive candidate for new applications in knowledge discovery.  Keywords: knowledge discovery, machine learning, informative patterns, data cleaning, information gain.  4.1 
923|Selecting Among Rules Induced from a Hurricane Database|can achieve orders of magnitude reduction in the volume of data For example, we applied a commercial tool (IXLtin) to a 1,819 record tropical storm database, yielding 161 rules. However, the human comprehension goals of Knowledge Discovery in Databases may require still more orders of magnitude. We present a rule refinement strategy, partly implemented in a Prolog program, that operationalizes &#034;interestingness &#034; into performance, simplicity, novelty, and significance. Applying the strategy to the induced rulebase yielded 10 &#034;genuinely interesting &#034; rules. I. PURPOSE OF THE STUDY At The Travelers Insurance Company, we are involved in applying statistics and artificial intelligence techniques to the solution of business problems. This work is part of an investigation into applications for Natural Hazards Research Services. The purpose of this study is not to deyelop a hurricane model or predictor. It is, rather, to assess the utility of rule induction technology and our particular rule refinement strategy. The object task of the study is to develop rules that
924|KDD for Science Data Analysis: Issues and Examples|Tile analysis of tile massive data sets collected by scientific instruments demands automation as a pre-requisite to analysis. There is an urgent need to cre-ate an intermediate level at which scientists can oper-ate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even renmtely ap-proach humans--namely, creative data analysis, the-ory and hypothesis formation, and drawing insights into underlying phe,mmena. We give an overview of the main issues in the exploitation of scientific data.sets, present five c,~se studies where KDD tools play important and enabling roles, and conclude with fi,ture challenges for data mining and KDD techniques in science data analysis. 1
925| 	 Active Data Mining   |We introduce an active data mining paradigm that combines the recent work in data mining with the rich literature on active database systems. In this paradigm, data is continuously mined at a desired frequency. As rules are discovered, they are added to a rulebase, and if they already exist, the history of the statistical parameters associated with the rules is updated. When the history starts exhibiting certain trends, specified as shape queries in the user-speci ed triggers, the triggers are red and appropriate actions are initiated. To be able to specify shape queries, we describe the constructs for de ning shapes, and discuss how the shape predicates are used in a query construct to retrieve rules whose histories exhibit the desired trends. We describe how this query capability is integrated into a trigger system to realize an active mining system. The system presented here has been validated using two sets of customer data.
926|Fast Spatio-Temporal Data Mining of Large Geophysical Datasets|The important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining techniques on a massive scale. Advances in parallel supercomputing technology, enabling high-resolution modeling, as well as in sensor technology, allowing data capture on an unprecedented scale, conspire to overwhelm present-day analysis approaches. We present here early experiences with a prototype exploratory data analysis environment, CONQUEST, designed to provide content-based access to such massive scientific datasets. CONQUEST (CONtent-based Querying in Space and Time) employs a combination of workstations and massively parallel processors (MPP&#039;s) to mine geophysical datasets possessing a prominent temporal component. It is designed to enable complex multi-modal interactive querying and knowledge discovery, while simultaneously coping with the extraordinary computational demands posed by the scope of the datasets involved. A...
927|Predicting Equity Returns from Securities Data|Our experiments with capital markets data suggest that the domain can be effectively modeled by classification rules induced from available historical data for the purpose of making gainful predictions for equityinvestments. New classification techniques developed at IBM Research, including minimal rule generation (R-MINI) and contextual feature analysis, seem robust enough for consistently extracting useful information from noisy domains such as financial markets. We will briefly introduce the rationale for our minimal rule generation technique, and the motivation for the use of contextual information in analyzing features. We will then describe our experience from several experiments with the S&amp;P 500 data, illustrating the general methodology, and the results of correlations and simulated managed investment based on classification rules generated by R-MINI. Wewillsketchhow the rules for classifications can be effectively used for numerical prediction, and eventually to an investment ...
928|Graphical Models for Discovering Knowledge|There are many different ways of representing knowledge, and for each of these ways there are many different discovery algorithms. How can we compare different representations? How can we mix, match and merge representations and algorithms on new problems with their own unique requirements? This chapter introduces probabilistic modeling as a philosophy for addressing these questions and presents graphical models for representing probabilistic models. Probabilistic graphical models are a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. 4.1 Introduction Perhaps one common element of the discovery systems described in this and previous books on knowledge discovery is that they are all different. Since the class of discovery problems is a challenging one, we cannot write a single program to address all of knowledge discovery. The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (199...
929|An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications|This paper surveys the growing number of indu5 trial applications of data mining and knowledge discovery. We look at the existing tools, describe some representative applications, and discuss the major issues and problems for building and deploying successful applications and their adoption by business users. Finally, we examine how to assess the potential of a knowledge discovery application. 1
930|Analyzing the Benefits of Domain Knowledge in Substructure Discovery|Discovering repetitive, interesting, and functional substructures in a structural  database improves the ability to interpret and compress the data. However, scientists  working with a database in their area of expertise often search for a predetermined  type of structure, or for structures exhibiting characteristics specic to the domain.  This paper presents methods for guiding the discovery process with domain-specic  knowledge. In this paper, the Subdue discovery system is used to evaluate the bene-  ts of using domain knowledge. The domain knowledge is incorporated into Subdue  following a single general methodology to guide the discovery process. Results show  using domain-specic knowledge improves the search for substructures which are useful  to the domain, and leads to greater compression of the data. To illustrate these bene-  ts, examples and experiments from the domain of computer programming, computer  aided design circuit, and a series of articially-generated domains...
932|A comparison of mechanisms for improving TCP performance over wireless links|Reliable transport protocols such as TCP are tuned to perform well in traditional networks where packet losses occur mostly because of congestion. However, networks with wireless and other lossy links also suffer from significant losses due to bit errors and handoffs. TCP responds to all losses by invoking congestion control and avoidance algorithms, resulting in degraded end-to-end performance in wireless and lossy systems. In this paper, we compare several schemes designed to improve the performance of TCP in such networks. We classify these schemes into three broad categories: end-to-end protocols, where loss recovery is performed by the sender; link-layer protocols, that provide local reliability; and split-connection protocols, that break the end-to-end connection into two parts at the base station. We present the results of several experiments performed in both LAN and WAN environments, using throughput and goodput as the metrics for comparison. Our results show that a reliable link-layer protocol that is TCP-aware provides very good performance. Furthermore, it is possible to achieve good performance without splitting the end-to-end connection at the base station. We also demonstrate that selective acknowledgments and explicit loss notifications result in significant performance improvements.
933|A mobility-based framework for adaptive clustering in wireless ad hoc networks|Abstract—This paper presents a novel framework for dynamically organizing mobile nodes in wireless ad hoc networks into clusters in which the probability of path availability can be bounded. The purpose of the ( ; t) cluster is to help minimize the far-reaching effects of topological changes while balancing the need to support more optimal routing. A mobility model for ad hoc networks is developed and is used to derive expressions for the probability of path availability as a function of time. It is shown how this model provides the basis for dynamically grouping nodes into clusters using an efficient distributed clustering algorithm. Since the criteria for cluster organization depends directly upon path availability, the structure of the cluster topology is adaptive with respect to node mobility. Consequently, this framework supports an adaptive hybrid routing architecture that can be more responsive and effective when mobility rates are low and more efficient when mobility rates are high. Index Terms—Ad hoc networks, dynamic clustering, hierarchical routing, mobile computing, mobility models, routing algorithms, wireless networks. I.
934|Increasing Network Throughput by Integrating Protocol Layers|Integrating protocol data manipulations is a strategy for increasing the throughput of network protocols. The idea is to combine a series of protocol layers into a pipeline so as to access message data more efficiently. This paper introduces a widely-applicable technique for integrating protocols. This technique not only improves performance, but also preserves the modularity of protocol layers by automatically integrating independently expressed protocols. The paper also describes a prototype integration tool, and studies the performance limits and scalability of protocol integration. Department of Computer Science The University of Arizona Tucson, AZ 85721 1 This research was sponsored in part by DARPA Contract DABT63-91-C-0030. 1 Introduction Data manipulation---e.g., encryption, presentation formatting, compression, computing checksums---is one of the costliest aspects of data transfer [3, 4, 6]. This is because reading, and possibly writing, each byte of data in a message invo...
935|Exact and Approximation Algorithms for Clustering|In this paper we present a n O(k1?1=d) time algorithm for solving the k-center problem in R d, under L1 and L2 metrics. The algorithm extends to other metrics, and can be used to solve the discrete k-center problem, as well. We also describe a simple (1 +)-approximation algorithm for the k-center problem, with running time O(n log k) + (k = ) O(k1?1=d). Finally, we present a n O(k1?1=d) time algorithm for solving the L-capacitated k-center problem, provided that L = (n=k 1?1=d) or L = O(1). We conclude with a simple approximation algorithm for the L-capacitated k-center problem.
936|Clustering with power control|Abstract – This paper proposes a stable, dynamic, distributed clustering for energy efficient networking. Via simulation, we evaluate the impacts of mobility and transmission power variation on network stability. 1.
937|A comparison of MAC protocols for wireless local networks based on battery power consumption|Abstract- Energy efficiency is an important issue in mobile wireless networks since the battery life of mobile terminals is limited. Conservation of battery power has been addressed using many techniques. This paper addresses energy efficiency in medium access control (MAC) protocols for wireless networks. The paper develops a framework to study the energy consumption of a MAC protocol from the transceiver usage perspective. This framework is then applied to compare the performance of a set of protocols that includes IEEE 802.11, EC-MAC, PRMA, MDR-TDMA, and DQRUMA a. The performance metrics considered are transmitter and receiver usage times for packet transmission and reception. The analysis here shows that protocols that aim to reduce the number of contentions perform better from a energy consumption perspective. The receiver usage time, however, tends to be higher for protocols that require the mobile to sense the medium before attempting transmission. 1
938|Embedded Computation Meets the World-Wide-Web|Two important trends are converging to bring about a radical transformation in the operation of our world. First, the computer industry&#039;s remarkable ability to squeeze more and more transistors into a smaller and smaller area of silicon is increasing the computational abilities of our devices, while simultaneously decreasing their cost and power consumption. Second, the proliferation of wired and wireless networking spurred by the development of the world-wide web and demands for mobile access are enabling low-cost connectivity among computing devices. It is now possible to connect not only our desktop machines, but every computing device into a true worldwide web that connects the physical world of sensors and actuators to the virtual world of our information utilities and services. What amazing new applications and services will result? How will ubiquitous computation affect our everyday lives? Will the long envisioned invisible computing paradigm finally be possible? This paper expl...
939|Energy-Scalable algorithms and Protocols for wireless Microsensor Networks|Wireless microsensor networks lend themselves to trade-offs in energy and quality. In these networks, the individual sensor data per se is not necessarily important to the end user. Rather, it is the combined knowledge of all the sensors that describes what is occurring in the environment. By allowing the algorithms and protocols to adapt the quality of this description, with a corresponding change in energy dissipation, sensor networks can be flexible to the end-user’s requirements. In this paper, we provide models for predicting quality and energy and show the advantages of trading off these two parameters. By ensuring that the system operates at a minimum energy for each quality point, the system can achieve both flexibility and energy efficiency, allowing the end-user to maximize system lifetime. 1.
940|Energy-Scalable Protocols for Battery-Operated MicroSensor Networks|To maximize battery lifetimes of distributed wireless sensors,  network protocols and data fusion algorithms should be designed with low  power techniques. Network protocols minimize energy by using localized communication  and control and by exploiting computation/communication tradeoffs.  In addition, data fusion algorithms such as beamforming aggregate data  from multiple sources to reduce data redundancy and enhance signal-to-noise  ratios, thus further reducing the required communications. We have developed  a sensor network system that uses a localized clustering protocol and beamforming  data fusion to enable energy-efficient collaboration. We have implemented  two beamforming algorithms, the Maximum Power and the Least  Mean Squares (LMS) beamforming algorithms, on the StrongARM (SA-1100)  processor. Results from our experiments show that the LMS algorithm  requires less than one-fifth the energy required by the Maximum Power beamforming  algorithm with onlya3dBloss in performa...
941|Bluetooth - A New Low-Power Radio Interface Providing Short-Range Connectivity|this paper, we review the Bluetooth technology, a new universal radio interface enabling electronic devices to connect and communicate wirelessly via short-range connections. Motivations for the air interface design and radio requirement decisions are discussed. Frequency hopping, interference resistance, and the concepts of ad hoc connectivity and scatternets are explained in detail. Furthermore, Bluetooth characteristics enabling low-cost single-chip implementations and supporting low power consumption are discussed
942|Software radio architecture with smart antennas: a tutorial on algorithms and complexity|Abstract — Recently, there has been considerable interest in using antenna arrays in wireless communication networks to increase the capacity and decrease the cochannel interference. Adaptive beamforming with smart antennas at the receiver increases the carrier-to-interference ratio (CIR) in a wireless link. This paper considers a wireless network with beamforming capabilities at the receiver which allows two or more transmitters to share the same channel to communicate with the base station. The concrete computational complexity and algorithm structure of a base station are considered in terms of a software radio system model, initially with an omnidirectional antenna. The software radio computational model is then expanded to characterize a network with smart antennas. The application of the software radio smart antenna is demonstrated through two examples. First, traffic improvement in a network with a smart antenna is considered, and the implementation of a hand-off algorithm in the software radio is presented. The blocking probabilities of the calls and total carried traffic in the system under different traffic policies are derived. The analytical and numerical results show that adaptive beamforming at the receiver reduces the probability of blocking and forced termination of the calls and increases the total carried traffic in the system. Then, a joint beamforming and power control algorithm is implemented in a software radio smart antenna in a CDMA network. This shows that, by using smart antennas, each user can transmit with much lower power, and therefore the system capacity increases significantly. Index Terms—Adaptive beamforming, handoff, power control, smart antennas, software radio. I.
943|Low power systems for wireless microsensors|Abstract-- Low power wireless sensor networks provide a new monitoring and control capability for civil and military applications in transportation, manufacturing, biomedical, environmental management, and safety and security systems. Wireless microsensor network nodes, operating at average and peak power levels constrained by compact power sources, offer a range of important challenges for low power methods. This paper reports advances in low power systems spanning network design, through power management, low power mixed signal circuits, and highly integrated RF network interfaces. Particular attention is focused on methods for low power RF receiver systems. I.
944|The design and implementation of HomeRF: A radio frequency wireless networking standard for the connected home|of over 100 companies from the computer, telecommunications, and consumer electronics industries. This group has developed an open specification called the Shared Wireless Access Protocol- Cordless Access (SWAP-CA) that enables radio frequency (RF) wireless connectivity between a diverse set of devices and computing resources in and around a typical home. Built around an RF spectrum with worldwide availability, SWAP-CA includes operational support for both managed and ad hoc network of devices. It combines and extends wireless networking and cordless telephony into a single unified protocol allowing mobile devices to communicate via both voice and data traffic simultaneously over the Internet and/or over the Public Switched Telephone Network (PSTN). For battery-operated devices it includes a power management mechanism that ensures connection longevity. The technology has been specifically optimized for consumer applications and price points and consequently the HomeRF WG has the broad backing of the major corporate stakeholders interested in enabling tether less networking within the home.
945|Design and implementation of a scalable encryption processor with embedded variable dc/dc converter|This work describes the design and implementation of an energy-efficient, scalable encryption processor that utilizes variable voltage supply techniques and a highefficiency embedded variable output DC/DC converter. The resulting implementation dissipates 134nJ/bit @ V DD = 2.5V, when encrypting at its maximum rate of 1Mb/s using a maximum datapath width of 512 bits. The embedded converter achieves an efficiency of 96 % at this peak load. The processor is 2-3 orders of magnitude more energy efficient than optimized assembly code running on a low-power processor such as the StrongARM. 2.
946|The H.263+ Video Coding Standard: Complexity and Performance|The ITU-T H.263+ low bit-rate video coding standard is Version 2 of the draft international standard ITU-T H.263. Currently, we are a contributing party in the H.263+ standardization effort. In this paper, we discuss this emerging video coding standard and present compression performance results based on our public domain implementation of H.263+.  
947|Random key predistribution schemes for sensor networks|Key establishment in sensor networks is a challenging problem because asymmetric key cryptosystems are unsuitable for use in resource constrained sensor nodes, and also because the nodes could be physically compromised by an adversary. We present three new mechanisms for key establishment using the framework of pre-distributing a random set of keys to each node. First, in the q-composite keys scheme, we trade off the unlikeliness of a large-scale network attack in order to significantly strengthen random key predistribution’s strength against smaller-scale attacks. Second, in the multipath-reinforcement scheme, we show how to strengthen the security between any two nodes by leveraging the security of other links. Finally, we present the random-pairwise keys scheme, which perfectly preserves the secrecy of the rest of the network when any node is captured, and also enables node-to-node authentication and quorum-based revocation.
948|A Key-Management Scheme for Distributed Sensor Networks|Distributed Sensor Networks (DSNs) are ad-hoc mobile networks that include sensor nodes with limited computation and communication capabilities. DSNs are dynamic in the sense that they allow addition and deletion of sensor nodes after deployment to grow the network or replace failing and unreliable nodes. DSNs may be deployed in hostile areas where communication is monitored and nodes are subject to capture and surreptitious use by an adversary. Hence DSNs require cryptographic protection of communications, sensorcapture detection, key revocation and sensor disabling. In this paper, we present a key-management scheme designed to satisfy both operational and security requirements of DSNs.
949|An Evaluation of Directory Schemes for Cache Coherence|The problem of cache coherence in shared-memory multiprocessors has been addressed using two basic approaches: directory schemes and snoopy cache schemes. Directory schemes have been given less attention in the past several years, while snoopy cache methods have become extremely popular. Directory schemes for cache coherence are potentially attractive in large multiprocessor systems that are beyond the scaling limits of the snoopy cache schemes. Slight modifications to directory schemes can make them competitive in performance with snoopy cache schemes for small multiprocessors. Trace driven simulation, using data collected from several real multiprocessor applications, is used to compare the performance of standard directory schemes, modifications to these schemes, and snoopy cache protocols. 1 Introduction In the past several years, shared-memory multiprocessors have gained wide-spread attention due to the simplicity of the shared-memory parallel programming model. However, allowing...
950|Self-Organized Public-Key Management for Mobile Ad Hoc Networks|In contrast with conventional networks, mobile ad hoc networks usually do not provide online access to trusted authorities or to centralized servers, and they exhibit frequent partitioning due to link and node failures and to node mobility. For these reasons, traditional security solutions that require online trusted authorities or certificate repositories are not well-suited for securing ad hoc networks. In this paper, we propose a fully self-organized public-key management system that allows users to generate their public-private key pairs, to issue certificates, and to perform authentication regardless of the network partitions and without any centralized services. Furthermore, our approach does not require any trusted authority, not even in the system initialization phase.
951|Reducing Memory and Traffic Requirements for Scalable Directory-Based Cache Coherence Schemes|As multiprocessors are scaled beyond single bus systems, there is renewed interest in directory-based cache coherence schemes. These schemes rely on a directory to keep track of all processors caching a memory block. When a write to that block occurs, pointto -point invalidation messages are sent to keep the caches coherent. A straightforward way of recording the identities of processors caching a memory block is to use a bit vector per memory block, with one bit per processor. Unfortunately, when the main memory grows linearly with the number of processors, the total size of the directory memory grows as the square of the number of processors, which is prohibitive for large machines. To remedy this problem several schemes that use a limited number of pointers per directory entry have been suggested. These schemes often cause excessive invalidation traffic. In this paper, we propose two simple techniques that significantly reduce invalidation traffic and directory memory requirements. ...
952|Combinatorial Bounds for Broadcast Encryption|Abstract. A broadcast encryption system allows a center to communi-cate securely over a broadcast channel with selected sets of users. Each time the set of privileged users changes, the center enacts a protocol to establish a new broadcast key that only the privileged users can obtain, and subsequent transmissions by the center are encrypted using the new broadcast key. We study the inherent trade-off between the number of establishment keys held by each user and the number of transmissions needed to establish a new broadcast key. For every given upper bound on the number of establishment keys held by each user, we prove a lower bound on the number of transmissions needed to establish a new broad-cast key. We show that these bounds are essentially tight, by describing broadcast encryption systems that come close to these bounds. 1
953|Long-lived broadcast encryption|Abstract. In a broadcast encryption scheme, digital content is encrypted to ensure that only privileged users can recover the content from the encrypted broadcast. Key material is usually held in a “tamper-resistant,” replaceable, smartcard. A coalition of users may attack such a system by breaking their smartcards open, extracting the keys, and building “pirate decoders ” based on the decryption keys they extract. In this paper we suggest the notion of long-lived broadcast encryption as a way of adapting broadcast encryption to the presence of pirate decoders and maintaining the security of broadcasts to privileged users while rendering all pirate decoders useless. When a pirate decoder is detected in a long-lived encryption scheme, the keys it contains are viewed as compromised and are no longer used for encrypting content. We provide both empirical and theoretical evidence indicating that there is a long-lived broadcast encryption scheme that achieves a steady state in which only a small fraction of cards need to be replaced in each epoch. That is, for any fraction ß, the parameter values may be chosen in such a way to ensure that eventually, at most ß of the cards must be replaced in each epoch. Long-lived broadcast encryption schemes are a more comprehensive solution to piracy than traitor-tracing schemes, because the latter only seek to identify the makers of pirate decoders and don’t deal with how to maintain secure broadcasts once keys have been compromised. In addition, long-lived schemes are a more efficient long-term solution than revocation schemes, because their primary goal is to minimize the amount of recarding that must be done in the long term. 1
954|Key establishment protocols for secure mobile communications: A selective survey|. We analyse several well-known key establishment protocols for mobile communications. The protocols are examined with respect to their security and suitability in mobile environments. In a number of cases weaknesses are pointed out, and in many cases refinements are suggested, either to improve the efficiency or to allow simplified security analysis. 1 Introduction  Security is a critical issue in mobile radio applications, both for the users and providers of such systems. Although the same may be said of all communications systems, mobile applications have special requirements and vulnerabilities, and are therefore of special concern. Once a call has been set up by establishing various security parameters, the problem is reduced to that of employing appropriate cryptographic algorithms to provide the required security services. The most important problem is undoubtedly that of designing protocols for authentication and key management as part of the call set-up process; security-criti...
955|Pgp in constrained wireless devices|Rights to individual papers remain with the author or the author&#039;s employer. Permission is granted for noncommercial reproduction of the work for educational or research purposes. This copyright notice must be included in the reproduced paper. USENIX acknowledges all trademarks herein.
956|Garcia-Luna-Aceves. Solutions to Hidden Terminal Problems in Wireless Networks|The floor acquisition multiple access (FAMA) discipline is ana-lyzed in networks with hidden terminals. According to FAMA, control of the channel (the floor) is assigned to at most one station in the network at any given time, and this station is guaranteed to be able to transmit one or more data packets to different destinations with no collisions. The FAMA protocols described consist of non-persistent carrier or packet sensing, plus a collision-avoidance dia-logue between a source and the intended receiver of a packet. Suf-ficient conditions under which these protocols provide correct floor acquisition are presented and verified for networks with hidden ter-minals; it is shown that FAMA protocols must use carrier sensing to support correct floor acquisition. The throughput of FAMA proto-cols is analyzed for single-channel networks with hidden terminals; it is shown that carrier-sensing FAMA protocols perform better than ALOHA and CSMA protocols in the presence of hidden terminals. 1
957|Storage alternatives for mobile computers|Mobile computers such as notebooks, subnotebooks, and palmtops require low weight, low power consumption, and good interactive performance. These requirements impose many challenges on architectures and operating systems. This paper investigates three alternative storage devices for mobile computers: magnetic hard disks, flash memory disk emulators, and flash memory cards. We have used hardware measurements and trace-driven simulation to evaluate each of the alternative storage devices and their related design strategies. Hardware measurements on an HP OmniBook 300 highlight differences in the performance of the three devices as used on the Omnibook, especially the poor performance of version 2.00 of the Microsoft Flash File System [11] when accessing large files. The traces used in our study came from different environments, including mobile computers (Macintosh Power-Books) and desktop computers (running Windows or HP-UX), as well as synthetic workloads. Our simulation study shows that flash memory can reduce energy consumptionby an order of magnitude, compared to magnetic disk, while providing good read performance and acceptable write performance. These energy savings can translate into a 22% extension of battery life. We also find that the amount of unused memory in a flash memory card has a substantial impact on energy consumption, performance, and endurance: compared to low storage utilizations (40 % full), running flash memory near its capacity (95 % full) can increase energy consumption by 70–190%, degrade write response time by 30%, and decrease the lifetime of the memory card by up to a third. For flash disks, asynchronous erasure can improve write response time by a factor of 2.5. 1
958|Reducing Power Consumption of Network Interfaces in Hand-Held Devices (Extended Abstract)  (1996) | An important issue to be addressed for the next generation of wirelessly-connected hand-held devices is battery longevity. In this paper we examine this issue from the point of view of the Network Interface (NI). In particular, we measure the power usage of two PDAs, the Apple Newton Messagepad and Sony Magic Link, and four NIs, the Metricom Ricochet Wireless Modem, the AT&amp;T Wavelan operating at 915 MHz and 2.4 GHz, and the IBM Infrared Wireless LAN Adapter. These measurements clearly indicate that the power drained by the network interface constitutes a large fraction of the total power used by the PDA. We also conduct trace-driven simulation experiments and show that by using application-specific policies it is possible to ...
959|Power efficient MAC protocol for multihop radio networks|In this paper we develop a new multiaccess protocol for multi-hop radio networks. The unique feature of our protocol is that it is energy conserving. Radios that are not actively transmitting or receiving a packet power themselves off. The manner in which nodes power themselves off does not influence the delay or throughput characteristics of our protocol. Simulation results indicate that power savings of between 10% and 70% are attainable in most systems.  1 Introduction  Ad Hoc networks are multi-hop wireless networks where all nodes cooperatively maintain network connectivity. These type of networks are useful in any situation where temporary network connectivity is needed such as in a region hit by some natural disaster. Nodes in an ad hoc network communicate via radio and since the radio channel is shared by all nodes, it becomes necessary to control access to this shared media. Several authors have developed channel access protocols for multi-hop radio networks where the goal has...
960|Energy Consumption Performance of a Class of Access Protocols for Mobile Data Networks|When user terminals powered by a finite battery source are used for wireless communications, energy constraints are likely to influence the choice of media access protocols. We use the average number of correctly transmitted packets for a given amount of allocated energy as an appropriate metric. In particular, we study different versions of a wireless access protocol operating over a mobile radio channel using a finite energy source with a flat power profile. The mobile radio channel iteslf is characterized by a correlated Rayleigh fading process, the correlation in the fading process being dependent on the speed of the user terminal. We show that the access protocol with an Error Detect feature is energy efficient for pedestrian user speeds, whereas for vehicular speeds a Retransmission  protocol is more efficient.  I. INTRODUCTION  Portable user terminals for mobile communications must rely on limited battery energy for their operation [1]-[6]. The design /choice of media access pro...
961|Low-power Access Protocols Based on Scheduling for Wireless and Mobile ATM Networks|This paper presents the design and analysis of a medium access control protocol called EC-MAC (Energy Conserving Medium Access Protocol) that supports multimedia traffic for wireless ATM networks. The objective of protocol design is to develop a low-power access protocol that will provide support for different traffic types with quality-of-service (QoS). The network architecture is derived from a testbed built at Bell Labs called SWAN (Seamless Wireless ATM network). The network is based on the infrastructure model where one base station serves all the mobiles currently in its cell. A reservation based approach is proposed, with appropriate scheduling of the requests from the mobiles. This strategy is utilized to accomplish the goals of reduced power consumption and support service quality provision in wireless links. Simulation based performance analysis of the protocol for voice, video and data traffic is presented.
962|Energy Management in Wireless Communications|In this paper, our goal is to study the &#034;bits per joule&#034; efficiency rating of a protocol in  the wireless environment. We explore and compare three approaches to evaluating the energy  efficiency and assess their accuracy and complexity. Although our technique allows us to  accommodate other profiles, for concreteness we model the battery as a device that has the  means to support the transmission of a fixed number of packets. We model the fading as a  Markov channel, and we present some particular results for link error control protocols. For the  particular examples considered, the exact recursive approach and an asymptotic approach were  found to predict results that were very close. In addition, a lower bound and an approximation  lead to analytical expressions. The quality of the bound and the accuracy of the approximation  improves quite quickly as the amount of available energy increases.  1 Introduction  In the mobile wireless environment, portable communicators can be seen as ...
963|Vivaldi: A Decentralized Network Coordinate System|Large-scale Internet applications can benefit from an ability to predict round-trip times to other hosts without having to contact them first. Explicit measurements are often unattractive because the cost of measurement can outweigh the benefits of exploiting proximity information. Vivaldi is a simple, light-weight algorithm that assigns synthetic coordinates to hosts such that the distance between the coordinates of two hosts accurately predicts the communication latency between the hosts.
964|Wide-area cooperative storage with CFS|The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers. CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.
965|Surface reconstruction from unorganized points|We describe and demonstrate an algorithm that takes as input an unorganized set of points fx1?:::?xng IR 3 on or near an unknown manifold M, and produces as output a simplicial surface that approximates M. Neither the topology, the presence of boundaries, nor the geometry of M are assumed to be known in advance — all are inferred automatically from the data. This problem naturally arises in a variety of practical situations such as range scanning an object from multiple view points, recovery of biological shapes from two-dimensional slices, and interactive surface sketching.
966|Modeling Internet Topology|The topology of a network, or a group of networks such as the Internet, has a strong bearing on many management and performance issues. Good models of the topological structure of a network are essential for developing and analyzing internetworking technology. This article discusses how graph-based models can be used to represent the topology of large networks, particularly aspects of locality and hierarchy present in the Internet. Two implementations that generate networks whose topology resembles that of typical internetworks are described, together with publicly available source code.  
967|Geographic Routing without Location Information|For many years, scalable routing for wireless communication systems was a compelling but elusive goal. Recently, several routing algorithms that exploit geographic information (e.g., GPSR) have been proposed to achieve this goal. These algorithms refer to nodes by their location, not address, and use those coordinates to route greedily, when possible, towards the destination. However, there are many situations where location information is not available at the nodes, and so geographic methods cannot be used. In this paper we define a scalable coordinate-based routing algorithm that does not rely on location information, and thus can be used in a wide variety of ad hoc and sensornet environments.  
968|IDMaps: A Global Internet Host Distance Estimation Service|There is an increasing need to quickly and efficiently learn network distances, in terms of metrics such as latency or bandwidth, between Internet hosts. For example, Internet content providers often place data and server mirrors throughout the Internet to improve access latency for clients, and it is necessary to direct clients to the closest mirrors based on some distance metric in order to realize the benefit of mirrors. We suggest a scalable Internet-wide architecture, called IDMaps, which measures and disseminates distance information on the global Internet. Higher-level services can collect such distance information to build a virtual distance map of the Internet and estimate the distance between any pair of IP addresses. We present our solutions to the measurement server placement and distance map construction problems in IDMaps. We show that IDMaps can indeed provide useful distance estimations to applications such as closest-mirror selection.
969|Locationing in distributed ad-hoc wireless sensor networks|Evolving networks of ad-hoc, wireless sensing nodes rely heavily on the ability to establish position information. The algorithms presented herein rely on range measurements between pairs of nodes and the aprioricoordinates of sparsely located anchor nodes. Clusters of nodes surrounding anchor nodes cooperatively establish confident position estimates through assumptions, checks, and iterative refinements. Once established, these positions are propagated to more distant nodes, allowing the entire network to create an accurate map of itself. Major obstacles include overcoming inaccuracies in range measurements as great as ±50%, as well as the development of initial guesses for node locations in clusters with few or no anchor nodes. Solutions to these problems are presented and discussed, using position error as the primary metric. Algorithms are compared according to position error, scalability, and communication and computational requirements. Early simulations yield average position errors of 5 % in the presence of both range and initial position inaccuracies. 1.
970|Development of the Domain Name System|(Originally published in the Proceedings of SIGCOMM ‘88,
971|Designing a DHT for low latency and high throughput|Designing a wide-area distributed hash table (DHT) that provides high-throughput and low-latency network storage is a challenge. Existing systems have explored a range of solutions, including iterative routing, recursive routing, proximity routing and neighbor selection, erasure coding, replication, and server selection. This
972|Virtual Landmarks for the Internet|Internet coordinate schemes have been proposed as a method for estimating minimum round trip time between hosts without direct measurement. In such a scheme, each host is assigned a set of coordinates, and Euclidean distance is used to form the desired estimate. Two key questions are: How accurate are coordinate schemes across the Internet as a whole? And: are coordinate assignment schemes fast enough, and scalable enough, for large scale use? In this paper we make contributions toward answering both those questions. Whereas the coordinate assignment problem has in the past been approached by nonlinear optimization, we develop a faster method based on dimensionality reduction of the Lipschitz embedding. We show that this method is reasonably accurate, even when applied to measurements spanning the Internet, and that it naturally leads to a scalable measurement strategy based on the notion of virtual landmarks.
973|PIC: Practical Internet Coordinates for Distance Estimation|mechanism to estimate Internet network distance (i.e., round-trip delay or network hops). Network distance estimation is important in many applications, for example, network-aware overlay construction and server selection. There are several proposals for distance estimation in the Internet but they all suffer from problems that limit their benefit. Most rely on a small set of infrastructure nodes that are a single point of failure and limit scalability. Others use sets of peers to compute coordinates but these coordinates can be arbitrarily wrong if one of these peers is malicious. While it may be reasonable to secure a small set of infrastructure nodes, it is unreasonable to secure all peers. PIC addresses these problems: it does not rely on infrastructure nodes and it can compute accurate coordinates even when some peers are malicious. We present PIC&#039;s design, experimental evaluation, and an application to network-aware overlay construction and maintenance.
974|Lighthouses for Scalable Distributed Location|This paper introduces Lighthouse, a scalable location mechanism  for wide-area networks. Unlike existing vector-based systems such  as GNP, we show how network-location can be established without using  a  xed set of reference points. This lets us avoid the communication bottlenecks  and single-points-of-failure that otherwise limit the practicality  of such systems.
975|Big-Bang Simulation for Embedding Network Distances in Euclidean Space|Embedding of a graph metric in Euclidean space efficiently and accurately is an important problem in general with applications in topology aggregation, closest mirror selection, and application level routing. We propose a new graph embedding scheme called Big-Bang Simulation (BBS), which simulates an explosion of particles under force field derived from embedding error. BBS is shown to be significantly more accurate, compared to all other embedding methods including GNP. We report an extensive simulation study of BBS compared with several known embedding schemes and show its advantage for distance estimation (as in the IDMaps project), mirror selection and topology aggregation.
976|Anchor-Free Distributed Localization in Sensor Networks|Many sensor network applications require that each node&#039;s sensor stream be annotated with its physical location in some common coordinate system. Manual measurement and configuration methods for obtaining location don&#039;t scale and are error-prone, and equipping sensors with GPS is often expensive and does not work in indoor and urban deployments. Sensor networks can therefore benefit from a self-configuring method where nodes cooperate with each other, estimate local distances to their neighbors, and converge to a consistent coordinate assignment. This paper describes a fully decentralized algorithm called AFL (Anchor-Free Localization) where nodes start from a random initial coordinate assignment and converge to a consistent solution using only local node interactions. The key idea in AFL is fold-freedom, where nodes first configure into a topology that resembles a scaled and unfolded version of the true configuration, and then run a force-based relaxation procedure.We show using extensive simulations under a variety of network sizes, node densities, and distance estimation errors that our algorithm is superior to previously proposed methods that incrementally compute the coordinates of nodes in the network, in terms of its ability to compute correct coordinates under a wider variety of conditions and its robustness to measurement errors.
977|Efficient Topology-Aware Overlay Network|Peer-to-peer (P2P) networking has become a household word in the past few years, being marketed as a work-around for server scalability problems and ``wonder drug&#039;&#039; to achieve resilience. Current widely-used P2P networks rely on central directory servers or massive message flooding, clearly not scalable solutions. Distributed Hash Tables (DHT) are expected to eliminate flooding and central servers, but can require many long-haul message deliveries. We introduce Mithos, an content-addressable overlay network that only uses minimal routing information and is directly suitable as an underlay network for P2P systems, both using traditional and DHT addressing. Unlike other schemes, it also efficiently provides locality-aware connectivity, thereby ensuring that a message reaches its destination with minimal overhead. Mithos provides for highly efficient forwarding, making it suitable for use in high-throughput applications. Paired with its ability to have addresses directly mapped into a subspace of the IPv6 address space, it provides a potential candidate for native deployment. Additionally, Mithos can be used to support third-party triangulation to quickly select a close-by replica of data or services.
978|A Network Positioning System for the Internet|Network positioning has recently been demonstrated to be a viable concept to represent the network distance relationships among Internet end hosts. Several subsequent studies have examined the potential benefits of using network position in applications, and proposed alternative network positioning algorithms. In this paper, we study the problem of designing and building a network positioning system (NPS). We identify several key system building issues such as the consistency, adaptivity and stability of host network positions over time. We propose a hierarchical network positioning architecture that maintains consistency while enabling decentralization, a set of adaptive decentralized algorithms to compute and maintain accurate, stable network positions, and finally present a prototype system deployed on PlanetLab nodes that can be used by a variety of applications. We believe our system is a viable first step to provide a network positioning capability in the Internet. 
979|Practical, Distributed Network Coordinates|Vivaldi is a distributed algorithm that assigns synthetic coordinates to Internet hosts, so that the Euclidean distance between two hosts&#039; coordinates predicts the network latency between them. Each node in Vivaldi computes its coordinates by simulating its position in a network of physical springs. Vivaldi is both distributed and efficient: no fixed infrastructure need be deployed and a new host can compute useful coordinates after collecting latency information from only a few other hosts. Vivaldi can rely on piggy-backing latency information on application traffic instead of generating extra traffic by sending its own probe packets. This paper
980|On the Curvature of the Internet and its usage for Overlay Construction and Distance Estimation|It was noted in recent years that the Internet structure resembles a star with a highly connected core and long stretched tendrils. In this work we present a new quantity, the Internet geometric curvature, that captures the above observation by a single number. We embed the Internet distance metric in a hyperbolic space with an optimal curvature and achieve an accuracy better than achieved before for the Euclidean space. This proves our hypothesis regarding the internet curvature. We demonstrate the strength of our embedding with two applications: selecting the closest server and building an application level multicast tree.
981|Efficient use of workstations for passive monitoring of local area networks|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
982|Routing and Data Location in Overlay Peer-to-Peer Networks|Peer-to-peer overlay networks offer a novel platform for a variety of scalable and decentralized distributed applications. Systems known as Distributed Hash Tables provide efficient and fault-tolerant routing, object location and load balancing within a self-organizing overlay network. The alternative solution we propose is an overlay location and routing infrastructure that efficiently uses minimal local information to achieve global routing. The main novelty of our approach consists in fitting the overlay network in a hyper-toroidal space and building it with *locality awareness.* Thanks to this specific network construction phase, forwarding decisions always take into account *locality preservation* in an implicit manner, leading to significant improvements in end-to-end delays and path lengths. With this overlay network it is possible to obtain global routing by adding minimal information to each single host and by making only local forwarding decisions. Our analysis shows how the average path length coming from the overlay routing is close to the optimal average path length of the underlaying network: on average, they only differ by a factor of 2. Furthermore, *locality preservation* has a significant impact on the end-to-end latency of the routing process as well. Such a system can be viewed as novel in the field of peer-to-peer data location and addressing, allowing the development of new applications in a real *low-latency* environment.
983|Peer-to-peer simulator|the development of a general force field for the molecular
984|Support-Vector Networks|The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the supportvector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.
985|A training algorithm for optimal margin classifiers|A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.  
986|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
987|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
988|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
989|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
990|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
991|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
992|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
993|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
994|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
995|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
996|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
997|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
998|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
999|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
1000|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
1001|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
1002|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
1003|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
1004|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
1005|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
1006|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
1007|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
1008|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
1009|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
1010|A distributed algorithm for minimum-weight spanning trees|A distributed algorithm is presented that constructs he minimum-weight spanning tree in a connected undirected graph with distinct edge weights. A processor exists at each node of the graph, knowing initially only the weights of the adjacent edges. The processors obey the same algorithm and exchange messages with neighbors until the tree is constructed. The total number of messages required for a graph of N nodes and E edges is at most 5N log2N + 2E, and a message contains at most one edge weight plus log28N bits. The algorithm can be initiated spontaneously at any node or at any subset of nodes.
1011|Routing for Maximum System Lifetime in Wireless Ad-hoc Networks|An ad-hoc network of wireless static nodes is considered as it arises in a rapidly  deployed, sensor based, monitoring system. Information is generated in certain  nodes and needs to reach some designated gateway node. Each node may adjust  its power within a certain range that determines the set of possible one hop away  neighbors. Traffic forwarding through multiple hops is employed when the intended  destination is not within immediate reach. The nodes have limited initial amounts  of energy that are consumed in different rates depending on the power level and the  intended receiver. We propose algorithms to select the routes and the corresponding  power levels such that the time until the batteries of the nodes drain-out is  maximized. The algorithms are local and amenable to distributed implementation.
1012|Decentralized Channel Management in Scalable Multihop Spread-Spectrum Packet Radio Networks|This thesis addresses the problems of managing the transmissions of stations in a spread-spectrum packet radio network so that the system can remain effective when scaled to millions of nodes concentrated in a metropolitan area. The principal difficulty in scaling a system of packet radio stations is interference from other stations in the system. Interference comes both from nearby stations and from distant stations. Each nearby interfering station is a particular problem, because a signal received from it may be as strong as or stronger than the desired signal from some other station. Far-off interfering stations are not individually a problem, since each of their signals will be weaker, but the combined effect may be the dominant source of interference. The thesis begins with an analysis of propagation and interference models. The overall noise level in the system (mainly caused by the many distant stations) is then analyzed, and found to remain manageable even as the system scales ...
1013|Data Streams: Algorithms and Applications|In the data stream scenario, input arrives very rapidly and there is limited memory to store the input. Algorithms have to work with one or few passes over the data, space less than linear in the input size or time significantly less than the input size. In the past few years, a new theory has emerged for reasoning about algorithms that work within these constraints on space, time, and number of passes. Some of the methods rely on metric embeddings, pseudo-random computations, sparse approximation theory and communication complexity. The applications for this scenario include IP network traffic analysis, mining text message streams and processing massive data sets in general. Researchers in Theoretical Computer Science, Databases, IP Networking and Computer Systems are working on the data stream challenges. This article is an overview and survey of data stream algorithmics and is an updated version of [175].1
1015|Greed is Good: Algorithmic Results for Sparse Approximation|This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho’s basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms. 
1016|Network Applications of Bloom Filters: A Survey|Abstract. ABloomfilter is a simple space-efficient randomized data structure for representing a set in order to support membership queries. Bloom filters allow false positives but the space savings often outweigh this drawback when the probability of an error is controlled. Bloom filters have been used in database applications since the 1970s, but only in recent years have they become popular in the networking literature. The aim of this paper is to survey the ways in which Bloom filters have been used and modified in a variety of network problems, with the aim of providing a unified mathematical and practical framework for understanding them and stimulating their use in future applications. 1.
1017|Approximate Frequency Counts over Data Streams|We present algorithms for computing frequency counts exceeding a user-specified threshold over data streams. Our algorithms are simple and have provably small memory footprints. Although the output is approximate, the error is guaranteed not to exceed a user-specified parameter. Our algorithms can easily be deployed for streams of singleton items like those found in IP network monitoring. We can also handle streams of variable sized sets of items exemplified by a sequence of market basket transactions at a retail store. For such streams, we describe an optimized implementation to compute frequent itemsets in a single pass.
1018|Bursty and Hierarchical Structure in Streams|A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a &#034;burst of activity,&#034; with certain features rising sharply in frequency as the topic emerges.
1019|Finding frequent items in data streams|Abstract. We present a 1-pass algorithm for estimating the most frequent items in a data stream using very limited storage space. Our method relies on a novel data structure called a count sketch, which allows us to estimate the frequencies of all the items in the stream. Our algorithm achieves better space bounds than the previous best known algorithms for this problem for many natural distributions on the item frequencies. In addition, our algorithm leads directly to a 2-pass algorithm for the problem of estimating the items with the largest (absolute) change in frequency between two data streams. To our knowledge, this problem has not been previously studied in the literature. 1
1020|Chromium: A Stream-Processing Framework for Interactive Rendering on Clusters|We describe Chromium, a system for manipulating streams of graphics API commands on clusters of workstations. Chromium&#039;s stream filters can be arranged to create sort-first and sort-last parallel graphics architectures that, in many cases, support the same applications while using only commodity graphics accelerators. In addition, these stream filters can be extended programmatically, allowing the user to customize the stream transformations performed by nodes in a cluster. Because our stream processing mechanism is completely general, any cluster-parallel rendering algorithm can be either implemented on top of or embedded in Chromium. In this paper, we give examples of real-world applications that use Chromium to achieve good scalability on clusters of workstations, and describe other potential uses of this stream processing technology. By completely abstracting the underlying graphics architecture, network topology, and API command processing semantics, we allow a variety of applications to run in different environments.
1021|Min-wise Independent Permutations|We define and study the notion of min-wise independent families of permutations. We say that F ? Sn is min-wise independent if for any set X ? [n] and any x ? X, when p is chosen at random in F we have Pr(min{p(X)}  = p(x))  = 1 |X |. In other words we require that all the elements of any fixed set X have an equal chance to become the minimum element of the image of X under p. Our research was motivated by the fact that such a family (under some relaxations) is essential to the algorithm used in practice by the AltaVista web index software to detect and filter near-duplicate documents. However, in the course of our investigation we have discovered interesting and challenging theoretical questions related to this concept – we present the solutions to some of them and we list the rest as open problems.
1022|What’s Hot and What’s Not: Tracking Most Frequent Items Dynamically|Most database management systems maintain statistics on the underlying relation. One of the important statistics is that of the “hot items” in the relation: those that appear many times (most frequently, or more than some threshold). For example, end-biased histograms keep the hot items as part of the histogram and are used in selectivity estimation. Hot items are used as simple outliers in data mining, and in anomaly detection in networking applications. We present a new algorithm for dynamically determining the hot items at any time in the relation that is undergoing deletion operations as well as inserts. Our algorithm maintains a small space data structure that monitors the transactions on the relation, and when required, quickly outputs all hot items, without rescanning the relation in the database. With user-specified probability, it is able to report all hot items. Our algorithm relies on the idea of “group testing”, is simple to implement, and has provable quality, space and time guarantees. Previously known algorithms for this problem that make similar quality and performance guarantees can not handle deletions, and those that handle deletions can not make similar guarantees without rescanning the database. Our experiments with real and synthetic data shows that our algorithm is remarkably accurate in dynamically tracking the hot items independent of the rate of insertions and deletions. 
1023|High-dimensional data analysis: The curses and blessings of dimensionality| The coming century is surely the century of data. A combination of blind faith and serious purpose makes our society invest massively in the collection and processing of data of all kinds, on scales unimaginable until recently. Hyperspectral Imagery, Internet Portals, Financial tick-by-tick data, and DNA Microarrays are just a few of the betterknown sources, feeding data in torrential streams into scientific and business databases worldwide. In traditional statistical data analysis, we think of observations of instances of particular phenomena (e.g. instance ? human being), these observations being a vector of values we measured on several variables (e.g. blood pressure, weight, height,...). In traditional statistical methodology, we assumed many observations and a few, wellchosen variables. The trend today is towards more observations but even more so, to radically larger numbers of variables – voracious, automatic, systematic collection of hyper-informative detail about each observed instance. We are seeing examples where the observations gathered on individual instances are curves, or spectra, or images, or
1025|How to Summarize the Universe: Dynamic Maintenance of Quantiles|Order statistics, i.e., quantiles, are frequently  used in databases both at the database server  as well as the application level. For example,  they are useful in selectivity estimation during  query optimization, in partitioning large relations,  in estimating query result sizes when  building user interfaces, and in characterizing  the data distribution of evolving datasets in  the process of data mining.
1026|Secure multiparty computation of approximations|Approximation algorithms can sometimes provide efficient solutions when no efficient exact computation is known. In particular, approximations are often useful in a distributed setting where the inputs are held by different parties and may be extremely large. Furthermore, for some applications, the parties want to compute a function of their inputs securely, without revealing more information than necessary. In this work we study the question of simultaneously addressing the above efficiency and security concerns via what we call secure approximations. We start by extending standard definitions of secure (exact) computation to the setting of secure approximations. Our definitions guarantee that no additional information is revealed by the approximation beyond what follows from the output of the function being approximated. We then study the complexity of specific secure approximation problems. In particular, we obtain a sublinear-communication protocol for securely approximating the Hamming distance and a polynomial-time protocol for securely approximating the permanent and related #P-hard problems. 1
1028|Comparing data streams using hamming norms (how to zero in)  (2003) | Massive data streams are now fundamental to many data processing applications. For example, Internet routers produce large scale diagnostic data streams. Such streams are rarely stored in traditional databases and instead must be processed “on the fly” as they are produced. Similarly, sensor networks produce multiple data streams of observations from their sensors. There is growing focus on manipulating data streams and, hence, there is a need to identify basic operations of interest in managing data streams, and to support them efficiently. We propose computation of the Hamming norm as a basic operation of interest. The Hamming norm formalizes ideas that are used throughout data processing. When applied to a single stream, the Hamming norm gives the number of distinct items that are present in that data stream, which is a statistic of great interest in databases. When applied to a pair of streams, the Hamming norm gives an important measure of (dis)similarity: the number of unequal item counts in the two streams. Hamming norms have many uses in comparing data streams. We present a novel approximation technique for estimating the Hamming norm for massive data streams; this relies on what we call the “l0 sketch ” and we prove its accuracy. We test our approximation method on a large quantity of synthetic and real stream data, and show that the estimation is accurate to within a few percentage points.
1029|A Small Approximately Min-Wise Independent Family of Hash Functions|In this paper we give a construction of a small approximately min-wise independent family of hash functions. The number of bits needed to represent each function is  O(logn \Delta log 1=ffl). This construction gives a solution to the main open problem of [2].  1 Introduction  A family of functions H ae [n] ! [n] (where [n] =  f0 : : : n \Gamma 1g) is called ffl-min-wise independent if for any  X ae [n] and x 2 [n] \Gamma X we have Pr  h2H  [h(x) ! minh(X)] = 1  jXj + 1 (1 \Sigma ffl)  1 This definition can be generalized to the case when  jXj is restricted to be smaller than a prespecified bound s. Such families (restricted to the case when all functions from H are permutations) were introduced and investigated in [2] and independently earlier in [6] (cf. [7]). The motivation for studying such families is to reduce amount of randomness used by algorithms [6, 2, 3]. In particular (as pointed out in [2]) they have immediate application to efficient detection of similar documents in large...
1030|Set Reconciliation with Nearly Optimal Communication Complexity|We consider the problem of efficiently reconciling two similar sets held by different hosts while minimizing the communication complexity. This type of problem arises naturally from gossip protocols used for the distribution of information. We describe an approach to set reconciliation based on the encoding of sets as polynomials. The resulting protocols exhibit tractable computational complexity and nearly optimal communication complexity. Also, these protocols can be adapted to work over a broadcast channel, allowing many clients to reconcile with one host based on a single broadcast, even if each client is missing a different subset.
1031|The String Edit Distance  Matching Problems with Moves|The edit distance between two strings S and R is defined to be the minimum number of character inserts, deletes and changes needed to convert R to S. Given a text string t of length n, and a pattern string p of length m, informally, the string edit distance matching problem is to compute the smallest edit distance between p and substrings of t. We relax the problem so that (a) we allow an additional operation, namely, substring moves, and (b) we allow approximation of this string edit distance. Our result is a near linear time deterministic algorithm to produce a factor of O(log n log * n) approximation to the string edit distance with moves. This is the first known significantly subquadratic algorithm for a string edit distance problem in which the distance involves nontrivial alignments. Our results are obtained by embedding strings into L1 vector space using a simplified parsing technique we call Edit
1032|Learn More, Sample Less: Control of Volume and Variance in Network Measurement |objects 289-43596         . We wish to estimate the sums   !#&#034; %$ &amp;(&#039;*)+&amp; ,  of the sizes of objects of a given color    , from a sampled subset of objects. How should the sampling distribution be chosen in order to jointly control both the variance of the estimators  -     ./   and the number of samples taken? This problem is motivated from network measurement, in which the    are the byte sizes of traffic flows reported by routers, and the     are the common properties of the packet of the flow, e.g., source and destination IP address. In this paper we propose a sampling scheme that optimally controls the volume of the measurements, and the variance of unbiased usage estimates  - 0/   , while retaining usage detail down to the finest level of granularity in the colors. We provide algorithms for dynamic control of sample volumes and evaluate them on flow data gathered from a commercial IP network. The algorithms are simple to implement and robust to variation in network conditions. The work reported here has been applied in the measurement infrastructure of the commercial IP network. To not have employed sampling would have entailed an order of magnitude greater capital expenditure to accommodate the measurement traffic and its processing.
1033|Mining database structure; or, how to build a data quality browser|ABSTRACT Data mining research typically assumes that the data to be analyzed has been identified, gathered, cleaned, and processed into a convenient form. While data mining tools greatly enhance the ability of the analyst to make datadriven discoveries, most of the time spent in performing an analysis is spent in data identification, gathering, cleaning and processing the data. Similarly, schema mapping tools have been developed to help automate the task of using legacy or federated data sources for a new purpose, but assume that the structure of the data sources is well understood. However the data sets to be federated may come from dozens of databases containing thousands of tables and tens of thousands of fields, with little reliable documentation about primary keys or foreign keys. We are developing a system, Bellman, which performs data mining on the structure of the database. In this paper, we present techniques for quickly identifying which fields have similar values, identifying join paths, estimating join directions and sizes, and identifying structures in the database. The results of the database structure mining allow the analyst to make sense of the database content. This information can be used to e.g., prepare data for data mining, find foreign key joins for schema mapping, or identify steps to be taken to prevent the database from collapsing under the weight of its complexity. 1.
1034|On the Dynamic Finger Conjecture for Splay Trees. Part II: The Proof|The following result is shown: On an n-node splay tree, the amortized cost of an access  at distance d from the preceding access is O(log(d + 1)). In addition, there is an O(n)  initialization cost. The accesses include searches, insertions and deletions.  1 Introduction  The reader is advised that this paper quotes results from the companion Part I paper [CMSS93]; in addition, the Part I paper introduces a number of the techniques used here, but in a somewhat less involved way.  The splay tree is a self-adjusting binary search tree devised by Sleator and Tarjan [ST85]. They showed that it is competitive with many of the balanced search tree schemes for maintaining a dictionary. Specifically, Sleator and Tarjan showed that a sequence of m accesses performed on a splay tree takes time O(m log n), where n is the maximum size attained by the tree (n  m). They also showed that in an amortized sense, up to a constant factor, on sufficiently long sequences of searches, the splay tree has as ...
1035|Communities of Interest|We consider problems that can be characterized by large dynamic  graphs. Communication networks provide the prototypical example  of such problems where nodes in the graph are network IDs and the  edges represent communication between pairs of network IDs. In such  graphs, nodes and edges appear and disappear through time so that  methods that apply to static graphs are not sufficient. We introduce a  data structure that captures, in an approximate sense, the graph and  its evolution through time. The data structure arises from a bottom-up  representation of the large graph as the union of small subgraphs, called  Communities of Interest (COI), centered on every node. These subgraphs  are interesting in their own right and we discuss two applications in the  area of telecommunications fraud detection to help motivate the ideas.
1036|QuickSAND: Quick Summary and Analysis of Network Data|Monitoring and analyzing traffic data generated from large ISP networks imposes challenges both at the data gathering phase as well as the data analysis itself. Still both tasks are crucial for responding to day to day challenges of engineering large networks with thousands of customers. In this paper we build on the premise that approximation is a necessary evil of handling massive datasets such as network data. We propose building compact summaries of the traffic data called sketches at distributed network elements and centers. These sketches are able to respond well to queries that seek features that stand out of the data. We call such features &#034;heavy hitters.&#034; In this paper, we describe sketches and show how to use sketches to answer aggregate and trend-related queries and identify heavy hitters. This may be used for exploratory data analysis of network operations interest. We support our proposal by experimentally studying AT&amp;T WorldNet data and performing a feasibility study on the Cisco NetFlow data collected at several routers.  1 
1038|Reverse Nearest Neighbor Aggregates Over Data Streams|Reverse Nearest Neighbor (RNN) queries  have been studied for finite, stored data  sets and are of interest for decision support.
1039|Sublinear Time Approximate Clustering|Clustering is of central importance in a number of disciplines including Machine Learning, Statistics, and Data Mining. This paper has two foci: (1) It describes how existing algorithms for clustering can benefit from simple sampling techniques arising from work in statistics [Pol84]. (2) It motivates and introduces a new model of clustering that is in the spirit of the &#034;PAC (probably approximately correct)&#034; learning model, and gives examples of efficient PAC-clustering algorithms.  
1040|Counting inversions in lists|issues we face here. In a recent paper, Ajtai et al. [1] give a streaming algorithm In the rest of the paper, , where is the to count the number of inversions in a stream ??using allowed error. We will make the reasonable assumption in two passes and space. Here, this paper that??. We have not made an attempt to we present a simple randomized streaming algorithm for the optimize constants. same problem that uses one pass and space. Our algorithm is based on estimating quantiles of the 2 The Algorithm items already seen in the stream, and using that to estimate Suppose for all ?  ?  ? ????, we knew the the number of inversions involving each element. quantiles ? of?exactly. We claim that we can use this to approximate??to within a factor of. 1
1041|Estimating dominance norms of multiple data streams|Abstract. There is much focus in the algorithms and database communities on designing tools to manage and mine data streams. Typically, data streams consist of multiple signals. Formally, a stream of multiple signals is (i, ai,j) where i’s correspond to the domain, j’s index the different signals and ai,j = 0 give the value of the jth signal at point i. We study the problem of finding norms that are cumulative of the multiple signals in the data stream. For example, consider the max-dominance norm, defined as i maxj{ai,j}. It may be thought as estimating the norm of the “upper envelope ” of the multiple signals, or alternatively, as estimating the norm of the “marginal ” distribution of tabular data streams. It is used in applications to estimate the “worst case influence” of multiple processes,for example in IP traffic analysis, electrical grid monitoring and financial domain. In addition, it is a natural measure, generalizing the union of data streams or counting distinct elements in data streams. We present the first known data stream algorithms for estimating max-dominance of multiple signals. In particular, we use workspace and time-per-item that are both sublinear (in fact, poly-logarithmic) in the input size. In contrast other notions of dominance on streams a, b — min-dominance ( i minj{ai,j}), countdominance (|{i|ai&gt; bi}|) or relative-dominance ( i ai / max{1, bi} )  — are all impossible to estimate accurately with sublinear space. 1
1042|Maintaining Statistics Counters in Router Line Cards|this article, we assume that an arriving packet increments only one counter. If we instead considered the case where each packet arrival updates C counters, the line rate on the interface would be CR
1043|Better algorithms for high-dimensional proximity problems via asymmetric embeddings|Abstract In this paper we give several results based on randomized embeddings of l2 into l1 (or &amp;quot;l1-like&amp;quot;) spaces. Our first result is a (1 + ffl)-distortion asymmetric embedding of n points in l2 into l1 with polylog(n) dimension, for any 1 + ffl. This gives the first known O(1)- approximate nearest neighbor algorithm with fast query time and almost polynomial space for a product of Euclidean norms, a common generalization of both l2 and l1 norms. Our embedding also clarifies the relative complexity of approximate nearest neighbor in l2 and l1 spaces. Our second result in a (1+ffl)-approximate algorithm for the diameter of n points in ld2, running in time ~O(dn1+1=(1+ffl)2); the algorithm is fully dynamic. This improves several previous algorithms for this problem (see Table 1 for more information). 1 Introduction Embeddings between normed spaces are known to be very useful tools for designing geometric algorithms. A classic example is an O(2dn)-time algorithm for computing diameter of n points in ld1 [GBT84]: since the problem seems difficult in the original space, we can embed1 ld1 isometrically into l2
1044|Three Thresholds for a Liar|Motivated by the problem of making correct computations from partly false information, we study a corruption of the classic game &#034;Twenty Questions&#034; in which the player who answers the yes-or-no questions is permitted to lie up to a fixed fraction r of the time. The other player is allowed q arbitrary questions with which to try to determine, with certainty, which of n objects his opponent has in mind; he &#034;wins&#034; if he can always do so, and &#034;wins quickly&#034; if he can do so using only O(log n) questions. It turns out that there is a threshold value for r below which the querier can win quickly, and above which he cannot win at all. However, the threshold value varies according to the precise rules of the game. Our &#034;three thresholds theorem&#034; says that when the answerer is forbidden at any point to have answered more than a fraction r of the questions incorrectly, then the threshold value is r =  1 2 ; when the requirement is merely that the total number of lies cannot exceed rq, the threshol...
1045|Inferring mixtures of markov chains|Abstract. We define the problem of inferring a “mixture of Markov chains ” based on observing a stream of interleaved outputs from these chains. We show a sharp characterization of the inference process. The problems we consider also has applications such as gene finding, intrusion detection, etc., and more generally in analyzing interleaved sequences. 1
1046|Petabyte Scale Data Mining: Dream or Reality?|Science is becoming very data intensive. Today&#039;s astronomy datasets with tens of millions of galaxies already present substantial challenges for data mining. In less than 10 years the catalogs are expected to grow to billions of objects, and image archives will reach Petabytes. Imagine having a 100GB database in 1996, when disk scanning speeds were  30MB/s, and database tools were immature. Such a task today is trivial, almost manageable with a laptop. We think that  the issue of a PB database will be very similar in six years. In this paper we scale our current experiments in data  archiving and analysis on the Sloan Digital Sky Survey  2,3  data six years into the future. We analyze these projections and look at the requirements of performing data mining on such data sets. We conclude that the task scales rather well: we could do the job today, although it would be expensive. There do not seem to be any show-stoppers that would prevent us from storing and using a Petabyte dataset six years from today.
1047|Application of the Two-Sided Depth Test to CSG Rendering|Shadow mapping is a technique for doing real-time shadowing. Recent work has shown that shadow mapping hardware can be used as a second depth test in addition to the z-test. In this paper, we explore the computational power provided by this second depth test by examining the problem of rendering objects described as CSG (Constructive Solid Geometry) expressions. We provide an algorithm that asymptotically improves the number of rendering passes required to display a CSG object by a factor of n by exploiting the two-sided depth test. Interestingly, a matching lower bound can be proved demonstrating that our algorithm is optimal.
1048|Space-efficient finger search on degree-balanced search trees|We show how to support the finger search operation on degree-balanced search trees in a space-efficient manner that retains a worst-case time bound of O(log d), where d is the difference in rank between successive search targets. While most existing tree-based designs allocate linear extra storage in the nodes (e.g., for side links and parent pointers), our design maintains a compact auxiliary data structure called the “hand” during the lifetime of the tree and imposes no other storage requirement within the tree. The hand requires O(log n) space for an n-node tree and has a relatively simple structure. It can be updated synchronously during insertions and deletions with time proportional to the number of structural changes in the tree. The auxiliary nature of the hand also makes it possible to introduce finger searches into any existing implementation without modifying the underlying data representation (e.g., any implementation of Red-Black trees can be used). Together these factors make finger searches more appealing in practice. Our design also yields a simple yet optimal inorder walk algorithm with worst-case O(1) work per increment (again without any extra storage requirement in the nodes), and we believe our algorithm can be used in database applications when the overall performance is very sensitive to retrieval latency.
1049|A Practical Bayesian Framework for Backprop Networks|A quantitative and practical Bayesian framework is described for learning of mappings in feedforward networks. The framework makes possible: (1) objective comparisons between solutions using alternative network architectures
1050|The Evidence Framework Applied to Classification Networks|Three Bayesian ideas are presented for supervised adaptive classifiers. First, it is argued that the output of a classifier should be obtained by marginalizing over the posterior distribution of the parameters; a simple approximation to this integral is proposed and demonstrated. This involves a &#034;moderation&#034; of the most probable classifier&#039;s outputs, and yields improved performance. Second, it is demonstrated that the Bayesian framework for model comparison described for regression models in MacKay (1992a,b) can also be applied to classification problems. This framework successfully chooses the magnitude of weight decay terms, and ranks solutions found using different numbers of hidden units. Third, an information-based data selection criterion is derived and demonstrated within this framework. 
1051|Bounds on the Sample Complexity of Bayesian Learning Using Information Theory and the VC Dimension|In this paper we study a Bayesian or average-case model of concept learning with a twofold goal: to provide more precise characterizations of learning curve (sample complexity) behavior that depend on properties of both the prior distribution over concepts and the sequence of instances seen by the learner, and to smoothly unite in a common framework the popular statistical physics and VC dimension theories of learning curves. To achieve this, we undertake a systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain. This study leads to a new understanding of the sample complexity of learning in several existing models. 1 Introduction  Consider a simple concept learning model in which the learner attempts to infer an unknown  target concept f , chosen from a known concept class F of f0; 1g-valued functions over an instance space X....
1052|Transforming Neural-Net Output Levels to Probability Distributions|(1) The outputs of a typical multi-output classification network do not satisfy the axioms of probability; probabilities should be positive and sum to one. This problem can be solved by treating the trained network as a preprocessor that produces a feature vector that can be further processed, for instance by classical statistical estimation techniques. (2) We present a method for computing the first two moments ofthe probability distribution indicating the range of outputs that are consistent with the input and the training data. It is particularly useful to combine these two ideas: we implement the ideas of section 1 using Parzen windows, where the shape and relative size of each window is computed using the ideas of section 2. This allows us to make contact between important theoretical ideas (e.g. the ensemble formalism) and practical techniques (e.g. back-prop). Our results also shed new light on and generalize the well-known &#034;soft max&#034; scheme. 1 Distribution of Categories in Output Space In many neural-net applications, it is crucial to produce a set of C numbers that serve as estimates of the probability of C mutually exclusive outcomes. For exam-ple, in speech recognition, these numbers represent the probability of C different phonemes; the probabilities of successive segments can be combined using a Hidden Markov Model. Similarly, in an Optical Character Recognition (&#034;OCR&#034;) applica-tion, the numbers represent C possible characters. Probability information for the &#034;best guess &#034; category (and probable runner-up categories) is combined with con-text, cost information, etcetera, to produce recognition of multi-character strings.
1055|Regularization and variable selection via the Elastic Net|Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.
1056|On Model Selection Consistency of Lasso|Sparsity or parsimony of statistical models is crucial for their proper interpretations, as  in sciences and social sciences. Model selection is a commonly used method to find such  models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani,  1996) is now being used as a computationally feasible alternative to model selection.
1057|Leave-One-Out Support Vector Machines|We present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave--one--out error [ Jaakkola and Haussler, 1999 ] proved for Support Vector Machines (SVMs) [ Vapnik, 1995; 1998 ] . The new approach directly minimizes the expression given by the bound in an attempt to minimize leave--one--out error. This gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. As such the algorithm possesses many of the same properties as SVMs. The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless -- the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs. First experiments using the method on benchmark datasets from the UCI repository show results similar to SVMs which have been tuned to have the best choice of parameter.  1 Introduction  Support Vector Machines (SVMs), motivated by minim...
1058|Sparse Principal Component Analysis|Principal component analysis (PCA) is widely used in data processing and dimensionality  reduction. However, PCA su#ers from the fact that each principal component is a linear combination  of all the original variables, thus it is often di#cult to interpret the results. We introduce  a new method called sparse principal component analysis (SPCA) using the lasso (elastic net)  to produce modified principal components with sparse loadings. We show that PCA can be  formulated as a regression-type optimization problem, then sparse loadings are obtained by imposing  the lasso (elastic net) constraint on the regression coe#cients. E#cient algorithms are  proposed to realize SPCA for both regular multivariate data and gene expression arrays. We  also give a new formula to compute the total variance of modified principal components. As  illustrations, SPCA is applied to real and simulated data, and the results are encouraging.
1059|Boosting with early stopping: convergence and consistency|Abstract Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulted estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency, and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting&#039;s greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early stopping strategies under which boosting is shown to be consistent based on iid samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step sizes, as known in practice through the works of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with ffl! 0 stepsize becomes an L1-margin maximizer when left to run to convergence. 1 Introduction In this paper we consider boosting algorithms for classification and regression. These algorithms present one of the major progresses in machine learning. In their original version, the computational aspect is explicitly specified as part of the estimator/algorithm. That is, the empirical minimization of an appropriate loss function is carried out in a greedy fashion, which means that at each step, a basis function that leads to the largest reduction of empirical risk is added into the estimator. This specification distinguishes boosting from other statistical procedures which are defined by an empirical minimization of a loss function without the numerical optimization details.
1060|Empty alternation|Abstract. We introduce the notion of empty alternation by investigating alternating automata which are restricted to empty their storage except for a logarithmically space-bounded tape before making an alternating transition. In particular, we consider the cases when the depth of alternation is bounded by a constant or a polylogarithmic function. In this way we get new characterizations of the classes AC k, SAC k and P using a push-down store and new characterizations of the class T P 2 using Turing tapes. 1
1061|The sources and consequences of embeddedness for the economic performance of organizations: The network effect|In this paper, I attempt to advance the concept of embeddedness beyond the level of a programmatic statement by developing a formulation that specifies how embeddedness and network structure affect economic action. On the basis of existing theory and original ethnographies of 23 apparel firms, I develop a systematic scheme that more fully demarcates the unique features, functions, and sources of embeddedness. From this scheme, I derive a set of refutable implications and test their plausibility, using another data set on the network ties of all better dress apparel firms in the New York apparel economy. Results reveal that embeddedness is an exchange system with unique opportunities relative to markets and that firms organized in networks have higher survival chances than do firms which maintain arm&#039;s-length market relationships. The positive effect of embeddedness reaches a threshold, however, after which point the positive effect reverses itself. Embeddedness 1
1062|Economic Action and Social Structure: The Problem of Embeddedness|How behavior and institutions are affected by social relations is one of the classic questions of social theory. This paper concerns the extent to which economic action is embedded in structures of social relations, in modern industrial society. Although the usual neoclassical accounts provide an &#034;undersocialized &#034; or atomized-actor explanation of such action, reformist economists who attempt to bring social structure back in do so in the &#034;oversocialized &#034; way criticized by Dennis Wrong. Under- and oversocialized accounts are paradoxically similar in their neglect of ongoing structures of social relations, and a sophisticated account of economic action must consider its embeddedness in such structures. The argument is illustrated by a critique of Oliver Williamson&#039;s &#034;markets and hierarchies &#034; research program. INTRODUCTION: THE PROBLEM OF EMBEDDEDNESS How behavior and institutions are affected by social relations is one of the
1063| Network Coding for Large Scale Content Distribution | We propose a new scheme for content distribution of large files that is based on network coding. With network coding, each node of the distribution network is able to generate and transmit encoded blocks of information. The randomization introduced by the coding process eases the scheduling of block propagation, and, thus, makes the distribution more efficient. This is particularly important in large unstructured overlay networks, where the nodes need to make decisions based on local information only. We compare network coding to other schemes that transmit unencoded information (i.e. blocks of the original file) and, also, to schemes in which only the source is allowed to generate and transmit encoded packets. We study the performance of network coding in heterogeneous networks with dynamic node arrival and departure patterns, clustered topologies, and when incentive mechanisms to discourage free-riding are in place. We demonstrate through simulations of scenarios of practical interest that the expected file download time improves by more than 20-30 % with network coding compared to coding at the server only and, by more than 2-3 times compared to sending unencoded information. Moreover, we show that network coding improves the robustness of the system and is able to smoothly handle extreme situations where the server and nodes departure the system.
1064|Incentives Build Robustness in BitTorrent|The BitTorrent file distribution system uses tit-for-tat as a method of seeking pareto efficiency. It achieves a higher level of robustness and resource utilization than any currently known cooperative technique. We explain what BitTorrent does, and how economic methods are used to achieve that goal.
1065|Free Riding on Gnutella|this paper, Gnutella is no exception to this finding, and an experimental study of its user patterns shows indeed that free riding is the norm rather than the exception. If distributed systems such as Gnutella rely on voluntary cooperation, rampant free riding may eventually render them useless, as few individuals will contribute anything that is new and high quality. Thus, the current debate over copyright might become a non-issue when compared to the possible collapse of such systems. This collapse can happen because of two factors, the tragedy of the digital commons, and increased system vulnerability, which we now discuss
1066|Modeling and performance analysis of bittorrentlike peer-to-peer networks|In this paper, we develop simple models to study the performance of BitTorrent, a second generation peerto-peer (P2P) application. We first present a simple fluid model and study the scalability, performance and efficiency of such a file-sharing mechanism. We then consider the built-in incentive mechanism of Bit-Torrent and study its effect on network performance. We also provide numerical results based on both simulations and real traces obtained from the Internet. 1
1067|SplitStream: High-Bandwidth Multicast in Cooperative Environments|In tree-based multicast systems, a relatively small number of interior nodes carry the load of forwarding multicast messages. This works well when the interior nodes are highly available, d d cated infrastructure routers but it poses a problem for application-level multicast in peer-to-peer systems. SplitStreamadV esses this problem by striping the content across a forest of interior-nodno# sjoint multicast trees that d stributes the forward ng load among all participating peers. For example, it is possible to construct efficient SplitStream forests in which each peer contributes only as much forwarding bandH d th as it receives. Furthermore, with appropriate content encod ngs, SplitStream is highly robust to failures because a nod e fai ure causes the oss of a single stripe on average. We present thed#&#039; gnand implementation of SplitStream and show experimental results obtained on an Internet testbed and via large-scale network simulation. The results show that SplitStreamd istributes the forward ing load among all peers and can accommod&#039;9 peers with different band0 d capacities while imposing low overhead for forest constructionand maintenance. 
1068|Practical Network Coding|We propose a distributed scheme for practical network coding that obviates  the need for centralized knowledge of the graph topology, the encoding functions,  and the decoding functions, and furthermore obviates the need for information to  be communicated synchronously through the network. The result is a practical  system for network coding that is robust to random packet loss and delay as well  as robust to any changes in the network topology or capacity due to joins, leaves,  node or link failures, congestion, and so on. We simulate such a practical network  coding system using the network topologies of several commercial Internet Service  Providers, and demonstrate that it can achieve close to the theoretically optimal  performance.
1069|Multiple Description Coding: Compression Meets the Network|This article focuses on the compressed representations of the pictures
1070|Bullet: High Bandwidth Data Dissemination Using an Overlay Mesh|In recent years, overlay networks have become an effective alternative to IP multicast for efficient point to multipoint communication across the Internet. Typically, nodes self-organize with the goal of forming an efficient overlay tree, one that meets performance targets without placing undue burden on the underlying network. In this paper, we target high-bandwidth data distribution from a single source to a large number of receivers. Applications include large-file transfers and real-time multimedia streaming. For these applications, we argue that an overlay mesh, rather than a tree, can deliver fundamentally higher bandwidth and reliability relative to typical tree structures. This paper presents Bullet, a scalable and distributed algorithm that enables nodes spread across the Internet to self-organize into a high bandwidth overlay mesh. We construct Bullet around the insight that data should be distributed in a disjoint manner to strategic points in the network. Individual Bullet receivers are then responsible for locating and retrieving the data from multiple points in parallel. Key contributions of this work include: i) an algorithm that sends data to di#erent points in the overlay such that any data object is equally likely to appear at any node, ii) a scalable and decentralized algorithm that allows nodes to locate and recover missing data items, and iii) a complete implementation and evaluation of Bullet running across the Internet and in a large-scale emulation environment reveals up to a factor two bandwidth improvements under a variety of circumstances. In addition, we find that, relative to tree-based solutions, Bullet reduces the need to perform expensive bandwidth probing.
1071|Distributing Streaming Media Content Using Cooperative Networking|In this paper, we discuss the problem of distributing streaming media content, both live and on-demand, to a large number of hosts in a scalable way. Our work is set in the context of the traditional client-server framework. Specifically, we consider the problem that arises when the server is overwhelmed by the volume of requests from its clients. As a solution, we propose Cooperative Networking (CoopNet), where clients cooperate to distribute content, thereby alleviating the load on the server. We discuss the proposed solution in some detail, pointing out the interesting research issues that arise, and present a preliminary evaluation using traces gathered at a busy news site during the flash crowd that occurred on September 11, 2001.
1072|The benefits of coding over routing in a randomized setting|Abstract — We present a novel randomized coding approach for robust, distributed transmission and compression of information in networks. We give a lower bound on the success probability of a random network code, based on the form of transfer matrix determinant polynomials, that is tighter than the Schwartz-Zippel bound for general polynomials of the same total degree. The corresponding upper bound on failure probability is on the order of the inverse of the size of the finite field, showing that it can be made arbitrarily small by coding in a sufficiently large finite field, and that it decreases exponentially with the number of codeword bits. We demonstrate the advantage of randomized coding over routing for distributed transmission in rectangular grid networks by giving, in terms of the relative grid locations of a source-receiver pair, an upper bound on routing success probability that is exceeded by a corresponding lower bound on coding success probability for sufficiently large finite fields. We also show that our lower bound on the success probability of randomized coding holds for linearly correlated sources. This implies that randomized coding effectively compresses linearly correlated information to the capacity of any network cut in a feasible connection problem. I.
1073|Dissecting BitTorrent: Five Months In Torrent&#039;s Lifetime|Popular content such as software updates is requested by a  large number of users. Traditionally, to satisfy a large number of requests,  lager server farms or mirroring are used, both of which are expensive. An  inexpensive alternative are peer-to-peer based replication systems, where  users who retrieve the file, act simultaneously as clients and servers. In  this paper, we study BitTorrent, a new and already very popular peerto  -peer application that allows distribution of very large contents to a  large set of hosts. Our analysis of BitTorrent is based on measurements  collected on a five months long period that involved thousands of peers.
1074|Informed content delivery across adaptive overlay networks|Abstract—Overlay networks have emerged as a powerful and highly flexible method for delivering content. We study how to optimize throughput of large transfers across richly connected, adaptive overlay networks, focusing on the potential of collaborative transfers between peers to supplement ongoing downloads. First, we make the case for an erasure-resilient encoding of the content. Using the digital fountain encoding approach, end hosts can efficiently reconstruct the original content of size from a subset of any symbols drawn from a large universe of encoding symbols. Such an approach affords reliability and a substantial degree of application-level flexibility, as it seamlessly accommodates connection migration and parallel transfers while providing resilience to packet loss. However, since the sets of encoding symbols acquired by peers during downloads may overlap substantially, care must be taken to enable them to collaborate effectively. Our main contribution is a collection of useful algorithmic tools for efficient summarization and approximate reconciliation of sets of symbols between pairs of collaborating peers, all of which keep message complexity and computation to a minimum. Through simulations and experiments on a prototype implementation, we demonstrate the performance benefits of our informed content-delivery mechanisms and how they complement existing overlay network architectures. Index Terms—Bloom filter, content delivery, digital fountain, erasure code, min-wise sketch, overlay, peer-to-peer, reconciliation. I.
1075|Improved Steiner Tree Approximation in Graphs|The Steiner tree problem in weighted graphs seeks a minimum weight connected subgraph  containing a given subset of the vertices (terminals). We present a new polynomial-time  heuristic with an approximation ratio approaching 1 +  2  1:55, which improves upon  the previously best-known approximation algorithm of [10] with performance ratio  1:59.
1076|Accessing multiple mirror sites in parallel: using tornado codes tospeed up downloads |Abstracr- Mirror sites enable client requests to be serviced by any of a number of servers, reducing load at individual servers and dispersing network load. Typically, a client requests service from a single mirror site. We consider enabling a client to access a file from multiple mirror sites in parallel to speed up the download. To eliminate complex client-server negotiations that a straightforward implementation of this approach would require, we develop a feedback-free protocol based on erasure codes. We demonstrate that a protocol using fast Tornado codes can deliver dramatic speedups at the expense of transmitting a moderate number of additional packets into the network. Our scalable solution extends naturally to allow multiple clients to access data from multiple mirror sites simultaneously. Our approach applies naturally to wireless networks and satellite networks as well. I.
1077|On-the-fly verification of rateless erasure codes for efficient content distribution|Abstract — The quality of peer-to-peer content distribution can suffer when malicious participants intentionally corrupt content. Some systems using simple block-by-block downloading can verify blocks with traditional cryptographic signatures and hashes, but these techniques do not apply well to more elegant systems that use rateless erasure codes for efficient multicast transfers. This paper presents a practical scheme, based on homomorphic hashing, that enables a downloader to perform on-the-fly verification of erasure-encoded blocks. I.
1078|Slurpie: A Cooperative Bulk Data Transfer Protocol|We present Slurpie: a peer-to-peer protocol for bulk data transfer. Slurpie is specifically designed to reduce client download times for large, popular files, and to reduce load on servers that serve these files. Slurpie employs a novel adaptive downloading strategy to increase client performance, and employs a randomized backoff strategy to precisely control load on the server. We describe a full implementation of the Slurpie protocol, and present results from both controlled localarea and wide-area testbeds. Our results show that Slurpie clients improve performance as the size of the network increases, and the server is completely insulated from large flash crowds entering the Slurpie network.
1079|Packing Steiner trees |  The Steiner packing problem is to find the maximum number of edge-disjoint subgraphs of a given graph G that connect a given set of required points S. This problem is motivated by practical applications in VLSI-layout and broadcasting, as well as theoretical reasons. In this paper, we study this problem and present an algorithm with an asymptotic approximation factor of |S|/4. This gives a sufficient condition for the existence of k edge-disjoint Steiner trees in a graph in terms of the edge-connectivity of the graph. We will show that this condition is the best possible if the number of terminals is 3. At the end, we consider the fractional version of this problem, and observe that it can be reduced to the minimum Steiner tree problem via the ellipsoid algorithm. 
1080|Dynamic parallel access to replicated content in the internet|Abstract—Popular content is frequently replicated in multiple servers or caches in the Internet to offload origin servers and improve end-user experience. However, choosing the best server is a non-trivial task and a bad choice may provide poor end user experience. In contrast to retrieving a file from a single server, we propose a parallel-access scheme where end users access multiple servers at the same time, fetching different portions of that file from different servers and reassembling them locally. The amount of data retrieved from a particular server depends on the resources available at that server or along the path from the user to the server. Faster servers will deliver bigger portions of a file while slower servers will deliver smaller portions. If the available resources at a server or along the path change during the download of a file, a dynamic parallel-access will automatically shift the load from congested locations to less loaded parts (server and links) of the Internet. The end result is that users experience significant speedups and very consistent response times. Moreover, there is no need for complicated server selection algorithms and load is dynamically shared among all servers. The dynamic parallel-access scheme presented in this paper does not require any modifications to servers or content and can be easily included in browsers, peer-to-peer applications or content distribution networks to speed up delivery of popular content. I.
1081|Rateless Codes and Big Downloads|This paper presents a novel algorithm for downloading big files from multiple sources in peer-to-peer networks. The algorithm is simple, but offers several compelling properties. It ensures low handshaking overhead between peers that download files (or parts of a files) from each other. It is computationally efficient, with cost linear in the amount of data transfered. Most importantly, when nodes leave the network in the middle of uploads, the algorithm minimizes the duplicate information shared by nodes with truncated downloads. Thus, any two peers with partial knowledge of a given file can almost always fully benefit from each other&#039;s knowledge. Our algorithm is made possible by the recent introduction of linear-time, rateless erasure codes.
1082|Multicast with Network Coding in Application-Layer Overlay Networks|All of the advantages of application-layer overlay networks arise from two fundamental properties: (1) The network nodes in an overlay network, as opposed to lower-layer network elements such as routers and switches, are end systems and have capabilities far beyond basic operations of storing and forwarding; and (2) The overlay topology, residing above a densely connected IP-layer wide-area network, can be constructed and manipulated to suit one&#039;s purposes. In this paper, we seek to significantly...
1083|A Framework for Architecting Peer-to-Peer Receiver-driven Overlays|This paper presents a simple and scalable framework for architecting peer-to-peer overlays called Peer-to-peer Receiverdriven Overlay (or PRO). PRO is designed for non-interactive streaming applications and its primary design goal is to maximize delivered bandwidth (and thus delivered quality) to peers with heterogeneous and asymmetric bandwidth. To achieve this goal, PRO adopts a receiver-driven approach where each receiver (or participating peer) (i) independently discovers other peers in the overlay through gossiping, and (ii) selfishly determines the best subset of parent peers through which to connect to the overlay to maximize its own delivered bandwidth. Participating peers form an unstructured overlay which is inherently robust to high churn rate. Furthermore, each receiver leverages congestion controlled bandwidth from its parents as implicit signal to detect and react to long-term changes in network or overlay condition without any explicit coordination with other participating peers. Independent parent selection by individual peers dynamically converge to an efficient overlay structure.
1084|Building scalable and robust peer-to-peer overlay networks for broadcasting using network coding|We propose a scheme for building peer-to-peer overlay networks for broadcasting using network coding. The scheme addresses many practical issues such as scalability, robustness, constraints on bandwidth, and locality of decisions. We analyze the system theoretically and prove near optimal bounds on the parameters defining robustness and scalability. As a result we show that the effects of failures are contained locally, allowing the network to grow exponentially with server load. We also argue that adversarial failures are no more harmful than random failures. 1
1085|Wide-Area Traffic: The Failure of Poisson Modeling|Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remotelogin and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into “connection bursts,” the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of widearea traffic.  
1086|On the Self-similar Nature of Ethernet Traffic (Extended Version)  (1994) | We demonstrate that Ethernet LAN traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal-like behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based networks, and that aggregating streams of such traffic typically intensifies the self-similarity (“burstiness”) instead of smoothing it. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We also present traffic models based on self-similar stochastic processes that provide simple, accurate, and realistic descriptions of traffic scenarios expected during B-ISDN deployment. 
1087|Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism|This paper considers the support of real-time applications in an
1088|The synchronization of periodic routing messages|Abstract — The paper considers a network with many apparently-independent periodic processes and discusses one method by which these processes can inadvertent Iy become synchronized. In particular, we study the synchronization of periodic routing messages, and offer guidelines on how to avoid inadvertent synchronization. Using simulations and analysis, we study the process of synchronization and show that the transition from unsynchronized to synchronized traffic is not one of gradual degradation but is instead a very abrupt ‘phase transition’: in general, the addition of a single router will convert a completely unsynchronized traffic stream into a completely synchronized one. We show that synchronization can be avoided by the addition of randomization to the tra~c sources and quantify how much randomization is necessary. In addition, we argue that the inadvertent synchronization of periodic processes is likely to become an increasing problem in computer networks.
1089|On Traffic Phase Effects in Packet-Switched Gateways|this paper we define the notion of traffic phase in a packet-switched network and describe how phase differences between competing traffic streams can be the dominant factor in relative throughput. Drop Tail gateways in a TCP/IP network with strongly periodic traffic can result in systematic discrimination against some connections. We demonstrate this
1090|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
1091|Packet Trains: Measurements and a New Model for Computer Network Traffic|Traffic measurements on a ring local area computer network
at the Massachusetts Institute of Technology are presented. The analysis of the arrival pattern shows that the arrival processes are neither Poisson nor compound Poisson. An alternative model called “packet train” is proposed.

In the train model, the traffic on the network consists of a number of packet streams between various pairs of nodes tohne network. Each node-pair stream (or node-pair process, as we call them) consists of a number of trains. Each train consists of a numbero f packets (or cars) going in either direction (from node A to B or from node B to A). The
intercar gap is large (compared to packet transmission time) and random. The intertrain time is even larger. The Poisson and the compound Poisson arrivals are shown to sbpee cial cases of the train arrival model. Another important observation is that the packet arrivals exhibit a
“source locality.” If a packet is seen on the network going from A to B, the probability of the next packet going from A to B or from B to A is very high. Implications of the train arrivals and of source locality on the design
of bridges, gateways, and reservation protocols are discussed. A number of open problems requiring development of analysis techniques for systems with train arrival processes are also described.
1092|Telnet Protocol Specification|The Internet Protocol (IP) [1] is used for host-to-host datagram service in a system of interconnected networks called the Catenet [2]. The network connecting devices are called Gateways. These gateways communicate between themselves for control purposes
1093|Local Area Network Traffic Characteristics, with Implications for Broadband Network Congestion Management |This paper examines the phenomenon of congestion in order to better understand the congestion management techniques that will be needed in high-speed, cell-based networks. The first step of this study is to use high time-resolution local area network (LAN) traffic data to explore the nature of LAN traffic variability. Then, we use the data for a trace-driven simulation of a connectionless service that provides LAN interconnection. The simulation allows us to characterize what congestion might look like in a high-speed, cell-based network. The most
1094|An Empirical Workload Model for Driving Wide-Area TCP/IP Network Simulations|We present an artificial workload model of wide-area internetwork traffic. The model can be used to drive simulation experiments of communication protocols and flow and congestion control experiments. The model is based on analysis of wide-area TCP/IP traffic collected from  one industrial and two academic networks. The artificial workload model uses both detailed knowledge and measured characteristics of the user application programs responsible for the traffic. Observations drawn from our measurements contradict some commonly held beliefs regarding wide-area TCP/IP network traffic.
1095|Statistical analysis of CCSN/SS7 traffic data from working CCS subnetworks|In this paper we report on an ongoing statistical analysis of actual CCSN traffic data. The data consist of approximately 170 million signaling messages collected from a variety of different working CCS subnetworks. The key findings from our analysis concern: (1) the characteristics of both the telephone call arrival process and the signaling message arrival process, (2) the tail behavior of the call holding time distribution, and (3) the observed performance of the CCSN with respect to a variety of performance and reliability measurements. 1.
