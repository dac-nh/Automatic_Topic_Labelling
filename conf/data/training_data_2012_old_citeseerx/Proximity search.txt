ID|Title|Summary
1|Proximity search in databases|An information retrieval (IR) engine can rank documents based on textual proximityofkeywords within each document. In this paper we apply this notion to search across an entire database for objects that are \near &amp;quot; other relevant objects. Proximity search enables simple \focusing &amp;quot; queries based on general relationships among objects, helpful for interactive query sessions. We view the database as a graph, with data in vertices (objects) and relationships indicated by edges. Proximity is dened based on shortest paths between objects. We have implemented a prototype search engine that uses this model to enable keyword searches over databases, and we have found it very e ective for quickly nding relevant information. Computing the distance between objects in a graph stored on disk can be very expensive. Hence, we show how to build compact indexes that allow us to quickly nd the distance between objects at search time. Experiments show that our algorithms are e-cient and scale well. 1
2|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
3|Lore: A database management system for semistructured data|Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources. 
4|Faster Shortest-Path Algorithms for Planar Graphs|We give a linear-time algorithm for single-source shortest paths in planar graphs with nonnegative edge-lengths. Our algorithm also yields a linear-time algorithm for maximum flow in a planar graph with the source and sink on the same face. The previous best algorithms for these problems  required\Omega\Gamma  n  p  log n) time where n is the number of nodes in the input graph.  For the case where negative edge-lengths are allowed, we give an algorithm requiring O(n  4=3  log nL) time, where L is the absolute value of the most negative length. Previous algorithms for shortest paths with negative edge-lengths required \Omega\Gamma n  3=2  ) time. Our shortest-path algorithm yields an O(n  4=3  log n)-time algorithm for finding a perfect matching in a planar bipartite graph. A similar improvement is obtained for maximum flow in a directed planar graph.  
5|Improved Algorithms and Data Structures for Solving Graph Problems in External Memory|Recently, the study of I/O-efficient algorithms has moved beyond fundamental problems of sorting and permuting and into wider areas such as computational geometry and graph algorithms. With this expansion has come a need for new algorithmic techniques and data structures. In this paper, we present I/O-efficient analogues of wellknown data structures that we show to be useful for obtaining simpler and improved algorithms for several graph problems. Our results include improved algorithms for minimum spanning trees, breadth-first and depth-first search, and single-source shortest paths. The descriptions of these algorithms are greatly simplified by their use of well-defined I/O-efficient data structures with good amortised performance bounds. We expect that I/O-efficient data structures such as these will be a useful tool for the design of I/O-efficient algorithms.  1 Introduction The design of I/O-efficient algorithms has received increasingly greater attention in recent years. This ha...
6|Shortest Paths in Digraphs of Small Treewidth. Part I: Sequential Algorithms|We consider the problem of preprocessing an n-vertex digraph with real edge weights so  that subsequent queries for the shortest path or distance between any two vertices can be  efficiently answered. We give algorithms that depend on the treewidth of the input graph.  When the treewidth is a constant, our algorithms can answer distance queries in O(ff(n))  time after O(n) preprocessing. This improves upon previously known results for the same  problem. We also give a dynamic algorithm which, after a change in an edge weight, updates  the data structure in time O(n  fi  ), for any constant 0 ! fi ! 1. Furthermore, an algorithm of  independent interest is given: computing a shortest path tree, or finding a negative cycle in  linear time.
7|DTL’s DataSpot: Database exploration using plain language|DTL’s DataSpot is a database publishing tool that enables non-technical end users to explore a database using free-form plain language queries combined with hypertext navigation. DataSpot is based on a novel representation of data in the form of a schema-less semi-structured graph called a hyperbase. The DataSpot Publisher takes one or more possibly heterogeneous databases, predefined knowledge banks such as a thesaurus, and user-defined associations, and creates the hyperbase. The DataSpot Search Server performs searches and navigation against the hyperbase, returning answers to the user either in HTML pages or through an object API. The DataSpot product has been successfilly deployed in diverse application areas including electronic catalogs, yellow pages, classified ads, help desks and finance. I...______........._............................... ataSpot Hyperbas
8|Ontology-based proximity search |This paper presents our developed general open source for ontology-based information retrieval to answer queries that involve named entities with their ontological features, namely, aliases, classes, and identifiers. We propose a novel approach for semantic search engines that exploit the ontology features of named entities in proximity search and develop an algorithm for computing dynamic distances between named entities and
9|Term proximity scoring for keyword-based retrieval systems|Abstract. This paper suggests the use of proximity measurement in combination with the Okapi probabilistic model. First, using the Okapi system, our investigation was carried out in a distributed retrieval framework to calculate the same relevance score as that achieved by a single centralized index. Second, by applying a term-proximity scoring heuristic to the top documents returned by a keyword-based system, our aim is to enhance retrieval performance. Our experiments were conducted using the TREC8, TREC9 and TREC10 test collections, and show that the suggested approach is stable and generally tends to improve retrieval effectiveness especially at the top documents retrieved. 1
10|An exploration of proximity measures in information retrieval|In most existing retrieval models, documents are scored primarily based on various kinds of term statistics such as within-document frequencies, inverse document frequencies, and document lengths. Intuitively, the proximity of matched query terms in a document can also be exploited to promote scores of documents in which the matched query terms are close to each other. Such a proximity heuristic, however, has been largely under-explored in the literature; it is unclear how we can model proximity and incorporate a proximity measure into an existing retrieval model. In this paper, we systematically explore the query term proximity heuristic. Specifically, we propose and study the effectiveness of five different proximity measures, each modeling proximity from a different perspective. We then design two heuristic constraints and use them to guide us in incorporating the proposed proximity measures into an existing retrieval model. Experiments on five standard TREC test collections show that one of the proposed proximity measures is indeed highly correlated with document relevance, and by incorporating it into the KL-divergence language model and the Okapi BM25 model, we can significantly improve retrieval performance.
11|Term Proximity Scoring for Ad-Hoc Retrieval on Very Large Text Collections |this paper, we chose to use our own method instead of theirs because it exhibited slightly better retrieval e#ectiveness in almost all of our experiments. Suppose a user submits the query Q = {T1 , . . . , Tn}. Then our implementation of BM25 fetches the posting lists for all query terms from the index and arranges them in a priority queue. It then starts consuming postings from all posting lists, one posting at a time, in ascending order, to find matching documents and simultaneously compute the relevance scores of all matching documents found (documentat -a-time approach). If an index with full positional information is used, Term proximity can be integrated into this process without much e#ort. With every query term, we associate an accumulator that contains that term&#039;s proximity score within the current document. Whenever the search system encounters a posting that belongs to the query term T j , it looks at the previous posting, belonging to the query term Tk , and determines the distance (number of postings) between the current posting and the previous one. If T j #= Tk , then both terms&#039; accumulators are incremented:  acc(T j ) := acc(T j ) + wT k  (dist(T j + Tk ))    acc(Tk ) := acc(Tk ) + wT j  (dist(T j + Tk ))    where wT i is T i &#039;s IDF weight (cf. equation 1). For T j = Tk , the accumulators remain unchanged. When the end of the current document is reached, the document&#039;s score is computed, and all proximity accumulators are reset to zero. The score of a document D is:  ScoreBM25TP (D)  = ScoreBM25 (D) + X  T#Q  min{1, wT }   acc(T )  (k1 + 1)  acc(T ) +K  where k1 and K are are the same as in the original Okapi equation. The di#erence to the strategy followed by Rasolofo and Savoy is that in our approach only neighboring query term&#039;s can a#ect each other&#039;...
12|Proximity-based document representation for named entity retrieval|One aspect in which retrieving named entities is different from retrieving documents is that the items to be retrieved – persons, locations, organizations – are only indirectly described by documents throughout the collection. Much work has been dedicated to finding references to named entities, in particular to the problems of named entity extraction and disambiguation. However, just as important for retrieval performance is how these snippets of text are combined to build named entity representations. We focus on the TREC expert search task where the goal is to identify people who are knowledgeable on a specific topic. Existing language modeling techniques for expert finding assume that terms and person entities are conditionally independent given a document. We present theoretical and experimental evidence that this simplifying assumption ignores information on how named entities relate to document content. To address this issue, we propose a new document representation which emphasizes text in proximity to entities and thus incorporates sequential information implicit in text. Our experiments demonstrate that the proposed model significantly improves retrieval performance. The main contribution of this work is an effective formal method for explicitly modeling the dependency between the named entities and terms which appear in a document.
13|Efficiency vs. Effectiveness in Terabyte-Scale Information Retrieval |We describe indexing and retrieval techniques that are suited to perform terabyte-scale information retrieval tasks on a standard desktop PC. Starting from an Okapi-BM25-based default baseline retrieval function, we explore both sides of the effectiveness spectrum. On one side, we show how term proximity can be integrated into the scoring function in order to improve the search results. On the other side, we show how index pruning can be employed to increase retrieval efficiency -- at the cost of reduced retrieval effectiveness. We show that
14|Keyword proximity search in XML trees|Abstract—Recent works have shown the benefits of keyword proximity search in querying XML documents in addition to text documents. For example, given query keywords over Shakespeare’s plays in XML, the user might be interested in knowing how the keywords cooccur. In this paper, we focus on XML trees and define XML keyword proximity queries to return the (possibly heterogeneous) set of minimum connecting trees (MCTs) of the matches to the individual keywords in the query. We consider efficiently executing keyword proximity queries on labeled trees (XML) in various settings: 1) when the XML database has been preprocessed and 2) when no indices are available on the XML database. We perform a detailed experimental evaluation to study the benefits of our approach and show that our algorithms considerably outperform prior algorithms and other applicable approaches.
15|The Anatomy of a Large-Scale Hypertextual Web Search Engine|In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/

To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.

Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
16|DBXplorer: A system for keyword-based search over relational databases|Internet search engines have popularized the keywordbased search paradigm. While traditional database management systems offer powerful query languages, they do not allow keyword-based search. In this paper, we discuss DBXplorer, a system that enables keywordbased search in relational databases. DBXplorer has been implemented using a commercial relational database and web server and allows users to interact via a browser front-end. We outline the challenges and discuss the implementation of our system including results of extensive experimental evaluation. 1.
17|Efficient Keyword Search for Smallest LCAs in XML Databases|Keyword search is a proven, user-friendly way to query HTML documents in the World Wide Web. We propose keyword search in XML documents, modeled as labeled trees, and describe corre-sponding efficient algorithms. The proposed keyword search re-
18|On K-Word Proximity Search|this paper, we propose two algorithms for finding documents in which all given keywords appear in neighboring places. One is based on plane-sweep algorithm and the other is based on divide-and-conquer approach. Both algorithms run in O(n log n) time where n is the number of occurrences of given keywords. We implement above algorithms and verify their effectiveness. 1 Introduction
19|Suffix arrays: A new method for on-line string searches|A new and conceptually simple data structure, called a suffix array, for on-line string searches is intro-duced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, &#034;Is W a substring of A?&#034; to be answered in time O(P + log N), where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in O(N) time in the worst case, versus O(N log N) time for suffix arrays. However, we give an augmented algorithm that, regardless of the alphabet size, constructs suffix arrays in O(N) expected time, albeit with lesser space efficiency. We believe that suffix arrays will prove to be better in practice than suffix trees for many applications.  
20|Keyword Proximity Search on XML Graphs|XKeyword provides efficient keyword proximity queries on large XML graph databases. A query is simply a list of keywords and does not require any schema or query language knowledge for its formulation. XKeyword is built on a relational database...
21|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
22|Relational Databases for Querying XML Documents:  Limitations and Opportunities|XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational 
23|Storing semistructured data with STORED |Systems for managing and querying semistructured-data sources often store data in proprietary object repositories or in a tagged-text format. We describe a technique that can use relational database management systems to store and manage semistructured data. Our technique relies on a mapping between the semistructured data model and the relational data model, expressed in a query language called STORED. When a semistrcutured data instance is given, a STORED mapping can be generated automatically using data-mining techniques. We are interested in applying STORED to XML data, which is an instance of semistructured data. We show how a document-type-descriptor (DTD), when present, can be exploited to further improve performance.
24|XIRQL: A Query Language for Information Retrieval in XML Documents|Based on the document-centric view of XML, we present the query language XIRQL. Current proposals for XML query languages lack most IR-related features, which are weighting and ranking, relevance-oriented search, datatypes with vague predicates, and semantic relativism. XIRQL integrates these features by using ideas from logic-based probabilistic IR models, in combination with concepts from the database area. For processing XIRQL queries, a path algebra is presented, that also serves as a starting point for query optimization.
25|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
26|From XML schema to relations: A cost-based approach to XML storage|bohannon,juliana,prasan,simeon¡ XML has become an important medium for data representation, particularly when that data is exchanged over or browsed on the Internet. As the volume of XML data increases, there is a growing interest in storing XML in relational databases so that the well-developed features of these systems (e.g., concurrency control, crash recovery, query processors) can be re-used. However, given the wide variety of XML applications and the mismatch between XML’s nested-tree structure and the flat tuples of the relational model, storing XML documents in relational databases presents interesting challenges. LegoDB is a cost-based XML-to-relational mapping engine that addresses this problem. It explores a space of possible mappings and selects the best mapping for a given application (defined by an XML Schema, XML data statistics, and an XML query workload). LegoDB leverages existing XML and relational technologies: it represents the target application using XML standards and constructs the space of configurations using XML-specific operations, and it uses a traditional relational optimizer to obtain accurate cost estimates of the derived configurations. In this paper, we describe the LegoDB mapping engine and provide experimental results that demonstrate the effectiveness of this approach. 1
27|Efficient Relational Storage and Retrieval of XML Documents|In this paper, we present a data and an execution model that allow for efficient storage and retrieval of XML documents in a relational database. The data model is strictly based on the notion of binary associations: by decomposing XML documents into small, exible and semantically homogeneous units we are able to exploit the performance potential of vertical fragmentation. Moreover, our approach provides clear and intuitive semantics, which facilitates the definition of a declarative query algebra. Our experimental results with large collections of XML documents demonstrate the effectiveness of the techniques proposed.
28|Tree Pattern Relaxation|Tree patterns are fundamental to querying tree-structured  data like XML. Because of the heterogeneity of XML data, it is often  more appropriate to permit approximate query matching and return  ranked answers, in the spirit of Information Retrieval, than to return only  exact answers. In this paper, we study the problem of approximate XML  query matching, based on tree pattern relaxations, and devise efficient  algorithms to evaluate relaxed tree patterns. We consider weighted tree  patterns, where exact and relaxed weights, associated with nodes and  edges of the tree pattern, are used to compute the scores of query answers. We are
29|Agora: Living with XML and Relational|Introduction  There has been a significant body of research in the last fifteen years dedicated to integration of data from various repositories, exhibiting heterogeneous formats, and sometimes access restrictions; for a survey of such systems see, for example, [12]. The main technical issues to be addressed in a mediation system are: how to semantically unify heterogeneous data formats and schemas, and how to use query processing capabilities of participant data sites and that of the mediator in order to answer a particular query.  Systems like the Information Manifold, and Garlic from IBM have chosen the relational and respectively the object-oriented model as the integration model. Given the popularity of XML as a data description format, more and more DBMS manufacturers have added to their systems the capability to export relational or object-oriented data to an XML format; other data formats (flat data files, regular HTML, PowerPoint presentations, annotated text) are also
30|Efficient Group of Permutants for Proximity Searching |Abstract. Modeling proximity searching problems in a metric space allows one to approach many problems in different areas, e.g. pattern recognition, multimedia search, or clustering. Recently there was proposed the permutation based approach, a novel technique that is unbeatable in practice but difficult to compress. In this article we introduce an improvement on that metric space search data structure. Our technique shows that we can compress the permutation based algorithm without loosing precision. We show experimentally that our technique is competitive with the original idea and improves it up to 46 % in real databases. 1
31|Searching in metric spaces|The problem of searching the elements of a set that are close to a given query element under some similarity criterion has a vast number of applications in many branches of computer science, from pattern recognition to textual and multimedia information retrieval. We are interested in the rather general case where the similarity criterion defines a metric space, instead of the more restricted case of a vector space. Many solutions have been proposed in different areas, in many cases without cross-knowledge. Because of this, the same ideas have been reconceived several times, and very different presentations have been given for the same approaches. We present some basic results that explain the intrinsic difficulty of the search problem. This includes a quantitative definition of the elusive concept of “intrinsic dimensionality. ” We also present a unified  
32|Comparing top k lists|Motivated by several applications, we introduce various distance measures between “top k lists.” Some of these distance measures are metrics, while others are not. For each of these latter distance measures, we show that they are “almost ” a metric in the following two seemingly unrelated aspects: (i) they satisfy a relaxed version of the polygonal (hence, triangle) inequality, and (ii) there is a metric with positive constant multiples that bound our measure above and below. This is not a coincidence—we show that these two notions of almost being a metric are formally identical. Based on the second notion, we define two distance measures to be equivalent if they are bounded above and below by constant multiples of each other. We thereby identify a large and robust equivalence class of distance measures. Besides the applications to the task of identifying good notions of (dis-)similarity between two top k lists, our results imply polynomial-time constant-factor approximation algorithms for the rank aggregation problem [DKNS01] with respect to a large class of distance measures. To appear in SIAM J. on Discrete Mathematics. Extended abstract to appear in 2003 ACM-SIAM Symposium on Discrete Algorithms (SODA ’03).
33|Index-driven similarity search in metric spaces|Similarity search is a very important operation in multimedia databases and other database applications involving complex objects, and involves finding objects in a data set S similar to a query object q, based on some similarity measure. In this article, we focus on methods for similarity search that make the general assumption that similarity is represented with a distance metric d. Existing methods for handling similarity search in this setting typically fall into one of two classes. The first directly indexes the objects based on distances (distance-based indexing), while the second is based on mapping to a vector space (mapping-based approach). The main part of this article is dedicated to a survey of distance-based indexing methods, but we also briefly outline how search occurs in mapping-based methods. We also present a general framework for performing search based on distances, and present algorithms for common types of queries that operate on an arbitrary “search hierarchy. ” These algorithms can be applied on each of the methods presented, provided a suitable search hierarchy is defined.
34|Effective Proximity Retrieval by Ordering Permutations|We introduce a new probabilistic proximity search algorithm for range and K-nearest neighbor (K-NN) searching in both coordinate and metric spaces. Although there exist solutions for these problems, they boil down to a linear scan when the space is intrinsically high-dimensional, as is the case in many pattern recognition tasks. This, for example, renders the K-NN approach to classification rather slow in large databases. Our novel idea is to predict closeness between elements according to how they order their distances towards a distinguished set of anchor objects. Each element in the space sorts the anchor objects from closest to farthest to it, and the similarity between orders turns out to be an excellent predictor of the closeness between the corresponding elements. We present extensive experiments comparing our method against state-of-the-art exact and approximate techniques, both in synthetic and real, metric and non-metric databases, measuring both CPU time and distance computations. The experiments demonstrate that our technique almost always improves upon the performance of alternative techniques, in some cases by a wide margin.  
35|Compact and Efficient Permutations for Proximity Searching |Abstract. Proximity searching consists in retrieving the most similar objects to a given query. This kind of searching is a basic tool in many fields of artificial intelligence, because it can be used as a search engine to solve problems like kNN searching. A common technique to solve proximity queries is to use an index. In this paper, we show a variant of the permutation based index, which, in his original version, has a great predicting power about which are the objects worth to compare with the query (avoiding the exhaustive comparison). We have noted that when two permutants are close, they can produce small differences in the order in which objects are revised, which could be responsible of finding the true answer or missing it. In this paper we pretend to mitigate this effect. As a matter of fact, our technique allows us both to reduce the index size and to improve the query cost up to 30%. 1
36|Proximity Searching in High Dimensional Spaces with a Proximity Preserving Order ? |Abstract. Kernel based methods (such as k-nearest neighbors classifiers) for AI tasks translate the classification problem into a proximity search problem, in a space that is usually very high dimensional. Unfortunately, no proximity search algorithm does well in high dimensions. An alternative to overcome this problem is the use of approximate and probabilistic algorithms, which trade time for accuracy. In this paper we present a new probabilistic proximity search algorithm. Its main idea is to order a set of samples based on their distance to each element. It turns out that the closeness between the order produced by an element and that produced by the query is an excellent predictor of the relevance of the element to answer the query. The performance of our method is unparalleled. For example, for a full 128-dimensional dataset, it is enough to review 10 % of the database to obtain 90 % of the answers, and to review less than 1 % to get 80 % of the correct answers. The result is more impressive if we realize that a full 128dimensional dataset may span thousands of dimensions of clustered data. Furthermore, the concept of proximity preserving order opens a totally new approach for both exact and approximated proximity searching. 1
37|Navigating nets: Simple algorithms for proximity search (Extended Abstract)  (2004) |Robert Krauthgamer # James R. Lee +  Abstract  We present a simple deterministic data structure for maintaining a set S of points in a general metric space, while supporting proximity search (nearest neighbor and range queries) and updates to S (insertions and deletions). Our data structure consists of a sequence of progressively finer #-nets of S, with pointers that allow us to navigate easily from one scale to the next.
38|Approximate Nearest Neighbors: Towards Removing the Curse of Dimensionality|The nearest neighbor problem is the following: Given a set of n points P = fp 1 ; : : : ; png in  some metric space X, preprocess P so as to efficiently answer queries which require finding the  point in P closest to a query point q 2 X. We focus on the particularly interesting case of the  d-dimensional Euclidean space where X = !  d  under some l p norm. Despite decades of effort,  the current solutions are far from satisfactory; in fact, for large d, in theory or in practice, they  provide little improvement over the brute-force algorithm which compares the query point to  each data point. Of late, there has been some interest in the approximate nearest neighbors  problem, which is: Find a point p 2 P that is an ffl-approximate nearest neighbor of the query  q in that for all p  0  2 P , d(p; q)  (1 + ffl)d(p  0  ; q).  We present two algorithmic results for the approximate version that significantly improve the  known bounds: (a) preprocessing cost polynomial in n and d, and a trul...
39|An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions|Consider a set S of n data points in real d-dimensional space, R d , where distances are measured using any Minkowski metric. In nearest neighbor searching we preprocess S into a data structure, so that given any query point q 2 R d , the closest point of S to q can be reported quickly. Given any positive real ffl, a data point p is a (1 + ffl)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + ffl) of the distance to the true nearest neighbor. We show that it is possible to preprocess a set of n points in R d in O(dn log n) time and O(dn) space, so that given a query point q 2 R d , and ffl ? 0, a (1 + ffl)-approximate nearest neighbor of q can be computed in O(c d;ffl log n) time, where c d;ffl d d1 + 6d=ffle d is a factor depending only on dimension and ffl. In general, we show that given an integer k 1, (1 + ffl)-approximations to the k nearest neighbors of q can be computed in additional O(kd log n) time.
40|Efficient Search for Approximate Nearest Neighbor in High Dimensional Spaces|We address the problem of designing data structures that allow efficient search for approximate nearest neighbors. More specifically, given a database consisting of a set of vectors in some high dimensional Euclidean space, we want to construct a space-efficient data structure that would allow us to search, given a query vector, for the closest or nearly closest vector in the database. We also address this problem when distances are measured by the L 1 norm, and in the Hamming cube. Significantly improving and extending recent results of Kleinberg, we construct data structures whose size is polynomial in the size of the database, and search algorithms that run in time nearly linear or nearly quadratic in the dimension (depending on the case; the extra factors are polylogarithmic in the size of the database).   Computer Science Department, Technion --- IIT, Haifa 32000, Israel. Email: eyalk@cs.technion.ac.il  y  Bell Communications Research, MCC-1C365B, 445 South Street, Morristown, NJ ...
41|Near neighbor search in large metric spaces|Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically difficult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT- Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT’s outperform previous data structures in a number of applications.
42|Bounded geometries, fractals, and low-distortion embeddings | The doubling constant of a metric space (X; d) is thesmallest value * such that every ball in X can be covered by * balls of half the radius. The doubling dimension of X isthen defined as dim(X)  = log2 *. A metric (or sequence ofmetrics) is called doubling precisely when its doubling dimension is bounded. This is a robust class of metric spaceswhich contains many families of metrics that occur in applied settings.We give tight bounds for embedding doubling metrics into (low-dimensional) normed spaces. We consider bothgeneral doubling metrics, as well as more restricted families such as those arising from trees, from graphs excludinga fixed minor, and from snowflaked metrics. Our techniques include decomposition theorems for doubling metrics, andan analysis of a fractal in the plane due to Laakso [21]. Finally, we discuss some applications and point out a centralopen question regarding dimensionality reduction in L2. 
43|Finding Nearest Neighbors in Growth-restricted Metrics|Most research on nearest neighbor algorithms in the literature has been focused on the Euclidean case. In many practical search problems however, the underlying metric is non-Euclidean. Nearest neighbor algorithms for general metric spaces are quite weak, which motivates a search for other classes of metric spaces that can be tractably searched.
44|Nearest neighbor queries in metric spaces|Given a set S of n sites (points), and a distance measure d, the nearest neighbor searching problem is to build a data structure so that given a query point q, the site nearest to q can be found quickly. This paper gives data structures for this problem when the sites and queries are in a metric space. One data structure, D(S), uses a divide-and-conquer recursion. The other data structure, M(S, Q), is somewhat like a skiplist. Both are simple and implementable. The data structures are analyzed when the metric space obeys a certain sphere-packing bound, and when the sites and query points are random and have distributions with an exchangeability property. This property implies, for example, that query point q is a random element of S ? {q}. Under these conditions, the preprocessing and space bounds for the algorithms are close to linear in n. They depend also on the sphere-packing bound, and on the logarithm of the distance ratio ?(S) of S, the ratio of the distance between the farthest pair of points in S to the distance between the closest pair. The data structure M(S, Q) requires as input data an additional set Q, taken to be representative of the query points. The resource bounds of M(S, Q) have a dependence on the distance ratio of S ? Q. While M(S, Q) can return wrong answers, its failure probability can be bounded, and is decreasing in a parameter K. Here K = |Q|/n is chosen when building M(S, Q). The expected query time for M(S, Q) is O(K log n) log ?(S ? Q), and the resource bounds increase linearly in K. The data structure D(S) has expected O(log n) O(1) query time, for fixed distance ratio. The preprocessing algorithm for M(S, Q) can be used to solve the all-nearest-neighbor problem for S in O(n(log n) 2 (log ?(S)) 2) expected time. 1
45|Pivot Selection Techniques for Proximity Searching in Metric Spaces|With few exceptions, proximity search algorithms in metric spaces based on the use of pivots select them at random among the objects of the metric space. However, it is well known that the way in which the pivots are selected can drastically a#ect the performance of the algorithm. Between two sets of pivots of the same size, better chosen pivots can largely reduce the search time. Alternatively, a better chosen small set of pivots (requiring much less space) can yield the same e#ciency as a larger, randomly chosen, set. We propose an e#ciency measure to compare two pivot sets, combined with an optimization technique that allows us to select good sets of pivots. We obtain abundant empirical evidence showing that our technique is e#ective, and it is the first that we are aware of in producing consistently good results in a wide variety of cases and in being based on a formal theory. We also show that good pivots are outliers, but that selecting outliers does not ensure that good pivots are selected.
46|Data Structures and Algorithms for Nearest Neighbor Search in General Metric Spaces|We consider the computational problem of finding nearest neighbors in general metric spaces. Of particular interest are spaces that may not be conveniently embedded or approximated in Euclidian space, or where the dimensionality of a Euclidian representation is very high.  Also relevant are high-dimensional Euclidian settings in which the distribution of data is in some sense of lower dimension and embedded in the space.  The vp-tree (vantage point tree) is introduced in several forms, together with associated algorithms, as an improved method for these difficult search problems. Tree construction executes in O(n log(n)) time, and search is under certain circumstances and in the limit, O(log(n)) expected time.  The theoretical basis for this approach is developed and the results of several experiments are reported. In Euclidian cases, kd-tree performance is compared.  
47|Distance-based indexing for high-dimensional metric spaces|In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces. The mvptree uses more than one vantage point to partition the space into spherical cuts at each level. It also utilizes the pre-computed (at construction time) distances between the data points and the vantage points. We have done experiments to compare mvp-trees with vp-trees which have a similar partitioning strategy, but use only one vantage point at each level, and do not make use of the pre-computed distances. Empirical studies show that mvptree outperforms the vp-tree 20 % to 80 % for varying query ranges and different distance distributions. 1.
48|Proximity Matching Using Fixed-Queries Trees |. We present a new data structure, called the fixed-queries tree, for the problem of finding all elements of a fixed set that are close, under some distance function, to a query element. Fixedqueries trees can be used for any distance function, not necessarily even a metric, as long as it satisfies the triangle inequality. We give an analysis of several performance parameters of fixed-queries trees and experimental results that support the analysis. Fixed-queries trees are particularly efficient for applications in which comparing two elements is expensive. 1 Introduction  Search structures such as hashing and trees are at the basis of many efficient computer science applications. But they usually support only exact queries. Finding things approximately, that is, allowing some errors in the query specifications, is much harder. The first question that a prominent biologist once asked one of the authors when finding that he is a computer scientist is whether it is possible to adapt bina...
49|Excluded Middle Vantage Point Forests for Nearest Neighbor Search|The excluded middle vantage point forest is a new data structure that supports worst case sublinear time searches in a metric space for nearest neighbors within a xed radius  of arbitrary queries. Worst case performance depends on the dataset but is not aected by the distribution of queries.  Our analysis predicts vp-forest performance in simple settings such as L p spaces with uniform random datasets | and experiments conrm these predictions. Another contribution of the analysis is a new perspective on the curse of dimensionality in the context of our methods and kd-trees as well. In our idealized setting the dataset is organized into a forest of O(N  1   ) trees, each of depth O(log N ). Here  may be viewed as depending on  , the distance function, and on the dataset. The radius of interest  is an input to the organization process and the result is a linear space data structure specialized to answer queries within this distance. Searches then require O(N  1   log N) time, or...
50|Fixed Queries Array: A Fast and Economical Data Structure for Proximity Searching|. Pivot-based algorithms are effective tools for proximity searching in metric spaces. They allow trading space overhead for number of distance evaluations performed at query time. With additional search structures (that pose extra space overhead) they can also reduce the amount of side computations. We introduce a new data structure, the Fixed Queries Array (FQA), whose novelties are (1) it permits sublinear extra CPU time without any extra data structure; (2) it permits trading number of pivots for their precision so as to make better use of the available memory. We show experimentally that the FQA is an efficient tool to search in metric spaces and that it compares favorably against other state of the art approaches. Its simplicity converts it into a simple yet effective tool for practitioners seeking for a black-box method to plug in their applications.  Keywords: Metric spaces, similarity search, range search, fixed queries tree.  1. 
51|Faster proximity searching in metric data |Abstract. A number of problems in computer science can be solved efficiently with the so called memory based or kernel methods. Among this problems (relevant to the AI community) are multimedia indexing, clustering, non supervised learning and recommendation systems. The common ground to this problems is satisfying proximity queries with an abstract metric database. In this paper we introduce a new technique for making practical indexes for metric range queries. This technique improves existing algorithms based on pivots and signatures, and introduces a new data structure, the Fixed Queries Trie to speedup metric range queries. The result is an O(n) construction time index, with query complexity O(n a), a = 1. The indexing algorithm uses only a few bits of storage for each database element. 1 Introduction and Related Work Proximity queries are those extensions of the exact searching where we want to
52|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
53|Efficient Multi-Object Dynamic Query Histograms|Dynamic Queries offer continuous feedback during range queries, and have been shown to be effective and satisfying. Recent work has extended them to datasets of 100,000 objects and, separately, to queries involving relations among multiple objects. The latter work enables filtering houses by properties of their owners, for instance. Our primary concern is providing feedback from histograms during Dynamic Query. The height of each histogram bar shows the count of selected objects whose attribute value falls into a given range. Unfortunately,  previous efficient algorithms for single object queries overcount in the case of multiple objects if, for instance, a house has multiple owners. This paper presents an efficient algorithm that with high probability closely approximates the true counts.  1. Previous Dynamic Query work  1.1. Single Object Interface  Figure 1 shows a Dynamic Query (DQ) interface as implemented in VQE, a Visual Query Environment for exploring data from a database [1]. ...
54|Efficient Engines for Keyword Proximity Search|This paper presents a formal framework for investigating
55|Keyword searching and browsing in databases using BANKS|With the growth of the Web, there has been a rapid increase in the number of users who need to access online databases without having a detailed knowledge of the schema or of query languages; even relatively simple query languages designed for non-experts are too complicated for them. We describe BANKS, a system which enables keyword-based search on relational databases, together with data and schema browsing. BANKS enables users to extract information in a simple manner without any knowledge of the schema or any need for writing complex queries. A user can get information by typing a few keywords, following hyperlinks, and interacting with controls on the displayed results. BANKS models tuples as nodes in a graph, connected by links induced by foreign key and other relationships. Answers to a query are modeled as rooted trees connecting tuples that match individual keywords in the query. Answers are ranked using a notion of proximity coupled with a notion of prestige of nodes based on inlinks, similar to techniques developed for Web search. We present an efficient heuristic algorithm for finding and ranking query results. 1.
56|XRANK: Ranked Keyword Search over XML Documents|We consider the problem of efficiently producing ranked results for keyword search queries over hyperlinked XML documents. Evaluating
57|Improved Steiner Tree Approximation in Graphs|The Steiner tree problem in weighted graphs seeks a minimum weight connected subgraph  containing a given subset of the vertices (terminals). We present a new polynomial-time  heuristic with an approximation ratio approaching 1 +  2  1:55, which improves upon  the previously best-known approximation algorithm of [10] with performance ratio  1:59.
58|ON GENERATING ALL MAXIMAL INDEPENDENT SETS|We present an algorithm that generates all maximal independent sets of a graph in lexicographic order, with only polynomial delay between the output of two successive independent sets. We also show that there is no polynomial-delay algorithm for generating all maximal independent sets in reverse lexicographic order, unless P = NP.
59|Efficient IR-Style Keyword Search over Relational Databases|Applications in which plain text coexists with structured data are pervasive. Commercial relational database management systems (RDBMSs) generally provide querying capabilities for text attributes that incorporate state-of-the-art information retrieval (IR) relevance ranking strategies, but this search functionality requires that queries specify the exact column or columns against which a given list of keywords is to be matched.
60|Approximation Algorithms for Directed Steiner Problems|We give the first non-trivial approximation algorithms for the Steiner tree problem and  the generalized Steiner network problem on general directed graphs. These problems have  several applications in network design and multicast routing. For both problems, the best  ratios known before our work were the trivial O(k)-approximations. For the directed Steiner  tree problem, we design a family of algorithms that achieves an approximation ratio of  i(i \Gamma 1)k  1=i  in time O(n  i  k  2i  ) for any fixed i ? 1, where k is the number of terminals. Thus,  an O(k  ffl  ) approximation ratio can be achieved in polynomial time for any fixed ffl ? 0.  Setting i = log k, we obtain an O(log  2  k) approximation ratio in quasi-polynomial time. For  the directed generalized Steiner network problem, we give an algorithm that achieves an  approximation ratio of O(k  2=3  log  1=3  k), where k is the number of pairs of vertices that are  to be connected. Related problems including the group Steiner...
61|XSEarch: A Semantic Search Engine for XML|XSEarch, a semantic search engine for XML,  is presented. XSEarch has a simple query language,  suitable for a naive user. It returns semantically  related document fragments that  satisfy the user&#039;s query. Query answers are  ranked using extended information-retrieval  techniques and are generated in an order similar  to the ranking. Advanced indexing techniques  were developed to facilitate e#cient implementation  of XSEarch. The performance  of the di#erent techniques as well as the recall  and the precision were measured experimentally.
62|DBXplorer: enabling keyword search over relational databases|While relational database systems offer powerfifl structured query languages such as SQL, there is no support for keyword search over databases. The simplicity of keyword search as a querying paradigm offers compelling values for data exploration. Specifically, keyword search does not require a priori knowledge of the schema. The above is significant as much information in a corporation is increasingly being available at its intranet. However, it is unrealistic to expect users who would browse and query such information to have detailed knowledge of the schema of available databases. Therefore, just as keyword search and classification hierarchies complement each other for document search, keyword search over databases can be effective. 2. Overview of DBXplorer We present DBXplorer, a system that enables keyword search over SQL databases. Technical details of our system may be found in [4]. Given a set of query keywords, our system returns all rows (either from single tables or by joining tables connected by foreign-key joins) such that each row contains all keywords. DBA ~1 Publish k r I Populate Search I ~ symbol table
63|Multi-Embedding and Path Approximation of Metric Spaces|Metric embeddings have become a frequent tool in the design of algorithms. The applicability is often dependent on how high the embedding&#039;s distortion is. For example embedding into ultrametrics (or arbitrary trees) requires linear distortion. Using probabilistic metric embeddings, the bound reduces to O(log n log log n). Yet, the lower bound is still logarithmic. We make
64|Generating relations from XML documents|Abstract. This paper discusses several mechanisms for creating relations out of XML documents. A relation generator consists of two parts: (1) a tuple of path expressions and (2) an index indicating which path expressions may not be assigned the null value. Evaluating a relation generator involves finding tuples of nodes that satisfy the path expressions and are related to one another in a meaningful fashion. Different semantics for evaluation are given that take into account the possible presence of incomplete information. The complexity of generating relations from documents is analyzed and evaluation algorithms are described. 1
65|Interconnection semantics for XML|A framework for defining and automatically discovering semantic relationships among nodes in XML documents is presented. A specific interconnection semantics in this framework consists of a set of patterns. Interconnection semantics can be specified explicitly or de-rived automatically. Several methods to automatically derive interconnection semantics are presented. The complexity of determining when nodes are interconnected under these se-mantics is analyzed. For many important cases, the complexity is tractable and hence, the proposed interconnection semantics can be efficiently applied to real-world documents. In particular, for acyclically-labeled documents, determining interconnection for a bounded-size set of nodes is polynomial for most of these semantics. The inverse problem of con-structing a document from a given set of objects and the interconnections that hold among those objects is also considered. It is shown that under a natural condition of unambiguity, a document that satisfies exactly the specified interconnections can be constructed efficiently, if such a document exists. If not, the set of new interconnections that are introduced by the construction is minimal.
66|Using Proximity Search to Estimate Authority Flow |Abstract—Authority flow and proximity search have been used extensively in measuring the association between entities in data graphs, ranging from the Web to relational and XML databases. These two ranking factors have been used and studied separately in the past. In addition to their semantic differences, a key advantage of proximity search is the existence of efficient execution algorithms. In contrast, due to the complexity of calculating the authority flow, current systems only use precomputed authority flows in runtime. This limitation prohibits authority flow to be used more effectively as a ranking factor. In this paper we present a comparative analysis of the two ranking factors. We present an efficient approximation of authority flow based on proximity search. We analytically estimate the approximation error and how this affects the ranking of the results of a query. I.
67|Authority-based keyword search in databases |The ObjectRank system applies authority-based ranking to keyword search in databases modeled as labeled graphs. Conceptually, authority originates at the nodes (objects) containing the keywords and flows to objects according to their semantic connections. Each node is ranked according to its authority with respect to the particular
68|Fast random walk with restart and its applications|How closely related are two nodes in a graph? How to compute this score quickly, on huge, disk-resident, real graphs? Random walk with restart (RWR) provides a good relevance score between two nodes in a weighted graph, and it has been successfully used in numerous settings, like automatic captioning of images, generalizations to the “connection subgraphs”, personalized PageRank, and many more. However, the straightforward implementations of RWR do not scale for large graphs, requiring either quadratic space and cubic pre-computation time, or slow response time on queries. We propose fast solutions to this problem. The heart of our approach is to exploit two important properties shared by many real graphs: (a) linear correlations and (b) blockwise, community-like structure. We exploit the linearity by using low-rank matrix approximation, and the community structure by graph partitioning, followed by the Sherman-Morrison lemma for matrix inversion. Experimental results on the Corel image and the DBLP dabasets demonstrate that our proposed methods achieve significant savings over the straightforward implementations: they can save several orders of magnitude in pre-computation and storage cost, and they achieve up to 150x speed up with 90%+ quality preservation. 1
69|Extrapolation Methods for Accelerating PageRank Computations|We present a novel algorithm for the fast computation of PageRank, a hyperlink-based estimate of the &#034;importance&#034; of Web pages. The original PageRank algorithm uses the Power Method to compute successive iterates that converge to the principal eigenvector of the Markov matrix representing the Web link graph. The algorithm presented here, called Quadratic Extrapolation, accelerates the convergence of the Power Method by periodically subtracting off estimates of the nonprincipal eigenvectors from the current iterate of the Power Method. In Quadratic Extrapolation, we take advantage of the fact that the first eigenvalueof a Markov matrix is known to be 1 to compute the nonprincipal eigenvectorsusing successiveiterates of the Power Method. Empirically, we show that using Quadratic Extrapolation speeds up PageRank computation by 50-300% on a Web graph of 80 million nodes, with minimal overhead.
70|Efficient Computation of PageRank|This paper discusses efficient techniques for computing PageRank, a ranking metric for hypertext documents. We show that PageRank can be computed for very large subgraphs of the web (up to hundreds of millions of nodes) on machines with limited main memory. Running-time measurements on various memory configurations are presented for PageRank computation over the 24-million-page Stanford WebBase archive. We discuss several methods for analyzing the convergence of PageRank based on the induced ordering of the pages. We present convergence results helpful for determining the number of iterations necessary to achieve a useful PageRank assignment, both in the absence and presence of search queries.
71|Bidirectional Expansion For Keyword Search On Graph Databases|Relational, XML and HTML data can be represented  as graphs with entities as nodes and  relationships as edges. Text is associated with  nodes and possibly edges. Keyword search  on such graphs has received much attention  lately. A central problem in this scenario  is to e#ciently extract from the data graph a  small number of the &#034;best&#034; answer trees. A  Backward Expanding search, starting at nodes  matching keywords and working up toward  confluent roots, is commonly used for predominantly  text-driven queries. But it can perform  poorly if some keywords match many  nodes, or some node has very large degree. In this paper
72|Towards scaling fully personalized PageRank|Abstract Personalized PageRank expresses backlink-based page quality around user-selected pages in a similar way as PageRank expresses quality over the entire Web. Existing personalized PageRank algorithms can however serve on-line queries only for a restricted choice of page selection. In this paper we achieve full personalization by a novel algorithm that computes a compact database of simulated random walks; this database can serve arbitrary personal choices of small subsets of web pages. We prove that for a fixed error probability, the size of our database is linear in the number of web pages. We justify our estimation approach by asymptotic worst-case lower bounds; we show that exact personalized PageRank values can only be obtained from a database of quadratic size. 1
73|I/O-Efficient Techniques for Computing Pagerank |Over the last few years, most major search engines have integrated link-based ranking techniques in order to provide more accurate search results. One widely known approach is the Pagerank technique, which forms the basis of the Google ranking scheme, and which assigns a global importance measure to each page based on the importance of other pages pointing to it. The main advantage of the Pagerank measure is that it is independent of the query posed by a user
74|Explaining and reformulating authority flow queries|Abstract — Authority flow is an effective ranking mechanism for answering queries on a broad class of data. Systems have been developed to apply this principle on the Web (PageRank and topic sensitive PageRank), bibliographic databases (ObjectRank), and biological databases (Hubs of Knowledge project). However, these systems have the following drawbacks: (a) There is no way to explain to the user why a particular result received its current score; (b) The authority flow rates, which have been shown to dramatically affect the results ’ quality in ObjectRank, have to be set manually by a domain expert; (c) There is no query reformulation methodology to refine the query results according to the user’s preferences. In this work, we address these shortcomings by introducing a framework and algorithms to explain query results and reformulate authority flow queries based on the user’s feedback. The query reformulation process can be used to learn the user’s preferences and automatically adjust the authority flow rates to facilitate personalized authority flow searching. We experimentally evaluate our algorithms in terms of performance and quality. I.
75|Beyond Single-Page Web Search Results |Abstract—Given a user keyword query, current Web search engines return a list of individual Web pages ranked by their “goodness” with respect to the query. Thus, the basic unit for search and retrieval is an individual page, even though information on a topic is often spread across multiple pages. This degrades the quality of search results, especially for long or uncorrelated (multitopic) queries (in which individual keywords rarely occur together in the same document), where a single page is unlikely to satisfy the user’s information need. We propose a technique that, given a keyword query, on the fly generates new pages, called composed pages, which contain all query keywords. The composed pages are generated by extracting and stitching together relevant pieces from hyperlinked Web pages and retaining links to the original Web pages. To rank the composed pages, we consider both the hyperlink structure of the original pages and the associations between the keywords within each page. Furthermore, we present and experimentally evaluate heuristic algorithms to efficiently generate the top composed pages. The quality of our method is compared to current approaches by using user surveys. Finally, we also show how our techniques can be used to perform query-specific summarization of Web pages. Index Terms—Internet search, search process, Web search.
76|List of Clustered Permutations for Proximity Searching ? |Abstract. The permutation based algorithm has been proved unbeatable in high dimensional spaces, requiring O(|P|) distance evaluations when solving similarity queries (where P is the set of permutants); but needs n evaluations of the permutant distance to compute the order to review the metric dataset, requires O(n|P|) space, and does not take much benefit from low dimensionality. There have been several proposals to avoid the n computations of the permutant distance, however all of them lost precision. Inspired in the list of cluster, in this paper we group the permutations and establish a criterion to discard whole clusters according the permutation of their centers. As a consequence of our proposal, we now reduce not only the space of the index and the number of distance evaluations but also the cpu time required when comparing the permutations themselves. Also, we can use the permutations in low dimensions. 1
77|A compact space decomposition for effective metric indexing|Abstract The metric space model abstracts many proximity search problems, from nearest-neighborclassifiers to textual and multimedia information retrieval. In this context, an index is a data structure that speeds up proximity queries. However, indexes lose their efficiency as the intrinsicdata dimensionality increases. In this paper we present a simple index called list of clusters (LC), which is based on a compact partitioning of the data set. The LC is shown to require little space,to be suitable both for main and secondary memory implementations, and most importantly, to be very resistant to the intrinsic dimensionality of the data set. In this aspect our structure isunbeaten. We finish with a discussion of the role of unbalancing in metric space searching, and how it permits trading memory space for construction time. 1 Introduction The problem of proximity searching has received much attention in recent times, due to an increasing interest in manipulating and retrieving the more and more common multimedia data. Multimedia data have to be classified, forecasted, filtered, organized, and so on. Their manipulation poses new challenges to classifiers and function approximators. The well-known k-nearest neighbor (knn) classifier is a favorite candidate for this task for being simple enough and well understood. One of the main obstacles, however, of using this classifier for massive data classification is its linear complexity to find a set of k neighbors for a given query.
78|Probabilistic Proximity Search: Fighting the Curse of Dimensionality in Metric Spaces |Proximity searches become very difficult on &#034;high dimensional&#034; metric spaces, that is, those whose histogram of distances has a large mean and/or a small variance. This so-called &#034;curse of dimensionality&#034;, well known in vector spaces, is also observed in metric spaces. The search complexity grows sharply with the dimension and with the search radius. We present a general probabilistic framework applicable to any search algorithm and whose net effect is to reduce the search radius. The higher the dimension, the more effective the technique. We illustrate empirically its practical performance on a particular class of algorithms, where large improvements in the search time are obtained at the cost of a very small error probability.
79|The Impact of Running Headers and Footers on Proximity Searching |Hundreds of experiments over the last decade on the retrieval of OCR documents performed by the Information Science Research Institute have shown that OCR errors do not significantly affect retrievability. We extend those results to show that in the case of proximity searching, the removal of running headers and footers from OCR text will not improve retrievability for such searches.
80|Results of Applying Probabilistic IR to OCR Text|Character accuracy of optically recognized text is considered a basic measure for evaluating OCR devices. In the broader sense, another fundamental measure of an OCR&#039;s goodness is whether its generated text is usable for retrieving information. In this study, we evaluate retrieval effectiveness from OCR text databases using a probabilistic IR system. We compare these retrieval results to their manually corrected equivalent. We show there is no statistical difference in precision and recall using graded accuracy levels from three OCR devices. However, characteristics of the OCR data have side effects that could cause unstable results with this IR model. In particular, we found individual queries can be greatly affected. Knowing the qualities of OCR text, we compensate for them by applying an automatic post-processing system that improves effectiveness.  1 Introduction  Anyone who has performed research in either optical character recognition (OCR) or information retrieval (IR) will atte...
81|Evaluation of model-based retrieval effectiveness with OCR text|We give a comprehensive report on our experiments with retrieval from OCR-generated text using systems based on standard models of retrieval. More specifically, we show that average precision and recall is not affected by OCR errors across systems for several collections. The collections used in these experiments include both actual OCR-generated text and standard information retrieval collections corrupted through the simulation of OCR errors. Both the actual and simulation experiments include full-text and abstract-length documents. We also demonstrate that the ranking and feedback methods associated with these models are generally not robust enough to deal with OCR errors. It is further shown that the OCR errors and garbage strings generated from the mistranslation of graphic objects increase the size of the index by a wide margin. We not only point out problems that can arise from applying OCR text within an information retrieval environment, we also suggest solutions to overcome some of these problems.
82|The effects of noisy data on text retrieval|We report on the results of our experiments on query evaluation in the presence of noisy data. In particular, an OCR generated database and its corresponding 99.8% correct version are used to process a set of queries to determine the effect the degraded version will have on retrieval. It is shown that with the set of scientific documents we use in our testing, the effect is insignificant. We further improve the result by applying an automatic post processing system designed to correct the kinds of errors generated by recognition devices.
83|Some aspects of proximity searching in text retrieval systems|Abstract. Describes and evaluates the proximity search facili-ties in external online systems and in-house retrieval software Discusses and illustrates capabilities, syntax and circum-stances ot use. Presents measurements of the overheads re-quired by proximity for storage, record import time and search time The search strategy narrowing effect of proximity is illustrated by Recall and Precision test results. Usage and problems lead to a number ot design ideas for better imple-mentation. some based on existing Boolean strategies. one on the use of weighted proximity to automatically produce ranked output A comparison of Boolean, quorum and proximate term pairs distance is included 1.
84|Efficient Proximity Search for 3-D Cuboids|In this paper, we give the definition for the voronoi diagram and its dual graph -- Delaunay triangulation for 3D cuboids. We prove properties of the 3D Delaunay triangulation, and provide algorithms to construct and update the Delaunay triangulation. The Delaunay triangulation data structure is used to perform proximity searches for both static and kinetic cases. We describe experimental results that show how the Delaunay triangulation is used on a mobile robot to model, understand and reason about the spatial information of the environment.
85|Voronoi diagrams -- a survey of a fundamental geometric data structure|This paper presents a survey of the Voronoi diagram, one of the most fundamental data structures in computational geometry. It demonstrates the importance and usefulness of the Voronoi diagram in a wide variety of fields inside and outside computer science and surveys the history of its development. The paper puts particular emphasis on the unified exposition of its mathematical and algorithmic properties. Finally, the paper provides the first comprehensive bibliography on Voronoi diagrams and related structures.
86|A fast procedure for computing the distance between complex objects in three space|Abstract-An efficient and reliable algorithm for computing the Euclidean distance between a pair of convex sets in Rm is described. Extensive numerical experience with a broad family of polytopes in R3 shows that the computational cost is approximately linear in the total number of vertices specifying the two polytopes. The algorithm has special features which makes its application in a variety of robotics problems attractive. These are discussed and an example of collision detection is given. I.
87|Data structures for mobile data|A kinetic data structure (KDS) maintains an attribute of interest in a system of geometric objects undergoing continuous motion. In this paper we develop a conceptual framework for kinetic data structures, propose a number of criteria for the quality of such structures, and describe a number of fundamental techniques for their design. We illustrate these general concepts by presenting kinetic data structures for maintaining the convex hull and the closest pair of moving points in the plane; these structures behavewell according to the proposed quality criteria for KDSs.
88|Kinetic Data Structures -- A State of the Art Report|... In this paper we present a general framework for addressing such problems and the tools for designing and analyzing relevant algorithms, which we call kinetic data structures. We discuss kinetic data structures for a variety of fundamental geometric problems, such as the maintenance of convex hulls, Voronoi and Delaunay diagrams, closest pairs, and intersection and visibility problems. We also briefly address the issues that arise in implementing such structures robustly and efficiently. The resulting techniques satisfy three desirable properties: (1) they exploit the continuity of the motion of the objects to gain efficiency, (2) the number of events processed by the algorithms is close to the minimum necessary in the worst case, and (3) any object may change its `flight plan&#039; at any moment with a low cost update to the simulation data structures. For computer applications dealing with motion in the physical world, kinetic data structures lead to simulation performance unattainable by other means. In addition, they raise fundamentally new combinatorial and algorithmic questions whose study may prove fruitful for other disciplines as well.
89|Box-Trees and R-trees with Near-Optimal Query Time|A box-tree is a bounding-volume hierarchy that uses axis-aligned boxes as bounding volumes. The query complexity of a box-tree with respect to a given type of query is the maximum number of nodes visited when answering such a query. We describe several new algorithms for constructing box-trees with small worst-case query complexity with respect to queries with axisparallel boxes and with points. We also prove lower bounds on the worst-case query complexity for box-trees, which show that our results are optimal or close to optimal. Finally, we present algorithms to convert box-trees to R-trees, resulting in R-trees with (almost) optimal query complexity. 1 
90|A compact piecewise-linear Voronoi diagram for convex sites in the plane|In the plane, the post-office problem, which asks for the closest site to a query site, and retraction motion planning, which asks for a one-dimensional retract of the free space of a robot, are both classically solved by computing a Voronoi diagram. When the sites are k disjoint convex sets, we give a compact representation of the Voronoi diagram, using O(k) line segments, that is sufficient for logarithmic time post-office location queries and motion planning. If these sets are polygons with n total vertices, we compute this diagram optimally in O ( k log n) deterministic time for the Euclidean metric and in O(k logn logm) deterministic time for the convex distance function defined by a convex m-gon. 
91|Compact Voronoi Diagrams for Moving Convex Polygons|We describe a kinetic data structure for maintaining a compact Voronoi-like diagram of  convex polygons moving around in the plane. We use a compact diagram for the polygons, dual  to the Voronoi, first presented in [MKS96]. A key feature of this diagram is that its size is  only a function of the number of polygons and not of their complexity. We demonstrate a local  certifying property of that diagram, akin to that of Delaunay triangulations of points. We then  obtain a method for maintaining this diagram that is output-sensitive and costs O(log n) per  update. Furthermore, we show that for a set of k polygons with a total of n vertices moving  along bounded degree algebraic motions, this dual diagram, and thus their compact Voronoi  diagram, changes  combinatorially##    ) and O(kn    #(k)#(n)) times, where #() is an extremely  slowly growing function. This compact Voronoi diagram can be used for collision detection or  retraction motion planning among the moving polygons.
93|A Unified Approach to Approximate Proximity Searching |Abstract. The inability to answer proximity queries efficiently for spaces of dimension d&gt;2 has led to the study of approximation to proximity problems. Several techniques have been proposed to address different approximate proximity problems. In this paper, we present a new and unified approach to proximity searching, which provides efficient solutions for several problems: spherical range queries, idempotent spherical range queries, spherical emptiness queries, and nearest neighbor queries. In contrast to previous data structures, our approach is simple and easy to analyze, providing a clear picture of how to exploit the particular characteristics of each of these problems. As applications of our approach, we provide simple and practical data structures that match the best previous results up to logarithmic factors, as well as advanced data structures that improve over the best previous results for all aforementioned proximity problems. 1
94|A Replacement for Voronoi Diagrams of Near Linear Size|For a set P of n points in R^d, we define a new type of space decomposition. The new diagram provides an &amp;epsilon;-approximation to the distance function associated with the Voronoi diagram of P, while being of near linear size, for d &amp;ge; 2. This contrasts with the standard Voronoi diagram that has complexity &amp;Omega;(n^&amp;lceil;d/2&amp;rceil;) in the worst case.
95|Approximate Range Searching|The range searching problem is a fundamental problem in computational geometry, with numerous important applications. Most research has focused on solving this problem exactly, but lower bounds show that if linear space is assumed, the problem cannot be solved in polylogarithmic time, except for the case of orthogonal ranges. In this paper we show that if one is willing to allow approximate ranges, then it is possible to do much better. In particular, given a bounded range  Q of diameter w and ffl ? 0, an approximate range query treats the range as a fuzzy object, meaning that points lying within distance fflw  of the boundary of Q either may or may not be counted. We show that in any fixed dimension d, a set of n points in R  d  can be preprocessed in O(n log n) time and O(n) space, such that approximate queries can be answered in O(logn + (1=ffl)  d  ) time. The only assumption we make about ranges is that the intersection of a range and a d-dimensional  cube can be answered in const...
96|Faster Core-Set Constructions and Data Stream Algorithms in Fixed Dimensions|We speed up previous (1 + &#034;)-factor approximation algorithms for a number of geometric  optimization problems in  xed dimensions: diameter, width, minimum-radius enclosing cylinder,  minimum-width annulus, minimum-volume bounding box, minimum-width cylindrical shell, etc.
97|Approximate Nearest Neighbor Queries Revisited|This paper proposes new methods to answer approximate nearest neighbor queries on a set of n points in d-dimensional Euclidean space. For any fixed constant d, a data structure with  O(&#034;  (1\Gammad)=2  n log n) preprocessing time and O(&#034;  (1\Gammad)=2  log n) query time achieves approximation factor 1 + &#034; for any given 0 ! &#034; ! 1; a variant reduces the &#034;-dependence by a factor of &#034;  \Gamma1=2  . For any arbitrary d, a data structure with O(d  2  n log n) preprocessing time and O(d  2  log n) query time achieves approximation factor O(d  3=2  ). Applications to various proximity problems are discussed. 1 Introduction  Let P be a set of n point sites in d-dimensional space IR  d  . In the well-known post office problem, we want to preprocess P into a data structure so that a site closest to a given query point q (called the  nearest neighbor of q) can be found efficiently. Distances are measured under the Euclidean metric. The post office problem has many applications within computational...
98|Balanced Aspect Ratio Trees: Combining the Advantages of k-d Trees and Octrees |Given a set S of n points in R^d, we show, for fixed d, how to construct in O(n log n) time a data structure we call the Balanced Aspect Ratio (BAR) tree. A BAR tree is a binary space partition tree on S that has O(logn) depth and in which every region is convex and “fat ” (that is, has a bounded aspect ratio). While previous hierarchical data structures, such as k-d trees, quadtrees, octrees, fair-split trees, and balanced box decompositions can guarantee some of these properties, we know of no previous data structure that combines alI of these properties simultaneously. The BAR tree data structure has numerous applications ranging from solving several geometric searching problems in fixed dimensional space to aiding in the visualization of graphs and three-dimensional worlds.  
99|Linear-Size Approximate Voronoi Diagrams|a (t; ffl)-approximate Voronoi diagram (AVD) is a partition of space into constant complexity cells, where each cell c is associated with t representative points of S, such that for any point in c, one of the associated representatives approximates the nearest neighbor to within a factor of (1+ ffl). The goal is to minimize the number and complexity of the cells in the AVD. We show that it is possible to construct an AVD consisting of O(n=ffl    cells for t = 1, and O(n) cells for t = O(1=ffl    ). In general, for a real parameter 2  fl  1=ffl, we show that it is possible to construct a (t; ffl)-AVD consisting of O(nfl    cells for t = O(1=(fflfl)    ). The cells in these AVDs are cubes or differences of two cubes. All these structures can be used to efficiently answer approximate nearest neighbor queries. Our algorithms are based on the well-separated pair decomposition and are very simple.
100|Closest-point problems simplified on the RAM|Basic proximity problems for low-dimensional point sets, such as closest pair (CP) and approximate nearest neighbor (ANN), have been studied extensively in the computational geometry literature, with well over a hundred papers published (we merely cite the survey by Smid [10] and omit most references). Generally, optimal algorithms designed for worst-case input require hierarchical spatial structures with sophisticated balancing conditions (we mention, for example, the BBD trees of Arya et al., balanced quadtrees, and Callahan and Kosaraju&#039;s fair-split trees); dynamization of these structures is even more involved (relying on Sleator and Tarjan&#039;s dynamic trees or Frederickson&#039;s topology trees). In this note, we point out that much simpler algorithms with the same performance are possible using standard, though nonalgebraic, RAM operations. This is interesting, considering that nonalgebraic operations have been used before in the literature (e.g., in the original version of the BBD tree [2], as well as in various randomized CP methods). The CP algorithm can be stated completely in one paragraph. Assume coordinates are positive integers bounded by U = 2 w. Given a point p in a constant dimension d where the i-th coordinate p i is the number p iw      p i0 in binary,  dene its shue (p) to be the number p 1w      pdw      p 10      p d0 in binary, and  dene shifts  i (p)  = (p 1 + bi2
101|Space-efficient approximate Voronoi diagrams|Given a set S of n points in IR d, a (t, o)-approximate Voronoi diagram (AVD) is a partition of space into constant complexity cells, where each cell c is associated with t representative points of S, such that for any point in c, one of the associated representatives approximates the nearest neighbor to within a factor of (1 + o). Like the Voronoi diagram, this structure defines a spatial subdivision. It also has the desirable properties of being easy to construct and providing a simple and practical data structure for answering approximate nearest neighbor queries. The goal is to minimize the number and complexity of the cells in the AVD. We assume that the dimension d is fixed. Given a real parameter ?, where 2 = ? = 1/o, we show that it is possible to construct a (t, o)-AVD consisting of O(no d-1 2 ? 3(d-1) 2 log ?) cells for t = O(1/(o?) (d-1)/2). This yields a data structure of O(n? d-1 log ?) space (including the space for representatives) that can answer o-NN queries in time O(log(n?) + 1/(o?) (d-1)/2). (Hidden constants may depend exponentially on d, but do not depend on o or ?). In the case ? = 1/o, we show that the additional log ? factor in space can be avoided, and so we have a data structure that answers o-approximate nearest neighbor queries in time O(log(n/o)) with space O(n/o d-1), improving upon the best known space bounds for this query time. In the case ? = 2, we have a data structure that can answer approximate nearest neighbor queries in O(log n + 1/o (d-1)/2) time using optimal O(n) space. This dramatically improves the * The work of the first two authors was supported by the
102|Space-Time Tradeoffs for Approximate Nearest Neighbor Searching|Nearest neighbor searching is the problem of preprocessing a set of n point points in d-dimensional space so that, given any query point q, it is possible to report the closest point to q rapidly. In approximate nearest neighbor searching, a parameter e&gt;0 is given, and a multiplicative error of (1 + e) is allowed. We assume that the dimension d is a constant and treat n and e as asymptotic quantities. Numerous solutions have been proposed, ranging from low-space solutions having space O(n) and query time O(log n +1/ed-1) to high-space solutions having space roughly O((n log n)/ed) and query time O(log(n/e)). We show that there is a single approach to this fundamental problem, which both improves upon existing results and spans the spectrum of space-time tradeoffs. Given a tradeoff parameter ?, where 2 = ? = 1/e, we show that there exists a data structure of space O(n? d-1 log(1/e)) that can answer queries in time O(log(n?) + 1/(e?)(d-1)/2). When ? = 2, this yields a data structure of space O(n log(1/e)) that can answer queries in time O(log n + 1/e (d-1)/2). When  
103|Space-time tradeoffs for approximate spherical range counting|Abstract We present space-time tradeoffs for approximate spherical range counting queries. Given a set S of n data points in Rdalong with a positive approximation factor ffl, the goal is to preprocess the points so that, given any Euclidean ball B,we can return the number of points of any subset of S that contains all the points within a (1- ffl)-factor contraction ofB, but contains no points that lie outside a (1 + ffl)-factor expansion of B.In many applications of range searching it is desirable to offer a tradeoff between space and query time. Wepresent here the first such tradeoffs for approximate range counting queries. Given 0 &lt; ffl &lt; = 1/2 and a parameterfl, where 2 &lt; = fl &lt; = 1/ffl, we show how to construct a data structure of space O(nfld log(1/ffl)) that allows us toanswer ffl-approximate spherical range counting queries in time O(log(nfl) + 1/(fflfl)d-1). The data structure can be built in time O(nfld log(n/ffl) log(1/ffl)). Here n, ffl, and fl areasymptotic quantities, and the dimension d is assumed to be a fixed constant.At one extreme (low space), this yields a data structure of space O(n log(1/ffl)) that can answer approximate range queries in time O(log n + (1/ffl)d-1) which, up to a factorof O(log 1/ffl) in space, matches the best known result
104|Fast Algorithms for Computing the Smallest k-Enclosing Disc|We consider the problem of  nding, for a given n point set P in the plane and  an integer k  n, the smallest circle enclosing at least k points of P . We present a  randomized algorithm that computes in O(nk) expected time such a circle, improving  over previously known algorithms.
105|On the importance of idempotence|Answering range queries is a problem of fundamental importance in spatial information retrieval and computational geometry. The objective is to store a set of n points P in R d, each associated with a weight, so that it is possible to count, or more generally to compute some function of the weights of the points lying inside a given query range. Range searching is among the most heavily studied problems, and many search structures have been proposed and analyzed [1, 7]. There is a spectrum of space-time tradeoffs. The most relevant work to ours involves halfspace range counting queries, which Matou?sek [6] has shown can be answered in n/m 1/d time from a data structure of space O(m). Nearly matching lower bounds were given by by Brönnimann, Chazelle and Pach [4] (or BCP). Given the relatively high complexity of range searching, it is natural to consider the problem in the context of approximation. We are given an approximation parameter e&gt; 0 and assume that ranges are bounded. Let ? denote a range, and let diam(?) denote its diameter. All the points that lie in the range must be counted, and any of the points that lie within distance e · diam(?) of the range’s boundary may be counted as well. Arya and Mount [3] showed that in any fixed dimension d with O(n log n) preprocessing time and O(n) space, e-approximate range queries for any bounded convex range can be answered in time O(log n+1/e d-1) [3]. Later, Chazelle, Liu, and Magen [5] considered approximate halfspace range and Euclidean ball searching in the high dimensional setting. Ignoring polylogarithmic factors, they showed that is possible to answer queries in O(d/e 2) time with O(dn O(1/e2) ) space.
108|Finding planar regions in a terrain: In practice and with a guarantee|We consider the problem of computing large connected regions in a triangulated terrain of size n for which the normals of the triangles deviate by at most some small fixed angle. In previous work an exact near-quadratic algorithm was presented, but only a heuristic implementation with no guarantee was practicable. We present a new approximation algorithm for the problem which runs in O(n/o 2) time and—apart from giving a guarantee on the quality of the produced solution—has been implemented and shows good performance on real data sets representing fracture surfaces consisting of around half a million triangles. Further we present a simple approximation algorithm for a related problem: given a set of n points in the plane, determine the placement of the unit disk which contains most points. This algorithm runs in linear time as well.
109|Fast Incremental Proximity Search in Large Graphs |In this paper we investigate two aspects of ranking problems on large graphs. First, we augment the deterministic pruning algorithm in Sarkar and Moore (2007) with sampling techniques to compute approximately correct rankings with high probability under random walk based proximity measures at query time. Second, we prove some surprising locality properties of these proximity measures by examining the short term behavior of random walks. The proposed algorithm can answer queries on the fly without caching any information about the entire graph. We present empirical results on a 600, 000 node author-word-citation graph from the Citeseer domain on a single CPU machine where the average query processing time is around 4 seconds. We present quantifiable link prediction tasks. On most of them our techniques outperform Personalized Pagerank, a well-known diffusion based proximity measure. 1.
110|The link-prediction problem for social networks|Given a snapshot of a social network, can we infer which new interactions among its members are likely to occur in the near future? We formalize this question as the link-prediction problem, and we develop approaches to link prediction based on measures for analyzing the “proximity” of nodes in a network. Experiments on large co-authorship networks suggest that information about future interactions can be extracted from network topology alone, and that fairly subtle measures for detecting node proximity can outperform more direct measures. 
111|Graph sparsification by effective resistances |We present a nearly-linear time algorithm that produces high-quality sparsifiers of weighted graphs. Given as input a weighted graph G = (V, E, w) and a parameter o&gt; 0, we produce a weighted subgraph H = (V,  ˜ E, ˜w) of G such that |  ˜ E |  = O(n log n/o 2) and for all vectors x ? R V (1 - o) ? (x(u)  - x(v)) 2 wuv = ? (x(u)  - x(v)) 2 ˜wuv = (1 + o) ? (x(u)  - x(v)) 2 wuv. (1) uv?E uv ?  ˜ E This improves upon the sparsifiers constructed by Spielman and Teng, which had O(n log c n) edges for some large constant c, and upon those of Benczúr and Karger, which only satisfied (1) for x ? {0, 1} V. We conjecture the existence of sparsifiers with O(n) edges, noting that these would generalize the notion of expander graphs, which are constant-degree sparsifiers for the complete graph. A key ingredient in our algorithm is a subroutine of independent interest: a nearly-linear time algorithm that builds a data structure from which we can query the approximate effective resistance between any two vertices in a graph in O(log n) time. uv?E
112|Dynamic Personalized Pagerank in Entity-Relation Graphs|Extractors and taggers turn unstructured text into entityrelation (ER) graphs where nodes are entities (email, paper, person, conference, company) and edges are relations (wrote, cited, works-for). Typed proximity search of the form type=person NEAR company~&amp;quot;IBM&amp;quot;, paper~&amp;quot;XML &amp;quot; is an increasingly useful search paradigm in ER graphs. Proximity search implementations either perform a Pagerank-like computation at query time, which is slow, or precompute, store and combine per-word Pageranks, which can be very expensive in terms of preprocessing time and space. We present HubRank, a new system for fast, dynamic, spaceefficient proximity searches in ER graphs. During preprocessing, HubRank computes and indexes certain “sketchy” random walk fingerprints for a small fraction of nodes, carefully chosen using query log statistics. At query time, a small “active ” subgraph is identified, bordered by nodes with indexed fingerprints. These fingerprints are adaptively loaded to various resolutions to form approximate personalized Pagerank vectors (PPVs). PPVs at remaining active nodes are now computed iteratively. We report on experiments with CiteSeer’s ER graph and millions of real Cite-Seer queries. Some representative numbers follow. On our testbed, HubRank preprocesses and indexes 52 times faster than whole-vocabulary PPV computation. A text index occupies 56 MB. Whole-vocabulary PPVs would consume 102 GB. If PPVs are truncated to 56 MB, precision compared to true Pagerank drops to 0.55; in contrast, HubRank has precision 0.91 at 63 MB. HubRank’s average query time is 200–300 milliseconds; query-time Pagerank computation takes 11 seconds on average.
113|A tractable approach to finding closest truncated-commute-time neighbors in large graphs|Recently there has been much interest in graph-based learning, with applications in collaborative filtering for recommender networks, link prediction for social networks and fraud detection. These networks can consist of millions of entities, and so it is very important to develop highly efficient techniques. We are especially interested in accelerating random walk approaches to compute some very interesting proximity measures of these kinds of graphs. These measures have been shown to do well empirically (Liben-Nowell &amp; Kleinberg, 2003; Brand, 2005). We introduce a truncated variation on a well-known measure, namely commute times arising from random walks on graphs. We present a very novel algorithm to compute all interesting pairs of approximate nearest neighbors in truncated commute times, without computing it between all pairs. We show results on both simulated and real graphs of size up to 100, 000 entities, which indicate near-linear scaling in computation time. 1
114|Reversible Markov Chains|ly, call f : [0; 1) ! [0; 1) completely monotone  (CM) if there is a non-negative measure  on [0; 1) such that  f(t) =  Z 1  0  e  \Gamma`t  (d`); 0  t ! 1: (41) Our applications will use only the special case of a finite sum  f(t) =  X  m  am e  \Gamma` m t  ; for some am ; ` m  0: (42) but finiteness plays no essential role. If f is CM then (provided they exist) so are  \Gammaf  0  (t)   F (t) j  Z 1  t  f(s)ds (43) A probability distribution  on [0; 1) is called CM if its tail distribution function   F (t) = (t; 1) is CM; equivalently, if its density function f is CM (except here we must in the general case allow the possibility f(0) =  1). In more probabilistic language,  is CM iff it can be expressed as the distribution of =, where  and are independent random variables such that   has exponential(1) distribution;  ? 0: (44) 19  Given a CM function or distribution, the spectral gap   0 can be defined consistently by   = infft ? 0 : [0; t] ? 0g in setting (41) = minf`m g in setting...
115|Abstract Navigating nets: Simple algorithms for proximity search |We present a simple deterministic data structure for maintaining a set S of points in a general metric space, while supporting proximity search (nearest neighbor and range queries) and updates to S (insertions and deletions). Our data structure consists of a sequence of progressively finer ?-nets of S, with pointers that allow us to navigate easily from one scale to the next. We analyze the worst-case complexity of this data structure in terms of the “abstract dimensionality ” of the metric S. Our data structure is extremely efficient for metrics of bounded dimension and is essentially optimal in a certain model of distance computation. Finally, as a special case, our approach improves over one recently devised by Karger and Ruhl [KR02]. 1
116|Navigating nets: Simple algorithms for proximity search |We present a simple deterministic data structure for maintaining a set S of points in a general metric space, while supporting proximity search (nearest neighbor and range queries) and updates to S (insertions and deletions). Our data structure consists of a sequence of progressively finer ?-nets of S, with pointers that allow us to navigate easily from one scale to the next. We analyze the worst-case complexity of this data structure in terms of the “abstract dimensionality ” of the metric S. Our data structure is extremely efficient for metrics of bounded dimension and is essentially optimal in a certain model of distance computation. Finally, as a special case, our approach improves over one recently devised by Karger and Ruhl [KR02]. 1
117|Probabilistic Proximity Searching Algorithms Based on Compact Partitions|The main bottleneck of the research in metric space searching  is the so-called curse of dimensionality, which makes the task of  searching some metric spaces intrinsically dicult, whatever algorithm  is used. A recent trend to break this bottleneck resorts to probabilistic  algorithms, where it has been shown that one can  nd 99% of the  relevant objects at a fraction of the cost of the exact algorithm. These  algorithms are welcome in most applications because resorting to metric  space searching already involves a fuzziness in the retrieval requirements.
118|M-tree: An Efficient Access Method for Similarity Search in Metric Spaces|A new access meth d, called M-tree, is proposed to organize and search large data sets from a generic &#034;metric space&#034;, i.e. whE4 object proximity is only defined by a distance function satisfyingth positivity, symmetry, and triangle inequality postulates. We detail algorith[ for insertion of objects and split management, whF h keep th M-tree always balanced - severalheralvFV split alternatives are considered and experimentally evaluated. Algorithd for similarity (range and k-nearest neigh bors) queries are also described. Results from extensive experimentationwith a prototype system are reported, considering as th performance criteria th number of page I/O&#039;s and th number of distance computations. Th results demonstratethm th Mtree indeed extendsth domain of applicability beyond th traditional vector spaces, performs reasonably well inhE[94Kv#E44V[vh data spaces, and scales well in case of growing files. 1 
119|Nearest Neighbor Queries|A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm. 
120|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
121|Efficient similarity search in sequence databases|We propose an indexing method for time sequences for processing similarity queries. We use the Discrete Fourier Transform (DFT) to map time sequences to the frequency domain, the crucial observation being that, for most sequences of practical interest, only the first few frequencies are strong. Another important observation is Parseval&#039;s theorem, which specifies that the Fourier transform preserves the Euclidean distance in the time or frequency domain. Having thus mapped sequences to a lower-dimensionality space by using only the first few Fourier coe cients, we use R-trees to index the sequences and e ciently answer similarity queries. We provide experimental results which show that our method is superior to search based on sequential scanning. Our experiments show that a few coefficients (1-3) are adequate to provide good performance. The performance gain of our method increases with the number and length of sequences. 
122|Efficient and Effective Querying by Image Content|In the QBIC (Query By Image Content) project we are studying methods to query large  on-line image databases using the images&#039; content as the basis of the queries. Examples of  the content we use include color, texture, and shape of image objects and regions. Potential  applications include medical (&#034;Give me other images that contain a tumor with a texture like this  one&#034;), photo-journalism (&#034;Give me images that have blue at the top and red at the bottom&#034;),  and many others in art, fashion, cataloging, retailing, and industry.  We describe a set of novel features and similarity measures allowing query by color, texture,  and shape of image object. We demonstrate the effectiveness of the QBIC system with normalized  precision and recall experiments on test databases containing over 1000 images and 1000  objects populated from commercially available photo clip art images, and of images of airplane  silhouettes. We also consider the efficient indexing of these features, specifically addre...
123|FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets|A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example&#039; type (which translates to a range query); the `all pairs&#039; query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret...
124|The R + -tree: A dynamic index for multidimensional objects|The problem of indexing multidimensional objects is considered. First, a classification of existing methods is given along with a discussion of the major issues involved in multidimensional data indexing. Second, a variation to Guttman’s R-trees (R +-trees) that avoids overlapping rectangles in intermediate nodes of the tree is introduced. Algorithms for searching, updating, initial packing and reorganization of the structure are discussed in detail. Finally, we provide analytical results indicating that R +-trees achieve up to 50 % savings in disk accesses compared to an R-tree when searching files of thousands of rectangles. 1
125|Content-based classification, search, and retrieval of audio|say that it belongs to the class of speech sounds or the class of applause sounds, where the system has previously been trained on other sounds in this class. I Acoustical/perceptual features: describing the sounds in terms of commonly understood physical characteristics such as brightness, pitch, and loudness. I Subjective features: describing the sounds using personal descriptive language. This requires training the system (in our case, by example) to understand the meaning of these descriptive terms. For example, a user might be looking for a “shimmering ” sound.
126|Density index and proximity search in large graphs|Given a large real-world graph where vertices are associated with labels, how do we quickly find interesting vertex sets according to a given query? In this paper, we study label-based proximity search in large graphs, which finds the top-k query-covering vertex sets with the smallest diameters. Each set has to cover all the labels in a query. Existing greedy al-gorithms only return approximate answers, and do not scale well to large graphs. We propose a novel framework, called gDensity, which uses density index and likelihood ranking to find vertex sets in an efficient and accurate manner. Promis-ing vertices are ordered and examined according to their likelihood to produce answers, and the likelihood calcula-tion is greatly facilitated by density indexing. Techniques such as progressive search and partial indexing are further proposed. Experiments on real-world graphs show the effi-ciency and scalability of gDensity.
128|LEDA: A Platform for Combinatorial and Geometric Computing|  We give an overview of the LEDA platform for combinatorial and geometric computing and an account of its development. We discuss our motivation for building LEDA and to what extent we have reached our goals. We also discuss some recent theoretical developments. This paper contains no new technical material. It is intended as a guide to existing publications about the system. We refer the reader also to our web-pages for more information.  
129|Evaluating Top-k Queries over Web-Accessible Databases|... In this article, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present a sequential algorithm for processing such queries, but observe that any sequential top-k query processing strategy is bound to require unnecessarily long query processing times, since web accesses exhibit high and variable latency. Fortunately, web sources can be probed in parallel, and each source can typically process concurrent requests, although sources may impose some restrictions on the type and number of probes that they are willing to accept. We adapt our sequential query processing technique and introduce an efficient algorithm that maximizes sourceaccess parallelism to minimize query response time, while satisfying source-access constraints. 
130|A Survey of Top-k Query Processing Techniques in Relational Database Systems |Efficient processing of top-k queries is a crucial requirement in many interactive environments that involve massive amounts of data. In particular, efficient top-k processing in domains such as the Web, multimedia search and distributed systems has shown a great impact on performance. In this survey, we describe and classify top-k processing techniques in relational databases. We discuss different design dimensions in the current techniques including query models, data access methods, implementation levels, data and query certainty, and supported scoring functions. We show the implications of each dimension on the design of the underlying techniques. We also discuss top-k queries in XML domain, and show their connections to relational approaches.
131|Minimal Probing: Supporting Expensive Predicates for Top-k Queries|This paper addresses the problem of evaluating ranked top-    queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries will be even more important: First, ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time sorted output, and must instead require per-object probe to evaluate. The current standard sort-merge framework for ranked queries cannot efficiently handle such predicates because it must completely probe all objects, before sorting and merging them to produce top-    answers. To minimize expensive probes, we thus develop the formal principle of &#034;necessary probes,&#034; which determines if a probe is absolutely required. We then propose Algorithm MPro which, by implementing the principle, is provably optimal with minimal probe cost. Further, we show that MPro can scale well and can be easily parallelized. Our experiments using both a real-estate benchmark database and synthetic datasets show that MPro enables significant probe reduction, which can be orders of magnitude faster than the standard scheme using complete probing.
132|Blinks: Ranked keyword searches on graphs|Query processing over graph-structured data is enjoying a growing number of applications. A top-k keyword search query on a graph nds the top k answers according to some ranking criteria, where each answer is a substructure of the graph containing all query keywords. Current techniques for supporting such queries on general graphs suffer from several drawbacks, e.g., poor worst-case performance, not taking full advantage of indexes, and high memory requirements. To address these problems, we propose BLINKS, a bi-level indexing and query processing scheme for top-k keyword search on graphs. BLINKS follows a search strategy with provable performance bounds, while additionally exploiting a bi-level index for pruning and accelerating the search. To reduce the index space, BLINKS partitions a data graph into blocks: The bilevel index stores summary information at the block level to initiate and guide search among blocks, and more detailed information for each block to accelerate search within blocks. Our experiments show that BLINKS offers orders-of-magnitude performance improvement over existing approaches.
133|Random texts exhibit Zipf&#039;s-law-like word frequency distribution|are scanned from a copy of the paper (apologize for the poor quality). It is shown that the distribution of word frequencies for randomly generated texts is very similar to Zipf’s law observed in natural languages such as the English. The facts that the frequency of occurrence of a word is almost an inverse power law function of its rank and the exponent of this inverse power law is very close to 1 are largely due to the transformation from the word’s length to its rank, which stretches an exponential function to a power law function. key words: statistical linguistics, Zipf’s law, power-law distribution, random texts. Zipf observed long time ago [1] that the distribution of word frequencies in English, if the words are aligned according to their ranks, is an inverse power law with the exponent very close to 1. In other words, if the most frequently occurring word appears in the text with the frequency P(1), the next most frequently occurring word has the frequency P(2), and the rank-r word has the frequency P(r), the frequency distribution is P(r)  = C ra, (1)
134|Walking in Facebook: A Case Study of Unbiased Sampling of OSNs|Abstract—With more than 250 million active users [1], Facebook (FB) is currently one of the most important online social networks. Our goal in this paper is to obtain a representative (unbiased) sample of Facebook users by crawling its social graph. In this quest, we consider and implement several candidate techniques. Two approaches that are found to perform well are the Metropolis-Hasting random walk (MHRW) and a re-weighted random walk (RWRW). Both have pros and cons, which we demonstrate through a comparison to each other as well as to the ”ground-truth ” (UNI- obtained through true uniform sampling of FB userIDs). In contrast, the traditional Breadth-First-Search (BFS) and Random Walk (RW) perform quite poorly, producing substantially biased results. In addition to offline performance assessment, we introduce onlineformal convergence diagnostics to assess sample quality during the data collection process. We show how these can be used to effectively determine when a random walk sample is of adequate size and quality for subsequent use (i.e., when it is safe to cease sampling). Using these methods, we collect the first, to the best of our knowledge, unbiased sample of Facebook. Finally, we use one of our representative datasets, collected through MHRW, to characterize several key properties of Facebook. IndexTerms—Measurements,onlinesocial networks,Facebook, graph sampling, crawling, bias. I.
135|Ease: an effective 3-in-1 keyword search method for unstructured, semi-structured and structured data|Conventional keyword search engines are restricted to a given data model and cannot easily adapt to unstructured, semistructured or structured data. In this paper, we propose an efficient and adaptive keyword search method, called EASE, for indexing and querying large collections of heterogenous data. To achieve high efficiency in processing keyword queries, we first model unstructured, semi-structured and structured data as graphs, and then summarize the graphs and construct graph indices instead of using traditional inverted indices. We propose an extended inverted index to facilitate keyword-based search, and present a novel ranking mechanism for enhancing search effectiveness. We have conducted an extensive experimental study using real datasets, and the results show that EASE achieves both high search efficiency and high accuracy, and outperforms the existing approaches significantly.
136|Motif search in graphs: application to metabolic networks |The classic view of metabolism as a collection of metabolic pathways is being questioned with the currently available possibility of studying whole networks. Novel ways of decomposing the network into modules and motifs that could be considered as the building blocks of a network are being sug-1 gested. In this work, we introduce a new definition of motif in the context of metabolic networks. Unlike in previous works on (other) biochemical networks, this definition is not based only on topological features. We propose instead to use an alternative definition based on the functional nature of the components that form the motif, which we call a reaction motif. After introducing a formal framework motivated by biological considerations, we present complexity results on the problem of searching for all occurrences of a reaction motif in a network, and introduce an algorithm that is fast in practice in most situations. We then show an initial application to the study of pathway evolution. Finally, we give some general features of the observed number of occurrences in order to highlight some structural features of metabolic networks. 1
137|Forming teams: an analytical approach|The selection of multi-functional teams is a key issue in problem solving. Currently there are no papers in the literature that discuss analytical approaches to forming teams. Furthermore, no comprehensive model exists to prioritize team membership based on customer requirements or product characteristics. To deal with the underlying complexities of the team selection process, a methodology for team formation is developed. The methodology is based on the Analytical Hierarchy Process (AHP) approach and the Quality Function Deployment (QFD) method. A QFD planning matrix is used to organize the factors considered in the team selection. The importance measure for each team member is determined with the AHP approach. A mathematical pro-gramming model is developed to determine the composition of a team. The methodology developed in this paper is tested by the selection of teams in concurrent engineering. A detailed discussion of the model implementation and how to reduce the number of comparisons in the AHP process is presented. Possible modifications of the model to include ‘‘soft factors’’, i.e., leadership, morale, personalities of group members, group values and so on are also discussed. 1.
138|Fast algorithms for top-k personalized pagerank queries|In entity-relation (ER) graphs (V,E), nodes V represent typed entities and edges E represent typed relations. For dynamic personalized PageRank queries, nodes are ranked by their steady-state probabilities obtained using the standard random surfer model. In this work, we propose a framework to answer top-k graph conductance queries. Our top-k ranking technique leads to a 4 × speedup, and overall, our system executes queries 200–1600 × faster than whole-graph PageRank. Some queries might contain hard predicates i.e. predicates that must be satisfied by the answer nodes. E.g., we may seek authoritative papers on public key cryptography, but only those written during 1997. We extend our system to handle hard predicates. Our system achieves these substantial query speedups while consuming only 10–20 % of the space taken by a regular text index.
139|Keyword Search in Graphs: Finding r-cliques |Keyword search over a graph finds a substructure of the
140|Querying Communities in Relational Databases |Abstract — Keyword search on relational databases provides users with insights that they can not easily observe using the traditional RDBMS techniques. Here, an l-keyword query is specified by a set of l keywords, {k1, k2,  ·  ·  · , kl}. It finds how the tuples that contain the keywords are connected in a relational database via the possible foreign key references. Conceptually, it is to find some structural information in a database graph, where nodes are tuples and edges are foreign key references. The existing work studied how to find connected trees for an l-keyword query. However, a tree may only show partial information about how those tuples that contain the keywords are connected. In this paper, we focus on finding communities for an l-keyword query. A community is an induced subgraph that contains all the l-keywords within a given distance. We propose new efficient algorithms to find all/top-k communities which consume small memory, for an l-keyword query. For topk l-keyword queries, our algorithm allows users to interactively enlarge k at run time. We conducted extensive performance studies using two large real datasets to confirm the efficiency of our algorithms. I.
141|Multi-skill Collaborative Teams based on Densest Subgraphs|We consider the problem of identifying a team of skilled individuals for collaboration, in the presence of a social network. Each node in the input social network may be an expert in one or more skills- such as theory, databases or data mining. The edge weights specify the affinity or collaborative compatibility between respective nodes. Given a project that requires a set of specified number of skilled individuals in each area of expertise, the goal is to identify a team that maximizes the collaborative compatibility. For example, the requirement may be to form a team that has at least three databases experts and at least two theory experts. We explore team formation where the collaborative compatibility objective is measured as the density of the induced subgraph on selected nodes. The problem of maximizing density is NP-hard even when the team requires a certain number of individuals of only one specific skill. We present a 3-approximation algorithm that improves upon a naive extension of the previously known algorithm for densest at least k subgraph problem. We further show how the same approximation can be extended to a special case of multiple skills as well. Our problem generalizes the formulation studied by Lappas et al. [KDD ’09]. Further, they measured collaborative compatibility in terms of diameter and the spanning tree costs. Our density based objective also turns out to be more robust in certain aspects. Experiments are performed on a crawl of the DBLP graph where individuals can be skilled in at most four areas- theory, databases, data mining, and artificial intelligence. In addition to our main algorithm, we also present heuristic extensions to trade off between the size of the solution and its induced density. These density-based algorithms outperform the diameter-based objective on several metrics for assessing the collaborative compatibility of teams. The solutions suggested are also intuitively meaningful and scale well with the increase in the number of skilled individuals required. 
142|Finding Approximate and Constrained Motifs in Graphs |Abstract. One of the emerging topics in the analysis of biological networks is the inference of motifs inside a network. In the context of metabolic network analysis, a recent approach introduced in [14], represents the network as a vertex-colored graph, while a motif M is represented as a multiset of colors. An occurrence of a motif M in a vertexcolored graph G is a connected induced subgraph of G whose vertex set is colored exactly as M. We investigate three different variants of the initial problem. The first two variants, Min-Add and Min-Substitute, deal with approximate occurrences of a motif in the graph, while the third variant, Constrained Graph Motif (or CGM for short), constrains the motif to contain a given set of vertices. We investigate the classical and parameterized complexity of the three problems. We show that Min-Add and Min-Substitute are NP-hard, even when M is a set, and the graph is a tree of degree bounded by 4 in which each color appears at most twice. Moreover, we show that Min-Substitute is in FPT when parameterized by the size of M. Finally, we consider the parameterized complexity of the CGM problem, and we give a fixed-parameter algorithm for graphs of bounded treewidth, while we show that the problem is W [2]-hard, even if the input graph has diameter 2. 1
143|Distance Based Indexing for String Proximity Search|In many database applications involving string data, it is common to have near neighbor queries (asking for strings that are similar to a query string) or nearest neighbor queries (asking for strings that are most similar to a query string). The similarity between strings is defined in terms of a distance function determined by the application domain. The most popular string distance measures are based on (a weighted) count of (i) character edit or (ii) block edit operations to transform one string into the other. Examples include the Levenshtein edit distance and the recently introduced compression distance. The main goal
144|When Is &#034;Nearest Neighbor&#034; Meaningful?|. We explore the effect of dimensionality on the &#034;nearest neighbor  &#034; problem. We show that under a broad set of conditions (much  broader than independent and identically distributed dimensions), as dimensionality  increases, the distance to the nearest data point approaches  the distance to the farthest data point. To provide a practical perspective,  we present empirical results on both real and synthetic data sets  that demonstrate that this effect can occur for as few as 10-15 dimensions.  These results should not be interpreted to mean that high-dimensional  indexing is never meaningful; we illustrate this point by identifying some  high-dimensional workloads for which this effect does not occur. However,  our results do emphasize that the methodology used almost universally  in the database literature to evaluate high-dimensional indexing  techniques is flawed, and should be modified. In particular, most such  techniques proposed in the literature are not evaluated versus simple...
145|Fast Similarity Search in the Presence of Noise, Scaling, and Translation in Time-Series Databases|We introduce a new model of similarity of time sequences that captures the intuitive notion that two sequences should be considered similar if they have enough non-overlapping time-ordered pairs of subsequences thar are similar. The model allows the amplitude of one of the two sequences to be scaled by any suitable amount and its offset adjusted appropriately. Two subsequences are considered similar if one can be enclosed within an envelope of a specified width drawn around the other. The model also allows non-matching gaps in the matching subsequences. The matching subsequences need not be aligned along the time axis. Given this model of similarity,we present fast search techniques for discovering all similar sequences in a set of sequences. These techniques can also be used to find all (sub)sequences similar to a given sequence. We applied this matching system to the U.S. mutual funds data and discovered interesting matches.
146|A Database Index to Large Biological Sequences|We present an approach to searching genetic DNA sequences using an adaptation of the suffix tree data structure deployed on the general purpose persistent Java platform, PJama. Our implementation technique is novel, in that it allows us to build suffix trees on disk for arbitrarily large sequences, for instance for the longest human chromosome consisting of 263 million letters. We propose to use such indexes as an alternative to the current practice of serial scanning. We describe our tree creation algorithm, analyse the performance of our index, and discuss the interplay of the data structure with object store architectures. Early measurements are presented.
147|A New Approach to Sequence Comparison: Normalized Sequence Alignment|The Smith-Waterman algorithm for local sequence alignment is one of the most important techniques in computational molecular biology. This ingenious dynamic programming approach was designed to reveal the highly conserved fragments by discarding poorly conserved initial and terminal segments. However, the existing notion of local similarity has a serious flaw: it does not discard poorly conserved intermediate segments. The Smith-Waterman algorithm finds the local alignment with maximal score but it is unable to find local alignment with maximum degree of similarity (e.g., maximal percent of matches). Moreover, there is still no efficient algorithm that answers the following natural question: do two sequences share a (sufficiently long) fragment with more than 70% of similarity? As a result, the local alignment sometimes produces a mosaic of well conserved fragments artificially connected by poorly conserved or even unrelated fragments. This may lead to problems in comparison of long genomic seque...
149|The principles of psychology|This Thesis is brought to you for free and open access. It has been accepted for inclusion in University Honors Theses by an authorized administrator of
150|A Model of Saliency-based Visual Attention for Rapid Scene Analysis|A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
151|The Laplacian Pyramid as a Compact Image Code| We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding. A
152|Shiftable Multi-scale Transforms|Orthogonal wavelet transforms have recently become a popular representation for multiscale signal and image analysis. One of the major drawbacks of these representations is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal, and in two dimensions, rotations of the input signal. We formalize these problems by defining a type of translation invariance that we call &#034;shiftability&#034;. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be considered in the context of other domains, particularly orientation and scale. We explore &#034;jointly shiftable&#034; transforms that are simultaneously shiftable in more than one domain. Two examples of jointly shiftable transforms are designed and implemented: a one-dimensional tran...
153|Preattentive texture discrimination with early vision mechanisms|mechanisms
154|Sustained and transient components of focal visual attention|Abstract-Human observers fixated the center of a search array and were required to discriminate the color of an odd target if it was present. The array consisted of horizontal or vertical black or white bars. In the simple case, only orientation was necessary to define the odd target, whereas in the conjunctive case, both orientation and color were necessary. A cue located at the critical target position was either visible all the time (sustained cuing) or it appeared at a short variable delay before the array presentation (transient cuing). Sustained visual cuing enhanced perception greatly in the conjunctive, but not in the simple condition. Perception of the odd target in the conjunctive display was improved even further by transient cuing, and peak discrimination performance occurred if the cue preceded the target array by 70-150 msec. Longer delays led to a marked downturn in performance. Control experiments indicated that this transient attentional component was independent of the observers ’ prior knowledge of target position and was not subject to voluntary control. We provide evidence to suggest hat the transient component does not originate at the earliest stages of visual processing, since it could not be extended in duration by flickering the cue, nor did it require a local sensory transient o trigger its onset. Neither the variation in retinal eccentricity nor changing the paradigm to a vernier acuity task altered the basic pattern of results. Our findings indicate the existence of a sustained and a transient component of attention, and we hypothesize that of the two, the transient component is operative at an earlier stage of visual cortical processing. Focal attention Visual search Pattern recognition Vernier acuity
155|A Comparison of Feature Combination Strategies for Saliency-Based Visual Attention Systems|Bottom-up or saliency-based visual attention allows primates to detect non-specific conspicuous targets in cluttered scenes. A classical metaphor, derived from electrophysiological and psychophysical studies, describes attention as a rapidly shiftable &#034;spotlight&#034;. The model described here reproduces the attentional scanpaths of this spotlight: Simple multi-scale &#034;feature maps&#034; detect local spatial discontinuities in intensity, color, orientation or optical flow, and are combined into a unique &#034;master&#034; or &#034;saliency&#034; map. The saliency map is sequentially scanned, in order of decreasing saliency, by the focus of attention. We study the problem of combining feature maps, from different visual modalities and with unrelated dynamic ranges (such as color and motion), into a unique saliency map. Four combination strategies are compared using three databases of natural color images: (1) Simple normalized summation, (2) linear combination with learned weights, (3) global non-linear normalization...
156|An Active Vision Architecture based on Iconic Representations|Active vision systems have the capability of continuously interacting with the environment. The rapidly changing environment of such systems means that it is attractive to replace static representations with visual routines that compute information on demand. Such routines place a premium on image data structures that are easily computed and used. The purpose of this paper is to propose a general active vision architecture based on efficiently computable iconic representations. This architecture employs two primary visual routines, one for identifying the visual image near the fovea (object identification), and another for locating a stored prototype on the retina (object location). This design allows complex visual behaviors to be obtained by composing these two routines with different parameters. The iconic representations are comprised of high-dimensional feature vectors obtained from the responses of an ensemble of Gaussian derivative spatial filters at a number of orientations and...
157|Clustered intrinsic connections in cat visual cortex|The intrinsic connections of the cortex have long been known to run vertically, across the cortical layers. In the present study we have found that individual neurons in the cat primary visual cortex can communicate over suprisingly long distances horizontally (up to 4 mm), in directions parallel to the cortical surface. For all of the cells having widespread projections, the collaterals within their axonal fields were distributed in repeating clusters, with an average periodicity of 1 mm. This pattern of extensive clustered projections has been revealed by combining the techniques of intracellular recording and injection of horseradish peroxidase with three-dimensional computer graphic reconstructions. The clustering pattern was most apparent when the cells were rotated to present a view parallel to the cortical surface. The pattern was observed in more than half of the pyramidal and spiny stellate cells in the cortex and was seen in all cortical layers. In our sample, cells made distant connections within their own layer and/or within another layer. The axon of one cell had clusters covering the same area in two layers, and the clusters in the deeper layer were located under those in the upper layer, suggesting a relationship between the clustering phenomenon and columnar cortical architecture. Some pyramidal cells did not project into the white matter,
158|Overcomplete steerable pyramid filters and rotation invariance|A given (overcomplete) discrete oriented pyramid may be converted into a steerable pyramid by interpolation. We present a technique for deriving the optimal interpolation functions (otherwise called steering coefficients). The proposed scheme is demonstrated on a computationally efficient oriented pyramid, which is a variation on the Burt and Adelson pyramid. We apply the generated steerable pyramid to orientation-invarianttexture analysis to demonstrate its excellent rotational isotropy. High classification rates and precise rotation identification are demonstrated. 1
159|Functional anatomy of macaque striate cortex. V. Spatial frequency|Macaque monkeys were shown retinotopically-specific vi-sual stimuli during %-2-deoxy-&amp;glucose (DG) infusion in a study of the retinotopic organization of primary visual cortex (Vl). In the central half of VI, the cortical magnification was found to be greater along the vertical than along the hori-zontal meridian, and overall magnification factors appeared to be scaled proportionate to brain size across different species. The cortical magnification factor (CMF) was found to reach a maximum of about 15 mm/deg at the represen-tation of the fovea, at a point of acute curvature in the Vl-V2 border. We find neither a duplication nor an overrepre-sentation of the vertical meridian. The magnification factor did not appear to be doubled in a direction perpendicular to the ocular dominance strips; it may not be increased at all. The DG borders in parvorecipient layer 4Cb were found to
160|Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque|We studied the effect of eye position on the light-sensitive, memory, and saccade-related activities of neurons of the lateral intraparietal area and area 7a in the posterior parietal cortex of rhesus monkeys. A majority of the cells showed significant effects of eye position, for each of the 3 types of response. The direction tuning of the light-sensitive, memory and saccade responses did not change with eye position but the magnitude of the response did. Since previous work showed a similar effect for the light-sensitive response of area 7a neurons (Andersen and Mountcastle, 1983; Ander-sen et al., 1985b), the present results indicate that this mod-ulating effect of eye position may be a general one, as it is found in 3 types of responses in 2 cortical areas. Gain fields were mapped by measuring the effect of eye position on the magnitude of the response at 9 different eye positions for
161|Incorporating Prior Information in Machine Learning by Creating Virtual Examples|One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training set size. We show that in some contexts, this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well-motivated. The process of creating virtual examples in real world pattern recognition tasks is highly non-trivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea.  1 Learning from Examples  Recently, machine learning techniques have become increasingly popular as an alternative to knowledge-based approaches to artificial intelligence pro...
162|Control of selective visual attention: Modelling the “where” pathway|Intermediate and higher vision processes require selection of a sub-set of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called &#034;focus of at-tention &#034; which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functional-ity of biological vision but also to be essential for the understanding of complex scenes in machine vision. 1 Introduction: &#034;What &#034; and &#034;Where &#034; In Vision It is a generally accepted fact that the computations of early vision are massively parallel operations, i.e., applied in parallel to all parts of the visual field. This high degree of parallelism cannot be sustained in in~ermediate and higher vision because
163|Multimodal integration for the representation of space in the posterior parietal cortex|The posterior parietal cortex has long been considered an a`ssociation&#039;area that combines information from di¡erent sensory modalities to form a cognitive representation of space. However, until recently little has been known about the neural mechanisms responsible for this important cognitive process. Recent experi-ments from the author&#039;s laboratory indicate that visual, somatosensory, auditory and vestibular signals are combined in areas LIP and 7a of the posterior parietal cortex. The integration of these signals can repre-sent the locations of stimuli with respect to the observer and within the environment. Area MSTd combines visual motion signals, similar to those generated during an observer&#039;s movement through the environment, with eye-movement and vestibular signals. This integration appears to play a role in specifying the path on which the observer is moving. All three cortical areas combine di¡erent modalities into common spatial frames by using a gain-¢eld mechanism. The spatial representations in areas LIP and 7a appear to be important for specifying the locations of targets for actions such as eye movements or reaching; the spatial representation within area MSTd appears to be important for navigation and the perceptual stabi-lity of motion signals. 1.
164|Withdrawing attention at little or no cost: detection and discrimination tasks. Percept Psychophys|Weused a concurrent-task paradigm to investigate the attentional cost of simple visual tasks. As in earlier studies, we found that detecting a unique orientation in an array of oriented elements (&#034;pop-out&#034;) carries little or no attentional cost. Surprisingly, this is true at all levels of performance and holds even when pop-out is barely discriminable. Wediscuss this finding in the context of our previous re-port that the attentional cost of stimulus detection is strongly influenced by the presence and nature of other stimuli in the display (Braun, 1994b). For discrimination tasks, we obtained a similarly mixed outcome: Discrimination of letter shape carried a high attentional cost whereas discrimination of color and orientation did not. Taken together, these findings lead us to modify our earlier position on the at-tentional costs of detection and discrimination tasks (Sagi &amp; Julesz, 1985). We now believe that ob-servers enjoy a significant degree of &#034;ambient &#034; visual awareness outside the focus of attention, per-mitting them to both detect and discriminate certain visual information. We hypothesize that the information in question is selected by a competition for saliency at the level of early vision. It has long been recognized that visual perception is in-fluenced by the observer&#039;s attentional state (Helmholtz, 1850/1962; James, 1890/1981). Psychophysical studies show that attention enhances visual sensitivity for stim-uli that are relevant to the observer and his/her behavior
165|Searching Distributed Collections With Inference Networks|The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, discusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.  
166|Okapi at TREC-3|this document length correction factor is #global&#034;: it is added at the end, after the weights for the individual terms have been summed, and is independentofwhich terms match.
167|The INQUERY Retrieval System|As larger and more heterogeneous text databases become available, information retrieval research will depend on the development of powerful, efficient and flexible retrieval engines. In this paper, we describe a retrieval system (INQUERY) that is based on a probabilistic retrieval model and provides support for sophisticated indexing and complex query formulation. INQUERY has been used successfully with databases containing nearly 400,000 documents. 1 Introduction  The increasing interest in sophisticated information retrieval (IR) techniques has led to a number of large text databases becoming available for research. The size of these databases, both in terms of the number of documents in them, and the length of the documents that are typically full text, has presented significant challenges to IR researchers who are used to experimenting with two or three thousand document abstracts. In order to carry out research with different types of text representations, retrieval models, learni...
168|Evaluation of an Inference Network-Based Retrieval Model|The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models. The performance of a retrieval system based on the inference network model is evaluated and compared to performance with conventional retrieval models,
169|Latent Semantic Indexing (LSI) and TREC-2  (1994) |this paper. The &#034;ltc&#034; weights were computed on this matrix. 3.2 SVD analysis
170|Information Retrieval Systems for Large Document Collections|Practical information retrieval systems must manage large volumes of data, often divided into several collections that may be held on separate machines. Techniques for locating matches to queries must therefore consider identification of probable collections as well as identification of documents that are probable answers. Furthermore, the large amounts of data involved motivates the use of compression, but in a dynamic environment compression is problematic, because as new text is added the compression model slowly becomes inappropriate. In this paper we describe solutions to both of these problems. We show that use of centralised blocked indexes can reduce overall query processing costs in a multi-collection environment, and that careful application of text compression techniques allow collections to grow by several orders of magnitude without recompression becoming necessary. 1 Introduction  Practical information systems are required to store many gigabytes of data while supporting ...
171|Distributed Indexing: A Scalable Mechanism for Distributed Information Retrieval|Despite blossoming computer network bandwidths and the emergence of hypertext and CD-ROM databases, little progress has been made towards uniting the world&#039;s library-style bibliographic databases. While a few advanced distributed retrieval systems can broadcast a query to hundreds of participating databases, experience shows that local users almost always clog library retrieval systems. Hence broadcast remote queries will clog nearly every system. The premise of this work is that broadcast-based systems do not scale to world-wide systems. This project describes an indexing scheme that will permit thorough yet efficient searches of millions of retrieval systems. Our architecture will work with an arbitrary number of indexing companies and information providers, and, in the market place, could provide economic incentive for cooperation between database and indexing services. We call our scheme distributed indexing, and believe it will help researchers disseminate and locate both publishe...
172|TREC-3 Ad-Hoc, Routing Retrieval and Thresholding Experiments using PIRCS|The PIRCS retrieval system has been upgraded in TREC-3 to handle the full English collections of 2 GB in an efficient manner. For ad-hoc retrieval, we use recurrent spreading of activation in our network to implement query learning and expansion based on the best-ranked subdocuments of an initial retrieval. We also augment our standard retrieval algorithm with a soft-Boolean component. For routing, we use learning from signal-rich short documents or subdocument segments. For the optional thresholding experiment, we tried two approaches to transforming retrieval status values (RSV&#039;s) so that they could be used to partition documents into retrieved and nonretrieved sets. The first method normalizes RSV&#039;s using a query self-retrieval score. The second, which requires training data, uses logistic regression to convert RSV&#039;s into estimates of probability of relevance. Overall, our results are highly competitive with those of other participants. 1. INTRODUCTION  PIRCS is an experimental info...
173|Searching in Metric Spaces by Spatial Approximation|We propose a new data structure to search in metric spaces. A metric space is formed by a collection of objects and a distance function defined among them, which satisfies the triangle inequality. The goal is, given a set of objects and a query, retrieve those objects close enough to the query. The complexity measure is the number of distances computed to achieve this goal. Our data structure, called sa-tree (&#034;spatial approximation tree&#034;), is based on approaching spatially the searched objects, that is, getting closer and closer to them, rather than the classical divide-and-conquer approach of other data structures. We analyze our method and show that the number of distance evaluations to search among n objects is sublinear. We show experimentally that the sa-tree is the best existing technique when the metric space is hard to search or the query has low selectivity. These are the most important unsolved cases in real applications. As a practical advantage, our data structure is one of the few that do not need to tune parameters, which makes it appealing for use by non-experts.
174|Locally Lifting the Curse of Dimensionality for Nearest Neighbor Search (Extended Abstract)  (1999) |We consider the problem of nearest neighbor search in the Euclidean hypercube [ 1, +1]^d with uniform distributions, and the additional natural assumption that the nearest neighbor is located within a constant fraction R of the maximum interpoint distance in this space, i.e. within distance 2R&amp;radic;d of the query. We introduce the idea of aggressive pruning and give a family of practical algorithms, an idealized analysis, and describe experiments. Our main result is that search complexity measured in terms of d-dimensional inner product operations, is i) strongly sublinear with respect to the data set size n for moderate R, ii) asymptotically, and as a practical matter, independent of dimension. Given a random data set, a random query within distance 2R&amp;radic;d of some database element, and a randomly constructed data structure, the search succeeds with a specified probability, which is a parameter of the search algorithm. On average a search performs...
175|A Probabilistic Spell for the Curse of Dimensionality|Range searches in metric spaces can be very difficult if the  space is &#034;high dimensional&#034;, i.e. when the histogram of distances has  a large mean and/or a small variance. This so-called &#034;curse of dimensionality  &#034;, well known in vector spaces, is also observed in metric spaces.
176|An Effective Clustering Algorithm to Index High Dimensional Metric Spaces |A metric space consists of a collection of objects and a distance function defined among them, which satisfies the triangular inequality. The goal is to preprocess the set so that, given a set of objects and a query, retrieve those objects close enough to the query. The number of distances computed to achieve this goal is the complexity measure. The problem is very difficult in the so-called high-dimensional metric spaces, where the histogram of distances has a large mean and a small variance. A recent survey on methods to index metric spaces has shown that the so-called clustering algorithms are better suited than their competitors, pivotbased  algorithms, to cope with high-dimensional metric spaces. In this paper we present a new clustering method that achieves much better performance than all the existing data structures. We present analytical and experimental results that support our claims and that give the users the tuning parameters to make optimal use of this data structure.
177|Approximate similarity queries: a survey|We review the major paradigms for similarity queries, in particular those that allow approximate results. We propose an original classification schema which easily allows existing approaches to be compared along several independent coordinates, such as quality of results, error metrics, and user interaction. 1
178|Efficient Term Proximity Search with Term-Pair Indexes |There has been a large amount of research on early termination techniques in web search and information retrieval. Such techniques return the top-k documents without scanning and evaluating the full inverted lists of the query terms. Thus, they can greatly improve query processing efficiency. However, only a limited amount of efficient top-k processing work considers the impact of term proximity, i.e., the distance between term occurrences in a document, which has recently been integrated into a number of retrieval models to improve effectiveness. In this paper, we propose new early termination techniques for efficient query processing for the case where term proximity is integrated into the retrieval model. We propose new index structures based on a term-pair index, and study new document retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Experimental results on large-scale data sets show that our techniques can significantly improve the efficiency of query processing.
179|Managing Gigabytes: Compressing and Indexing Documents and Images - Errata|&gt; ! &#034;GZip&#034; page 64, Table 2.5, line &#034;progp&#034;: &#034;43,379&#034; ! &#034;49,379&#034; page 68, Table 2.6: &#034;Mbyte/sec&#034; ! &#034;Mbyte/min&#034; twice in the body of the table, and in the caption &#034;Mbyte/second&#034; ! &#034;Mbyte/minute&#034;  page 70, para 4, line 5: &#034;Santos&#034; ! &#034;Santis&#034; page 71, line 11: &#034;Fiala and Greene (1989)&#034; ! &#034;Fiala and Green (1989)&#034;  Chapter Three  page 89, para starting &#034;Using this method&#034;, line 2: &#034;hapax legomena &#034; !  &#034;hapax legomenon &#034; page 96, line 5: &#034;a such a&#034; ! &#034;such a&#034; page 98, line 6: &#034;shows that in fact none is an answer to this query&#034; !  &#034;shows that only document 2 is an answer to this query&#034; page 106, para 3, line 9: &#034;the bitstring in Figure 3.7b&#034; ! &#034;the bitstring in Figure 3.7c&#034; page 107, Figure 3.7: The coding shown in part (c) cannot be decoded ambiguously. For example, the sequence &#034;1010 0000 0001 0000
180|Optimal Aggregation Algorithms for Middleware|Assume that each object in a database has m grades, or scores, one for each of m attributes. For example, an object can have a color grade, that tells how red it is, and a shape grade, that tells how round it is. For each attribute, there is a sorted list, which lists each object and its grade under that attribute, sorted by grade (highest grade first). There is some monotone aggregation function, or combining rule, such as min or average, that combines the individual grades to obtain an overall grade. To determine the top k objects (that have the best overall grades), the naive algorithm must access every object in the database, to find its grade under each attribute. Fagin has given an algorithm (“Fagin’s Algorithm”, or FA) that is much more efficient. For some monotone aggregation functions, FA is optimal with high probability in the worst case. We analyze an elegant and remarkably simple algorithm (“the threshold algorithm”, or TA) that is optimal in a much stronger sense than FA. We show that TA is essentially optimal, not just for some monotone aggregation functions, but for all of them, and not just in a high-probability worst-case sense, but over every database. Unlike FA, which requires large buffers (whose size may grow unboundedly as the database size grows), TA requires only a small, constant-size buffer. TA allows early stopping, which yields, in a precise sense, an approximate version of the top k answers. We distinguish
181|Combining fuzzy information from multiple systems (Extended Abstract)  (1996) |In a traditional database system, the result of a query is a set of values (those values that satisfy the query). In other data servers, such as a system with queries baaed on image content, or many text retrieval systems, the result of a query is a sorted list. For example, in the case of a system with queries based on image content, the query might aak for objects that are a particular shade of red, and the result of the query would be a sorted list of objects in the database, sorted by how well the color of the object matches that given in the query. A multimedia system must somehow synthesize both types of queries (those whose result is a set, and those whose result is a sorted list) in a consistent manner. In this paper we discuss the solution adopted by Garlic, a multimedia information system being developed at
182|Inverted files for text search engines|The technology underlying text search engines has advanced dramatically in the past decade. The development of a family of new index representations has led to a wide range of innovations in index storage, index construction, and query evaluation. While some of these developments have been consolidated in textbooks, many specific techniques are not widely known or the textbook descriptions are out of date. In this tutorial, we introduce the key techniques in the area, describing both a core implementation and how the core can be enhanced through a range of extensions. We conclude with a comprehensive bibliography of text indexing literature.
183|Filtered Document Retrieval with Frequency-Sorted Indexes|Ranking techniques are effective at finding answers in document collections but can be expensive to evaluate. We propose an evaluation technique that uses early recognition of which documents are likely to be highly ranked to reduce costs; for our test data, queries are evaluated in 2% of the memory of the standard implementation without degradation in retrieval effectiveness. cpu time and disk traffic can also be dramatically reduced by designing inverted indexes explicitly to support the technique. The principle of the index design is that inverted lists are sorted by decreasing within-document frequency rather than by document number, and this method experimentally reduces cpu time and disk traffic to around one third of the original requirement. We also show that frequency sorting can lead to a net reduction in index size, regardless of whether the index is compressed.
184|Efficient query evaluation using a two-level retrieval process|We present an efficient query evaluation method based on a two level approach: at the first level, our method iterates in parallel over query term postings and identifies candidate documents using an approximate evaluation taking into account only partial information on term occurrences and no query independent factors; at the second level, promising candidates are fully evaluated and their exact scores are computed. The efficiency of the evaluation process can be improved significantly using dynamic pruning techniques with very little cost in effectiveness. The amount of pruning can be controlled by the user as a function of time allocated for query evaluation. Experimentally, using the TREC Web Track data, we have determined that our algorithm significantly reduces the total number of full evaluations by more than 90%, almost without any loss in precision or recall. At the heart of our approach there is an efficient implementation of a new Boolean construct called WAND or Weak AND that might be of independent interest.
185|Pruned query evaluation using pre-computed impacts|Exhaustive evaluation of ranked queries can be expensive, particularly when only a small subset of the overall ranking is required, or when queries contain common terms. This concern gives rise to techniques for dynamic query pruning, that is, methods for eliminating redundant parts of the usual exhaustive evaluation, yet still generating a demonstrably “good enough ” set of answers to the query. In this work we propose new pruning methods that make use of impact-sorted indexes. Compared to exhaustive evaluation, the new methods reduce the amount of computation performed, reduce the amount of memory required for accumulators, reduce the amount of data transferred from disk, and at the same time allow performance guarantees in terms of precision and mean average precision. These strong claims are backed by experiments using the TREC Terabyte collection and queries. Categories and Subject Descriptors H.3.1 [Information Storage and Retrieval]: Content analysis and indexing – indexing methods; H.3.2 [Information Storage and Retrieval]:
186|Optimized Query Execution in Large Search Engines with Global Page Ordering|Large web search engines have to answer thousands of queries per second with interactive response times. A major factor in the cost of executing a query is given by the lengths of the inverted lists for the query terms, which increase with the size of the document collection and are often in the range of many megabytes. To address this issue, IR and database researchers have proposed pruning techniques that compute or approximate term-based ranking functions without scanning over the full inverted lists.
187|Three-level caching for efficient query processing in large web search engines|Large web search engines have to answer thousands of queries per second with interactive response times. Due to the sizes of the data sets involved, often in the range of multiple terabytes, a single query may require the processing of hundreds of megabytes or more of index data. To keep up with this immense workload, large search engines employ clusters of hundreds or thousands of machines, and a number of techniques such as caching, index compression, and index and query pruning are used to improve scalability. In particular, two-level caching techniques cache results of repeated identical queries at the frontend, while index data for frequently used query terms are cached in each node at a lower level. We propose and evaluate a three-level caching scheme that adds an intermediate level of caching for additional performance gains. This intermediate level attempts to exploit frequently occurring pairs of terms by caching intersections or projections of the corresponding inverted lists. We propose and study several offline and online algorithms for the resulting weighted caching problem, which turns out to be surprisingly rich in structure. Our experimental evaluation based on a large web crawl and real search engine query log shows significant performance gains for the best schemes, both in isolation and in combination with the other caching levels. We also observe that a careful selection of cache admission and eviction policies is crucial for best overall performance.
188|Efficient document retrieval in main memory|Disk access performance is a major bottleneck in traditional information retrieval systems. Compared to system memory, disk bandwidth is poor, and seek times are worse. We circumvent this problem by considering query evaluation strategies in main memory. We show how new accumulator trimming techniques combined with inverted list skipping can produce extremely high performance retrieval systems without resorting to methods that may harm effectiveness. We evaluate our techniques using Galago, a new retrieval system designed for efficient query processing. Our system achieves a 69 % improvement in query throughput over previous methods.
189|Efficient Passage Ranking for Document Databases|Queries to text collections are resolved by ranking the documents in the collection and returning the highest-scoring documents to the user. An alternative retrieval method is to rank passages, that is, short fragments of documents, a strategy that can improve effectiveness and identify relevant material in documents that are too large for users to consider as a whole. However, ranking of passages can considerably increase retrieval costs. In this paper we explore alternative query evaluation techniques, and develop new techniques for evaluating queries on passages. We show experimentally that, appropriately implemented, effective passage retrieval is practical in limited memory on a desktop machine. Compared to passage ranking with adaptations of current document ranking algorithms, our new &#034;DO-TOS&#034; passage ranking algorithm requires only a fraction of the resources, at the cost of a small loss of effectiveness.
190|A Document-Centric Approach to Static Index Pruning in Text Retrieval Systems|We present a static index pruning method, to be used in ad-hoc document retrieval tasks, that follows a documentcentric approach to decide whether a posting for a given term should remain in the index or not. The decision is made based on the term&#039;s contribution to the document&#039;s Kullback-Leibler divergence from the text collection&#039;s global language model. Our technique can be used to decrease the size of the index by over 90%, at only a minor decrease in retrieval e#ectiveness. It thus allows us to make the index small enough to fit entirely into the main memory of a single PC, even for large text collections containing millions of documents. This results in great e#ciency gains, superior to those of earlier pruning methods, and an average response time around 20 ms on the GOV2 document collection.
191|Efficient Phrase Querying with an Auxiliary Index|Search engines need to evaluate queries extremely fast, a challenging task given the vast quantities of data being indexed. A significant proportion of the queries posed to search engines involve phrases. In this paper we consider how phrase queries can be efficiently supported with low disk overheads. Previous research has shown that phrase queries can be rapidly evaluated using nextword indexes, but these indexes are twice as large as conventional inverted files. We propose a combination of nextword indexes with inverted files as a solution to this problem. Our experiments show that combined use of an auxiliary nextword index and a conventional inverted file allow evaluation of phrase queries in half the time required to evaluate such queries with an inverted file alone, and the space overhead is only 10% of the size of the inverted file. Further time savings are available with only slight increases in disk requirements.
192|Fast Ranking in Limited Space|Ranking techniques have long been suggested as alternatives to more conventional Boolean methods for searching document collections. The cost of computing a ranking is, however, greater than the cost of performing a Boolean search, in terms of both memory space and processing time. Here we consider the resources required by the cosine method of ranking, and show that with a careful application of indexing and selection techniques both the space and time required by ranking can be substantially reduced. The methods described in this paper have been used to build a retrieval system for a collection of over two million pages of text, with which it is possible to process ranked queries of 50--60 terms in about 5% of the space required by previous implementations; in as little as 25% of the time; and with no loss of retrieval effectiveness. 1 Introduction  Ranking techniques are used to find the documents in a document collection that are most likely to be relevant to an informally-phrased ...
193|ResIn: A Combination of Results Caching and Index Pruning for High-performance Web Search Engines |Results caching is an efficient technique for reducing the query processing load, hence it is commonly used in real search engines. This technique, however, bounds the maximum hit rate due to the large fraction of singleton queries, which is an important limitation. In this paper we propose ResIn- an architecture that uses a combination of results caching and index pruning to overcome this limitation. We argue that results caching is an inexpensive and efficient way to reduce the query processing load and show that it is cheaper to implement compared to a pruned index. At the same time, we show that index pruning performance is fundamentally affected by the changes in the query traffic that the results cache induces. We experiment with real query logs and a large document collection, and show that the combination of both techniques enables efficient reduction of the query processing costs and thus is practical to use in Web search engines.
194|Compressing Term Positions in Web Indexes |Large search engines process thousands of queries per second on billions of pages, making query processing a major factor in their operating costs. This has led to a lot of research on how to improve query throughput, using techniques such as massive parallelism, caching, early termination, and inverted index compression. We focus on techniques for compressing term positions in web search engine indexes. Most previous work has focused on compressing docID and frequency data, or position information in other types of text collections. Compression of term positions in web pages is complicated by the fact that term occurrences tend to cluster within documents but not across document boundaries, making it harder to exploit clustering effects. Also, typical access patterns for position data are different from those for docID and frequency data. We perform a detailed study of a number of existing and new techniques for compressing position data in web indexes. We also study how to efficiently access position data for ranking functions that take proximity features into account.
195|Dynamic index pruning for effective caching|Search engines make use of inverted list caching in RAM and dynamic pruning schemes to reduce query evaluation times. While only a small portion of lists are processed with dynamic pruning, current systems still store the entire inverted list in cache. In this paper we investigate caching only the pieces of the inverted lists that are actually used to answer a query during dynamic pruning. We examine an LRU cache model, and two recently proposed models. We also introduce a new dynamic pruning scheme for impactordered inverted lists. Using two large web collections and corresponding query logs we show that, using an LRU cache, our new pruning scheme reduces the number of disk accesses during query processing time by 7%–15 % over the state-of-the-art impact-ordered baseline, without reducing answer quality. Surprisingly, however, we discover that using our new pruning scheme makes little difference to disk traffic when the more sophisticated caching schemes are employed.
196|Top-k Aggregation Using Intersections of Ranked Inputs |There has been considerable past work on efficiently computing top k objects by aggregating information from multiple ranked lists of these objects. An important instance of this problem is query processing in search engines: One has to combine information from several different posting lists (rankings) of web pages (objects) to obtain the top k web pages to answer user queries. Two particularly well-studied approaches to achieve efficiency in top-k aggregation include early-termination algorithms (e.g., TA and NRA) and preaggregation of some of the input lists. However, there has been little work on a rigorous treatment of combining these approaches. We generalize the TA and NRA algorithms to the case when preaggregated intersection lists are available in addition to the original lists. We show that our versions of TA and NRA continue to remain “instance optimal, ” a very strong optimality notion that is a highlight of the original TA and NRA algorithms. Using an index of millions of web pages and real-world search engine queries, we empirically characterize the performance gains offered by our new algorithms. We show that the practical benefits of intersection lists can be fully realized only with an early-termination algorithm.
197|Revisiting globally sorted indexes for efficient document retrieval|There has been a large amount of research on efficient document retrieval in both IR and web search areas. One important technique to improve retrieval efficiency is early termination, which speeds up query processing by avoiding scanning the entire inverted lists. Most early termination techniques first build new inverted indexes by sorting the inverted lists in the order of either the term-dependent information, e.g., term frequencies or term IR scores, or the term-independent information, e.g., static rank of the document; and then apply appropriate retrieval strategies on the resulting indexes. Although the methods based only on the static rank have been shown to be ineffective for the early termination, there are still many advantages of using the methods based on term-independent information. In this paper, we propose new techniques to organize inverted indexes based on the termindependent information beyond static rank and study the new retrieval strategies on the resulting indexes. We perform a detailed experimental evaluation on our new techniques and compare them with the existing approaches. Our results on the TREC GOV and GOV2 data sets show that our techniques can improve query efficiency significantly.
198|Text Retrieval by using k-word Proximity Search|When we search from a huge amount of documents, we often specify several keywords and use conjunctive queries to narrow the result of the search. Though the searched documents contain all keywords, positions of the keywords are usually not considered. As the result, the search result contains some meaningless documents. It is therefore effective to rank documents according to proximity of keywords in the documents. This ranking is regarded as a kind of text data mining. In this paper, we propose two algorithms for finding documents in which all given keywords appear in neighboring places. One is based on plane-sweep algorithm and the other is based on divide-and-conquer approach. Both algorithms run in O(n log n) time where n is the number of occurrences of given keywords. We run the plane-sweep algorithm on a large collection of html files and verify its effectiveness.
199|A System for Keyword Proximity Search on XML Databases|ding to their size. Trees of smaller sizes denote higher association between the keywords, which is generally true for reasonable schema designs. For example, consider the keyword query &#034;Yannis, Vasilis&#034; on the Permission to copy without fee all or part of this material is granted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment.  Proceedings of the 29th VLDB Conference,  Berlin, Germany, 2003  Conference Conference Conference  Person  [SergeAbiteboul]  Person  [Chen Li]  Person  [YannisPapakonstantinou]  Person  [VasilisVassalos]  Name  [VLDB]  Issue  Year  [1998]  Paper  Title  [&#034;MedMaker:..&#034;] Author  Name  [ICDE]  Issue  Year  [1996]  Paper  Author Author  Name  [Sigmod]  Issue  Year  [
200|Integrating Keyword Search into XML Query Processing|Due to the popularity of the XML data format, several query languages for XML have been proposed, specially devised to handle data whose structure is unknown, loose, or absent. While these languages are rich enough to allow for querying the content and structure of an XML document, a varying or unknown structure can make formulating queries a very difficult task. We propose an extension to XML query languages that enables keyword search at the granularity of XML elements, that helps novice users formulate queries, and also yields new optimization opportunities for the query processor. We present an implementation of this extension on top of a commercial RDBMS; we then discuss implementation choices and performance results.  Keywords  XML query processing, full-text index  1 Introduction  There is no doubt that XML is rapidly becoming one of the most important data formats. It is already used for scientific data (e.g., DNA sequences), in linguistics (e.g., the Treebank database at the U...
201|Networks versus Markets in International Trade|I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative
202|CONTINENTAL TRADING BLOCS: ARE THEY NATURAL, OR SUPER-NATURAL?|Using the gravity model, we find evidence of three continental trading blocs: the Americas, Europe and Pacific Asia. Intra-regional trade exceeds what can be explained by the proximity of a pair of countries, their sizes and GNP/capitas, and whether they share a common border or language. We then turn from the econometrics to the economic welfare implications. Krugman has supplied an argument against a three-bloc world, assuming no transport costs, and another argument in favor, assuming prohibitively high transportation costs between continents. We complete the model for the realistic case where intercontinental transport costs are neither prohibitive nor zero. If transport costs are low, continental Free Trade Areas can reduce welfare. We call such blocs super-natural. Partial liberalization is better than full liberalization within regional Preferential Trading Arrangements, despite the GATT&#039;s Article 24. The super-natural zone occurs when the regionalization of trade policy exceeds what is justified by natural factors.
203|Fast Algorithms for k-word Proximity Search|When we search from a huge amount of... In this paper, we propose two algorithms for finding documents in which all given keywords appear in neighboring places. One is based on plane-sweep algorithm and the other is based on divide-and-conquer approach. Both algorithms run in O(n log n) time where n is the number of occurrences of given keywords. We run the algorithms on a large collection of html files and verify its effectiveness.
204|Authoritative Sources in a Hyperlinked Environment|The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authoritative ” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages ” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.
205|Plane-Sweep Algorithms for Intersecting Geometric Figures|Algorithms in computational geometry are of increasing importance in computer-aided design, for example, in the layout of integrated circuits. The efficient computation of the intersection of several superimposed figures is a basic problem. We consider plane figures defined by points connected by straight line segments, for example, polygons (not necessarily simple) and maps (embedded planar graphs). The regions into which the plane is partitioned by these intersecting figures are to be processed in various ways such as listing the boundary of each region in cyclic order or sweeping the interior of each region. Let n be the total number of points of all the figures involved and s be the total number of inter-sections of all line segments. We present two plane-sweep algorithms that solve the problems above; in the general case (no convexity) in time O((n + s)log n) and space O(n + s); when the regions of each given figure are convex, the same can be achieved in time O(n log n + s) and space O(n).
206|Cut as a Querying Unit for WWW, Netnews, and E-mail|In this paper, we propose a query framework for hypertext data in general, and for WWW pages, Netnews articles, and e-mails in particular. In existing query tools for hypertext data, such as search engines for WWW or intelligent news/mail readers, data units in query are typically individual nodes. In actual hypertext data, however, one topic is often described over a series of connected nodes, and therefore, the logical data unit should be such a series of nodes corresponding to one topic. This discrepancy between the data unit in query and the logical data unit hinders the efficient information discovery from hypertext data. To solve this problem, in our framework, we divide hypertexts into connected subgraphs corresponding to individual topics, and we use those subgraphs as the data units in queries.
207|Proximity search with a triangulated spatial mode|The proximity relations inherent in triangulations of geometric data can be exploited in the implementation of nearest-neighbour search procedures. This is relevant to applications such as terrain analysis, cartography and robotics, in which triangulations may be used to model the spatial data. Here we describe neighbourhood search procedures within constrained Delaunay triangulations of the vertices of linear objects, for the queries of nearest object to an object and the nearest object to an arbitrary point. The procedures search locally from object edges, or from a query point, to build triangulated regions that extend from the source edge or point by a distance at least equal to that to its nearest neighbouring feature. Several geographical datasets have been used to evaluate the procedures experimentally. Average numbers of edge–edge distance calculations to find the nearest line feature edge disjoint to another line feature edge ranged between 15 and 39 for the different datasets examined, while the average numbers of point–edge distance calculations to determine the nearest edge to an arbitrary point ranged between 7 and 35.
208|Optimal search in planar subdivisions|Abstract. A planar subdivision is any partition of the plane into (possibly unbounded) polygonal regions. The subdivision search problem is the following: given a subdivision S with n line segments and a query point P, determine which region of S contains P. We present a practical algorithm for subdivision search that achieves the same (optimal) worst case complexity bounds as the significantly more complex algorithm of Lipton and Tarjan, namely O (log n) search time with O (n) storage. Our subdivision search structure can be constructed in linear time from the subdivision representation used in many applications. Key words, computational geometry, analysis of algorithms, point location, planar graphs, hierarchical search
209|Constrained Delaunay triangulations|Given a set of n vertices in the plane together with a set of noncrossing edges, the constrained Delaunay triangulation (CDT) is the triangulation of the vertices with the following properties: (1) the prespecified edges are included in the triangulation, and (2) it is as close as possible to the Delaunay triangulation. We show that the CDT can be built in optimal O(n log n) time using a divide-and-conquer technique. This matches the time required to build an arbitrary (unconstrained) Delaunay triangulation and the time required to build an arbitrary constrained (nonDelaunay) triangulation. CDTs, because of their relationship with Delaunay triangulations, have a number of properties that should make them useful for the finite-element method. Applications also include motion planning in the presence of polygonal obstacles in the plane and constrained Euclidean minimum spanning trees, spanning trees subject to the restriction that some edges are prespecified. I’wnishi0tt to copy without tix all or part of thk material is granlcd provided thal IIIC wpics arc not nude or distributed li)r direct commercial advanlagc, the ACM copyright wficc and the title of lhc publication and its date appear. and notice is given that copying is hy permission ol the Association Car Computing Machinery. ‘To copy otherwise. or to republish. requires a fee and/or specific permission.
210|Generalized Delaunay Triangulation for Planar Graphs| We introduce the notion of generalized Delaunay triangulation of a planar straight-line graph G = (V, E) in the Euclidean plane and present some characterizations of the triangulation. It is shown that the generalized Delaunay triangulation has the property that the minimum angle of the triangles in the triangulation is maximum among all possible triangulations ofthe graph. A general algorithm that runs in O(|V|²) time for computing the generalized Delaunay triangu-lation is presented. When the underlying raph is a simple polygon, a divide-and-conquer algorithm based on the polygon cutting theorem of Chazelle is given that runs in O (|V| log(|V|) time. 
212|Spatial Queries on a Hierarchical Terrain Model|In this paper we consider the problem of defining and answering spatial queries on hierarchical terrain models that provide a multiresolution representation. In particular, we focus our attention on interference queries in which the query object is a spatial entity not belonging to the model. We propose algorithms for efficiently answering such queries on a triangle-based hierarchical model.
213|Finding and Approximating Top-k Answers in Keyword Proximity Search|Various approaches for keyword proximity search have been implemented in relational databases, XML and the Web. Yet, in all of them, an answer is a Q-fragment, namely, a subtree T of the given data graph G, such that T contains all the keywords of the query Q and has no proper subtree with this property. The rank of an answer is inversely propor-tional to its weight. Three problems are of interest: finding an optimal (i.e., top-ranked) answer, computing the top-k answers and enumerating all the answers in ranked order. It is shown that, under data complexity, an efficient algorithm for solving the first problem is sufficient for solving the other two problems with polynomial delay. Similarly, an efficient algorithm for finding a ?-approximation of the optimal an-swer suffices for carrying out the following two tasks with polynomial delay, under query-and-data complexity. First, enumerating in a (? + 1)-approximate order. Second, com-puting a (? + 1)-approximation of the top-k answers. As a corollary, this paper gives the first efficient algorithms, under data complexity, for enumerating all the answers in ranked order and for computing the top-k answers. It also gives the first efficient algorithms, under query-and-data complexity, for enumerating in a provably approximate order and for computing an approximation of the top-k answers.
214|A polylogarithmic approximation algorithm for the group Steiner tree problem|The group Steiner tree problem is a generalization of the Steiner tree problem where we ae given several subsets (groups) of vertices in a weighted graph, and the goal is to find a minimum-weight connected subgraph containing at least one vertex from each group. The problem was introduced by Reich and Widmayer and finds applications in VLSI design. The group Steiner tree problem generalizes the set covering problem, and is therefore at least as had. We give a randomized O(log 3 n log k)-approximation algorithm for the group Steiner tree problem on an n-node graph, where k is the number of groups. The best previous ink)v/ (Bateman, Helvig, performance guarantee was (1 + -  Robins and Zelikovsky).
215|The complexity of relational query languages (extended abstract  (1982) |Two complexity measures for query languages are proposed. Data complexity is the complexity of evaluating a query in the language as a function of the size of the database, and expression complexity is the complexity of ewduating a query in the language as a function of the size of the expression defining the query. We study the data and expression complexity of logical langnages- relational calculus and its extensions by transitive closure, fixpoint and second order existential quantification- and algebraic languages- relational algebra and its extensions by bounded and unbounded looping. The pattern which will bc shown is that the expression complexity of the investi-gated languages is one exponential higher then their data complexity, and for both types of complexity we show completeness in some complexity class. Research supported by a Weizrnann Post-doctoral Fellowship,
216|Efficiently enumerating results of keyword search|Abstract. Various approaches for keyword search have been explored in different settings, including databases, XML and the Web. It is shown that in many cases, systems that incorporate keyword search actually solve similar problems. This paper describes, for this type of problems, the first algorithms that are provably efficient, that is, run with polynomial delay. Specifically, algorithms for enumerating K-fragments are given, where a K-fragment is a subtree T of the given data graph, such that T contains all the keywords of K and no proper subtree of T has this property. Three types of K-fragments are considered: rooted, undirected and strong. For all three types, there are algorithms that enumerate all K-fragments with polynomial delay. For rooted K-fragments and acyclic data graphs, there is an algorithm that enumerates with polynomial delay in the order of increasing weight, assuming that K is of a fixed size. 1
217|The directed steiner network problem is tractable for a constant number of terminals|We consider the DIRECTED STEINER NETWORK problem, also called the POINT-TO-POINT CONNECTION problem, where given a directed graph G and p pairs s1 t1 sp tp of nodes in the graph, one has to find the smallest subgraph H of G that contains paths from si to ti for all i. The problem is NP-hard for general p, since the DIRECTED STEINER TREE problem is a special case. Until now, the complexity was unknown for constant p 3. We prove that the problem is polynomially solvable if p is any constant number, even if nodes and edges in G are weighted and the goal is to minimize the total weight of the subgraph H. In addition, we give an efficient algorithm for the STRONGLY CONNECTED STEINER SUBGRAPH problem for any constant p, where given a directed graph and p nodes in the graph, one has to compute the smallest strongly connected subgraph containing the p nodes.
219|Searching in Metric Spaces with User-Defined and Approximate Distances|Metric access methods (MAMs), such as the M-tree, are powerful index structures for supporting similarity queries on metric spaces, which represent a common abstraction forthIj searchrc problems tho arise in many modern application areas, such as multimedia, data mining, decision support, pattern recognition, and genomic databases. As compared to multi-dimensional (spatial) access methods (SAMs), MAMs are more general, yet they are reputed to lose in flexibility, since it is commonly deemed th= th= can only answer queries using th same distance function used to buildth index. In thj paper we sh wth&#034; th&#034; limitation is only apparent -- thus MAMs are far more flexible than believed -- and extend the M-tree so as to be able to support user-defined distance criteria, approximate distance functions to speed up query evaluation, as well as dissimilarity functions whD h are not metrics. The so-extended M-tree, also called QIC-M-tree, can deal with three distinct distances at a time: 1) a query (user-defined) distance,2)anindex distance (used to buildth tree), and 3) a comparison(iso oximate) distance (used to quickly discard from th search uninteresting parts of th tree). We develop an analytical cost model thl accurately characterizes the performance of QIC-M-tree and validate such model thjj&#034;[ extensive experimentation on real metric data sets. In particular, our analysis is able to predict th best evaluation strategy (i.e.whe h distances to use) under a variety of configurations, by properly taking into account relevant factors such as th distribution of distances, th cost of computing distances, and th actual index structure. We also prove thF the overall saving in CPU search costs whj using an approximate distance can be estimated by using information on the data set only -- thus...
220|A Fast Branch &amp; Bound Nearest Neighbour Classifier in Metric Spaces|The recently introduced algorithm LAESA finds the nearest neighbour  prototype in a metric space. The average number of distances computed  in the algorithm does not depend on the number of prototypes but it  shows linear space and time complexities. In this paper, a new algorithm  (TLAESA) is proposed which has a sublinear time complexity and keeps the  other features unchanged.  Keywords. Metric spaces, nearest-neighbour, pattern recognition.  1 Introduction  One of the most popular techniques in Pattern Recognition is the nearest neighbour search. In this method, the sample is classified in the same class as the prototype which minimizes a predefined distance function with respect to the sample. For instance, in handwritten character recognition the set of prototypes consists of many (classified) examples of letters and numerals, the sample is the character to be classified and the edit distance has been often used in order Work partially supported under grant TIC93-0633-C02-02  y  ...
221|Analysis and Comparison of Eigenspace-based Face Recognition Approaches|Different eigenspace-based approaches have been proposed for the  recognition of faces. They differ mostly in the kind of projection method been  used and in the similarity matching criterion employed. The aim of this paper  is to present a comparative study between some of these different approaches.
222|t-Spanners as a Data Structure for Metric Space Searching|A \emph{t-spanner}, a subgraph that approximates graph distances within a precision factor $t$, is a well known concept in graph theory. In this paper we use it in a novel way, namely as a data structure for searching metric spaces. The key idea is to consider the $t$-spanner as an approximation of the complete graph of distances among the objects, and use it as a compact device to simulate the large matrix of distances required by successful search algorithms like AESA [Vidal 1986]. The $t$-spanner provides a time-space tradeoff where full AESA is just one extreme. We show that the resulting algorithm is competitive against current approaches, e.g., 1.5 times the time cost of AESA using only 3.21\% of its space requirement, in a metric space of strings; and 1.09 times the time cost of AESA using only 3.83 \% of its space requirement, in a metric space of documents. We also show that $t$-spanners provide better space-time tradeoffs than classical alternatives such as pivot-based indexes. Furthermore, we show that the concept of $t$-spanners has potential for large improvements.
223|Reinforcement Learning I: Introduction|In which we try to give a basic intuitive sense of what reinforcement learning is and how it differs and relates to other fields, e.g., supervised learning and neural networks, genetic algorithms and artificial life, control theory. Intuitively, RL is trial and error (variation and selection, search) plus learning (association, memory). We argue that RL is the only field that seriously addresses the special features of the problem of learning from interaction to achieve long-term goals.
224|Multidimensional Access Methods|Search operations in databases require special support at the physical level. This is true for conventional databases as well as spatial databases, where typical search operations include the point query (find all objects that contain a given search point) and the region query (find all objects that overlap a given search region).
225|The quadtree and related hierarchical data structures|A tutorial survey is presented of the quadtree and related hierarchical data structures. They are based on the principle of recursive decomposition. The emphasis is on the representation of data used in applications in image processing, computer graphics, geographic information systems, and robotics. There is a greater emphasis on region data (i.e., two-dimensional shapes) and to a lesser extent on point, curvilinear, and threedimensional data. A number of operations in which such data structures find use are examined in greater detail.
227|Beyond uniformity and independence: Analysis of r-trees using the concept of fractal dimension|We propose the concept of fractal dimension of a set of points, in order to quantify the deviation from the uniformity distribution. Using measurements on real data sets (road intersections of U.S. counties, star coordinates from NASA’s Infrared-Ultraviolet Explorer etc.) we provide evidence that real data indeed are skewed, and, moreover, we show that they behave as mathematical fractals, with a measurable, non-integer fract al dimension. Armed with this tool, we then show its practical use in predicting the performance of spatial access methods, and specifically of the R-trees. We provide the jirst analysis of R-trees for skewed distributions of points: We develop a formula that estimates the number of disk accesses for range queries, given only the fractal dimension of the point set, and its count. Experiments on real data sets show that the formula is very accurate: the relative error is usually below 5%, and it rarely exceeds 10%. We believe that the fractal dimension will help replace the uniformity and independence assumptions, allowing more accurate analysis for any spatial access method, as well as better estimates for query optimization on multi-attribute queries. 1
228|A Simple Algorithm for Nearest Neighbor Search in High Dimensions|Abstract—The problem of finding the closest point in high-dimensional spaces is common in pattern recognition. Unfortunately, the complexity of most existing search algorithms, such as k-d tree and R-tree, grows exponentially with dimension, making them impractical for dimensionality above 15. In nearly all applications, the closest point is of interest only if it lies within a user-specified distance e. We present a simple and practical algorithm to efficiently search for the nearest neighbor within Euclidean distance e. The use of projection search combined with a novel data structure dramatically improves performance in high dimensions. A complexity analysis is presented which helps to automatically determine e in structured problems. A comprehensive set of benchmarks clearly shows the superiority of the proposed algorithm for a variety of structured and unstructured search problems. Object recognition is demonstrated as an example application. The simplicity of the algorithm makes it possible to construct an inexpensive hardware search engine which can be 100 times faster than its software equivalent. A C++ implementation of our algorithm is available upon request to search@cs.columbia.edu/CAVE/.
229|A survey on content-based retrieval for multimedia databases|Abstract—Conventional database systems are designed for managing textual and numerical data, and retrieving such data is often based on simple comparisons of text/numerical values. However, this simple method of retrieval is no longer adequate for the multimedia data, since the digitized representation of images, video, or data itself does not convey the reality of these media items. In addition, composite data consisting of heterogeneous types of data also associates with the semantic content acquired by a user’s recognition. Therefore, content-based retrieval for multimedia data is realized taking such intrinsic features of multimedia data into account. Implementation of the content-based retrieval facility is not based on a single fundamental, but is closely related to an underlying data model, a priori knowledge of the area of interest, and the scheme for representing queries. This paper surveys recent studies on content-based retrieval for multimedia databases from the point of view of three fundamental issues. Throughout
230|Combining Textual and Visual Cues for Content-based Image Retrieval on the World Wide Web|A system is proposed that combines textual and visual statistics in a single index vector for content-based search of a WWW image database. Textual statistics are captured in vector form using latent semantic indexing (LSI) based on text in the containing HTML document. Visual statistics are captured in vector form using color and orientation histograms. By using an integrated approach, it becomes possible to take advantage of possible statistical couplings between the content of the document (latent semantic content) and the contents of images (visual statistics). The combined approach allows improved performance in conducting content-based search. Search performance experiments are reported for a database containing 100,000 images collected from the WWW.  1 Introduction  The growing importance of the world wide web has led to the birth of a number of image search engines [6, 7, 11, 12]. The web&#039;s staggering scale puts severe limitations on the types of indexing algorithms that can be...
231|Optimal expected-time algorithms for closest point problems|Geometric closest potnt problems deal with the proxLmity relationships in k-dimensional point sets. Examples of closest point problems include building minimum spanning trees, nearest neighbor searching, and triangulation constructmn Shamos and Hoey [17] have shown how the Voronoi dtagram can be used to solve a number of planar closest point problems in optimal worst case tune. In this paper we extend thmr work by giving optimal expected.trine algorithms for solving a number of closest point problems in k-space, including nearest neighbor searching, finding all nearest neighbors, and computing planar minimum spanning trees. In addition to establishing theoretical bounds, the algorithms in this paper can be implemented to solve practical problems very efficiently. Key Words and Phrases &#039; computational geometry, closest point problems, minunum spanning trees, nearest neighbor searching, optimal algorithms, probabfllstm analysis of algorithms, Voronoi diagrams CR Categories: 3.74, 5 25, 5.31, 5.32 1.
232|Content-Based Image Indexing|We formulate the content-based image in-dexing problem as a multi-dimensional nearest-neighbor search problem, and de-velop/implement an optimistic vantage-point tree algorithm that can dynamically adapt the indexed search process to the character-istics of given queries. Based on our perfor-mance study, the system typically only needs to touch less than 20 % of the index entries for well-behaved queries, i.e., when the query images are relatively close to their nearest neighbors in the database. We also report in this paper the results of extensive perfor-mance experiments, which characterise the
233|A Cost Model for Similarity Queries in Metric Spaces|Wu consider tho problem of estimating CPU (distance com-putntlons) nnd I/O costs for processing range and k-nearest neighbors qucrics over metric spaces. Unlike the specific case of vector spaces, where information on data distribution has been exploited to derive cost models for predicting the per-formanco of multi-dimensional access methods, in a generic metric space there is no such a possibility, which makes the problem quite different and requires a novel approach. We insist that the distance distribution of objects can be prof-itably used to solve the problem, and consequently develop a concrete cost model for the M-tree access method [lo]. Our results rely on the assumption that the indexed dataset comes from a metric space which is “homogeneous ” enough (in a probabilistic sense) to allow reliable cost estimations even if the distance distribution with respect to a specific query object is unknown. We experimentally validate the modol ovor both real and synthetic datasets, and show how the model can be used to tune the M-tree in order to min-imlzo a combination of CPU and I/O costs. Finally, we sketch how the same approach can be applied to derive a cost model for the up-tree index structure [8]. 
234|Fast Approximate String Matching in a Dictionary|A successful technique to search large textual databases allowing errors relies on an online search in the vocabulary of the text. To reduce the time of that online search, we index the vocabulary as a metric space. We show that with reasonable space overhead we can improve by a factor of two over the fastest online algorithms, when the tolerated error level is low (which is reasonable in text searching). 1 Introduction  Approximate string matching is a recurrent problem in many branches of computer science, with applications to text searching, computational biology, pattern recognition, signal processing, etc. The problem can be stated as follows: given a long text of length n, and a (comparatively short) pattern of length m, retrieve all the segments (or &#034;occurrences&#034;) of the text whose edit distance to the pattern is at most  k. The edit distance ed() between two strings is defined as the minimum number of character insertions, deletions and replacements needed to make them equal. I...
235|Learning Feature Relevance and Similarity Metrics in Image Databases|Most of the current image retrieval systems use &#034;one-shot&#034; queries to a database to retrieve similar images. Typically a K-NN (nearest neighbor) kind of algorithm is used where the weights of the features that are used to represent images remain fixed (or manually tweaked by the user) in the computation of a given similarity metric. However, neither all of the features are equally important for a given query nor a similarity metric is optimal for all kinds of images in a database. The manual adjustment of these weights and the selection of similarity metric are exhausting. Moreover, they require a very sophisticated user. In this paper we present a novel image retrieval system that continuously learns the weights of features and selects an appropriate similarity metric based on the user&#039;s feedback given as positive or negative image examples. Experimental results are presented that provide the objective evaluation of learning behavior of the system for image retrieval.
236|Tractable Algorithms for Proximity Search on Large Graphs|  Identifying the nearest neighbors of a node in a graph is a key ingredient in a diverse set of ranking problems, e.g. friend suggestion in social networks, keyword search in databases, web-spam detection etc. For finding these “near” neighbors, we need graph theoretic measures of similarity or proximity. Most popular graph-based similarity measures, e.g. length of shortest path, the number of common neighbors etc., look at the paths between two nodes in a graph. One such class of similarity measures arise from random walks. In the context of using these measures, we identify and address two important
237|Webspam Identification Through Content and Hyperlinks|We present an algorithm, witch, that learns to detect spam hosts or pages on the Web. Unlike most other approaches, it simultaneously exploits the structure of the Web graph as well as page contents and features. The method is efficient, scalable, and provides state-of-the-art accuracy on a standard Web spam benchmark.
238|Local Partitioning for Directed Graphs Using PageRank |Abstract. A local partitioning algorithm finds a set with small conductance near a specified seed vertex. In this paper, we present a generalization of a local partitioning algorithm for undirected graphs to strongly connected directed graphs. In particular, we prove that by computing a personalized PageRank vector in a directed graph, starting from a single seed vertex within a set S that has conductance at most a, and by performing a sweep over that vector, we can obtain a set of vertices S ' with conductance FM (S ' )  = O ( p a log |S|). Here, the conductance function FM is defined in terms of the stationary distribution of a random walk in the directed graph. In addition, we describe how this algorithm may be applied to the PageRank Markov chain of an arbitrary directed graph, which provides a way to partition directed graphs that are not strongly connected. 1
239|9 Efficient Proximity Search with Query Logs |In information retrieval technology there are various techniques for fetching data from resources. And that technique also contains various issues. Information retrieval techniques require advanced manipulating schemes which improves keyword search. There are many techniques have been proposed but results get down when large amount data interrupted. In this paper, have tendency to achieve efficient time and space complexities by integrating proximity information. This system improves the performance by using previous searching results. All the previous system consist basic solutions for extracting results and ranking them. Query logs consists the last searching results and use that results for next search. Fuzzy keyword search truly enhance the system usability. Existing system in databases requires to write complete keyword for searching but by using auto-complete scheme it is easy to type less and find more. In this system proper demand paging algorithm is used for finding previous results.
240|Efficient Interactive Fuzzy Keyword Search|Traditional information systems return answers after a user submits a complete query. Users often feel “left in the dark” when they have limited knowledge about the underlying data, and have to use a try-and-see approach for finding information. A recent trend of supporting autocomplete in these systems is a first step towards solving this problem. In this paper, we study a new information-access paradigm, called “interactive, fuzzy search,” in which the system searches the underlying data “on the fly” as the user types in query keywords. It extends autocomplete interfaces by (1) allow- ing keywords to appear in multiple attributes (in an arbi- trary order) of the underlying data; and (2) finding relevant records that have keywords matching query keywords ap- proximately. This framework allows users to explore data as they type, even in the presence of minor errors. We study research challenges in this framework for large amounts of data. Since each keystroke of the user could invoke a query on the backend, we need efficient algorithms to process each query within milliseconds. We develop various incremental- search algorithms using previously computed and cached re- sults in order to achieve an interactive speed. We have deployed several real prototypes using these techniques. One of them has been deployed to support interactive search on the UC Irvine people directory, which has been used regularly and well received by users due to its friendly interface and high efficiency.
241|Effective Phrase Prediction|Autocompletion is a widely deployed facility in systems that require user input. Having the system complete a partially typed “word ” can save user time and effort. In this paper, we study the problem of autocompletion not just at the level of a single “word”, but at the level of a multi-word “phrase”. There are two main challenges: one is that the number of phrases (both the number possible and the number actually observed in a corpus) is combinatorially larger than the number of words; the second is that a “phrase”, unlike a “word”, does not have a well-defined boundary, so that the autocompletion system has to decide not just what to predict, but also how far. We introduce a FussyTree structure to address the first challenge and the concept of a significant phrase to address the second. We develop a probabilistically driven multiple completion choice model, and exploit features such as frequency distributions to improve the quality of our suffix completions. We experimentally demonstrate the practicability and value of our technique for an email composition application and show that we can save approximately a fifth of the keystrokes typed. 
242|Finding Good Permutants for Proximity Searching in Metric Spaces |Abstract—The permutation index has shown to be very effective in medium and high dimensional metric spaces, even in difficult problems, for instance, when solving reverse k-nearest neighbor queries. Nevertheless, currently there is no study about which are the desirable features one can ask to a permutant set, or how to select good permutants. Similar to the case of pivots, our experimental results show that, compared with a randomly chosen set, a good permutant set yields to fast query response or to reduce the amount of space used by the index. In this paper we start by characterizing permutants and studying their discrimination power, and then we propose an effective heuristic to select a good permutant candidate set. We also show empirical evidence that supports our technique. Keywords-component; metric space indexing; probabilistic € algorithms; indexing permutations I.
243|Spatial selection of sparse pivots for similarity search in metric spaces|Similarity search is a fundamental operation for applications that deal with unstructured data sources. In this paper we propose a new pivot-based method for similarity search, called Sparse Spatial Selection (SSS). The main characteristic of this method is that it guarantees a good pivot selection more efficiently than other methods previously proposed. In addition, SSS adapts itself to the dimensionality of the metric space we are working with, without being necessary to specify in advance the number of pivots to use. Furthermore, SSS is dynamic, that is, it is capable to support object insertions in the database efficiently, it can work with both continuous and discrete distance functions, and it is suitable for secondary memory storage. In this work we provide experimental results that confirm the advantages of the method with several vector and metric spaces. We also show that the efficiency of our proposal is similar to that of other existing ones over vector spaces, although it is better over general metric spaces.
244|A Dynamic Pivot Selection Technique for Similarity Search * |All pivot-based algorithms for similarity search use a set of reference points called pivots. The pivot-based search algorithm precomputes some distances to these reference points, which are used to discard objects during a search without comparing them directly with the query. Though most of the algorithms proposed to date select these reference points at random, previous works have shown the importance of intelligently selecting these points for the index performance. However, the proposed pivot selection techniques need to know beforehand the complete database to obtain good results, which inevitably makes the index static. More recent works have addressed this problem, proposing techniques that dynamically select pivots as the database grows. This paper presents a new technique for choosing pivots, that combines the good properties of previous proposals with the recently proposed dynamic selection. The experimental evaluation provided in this paper shows that the new proposed technique outperforms the state-of-art methods for selecting pivots. 1
245|Space-Time Tradeoffs for Proximity Searching in Doubling Spaces |Abstract. We consider approximate nearest neighbor searching in metric spaces of constant doubling dimension. More formally, we are given a set S of n points and an error bound e&gt; 0. The objective is to build a data structure so that given any query point q in the space, it is possible to efficiently determine a point of S whose distance from q is within a factor of (1 + e) of the distance between q and its nearest neighbor in S. In this paper we obtain the following space-time tradeoffs. Given a parameter ? ? [2, 1/e], we show how to construct a data structure of space n? O(dim) log(1/e) space that can answer queries in time O(log(n?))+(1/(e?)) O(dim). This is the first result that offers space-time tradeoffs for approximate nearest neighbor queries in doubling spaces. At one extreme it nearly matches the best result currently known for doubling spaces, and at the other extreme it results in a data structure that can answer queries in time O(log(n/e)), which matches the best query times in Euclidean space. Our approach involves a novel generalization of the AVD data structure from Euclidean space to doubling space. 1
246|A Decomposition of Multi-Dimensional Point Sets with Applications to k-Nearest-Neighbors and n-Body Potential Fields|We define the notion of a well-separated pair decomposition of points in d-dimensional space. We then develop efficient sequential and parallel algorithms for computing such a decomposition. We apply the resulting decomposition to the efficient computation of k-nearest neighbors and n-body potential fields.
247|Cover trees for nearest neighbor|ABSTRACT. We present a tree data structure for fast nearest neighbor operations in general-point metric spaces. The data structure requires space regardless of the metric’s structure. If the point set has an expansion constant ? in the sense of Karger and Ruhl [KR02], the data structure can be constructed in ? time. Nearest neighbor queries obeying the expansion bound require ? time. In addition, the nearest neighbor of points can be queried in time. We experimentally test the algorithm showing speedups over the brute force search varying between 1 and 2000 on natural machine learning datasets. 1.
248|Searching dynamic point sets in spaces with bounded doubling dimension|We present a new data structure that facilitates approximate nearest neighbor searches on a dynamic set of points in a metric space that has a bounded doubling dimension. Our data structure has linear size and supports insertions and deletions in O(log n) time, and finds a (1 + ?)-approximate nearest neighbor in time O(log n) +(1/?) O(1). The search and update times hide multiplicative factors that depend on the doubling dimension; the space does not. These performance times are independent of the aspect ratio (or spread) of the points. Categories and Subject Descriptors: F.2.2 [Nonnumerical Algorithms and Problems]:Sorting and searching, computations on discrete structures; E.1 [Data Structures]:Graphs and networks, trees.
249|The black-box complexity of nearest neighbor search|We define a natural notion of efficiency for approximate nearest-neighbor (ANN) search in general n-point metric spaces, namely the existence of a randomized algorithm which answers (1 + e)-approximate nearest neighbor queries in polylog(n) time using only polynomial space. We then study which families of metric spaces admit efficient ANN schemes in the black-box model, where only oracle access to the distance function is given, and any query consistent with the triangle inequality may be asked. For e &lt; 2 5, we offer a complete answer to this problem. Using the notion of metric dimension defined in [GKL03] (à la [Ass83]), we show that a metric space X admits an efficient (1+e)-ANN scheme for any e &lt; 2 5 if and only if dim(X)  = O(log log n). For coarser approximations, clearly the upper bound continues to hold, but there is a threshold at which our lower bound breaks down—this is precisely when points in the “ambient space ” may begin to affect the complexity of “hard ” subspaces S ? X. Indeed, we give examples which show that dim(X) does not characterize the black-box complexity of ANN above the threshold. Our scheme for ANN in low-dimensional metric spaces is the first to yield efficient algorithms without relying on any additional assumptions on the input. In previous approaches (e.g., [Cla99, KR02, KL04, HKMR04]), even spaces with dim(X)  = O(1) sometimes required ?(n) query times. 1
250|Approximating minimization diagrams and generalized proximity search|We investigate the classes of functions whose minimization diagrams can be approximated efficiently in IRd. We present a general framework and a data-structure that can be used to approximate the minimization diagram of such functions. The resulting data-structure has near linear size and can answer queries in logarithmic time. Applications include approximating the Voronoi diagram of (additively or multiplicatively) weighted points. Our technique also works for more general distance functions, such as metrics induced by convex bodies, and the nearest furthest-neighbor distance to a set of point sets. Interestingly, our framework works also for distance functions that do not comply with the triangle inequality. For many of these functions no near-linear size approximation was known before. 1.
251|Near-optimal hashing algorithms for approximate nearest neighbor in high dimensions|In this article, we give an overview of efficient algorithms for the approximate and exact nearest neighbor problem. The goal is to preprocess a dataset of objects (e.g., images) so that later, given a new query object, one can quickly return the dataset object that is most similar to the query. The problem is of significant interest in a wide variety of areas.  
252|Geometric Range Searching and Its Relatives |... process a set S of points in so that the points of S lying inside a query R region can be reported or counted quickly. Wesurvey the known techniques and data structures for range searching and describe their application to other related searching problems. 
253|RAY SHOOTING AND PARAMETRIC SEARCH|Efficient algorithms for the ray shooting problem are presented: Given a collection F of objects in d, build a data structure so that, for a query ray, the first object of F hit by the ray can be quickly determined. Using the parametric search technique, this problem is reduced to the segment emptiness problem. For various ray shooting problems, space/query-time trade-offs of the following type are achieved: For some integer b and a parameter m (n _&lt; m &lt; n b) the queries are answered in time O((n/m /b) log &lt;) n), with O(m!+) space and preprocessing time (t&gt; 0 is arbitrarily small but fixed constant), b Ld/2J is obtained for ray shooting in a convex d-polytope defined as an intersection of n half spaces, b d for an arrangement of n hyperplanes in d, and b 3 for an arrangement of n half planes in 3. This approach also yields fast procedures for finding the first k objects hit by a query ray, for searching nearest and farthest neighbors, and for the hidden surface removal. All the data structures can be maintained dynamically in amortized time O (m + / n) per insert/delete operation.
254|Nearest-neighbor searching and metric space dimensions|Given a set S of n sites (points), and a distance measure d, the nearest neighbor searching problem is to build a data structure so that given a query point q, the site nearest to q can be found quickly. This paper gives a data structure for this problem; the data structure is built using the distance function as a “black box”. The structure is able to speed up nearest neighbor searching in a variety of settings, for example: points in low-dimensional or structured Euclidean space, strings under Hamming and edit distance, and bit vector data from an OCR application. The data structures are observed to need linear space, with a modest constant factor. The preprocessing time needed per site is observed to match the query time. The data structure can be viewed as an application of a “kd-tree ” approach in the metric space setting, using Voronoi regions of a subset in place of axis-aligned boxes. 1
255|Efficient Partition Trees|We prove a theorem on partitioning point sets in Ed (d fixed) and give an efficient construction of partition trees based on it. Such a partition tree for n points uses O(n) space, can be constructed in O(n log n) de-terministic time, and it can be used to answer simplex range queries (counting or general semigroup ones) in time O(nl–lld(log n) O[lJ). If we allow O(nl+d) pre-processing time, where 6 is some positive constant, a more complicated data structure yields query time O(nl-lld(loglog n) O(lJ). J?or dimensions d = 2,3 and if the weights of the points lie in a group, we get query time 0(nl-11d20(10g ” ‘)). This attains the lower bounds due to [Chazelle 89] upto polylogarithrnic factors, im-proving the results of [Chazelle et al. 90], making the preprocessing deterministic without loss of efficiency and simplifying the query answering algorithm. Dy-namization of our data structures and tradeoffs between preprocessing (and storage) and query time are also possible. The partition result is also applied for a deterministic computation of c-cuttings. E.g., given a collection of n lines in the plane and a parameter r &lt; nl- $ for a fixed 6&gt;0, the plane can be subdivided into O(r2) triangles, each intersected by at most n/r of the given lines, in time O(nl/2r3/2 + n log r), thus in almost linear time for r &lt; n113. ?Part of thk research waa performed while the author was visiting at the Freie Universit5t Berlin.
257|Optimal Partition Trees|We revisit one of the most fundamental classes of data structure problems in computational geometry: range searching. Back in SoCG’92, Matou?sek gave a partition tree method for d-dimensional simplex range searching achieving O(n) space and O(n 1-1/d) query time. Although this method is generally believed to be optimal, it is complicated and requires O(n 1+e) preprocessing time for any fixed e&gt; 0. An earlier method by Matou?sek (SoCG’91) requires O(n log n) preprocessing time but O(n1-1/d log O(1) n) query time. We give a new method that achieves simultaneously O(n log n) preprocessing time, O(n) space, and O(n1-1/d) query time with high probability. Our method has several advantages: • It is conceptually simpler than Matou?sek’s SoCG’92 method. Our partition trees satisfy many ideal properties (e.g., constant degree, optimal crossing number at almost all layers, and disjointness of the children’s cells at each node). • It leads to more efficient multilevel partition trees, which are important in many data structural applications (each level adds at most one logarithmic factor to the space and query bounds, better than in all previous methods). • A similar improvement applies to a shallow version of partition trees, yielding O(n log n) time, O(n) space, and O(n 1-1/?d/2 ? ) query time for halfspace range emptiness in even dimensions d = 4. Numerous consequences follow (e.g., improved results for computing spanning trees with low crossing number, ray shooting among line segments, intersection searching, exact nearest neighbor search, linear programming queries, finding extreme points,...). 1
258|Nearest-Neighbor Searching Under Uncertainty|Nearest-neighbor queries, which ask for returning the nearest neighbor of a query point in a set of points, are important and widely studied in many fields because of a wide range of applications. In many of these applications, such as sensor databases, location based services, face recognition, and mobile data, the location of data is imprecise. We therefore study nearest neighbor queries in a probabilistic framework in which the location of each input point and/or query point is specified as a probability density function and the goal is to return the point that minimizes the expected distance, which we refer to as the expected nearest neighbor (ENN). We present methods for computing an exact ENN or an e-approximate ENN, for a given error parameter 0 &lt; e &lt; 1, under different distance functions. These methods build an index of near-linear size and answer ENN queries in polylogarithmic or sublinear time, depending on the underlying function. As far as we know, these are the first nontrivial methods for answering exact or e-approximate ENN queries with provable performance guarantees.  
259|Approximate Nearest Neighbor: Towards Removing the Curse of Dimensionality|We present two algorithms for the approximate nearest neighbor problem in high-dimensional spaces. For data sets of size n living in Rd, the algorithms require space that is only polynomial in n and d, while achieving query times that are sub-linear in n and polynomial in d. We also show applications to other high-dimensional geometric problems, such as the approximate minimum spanning tree. The article is based on the material from the authors’ STOC’98 and FOCS’01 papers. It unifies, generalizes and simplifies the results from those papers. 
260|Using the k-nearest neighbor graph for proximity searching in metric spaces|Abstract. Proximity searching consists in retrieving from a database, objects that are close to a query. For this type of searching problem, the most general model is the metric space, where proximity is defined in terms of a distance function. A solution for this problem consists in building an offline index to quickly satisfy online queries. The ultimate goal is to use as few distance computations as possible to satisfy queries, since the distance is considered expensive to compute. Proximity searching is central to several applications, ranging from multimedia indexing and querying to data compression and clustering. In this paper we present a new approach to solve the proximity searching problem. Our solution is based on indexing the database with the knearest neighbor graph (knng), which is a directed graph connecting each element to its k closest neighbors. We present two search algorithms for both range and nearest neighbor queries which use navigational and metrical features of the knng graph. We show that our approach is competitive against current ones. For instance, in the document metric space our nearest neighbor search algorithms perform 30 % more distance evaluations than AESA using only a 0.25 % of its space requirement. In the same space, the pivot-based technique is completely useless. 1
261|Iterated Nearest Neighbors and Finding Minimal Polytopes|Weintroduce a new method for finding several types of optimal k-point  sets, minimizing perimeter, diameter, circumradius, and related measures, by  testing sets of the O(k) nearest neighbors to each point. We argue that this is  better in a number of ways than previous algorithms, whichwere based on high  order Voronoi diagrams. Our technique allows us for the first time to efficiently  maintain minimal sets as new points are inserted, to generalize our algorithms  to higher dimensions, to find minimal convex k-vertex polygons and polytopes,  and to improvemany previous results. Weachievemany of our results via  a new algorithm for finding rectilinear nearest neighbors in the plane in time  O(n log n+kn). We also demonstrate a related technique for finding minimum  area k-point sets in the plane, based on testing sets of nearest vertical neighbors  to each line segment determined by a pair of points. A generalization of this  technique also allows us to find minimum volume and boundary measure sets  in arbitrary dimensions.
262|A Fast Algorithm for the All k Nearest Neighbors Problem in General Metric Spaces |Given a database of sites or elements of a metric space, the all-k-nearest-neighbor problem consists in finding, for each element, its k nearest neighbors. Using the brute force approach the problem is solved with O(n&amp;sup2;) distance computations. An obvious idea is to index the set somehow so as to reduce this complexity. Yet, it is not obvious what kind of indexing can be helpful and there exist few proposals valid for general metric spaces. In this paper we present a general method to compute the k nearest neighbors of each element which takes advantage of any index able to answer fixed radius queries, a much better known problem. The approach is incremental, starting with a rough guess of the set of neighbors and refining it as more information is available throughout the process. The elements are solved one by one, the cost for the first being high, but dropping as we solve more and more elements. Assuming that the set has n elements and the index examines  n (0    1) elements for a fixed radius query that retrieves n elements from the set, our amortized analysis shows that our algorithm takes average time O  n 2+ 1+  . Our experimental results confirm the subquadraticity of the algorithm and show that it is very efficient in practice.
263|Practical Construction of Metric t-Spanners|Let $G(V,A)$ be a connected graph with a nonnegative cost function $d: A \rightarrow \mathbb{R}^{+}$. Let $d_G(u,v)$ be the cost of the cheapest path between $u,v \in V$. A $t$-spanner of $G$ is a subgraph $G&#039;(V,E)$, $E \subseteq A$, such that $\forall~u,v \in V,~d_{G&#039;}(u,v) \le t \cdot d_G(u,v),~t &gt; 1$. We focus on the metric space context, which means that $A = V \times V$, $d$ is a metric, and $t \le 2$. Several algorithms to build $t$-spanners are known, but they do not seem to apply well to our case. We present four practical algorithms to build $t$-spanners with empirical time costs of the form $C_t \cdot n^{2+\frac{0.1 \ldots 0.2}{t-1}}$ and number of edges of the form $C_e \cdot n^{1+\frac{0.1 \ldots 0.2}{t-1}}$. These algorithms are useful on general graphs as well.
264|Practical construction of k nearest neighbor graphs in metric spaces|Abstract. Let U be a set of elements and d a distance function defined among them. Let NNk(u)d be the k elements in U - {u} which have the smallest distance to u. The k-nearest neighbors graph (knng) is a directed graph G(U, E) such that E = {(u, v, d(u, v)), v ? NNk(u)d}. We focus on the metric space context, so d is a metric. Several knngs construction algorithms are known, but they are not suitable to general metric spaces. We present two practical algorithms to construct knngs that exploit several features of metric spaces, obtaining time costs of the form O(n 1.63..2.24 k 0.02..0.59), and using O(n 0.91..1.96 k 0.04..0.66) distance computations. 1
265|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
266|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
267|Test stimulus representation and experimental context effects in memory scanning |The 5s performed a memory-scanning task in which they indicated whether or not a given test stimulus (letter or picture) matched one of a previously memorized set of letters. The test stimuli presented during a given session were either exclusively letters (a letter session), exclusively pictures (a picture session), or a random sequence of both (a mixed session). Reactiontime functions relating response latency to the size of the memorized set of letters were plotted, and the data are discussed in the context of the scanning models previously proposed by S. Sternberg. The reaction time functions of letter sessions and picture sessions were found to be consistent with the exhaustive model for memory scanning. However, the functions for mixed sessions deviated markedly from the predictions of such a model. The context in which a scanning task is imbedded appears to have a substantial effect on reaction time functions. Evidence that scans of information stored in short-term memory are serial and exhaustive
268|Stimulus probability and stimulus set size in memory scanning|In many recent studies of speeded scanning of immediate memory, variations in the size of the positive set (s) were confounded with variations in the probability (P) of the individual items within the positive set: As s increased, P decreased. The present experiment sought to determine whether the effect on RT attributed to s could be accounted for by variations in P. This was accomplished by factorially varying both s and P. Probability effects were confined to items in the positive set and were insufficient to account for the effect of s. The results are discussed in terms of a model in which s and P affect different information-processing stages. The s affects the number of compari-sons between the encoded item and the items stored in the memory of the positive set, as proposed by Sternberg. The P affects response selection— information as to the particular digit that was presented is available to the mechanisms for response selection along with the knowledge that there was or was not a match. The response selection mechanisms are assumed to be biased in tune with the P values of the items within the positive set. The number of things that one has to think about and the expectancy as to the likelihood of occurrence of these things— stimulus number and stimulus probability —have long been regarded as fundamental variables in the study of cognition. The common rinding that longer RTs would be produced by an increase in the number of possible stimuli or a decrease in stimulus probability was a result that was compati-ble with most theories of stimulus recogni-tion. Discriminating among the various theoretical accounts for these effects has been a more elusive task. One class of models holds that variations in stimulus probability and stimulus num-ber affect only a single commodity such as information (in bits) or repetitions. Ex-amples of such models are those that posit
269|Analysis of Search Algorithms and Tree Structures for Proximity Search in Metric Spaces by |Proximity search in metric spaces involves searching the elements of a set that are close to a specified query point when the data elements form a metric space. The triangle inequality is a fundamental property of metric spaces and can be utilized in various ways to prune the metric search space. There are various frameworks under which metric spaces have been organized and the algorithms used to perform proximity queries are dependent on how the metric space tree has been structured. We present a classification of the search strategies based on triangle inequalities and the metric tree indexing algorithms. Algorithms are presented for various combinations of these strategies which result in different trade-offs of the time and space required for the search. Experimental analysis of these algorithms is performed in the context of the biological database management system called MoBIoS (Molecular Biological Information System) that we are developing. 1. 
270|An Assessment of a Metric Space Database Index to Support Sequence Homology|Hierarchical metric-space clustering methods have been commonly used to organize proteomes into taxonomies. Consequently, it is often anticipated that hierarchical clustering can be leveraged as a basis for scalable database index structures capable of managing the hyper-exponential growth of sequence data. M-tree is one such data structure specialized for the management of large data sets on disk. We explore the application of M-trees to the storage and retrieval of peptide sequence data. Exploiting a technique first suggested by Myers, we organize the database as records of fixed length substrings. Empirical results are promising. However, metric-space indexes are subject to “the curse of dimensionality ” and the ultimate performance of an index is sensitive to the quality of the initial construction of the index. We introduce new hierarchical bulk-load algorithm that alternates between top-down and bottom-up clustering to initialize the index. Using the Yeast Proteomes, the bi-directional bulk load produces a more effective index than the existing M-tree initialization algorithms. 1.
271|Visual detection in relation to display size and redundancy of critical elements |Visual detection was studied in relation to displays of discrete elements, randomly selected consonant letters, distributed in random subsets of cells of a matrix, the subject being required on each trial to indicate only which member of a predesignated pair of critical elements was present in a given display. Experimental variables were number of elements per display and number of redundant critical elements per display. Estimates of the number of elements effectively processed by a subject during a 50 ms. exposure increased with display size, but not in the manner that would be expected if the subject sampled a fixed proportion of the elements present in a display of given area. Test-retest data indicated substantial correlations over long intervals of time in the particular elements sampled by a subject
272|Photo tourism: Exploring photo collections in 3D|Figure 1: Our system takes unstructured collections of photographs such as those from online image searches (a) and reconstructs 3D points and viewpoints (b) to enable novel ways of browsing the photos (c). We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.
273|Distinctive Image Features from Scale-Invariant Keypoints|This paper presents a method for extracting distinctive invariant features from  images, which can be used to perform reliable matching between different images  of an object or scene. The features are invariant to image scale and rotation,  and are shown to provide robust matching across a a substantial range  of affine distortion, addition of noise, change in 3D viewpoint, and change in  illumination. The features are highly distinctive, in the sense that a single feature  can be correctly matched with high probability against a large database of  features from many images. This paper also describes an approach to using  these features for object recognition. The recognition proceeds by matching  individual features to a database of features from known objects using a fast  nearest-neighbor algorithm, followed by a Hough transform to identify clusters  belonging to a single object, and finally performing verification through leastsquares  solution for consistent pose parameters. This approach to recognition  can robustly identify objects among clutter and occlusion while achieving near  real-time performance.
275|Video google: A text retrieval approach to object matching in videos|We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieval is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching on two full length feature films. 1.
276|Light Field Rendering|A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function - the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a 
278|The Lumigraph|This paper discusses a new method for capturing the complete appearanceof both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation. 1
279|Plenoptic Modeling: An Image-Based Rendering System|Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convincing animations without an explicit geometric representation. We use the “plenoptic function” of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a parameterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections.
280|Labeling Images with a Computer Game|We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people  play the game they help determine the contents of images  by providing meaningful labels for them. If the game is  played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the  Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users  block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don&#039;t work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.
281|LabelMe: A Database and Web-Based Tool for Image Annotation|  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.
282| View Interpolation for Image Synthesis |Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today’s high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources.
283|A Touring Machine: Prototyping 3D Mobile Augmented Reality Systems for Exploring the Urban Environment|We describe a prototype system that combines together the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university’s campus, using a head-tracked, see-through, headworn, 3D display, and an untracked, opaque, handheld, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.
284|A comparison of affine region detectors|The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris [24, 34] and Hessian points [24], as proposed by Mikolajczyk and Schmid and by Schaffalitzky and Zisserman; a detector of ‘maximally stable extremal regions’, proposed by Matas et al. [21]; an edge-based region detector [45] and a detector based on intensity extrema [47], proposed by Tuytelaars and Van Gool; and a detector of ‘salient regions’, proposed by Kadir, Zisserman and Brady [12]. The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework. 1
285|Unstructured lumigraph rendering|We describe an image based rendering approach that generalizes many image based rendering algorithms currently in use including light field rendering and view-dependent texture mapping. In particular it allows for lumigraph style rendering from a set of input cameras that are not restricted to a plane or to any specific manifold. In the case of regular and planar input camera positions, our algorithm reduces to a typical lumigraph approach. In the case of fewer cameras and good approximate geometry, our algorithm behaves like view-dependent texture mapping. Our algorithm achieves this flexibility because it is designed to meet a set of desirable goals that we describe. We demonstrate this flexibility with a variety of examples. Keyword Image-Based Rendering 1
286|View morphing|Image morphing techniques can generate compelling 2D transitions between images. However, differences in object pose or viewpoint often cause unnatural distortions in image morphs that are difficult to correct manually. Using basic principles of projective geometry, this paper introduces a simple extension to image morphing that correctly handles 3D projective camera and scene transformations. The technique, called view morphing, works by prewarping two images prior to computing a morph and then postwarping the interpolated images. Because no knowledge of 3D shape is required, the technique may be applied to photographs and drawings, as well as rendered scenes. The ability to synthesize changes both in viewpoint and image structure affords a wide variety of interesting 3D effects via simple image transformations.
287|How Do People Manage Their Digital Photographs|In this paper we present and discuss the findings of a study that investigated how people manage their collections of digital photographs. The six-month, 13-participant study included interviews, questionnaires, and analysis of usage statistics gathered from an instrumented digital photograph management tool called Shoebox. Alongside simple browsing features such as folders, thumbnails and timelines, Shoebox has some advanced multimedia features: content-based image retrieval and speech recognition applied to voice annotations. Our results suggest that participants found their digital photos much easier to manage than their non-digital ones, but that this advantage was almost entirely due to the simple browsing features. The advanced features were not used very often and their perceived utility was low. These results should help to inform the design of improved tools for managing personal digital photographs.
288|Video Indexing Based on Mosaic Representations|Video is a rich source of information. It provides visual information about scenes. However, this information is implicitly buried inside the raw video data, and is provided with the cost of very high temporal redundancy. While the standard sequential form of video storage is adequate for viewing in a &#034;movie mode&#034;, it fails to support rapid access to information of interest that is required in many of the emerging applications of video. This paper presents an approach for efficient access, use and manipulation of video data. The video data is first transformed from its sequential and redundant frame-based representation in which the information about the scene is distributed over many frames, to an explicit and compact scene-based representation, to which each frame can be directly related. This compact reorganization of the video data supports  non-linear browsing and efficient indexing to provide rapid access directly to information of interest. The paper describes a new set of metho...
289|Modelling and interpretation of architecture from several images |The modelling of 3-dimensional (3D) environments has become a requirement for many applications in engineering design, virtual reality, visualisation and entertainment. However the scale and complexity demanded from such models has risen to the point where the acquisition of 3D models can require a vast amount of specialist time and equipment. Because of this much research has been undertaken in the computer vision community into automating all or part of the process of acquiring a 3D model from a sequence of images. This thesis focuses specifically on the automatic acquisition of architectural models from short image sequences. An architectural model is defined as a set of planes corresponding to walls which contain a variety of labelled primitives such as doors and windows. As well as a label defining its type, each primitive contains parameters defining its shape and texture. The key advantage of this representation is that the model defines not only geometry and texture, but also an interpretation of the scene. This is crucial as it enables reasoning about the scene; for instance, structure and texture can be inferred in areas of the model which are unseen in any
290|Image alignment and stitching: a tutorial|This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce
291|The design and implementation of a generic sparse bundle adjustment software package based on the levenberg-marquardt algorithm|The most recent revision of this document will always be found at
292|Automatic Line matching across views|HAL is a multi-disciplinary open access archive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destine´e au de´po^t et a ` la diffusion de documents scientifiques de niveau recherche, publie´s ou non, e´manant des e´tablissements d’enseignement et de recherche franc¸ais ou e´trangers, des laboratoires publics ou prive´s.
293|Temporal event clustering for digital photo collections |Organizing digital photograph collections according to events such as holiday gatherings or vacations is a common practice among photographers. To support photographers in this task, we present similarity-based methods to cluster digital photos by time and image content. The approach is general and unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity. First, interphoto similarity is quantified at multiple temporal scales to identify likely event clusters. Second, the final clusters are determined according to one of three clustering goodness criteria. The clustering criteria trade off computational complexity and performance. We also describe a supervised clustering method based on learning vector quantization. Finally, we review the results of an experimental evaluation of the proposed algorithms and existing approaches on two test collections.
294|Automatic organization for digital photographs with geographic coordinates|We describe PhotoCompas, a system that utilizes the time and location information embedded in digital photographs to automatically organize a personal photo collection. PhotoCompas produces browseable location and event hierarchies for the collection. These hierarchies are created using algorithms that interleave time and location to produce an organization that mimics the way people think about their photo collections. In addition, the algorithm annotates the generated hierarchy with geographical names. We tested our approach in case studies of three real-world collections and verified that the results are meaningful and useful for the collection owners.
295|Calibrated, Registered Images of an Extended Urban Area|We describe a dataset of several thousand calibrated, time-stamped, geo-referenced, high  dynamic range color images, acquired under uncontrolled, variable illumination conditions in  an outdoor region spanning several hundred meters. The image data is grouped into several  regions which have little mutual inter-visibility. For each group, the calibration data is globally  consistent on average to roughly five centimeters and 0.1 # , or about four pixels of epipolar  registration. All image, feature and calibration data is available for interactive inspection and  downloading at http://city.lcs.mit.edu/data.
296|From where to what: Metadata sharing for digital photographs with geographic coordinates|Abstract. We describe LOCALE, a system that allows cooperating information systems to share labels for photographs. Participating photographs are enhanced with a geographic location stamp – the latitude and longitude where the photograph was taken. For a photograph with no label, LOCALE can use the shared information to assign a label based on other photographs that were taken in the same area. LOCALE thus allows (i) text search over unlabeled sets of photos, and (ii) automated label suggestions for unlabeled photos. We have implemented a LOCALE prototype where users cooperate in submitting labels and locations, enhancing search quality for all users in the system. We ran an experiment to test the system in centralized and distributed settings. The results show that the system performs search tasks with surprising accuracy, even when searching for specific landmarks. 1
297|Interactive Design of Multi-Perspective Images For Visualizing Urban Landscapes|Multi-perspective images are a useful way to visualize extended, roughly planar scenes such as landscapes or city blocks. However, constructing effective multi-perspective images is something of an art. In this paper, we describe an interactive system for creating multi-perspective images composed of serially blended cross-slits images. Beginning with a sideways-looking video of the scene as might be captured from a moving vehicle, we allow the user to interactively specify a set of cross-slits cameras, possibly with gaps between them. In each camera, one of the slits is defined to be the camera path, which is typically horizontal, and the user is left to choose the second slit, which is typically vertical. The system then generates intermediate views between these cameras using a novel interpolation scheme, thereby producing a multi-perspective image with no seams. The user can also choose the picture surface in space onto which viewing rays are projected, thereby establishing a parameterization for the image. We show how the choice of this surface can be used to create interesting visual effects. We demonstrate our system by constructing multi-perspective images that summarize city blocks, including corners, blocks with deep plazas and other challenging urban situations.
298|A System Architecture for Ubiquitous Video |Realityflythrough is a telepresence/tele-reality system that works in the dynamic, uncalibrated environments typically associated with ubiquitous computing. By harnessing networked mobile video cameras, it allows a user to remotely and immersively explore a physical space. RealityFlythrough creates the illusion of complete live camera coverage in a physical environment. This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: (1) its support of the abstractions for infinite camera coverage, (2) its scalability, and (3) its robustness to changing user requirements. 1
299|A System for Automatic Pose-Estimation from a Single Image in a City Scene|We describe an automatic system for pose-estimation from a single image in a city scene. Each building has a model consisting of a number of parallel planes associated with it. The homographies for the best match of the planes to the image is estimated automatically for each of the possible buildings. We show how the estimation of homographies can be done effectively by reducing the search space and using fast convolution. The model having the best match is then used to determine the position and orientation of the camera. The results
300|Sea of Images|A long-standing research problem in computer graphics is to reproduce the visual experience of walking through a large photorealistic environment interactively. On one hand, traditional geometry-based rendering systems fall short of simulating the visual realism of a complex environment. On the other hand, image-based rendering systems have to date been unable to capture and store a sampled representation of a large environment with complex lighting and visibility effects.
301|Spectral Partitioning for Structure from Motion |We propose a spectral partitioning approach for large-scale optimization problems, specifically structure from motion. In structure from motion, partitioning methods reduce the problem into smaller and better conditioned subproblems which can be efficiently optimized. Our partitioning method uses only the Hessian of the reprojection error and its eigenvectors. We show that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized. We create partitions by clustering the entries of the eigenvectors of the Hessian corresponding to small eigenvalues. This is a more general technique than relying on domain knowledge and heuristics such as bottom-up structure from motion approaches. Simultaneously, it takes advantage of more information than generic matrix partitioning algorithms.
302|Interactive Image-Based Rendering Using Feature Globalization|Image-based rendering (IBR) systems enable virtual walkthroughs of photorealistic environments by warping and combining reference images to novel viewpoints under interactive user control. A significant challenge in such systems is to automatically compute image correspondences that enable accurate image warping.
303|Eye movements in reading and information processing: 20 years of research|Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed. Many studies using eye movements to investigate cognitive processes have appeared over the past 20 years. In an earlier review, I (Rayner, 1978b) argued that since the mid-1970s we have been in a third era of eye movement research and that the success of research in the current era would depend on the ingenuity of researchers in designing interesting and informative
304|What one intelligence test measures: A theoretical account of the processing|Raven Progressive
305|Scene perception: detecting and judging objects undergoing relational violations|Five classes of relations between an object and its setting can characterize the organization of objects into real-world scenes. The relations are (1) Interposition (objects interrupt their background), (2) Support (objects tend to rest on surfaces), (3) Probability (objects tend to be found in some scenes but not others), (4) Position (given an object is probable in a scene, it often is found in some positions and not others), and (5) familiar Size (objects have a limited set of size relations with other objects). In two experiments subjects viewed brief (150 msec) presentations of slides of scenes in which an object in a cued location in the scene was either in a normal relation to its background or violated from one to three of the relations. Such objects appear to (1) have the background pass through them, (2) float in air, (3) be unlikely in that particular scene, (4) be in an inappropriate position, and (5) be too large or too small relative to the other objects in the scene. In Experiment I, subjects attempted to determine whether the cued object corresponded to a target object which had been specified in advance by name. With the exception of the Interposition violation, violation costs were incurred in that the
306|An analysis of the saccadic system by means of double step stimuli|Abatrac-The characteristics of saccadic reactions to double steps of a target were analysed as a function of the time lapse between the second target step and the onset of the response. The analysis suggests that goal-directed saccades are prepared in two steps; first a decision as to their direction is taken which requires a randomly varying time, and subsequently their amplitude is calculated as a time average of the fixation error. In addition. the analysis demonstrates that the preparatory processes of two different saccades may overlap in time (“parallel programming”) and that. although reacting in a discontinuous manner, the saccadic system continuously processes the afferent visual information. A conceptual model based on an internal predictive feedback pathway and on a non-linear decision mechanism is proposed that accounts for the observed behaviour. (Trunslated abstract at end of paper) 1. INTRODUCIION The saccadic branch of the human oculomotor system has been studied quite extensively with methods adapted from control theory, and several models have been proposed to account for the observed behaviour
307|Eye fixations and memory for emotional events|Subjects watched either an emotional, neutral, or unusual sequence of slides containing 1 critical slide in the middle. Experiments 1 and 2 allowed only a single eye fixation on the critical slide by presenting it for 180 ms (Experiment 1) or 150 ms (Experiment 2). Despite this constraint, memory for a central detail was better for the emotional condition. In Experiment 3, subjects were allowed 2.70 s to view the critical slide while their eye movements were monitored. When subjects who had devoted the same number of fixations were compared, memory for the central detail of the emotional slide was again better. The results suggest that enhanced memory for detail information of an emotional event does not occur solely because more attention is devoted to the emotional information. The purpose of this series of studies was to examine the role of attention and focusing patterns in memory for emotional versus neutral events. We use the term emotional events in this paper to refer to scenes that have unpleasant visual features (e.g., blood) and that have the potential to evoke negative emotional feelings in the viewer. How well are details
308|Dropout from higher education: A theoretical synthesis of recent research|Despite the very extensive literature on dropout from higher education, much remains unknown about the nature of the dropout process. In large measure, the failure of past research to delineate more clearly the multiple characteristics of dropout can be traced to two major shortcomings; namely, inadequate atten-tion given to questions of definition and to the development of theoretical models that seek to explain, not simply to describe, the processes that bring individuals to leave institutions of higher education. With regard to the former, inadequate attention given to definition has often led researchers to lump together, under the rubric of dropout, forms of leaving behavior that are very differ-ent in character. It is not uncommon to find, for instance, research on dropout that fails to distinguish dropout resulting from academic failure from that which is the outcome of voluntary withdrawal. Nor is it uncommon to find permanent dropouts placed together with persons whose leaving may be temporary in I am indebted to my research assistant, John B. Cullen, for having made an extensive literature search and compiling summaries of the literature for me. I am also indebted to Professors Peter Moock, to John Weidman, and to an unknown reviewer for their insightful comments on an earlier draft of this paper. The work reported here overlaps to a large extent work performed for the Office of
309|Attrition among college students|In the fall of 1961 a study of all entering freshmen students at a national sample of 248 colleges and universities was conducted at the National Merit Scholarship Corporation (Astin, 1965). The 127,212 students who participated in the study provided informa-tion on their socioeconomic backgrounds, high school activities and achievements, and educational and vocational aspirations.1 In the summer of 1965 the Office of Research of the American Council on Education2 followed up randomly selected samples of students from each of 246 colleges and universities included in the 1961 survey.3 Questionnaires were mailed to 60,078 of the original 127,212 students—or approximately 250 students per institution.4
310|Structural Proximity Searching for Large Collections of Semi-Structured Data|The richness of the XML data format allows data to be structured in a way which precisely captures the semantics required by the author. It is the structure of the data, however, which forms the basis of all XML query languages. Without at least some notion of the structure, a user cannot meaningfully query the data. This problem is compounded when one considers that heterogeneous data adhering to different schema are likely to exist in the database(s) being queried. This paper proposes a solution based on an e- cient proximity index. In particular, we describe a family of encoding and compression schemes which enable us to build an index to eciently implement the proximity search. Our index is extremely small, and can reect updates in the underlying database in modest time. Experiments show that our algorithm and implementation are fast and scale well.
311|Querying Semi-Structured Data|

312|Proximal Nodes: A Model to Query Document Databases by Contents and Structure|  A model to query document databases by both their content and structure is presented. The goal is to obtain a query language which is expressive in practice while being efficiently implementable, features not present at the same time in previous work. The key ideas of the model are a set-oriented query language based on operations on nearby structure elements of one or more hierarchies, together with content and structural indexing and bottom-up evaluation. The model is evaluated regarding expressiveness and efficiency, showing that it provides a good trade-off between both goals. Finally, it is shown how to include in the model other media different from text.  
313|Scalable Recognition with a Vocabulary Tree|A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD&#039;s. The scheme
314|A PERFORMANCE EVALUATION OF LOCAL DESCRIPTORS|In this paper we compare the performance of descriptors computed for local interest regions, as for example extracted by the Harris-Affine detector [32]. Many different descriptors have been proposed in the literature. However, it is unclear which descriptors are more appropriate and how their performance depends on the interest region detector. The descriptors should be distinctive and at the same time robust to changes in viewing conditions as well as to errors of the detector. Our evaluation uses as criterion recall with respect to precision and is carried out for different image transformations. We compare shape context [3], steerable filters [12], PCA-SIFT [19], differential invariants [20], spin images [21], SIFT [26], complex filters [37], moment invariants [43], and cross-correlation for different types of interest regions. We also propose an extension of the SIFT descriptor, and show that it outperforms the original method. Furthermore, we observe that the ranking of the descriptors is mostly independent of the interest region detector and that the SIFT based descriptors perform best. Moments and steerable filters show the best performance among the low dimensional descriptors.
315|An affine invariant interest point detector|Abstract. This paper presents a novel approach for detecting affine invariant interest points. Our method can deal with significant affine transformations including large scale changes. Such transformations introduce significant changes in the point location as well as in the scale and the shape of the neighbourhood of an interest point. Our approach allows to solve for these problems simultaneously. It is based on three key ideas: 1) The second moment matrix computed in a point can be used to normalize a region in an affine invariant way (skew and stretch). 2) The scale of the local structure is indicated by local extrema of normalized derivatives over scale. 3) An affine-adapted Harris detector determines the location of interest points. A multi-scale version of this detector is used for initialization. An iterative algorithm then modifies location, scale and neighbourhood of each point and converges to affine invariant points. For matching and recognition, the image is characterized by a set of affine invariant points; the affine transformation associated with each point allows the computation of an affine invariant descriptor which is also invariant to affine illumination changes. A quantitative comparison of our detector with existing ones shows a significant improvement in the presence of large affine deformations. Experimental results for wide baseline matching show an excellent performance in the presence of large perspective transformations including significant scale changes. Results for recognition are very good for a database with more than 5000 images.
316|Robust wide baseline stereo from maximally stable extremal regions|The wide-baseline stereo problem, i.e. the problem of establishing correspon-dences between a pair of images taken from different viewpoints is studied. A new set of image elements that are put into correspondence, the so called extremal regions, is introduced. Extremal regions possess highly de-sirable properties: the set is closed under 1. continuous (and thus projective) transformation of image coordinates and 2. monotonic transformation of im-age intensities. An efficient (near linear complexity) and practically fast de-tection algorithm (near frame rate) is presented for an affinely-invariant stable subset of extremal regions, the maximally stable extremal regions (MSER). A new robust similarity measure for establishing tentative correspon-dences is proposed. The robustness ensures that invariants from multiple measurement regions (regions obtained by invariant constructions from ex-tremal regions), some that are significantly larger (and hence discriminative) than the MSERs, may be used to establish tentative correspondences. The high utility of MSERs, multiple measurement regions and the robust metric is demonstrated in wide-baseline experiments on image pairs from both indoor and outdoor scenes. Significant change of scale (3.5×), illumi-nation conditions, out-of-plane rotation, occlusion, locally anisotropic scale change and 3D translation of the viewpoint are all present in the test prob-lems. Good estimates of epipolar geometry (average distance from corre-sponding points to the epipolar line below 0.09 of the inter-pixel distance) are obtained. 1
317|The pyramid match kernel: Discriminative classification with sets of image features|Discriminative learning is challenging when examples are sets of features, and the sets vary in cardinality and lack any sort of meaningful ordering. Kernel-based classification methods can learn complex decision boundaries, but a kernel over unordered set inputs must somehow solve for correspondences – generally a computationally expensive task that becomes impractical for large set sizes. We present a new fast kernel function which maps unordered feature sets to multi-resolution histograms and computes a weighted histogram intersection in this space. This “pyramid match” computation is linear in the number of features, and it implicitly finds correspondences based on the finest resolution histogram cell where a matched pair first appears. Since the kernel does not penalize the presence of extra features, it is robust to clutter. We show the kernel function is positive-definite, making it valid for use in learning algorithms whose optimal solutions are guaranteed only for Mercer kernels. We demonstrate our algorithm on object recognition tasks and show it to be accurate and dramatically faster than current approaches. 
318|Shape matching and object recognition using low distortion correspondence|We approach recognition in the framework of deformable shape matching, relying on a new algorithm for finding correspondences between feature points. This algorithm sets up correspondence as an integer quadratic programming problem, where the cost function has terms based on similarity of corresponding geometric blur point descriptors as well as the geometric distortion between pairs of corresponding feature points. The algorithm handles outliers, and thus enables matching of exemplars to query images in the presence of occlusion and clutter. Given the correspondences, we estimate an aligning transform, typically a regularized thin plate spline, resulting in a dense correspondence between the two shapes. Object recognition is then handled in a nearest neighbor framework where the distance between exemplar and query is the matching cost between corresponding points. We show results on two datasets. One is the Caltech 101 dataset (Fei-Fei, Fergus and Perona), an extremely challenging dataset with large intraclass variation. Our approach yields a 48 % correct classification rate, compared to Fei-Fei et al’s 16%. We also show results for localizing frontal and profile faces that are comparable to special purpose approaches tuned to faces. 1.
319|Evaluation of Interest Point Detectors| Many different low-level feature detectors exist and it is widely agreed that the evaluation of detectors is important. In this paper we introduce two evaluation criteria for interest points: repeatability rate and information content. Repeatability rate evaluates the geometric stability under different transformations. Information content measures the distinctiveness of features. Different interest point detectors are compared using these two criteria. We determine which detector gives the best results and show that it satisfies the criteria well.
320|Reliable Feature Matching Across Widely Separated Views|In this paper we present a robust method for automatically matching features in images corresponding to the same physical point on an object seen from two arbitrary viewpoints. Unlike conventional stereo matching approaches we assume no prior knowledge about the relative camera positions and orientations. In fact in our application this is the information we wish to determine from the image feature matches. Features are detected in two or more images and characterised using affine texture invariants. The problem of window effects is explicitly addressed by our method - our feature characterisation is invariant to linear transformations of the image data including rotation, stretch and skew. The feature matching process is optimised for a structure-from-motion application where we wish to ignore unreliable matches at the expense of reducing the number of feature matches.
321|Randomized trees for realtime keypoint recognition|In earlier work, we proposed treating wide baseline matching of feature points as a classification problem, in which each class corresponds to the set of all possible views of such a point. We used a K-mean plus Nearest Neighbor classifier to validate our approach, mostly because it was simple to implement. It has proved effective but still too slow for real-time use. In this paper, we advocate instead the use of randomized trees as the classification technique. It is both fast enough for real-time performance and more robust. It also gives us a principled way not only to match keypoints but to select during a training phase those that are the most recognizable ones. This results in a real-time system able to detect and position in 3D planar, non-planar, and even deformable objects. It is robust to illuminations changes, scale changes and occlusions. 1.
322|3D Object modeling and recognition using local affine-invariant image descriptors and multi-view spatial constraints|Abstract. This article introduces a novel representation for three-dimensional (3D) objects in terms of local affine-invariant descriptors of their images and the spatial relationships between the corresponding surface patches. Geometric constraints associated with different views of the same patches under affine projection are combined with a normalized representation of their appearance to guide matching and reconstruction, allowing the acquisition of true 3D affine and Euclidean models from multiple unregistered images, as well as their recognition in photographs taken from arbitrary viewpoints. The proposed approach does not require a separate segmentation stage, and it is applicable to highly cluttered scenes. Modeling and recognition results are presented.
323|Shape-adapted smoothing in estimation of 3-D depth cues from affine distortions of local 2-D brightness structure|Rotationally symmetric operations in the image domain may give rise to shape distortions. This article describes a way of reducing this effect for a general class of methods for deriving 3-D shape cues from 2-D image data, which are based on the estimation of locally linearized distortion of brightness patterns. By extending the linear scale-space concept into an affine scale-space representation and performing affine shape adaption of the smoothing kernels, the accuracy of surface orientation estimates derived from texture and disparity cues can be improved by typically one order of magnitude. The reason for this is that the image descriptors, on which the methods are based, will be relative invariant under a ne transformations, and the error will thus be confined to the higher-order terms in the locally linearized perspective mapping. 
324|Evaluation of Local Detectors on Non-Planar Scenes|This paper presents for the first time a method to evaluate the performance of local detectors under viewpoint changes on complex, realistic, and practically relevant scenes. The main contribution is a method which allows the automatic verification of detected corresponding points and regions for non-planar scenes. Using this method the performances of 10 di#erent local detectors were evaluated in a large scale experiment. A ranking of the di#erent detectors has been established based on this evaluation.
325|SNOPT: An SQP Algorithm For Large-Scale Constrained Optimization|Sequential quadratic programming (SQP) methods have proved highly effective for solving constrained optimization problems with smooth nonlinear functions in the objective and constraints. Here we consider problems with general inequality constraints (linear and nonlinear). We assume that first derivatives are available, and that the constraint gradients are sparse. We discuss
327|LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares|An iterative method is given for solving Ax ~ffi b and minU Ax- b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. Reliable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned. Categories and Subject Descriptors: G.1.2 [Numerical Analysis]: ApprorJmation--least squares approximation; G.1.3 [Numerical Analysis]: Numerical Linear Algebra--linear systems (direct and
328|A Limited Memory Algorithm for Bound Constrained Optimization|An algorithm for solving large nonlinear optimization problems with simple bounds is described. It is based
329|Benchmarking Optimization Software with Performance Profiles|We propose performance profiles --- distribution functions for a performance metric ---  as a tool for benchmarking and comparing optimization software. We show that performance  profiles combine the best features of other tools for performance evaluation.  1 Introduction  The benchmarking of optimization software has recently gained considerable visibility. Hans Mittlemann&#039;s [13] work on a variety of optimization software has frequently uncovered deficiencies in the software and has generally led to software improvements. Although Mittelmann&#039;s efforts have gained the most notice, other researchers have been concerned with the evaluation and performance of optimization codes. As recent examples, we cite [1, 2, 3, 4, 6, 12, 17].  The interpretation and analysis of the data generated by the benchmarking process are the main technical issues addressed in this paper. Most benchmarking efforts involve tables displaying the performance of each solver on each problem for a set of metrics such...
330|Nonlinear Programming without a penalty function|In this paper the solution of nonlinear programming problems by a Sequential Quadratic Programming (SQP) trust-region algorithm is considered. The aim of the present work is to promote global convergence without the need to use a penalty function. Instead, a new concept of a &#034;filter&#034; is introduced which allows a step to be accepted if it reduces either the objective function or the constraint violation function. Numerical tests on a wide range of test problems are very encouraging and the new algorithm compares favourably with LANCELOT and an implementation of Sl 1 QP.
331|CUTE: Constrained and unconstrained testing environment|The purpose of this paper is to discuss the scope and functionality of a versatile environment  for testing small and large-scale nonlinear optimization algorithms. Although many  of these facilities were originally produced by the authors in conjunction with the software  package LANCELOT, we believe that they will be useful in their own right and should be available  to researchers for their development of optimization software. The tools are available by  anonymous ftp from a number of sources and may, in many cases, be installed automatically.  The scope of a major collection of test problems written in the standard input format (SIF)  used by the LANCELOT software package is described. Recognising that most software was  not written with the SIF in mind, we provide tools to assist in building an interface between  this input format and other optimization packages. These tools already provide a link between  the SIF and an number of existing packages, including MINOS and OSL. In ad...
332|Representations Of Quasi-Newton Matrices And Their Use In Limited Memory Methods|We derive compact representations of BFGS and symmetric rank-one matrices for optimization. These representations allow us to efficiently implement limited memory methods for large constrained optimization problems. In particular, we discuss how to compute projections of limited memory matrices onto subspaces. We also present a compact representation of the matrices generated by Broyden&#039;s update for solving systems of nonlinear equations. 
333|A trust region method based on interior point techniques for nonlinear programming|Jorge Nocedal z An algorithm for minimizing a nonlinear function subject to nonlinear inequality constraints is described. It applies sequential quadratic programming techniques to a sequence of barrier problems, and uses trust regions to ensure the robustness of the iteration and to allow the direct use of second order derivatives. This framework permits primal and primal-dual steps, but the paper focuses on the primal version of the new algorithm. An analysis of the convergence properties of this method is presented. Key words: constrained optimization, interior point method, large-scale optimization, nonlinear programming, primal method, primal-dual method, SQP iteration, barrier method, trust region method.
334|LARGE-SCALE LINEARLY CONSTRAINED OPTIMIZATION|An algorithm for solving large-scale nonlinear &#039; programs with linear constraints is presented. The method combines efficient sparse-matrix techniques as in the revised simplex method with stable quasi-Newton methods for handling the nonlinearities. A general-purpose production code (MINOS) is described, along with computational experience on a wide variety of problems.
335|A Sqp Method For General Nonlinear Programs Using Only Equality Constrained Subproblems|In this paper we describe a new version of a sequential equality constrained quadratic programming method for general nonlinear programs with mixed equality and inequality constraints. Compared with an older version [34] it is much simpler to implement and allows any kind of changes of the working set in every step. Our method relies on a strong regularity condition. As far as it is applicable the new approach is superior to conventional SQP-methods, as demonstrated by extensive numerical tests. 
337|On the implementation of an algorithm for large-scale equality constrained optimization|Abstract. This paper describes a software implementation of Byrd and Omojokun’s trust region algorithm for solving nonlinear equality constrained optimization problems. The code is designed for the efficient solution of large problems and provides the user with a variety of linear algebra techniques for solving the subproblems occurring in the algorithm. Second derivative information can be used, but when it is not available, limited memory quasi-Newton approximations are made. The performance of the code is studied using a set of difficult test problems from the CUTE collection.
338|Inertia-controlling methods for general quadratic programming|Abstract. Active-set quadratic programming (QP) methods use a working set to define the search direction and multiplier estimates. In the method proposed by Fletcher in 1971, and in several subsequent mathematically equivalent methods, the working set is chosen to control the inertia of the reduced Hessian, which is never permitted to have more than one nonpositive eigenvalue. (We call such methods inertia-controlling.) This paper presents an overview of a generic inertia-controlling QP method, including the equations satisfied by the search direction when the reduced Hessian is positive definite, singular and indefinite. Recurrence relations are derived that define the search direction and Lagrange multiplier vector through equations related to the Karush-Kuhn-Tucker system. We also discuss connections with inertia-controlling methods that maintain an explicit factorization of the reduced Hessian matrix. Key words. Nonconvex quadratic programming, active-set methods, Schur complement, Karush-
339|A sequential quadratic programming algorithm using an incomplete solution of the subproblem|Ary opinions, findings, and conclusions or recommendations expressed in this publication are those of the authors and do NOT necessarily reflect the views of the above sponsors.
340|A Practical Algorithm For General Large Scale Nonlinear Optimization Problems|. We provide an effective and efficient implementation of a sequential quadratic programming (SQP) algorithm for the general large scale nonlinear programming problem. In this algorithm the quadratic programming subproblems are solved by an interior point method that can be prematurely halted by a trust region constraint. Numerous computational enhancements to improve the numerical performance are presented. These include a dynamic procedure for adjusting the merit function parameter and procedures for adjusting the trust region radius. Numerical results and comparisons are presented.  Key words: nonlinear programming, interior point, SQP, merit function, trust region, large scale 1. Introduction. In a series of recent papers, [3], [6], and [8], the authors have developed a new algorithmic approach for solving large, nonlinear, constrained optimization problems. This proposed procedure is, in essence, a sequential quadratic programming (SQP) method that uses an interior point algorithm...
341|Some theoretical properties of an augmented Lagrangian merit function|Sequential quadratic programming (SQP) methods for nonlinearly constrained op-timization typically use a merit function to enforce convergence from an arbitrary starting point. We define a smooth augmented Lagrangian merit function in which the Lagrange multiplier estimate is treated as a separate variable, and inequality con-straints are handled by means of non-negative slack variables that are included in the linesearch. Global convergence is proved for an SQP algorithm that uses this merit function. We also prove that steps of unity are accepted in a neighborhood of the solution when this merit function is used in a suitable superlinearly convergent algo-rithm. Finally, some numerical results are presented to illustrate the performance of the associated SQP method.
342|A globally convergent linearly constrained Lagrangian method for nonlinear optimization|Abstract. For optimization problems with nonlinear constraints, linearly constrained Lagrangian (LCL) methods solve a sequence of subproblems of the form “minimize an augmented Lagrangian function subject to linearized constraints. ” Such methods converge rapidly near a solution but may not be reliable from arbitrary starting points. Nevertheless, the well-known software package MINOS has proved effective on many large problems. Its success motivates us to derive a related LCL algorithm that possesses three important properties: it is globally convergent, the subproblem constraints are always feasible, and the subproblems may be solved inexactly. The new algorithm has been implemented in Matlab, with an option to use either MINOS or SNOPT (Fortran codes) to solve the linearly constrained subproblems. Only first derivatives are required. We present numerical results on a subset of the COPS, HS, and CUTE test problems, which include many large examples. The results demonstrate the robustness and efficiency of the stabilized LCL procedure.
343|User manual for filterSQP|This paper describes a software package for the solution of Nonlinear Programming (NLP) problems. The package implements a Sequential Quadratic Programming solver with a “filter ” to promote global convergence. The solver runs with a dense or a sparse linear algebra package and a robust QP solver.
344|SPARSE MATRIX METHODS IN OPTIMIZATION|  Optimization algorithms typically require the solution of many systems of linear equations Bkyk b,. When large numbers of variables or constraints are present, these linear systems could account for much of the total computation time. Both direct and iterative equation solvers are needed in practice. Unfortunately, most of the off-the-shelf solvers are designed for single systems, whereas optimization problems give rise to hundreds or thousands of systems. To avoid refactorization, or to speed the convergence of an iterative method, it is essential to note that B is related to Bk _ 1. We review various sparse matrices that arise in optimization, and discuss compromises that are currently being made in dealing with them. Since significant advances continue to be made with single-system solvers, we give special attention to methods that allow such solvers to be used repeatedly on a sequence of modified systems (e.g., the product-form update; use of the Schur complement). The speed of factorizing a matrix then becomes relatively less important than the efficiency of subsequent solves with very many right-hand sides. At the same time, we hope that future improvements to linear-equation software will be oriented more specifically to the case of related matrices B k.
346|Limited-memory reduced-Hessian methods for unconstrained optimization, Numerical Analysis|Abstract. Limited-memory BFGS quasi-Newton methods approximate the Hessian matrix of second derivatives by the sum of a diagonal matrix and a fixed number of rank-one matrices. These methods are particularly effective for large problems in which the approximate Hessian cannot be stored explicitly. It can be shown that the conventional BFGS method accumulates approximate curvature in a sequence of expanding subspaces. This allows an approximate Hessian to be represented using a smaller reduced matrix that increases in dimension at each iteration. When the number of variables is large, this feature may be used to define limited-memory reduced-Hessian methods in which the dimension of the reduced Hessian is limited to save storage. Limited-memory reduced-Hessian methods have the benefit of requiring half the storage of conventional limited-memory methods. In this paper, we propose a particular reduced-Hessian method with substantial computational advantages compared to previous reduced-Hessian methods. Numerical results from a set of unconstrained problems in the CUTE test collection indicate that our implementation is competitive with the limited-memory codes L-BFGS and L-BFGS-B. Key words. Unconstrained optimization, quasi-Newton methods, BFGS method, reduced-Hessian methods, conjugate-direction methods AMS subject classifications. 65K05, 90C30
347|Shallow Parsing with Conditional Random Fields|Conditional random fields for sequence labeling  offer advantages over both generative models  like HMMs and classifiers applied at each  sequence position. Among sequence labeling  tasks in language processing, shallow parsing  has received much attention, with the development  of standard evaluation datasets and extensive  comparison among methods. We show  here how to train a conditional random field to  achieve performance as good as any reported  base noun-phrase chunking method on the  CoNLL task, and better than any reported single  model. Improved training methods based  on modern optimization algorithms were critical  in achieving these results. We present extensive  comparisons between models and training  methods that confirm and strengthen previous  results on shallow parsing and training  methods for maximum-entropy models.
348|Conditional random fields: Probabilistic models for segmenting and labeling sequence data|We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1.
349|A Maximum Entropy approach to Natural Language Processing|The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.  
350|Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging|this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging
351|Inducing Features of Random Fields|We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classifica...
352|A Maximum Entropy Model for Part-Of-Speech Tagging|This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual &#034;features&#034; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
353|Text Chunking using Transformation-Based Learning|Eric Brill introduced transformation-based learning and showed that it can do part-ofspeech  tagging with fairly high accuracy. The same method can be applied at a higher  level of textual interpretation for locating chunks in the tagged text, including non-recursive  &#034;baseNP&#034; chunks. For this purpose, it is convenient to view chunking as a tagging problem  by encoding the chunk structure in new tags attached to each word. In automatic tests using  Treebank-derived data, this technique achieved recall and precision rates of roughly 92% for  baseNP chunks and 88% for somewhat more complex chunks that partition the sentence. Some  interesting adaptations to the transformation-based learning approach are also suggested by  this application.
355|Discriminative probabilistic models for relational data|In many supervised learning tasks, the entities to be labeled are related to each other in complex ways and their labels are not independent. For example, in hypertext classification, the labels of linked pages are highly correlated. A standard approach is to classify each entity independently, ignoring the correlations between them. Recently, Probabilistic Relational Models, a relational version of Bayesian networks, were used to define a joint probabilistic model for a collection of related entities. In this paper, we present an alternative framework that builds on (conditional) Markov networks and addresses two limitations of the previous approach. First, undirected models do not impose the acyclicity constraint that hinders representation of many important relational dependencies in directed models. Second, undirected models are well suited for discriminative training, where we optimize the conditional likelihood of the labels given the features, which generally improves classification accuracy. We show how to train these models effectively, and how to use approximate probabilistic inference over the learned model for collective classification of multiple related entities. We provide experimental results on a webpage classification task, showing that accuracy can be significantly improved by modeling relational dependencies. 1
356|Parsing By Chunks|Introduction I begin with an intuition: when I read a sentence, I read it a chunk at a time. For example, the previous sentence breaks up something like this: (1) [I begin] [with an intuition]: [when I read] [a sentence], [I read it] [a chunk] [at a time] These chunks correspond in some way to prosodic patterns. It appears, for instance, that the strongest stresses in the sentence fall one to a chunk, and pauses are most likely to fall between chunks. Chunks also represent a grammatical watershed of sorts. The typical chunk consists of a single content word surrounded by a constellation of function words, matching a fixed template. A simple context-free grammar is quite adequate to describe the structure of chunks. By contrast, the relationships between chunks are mediated more by lexical selection than by rigid templates. Co-occurence of chunks is determined not just by their syntactic categories, but is sensitive to the precise words that head them
357|An Algorithm that Learns What&#039;s in a Name|In this paper, we present IdentiFinder^TM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder&#039;s performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.
358|A Comparison of Algorithms for Maximum Entropy Parameter Estimation |A comparison of algorithms for maximum entropy parameter estimation Conditional maximum entropy (ME) models provide a general purpose machine learning technique which has been successfully applied to fields as diverse as computer vision and econometrics, and which is used for a wide variety of classification problems in natural language processing. However, the flexibility of ME models is not without cost. While parameter estimation for ME models is conceptually straightforward, in practice ME models for typical natural language tasks are very large, and may well contain many thousands of free parameters. In this paper, we consider a number of algorithms for estimating the parameters of ME models, including iterative scaling, gradient ascent, conjugate gradient, and variable metric methods. Surprisingly, the standardly used iterative scaling algorithms perform quite poorly in comparison to the others, and for all of the test problems, a limited-memory variable metric algorithm outperformed the other choices. 
359|New Ranking Algorithms for Parsing and Tagging: Kernels over Discrete Structures, and the Voted Perceptron|This paper introduces new learning algorithms for natural language processing based on  the perceptron algorithm. We show how the algorithms can be efficiently applied to  exponential sized representations of parse trees, such as the &#034;all subtrees&#034; (DOP)  representation described by (Bod 98), or a representation tracking all sub-fragments of a  tagged sentence. We give experimental results showing significant improvements on two  tasks: parsing Wall Street Journal text, and named-entity extraction from web data.
360|A Gaussian prior for smoothing maximum entropy models|In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood train-ing for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods com-pare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty [1] performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parame-ters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.
361|Introduction to the CoNLL-2000 Shared Task: Chunking|We describe the CoNLL-2000 shared task: dividing text into syntactically related nonoverlapping groups of words, so-called text chunking. We give background information on the data setst present a general overview of the systems that have taken part in the shared task and briefly discuss their performance.
362|A Linear Observed Time Statistical Parser Based on Maximum Entropy Models|This paper presents a statistical parser for  natural language that obtains a parsing  accuracy--roughly 87% precision and 86%  recall--which surpasses the best previously  published results on the Wall St. Journal  domain. The parser itself requires very little  human intervention, since the information  it uses to make parsing decisions is  specified in a concise and simple manner,  and is combined in a fully automatic way  under the maximum entropy framework.
363|Parsing the Wall Street Journal using a Lexical-Functional Grammar and Discriminative Estimation Techniques|We present a stochastic parsing system  consisting of a Lexical-Functional Grammar  (LFG), a constraint-based parser and  a stochastic disambiguation model. We report  on the results of applying this system  to parsing the UPenn Wall Street  Journal (WSJ) treebank. The model combines  full and partial parsing techniques  to reach full grammar coverage on unseen  data. The treebank annotations are used  to provide partially labeled data for discriminative  statistical estimation using exponential  models. Disambiguation performance  is evaluated by measuring matches  of predicate-argument relations on two  distinct test sets. On a gold standard of  manually annotated f-structures for a subset  of the WSJ treebank, this evaluation  reaches 79% F-score. An evaluation on a  gold standard of dependency relations for  Brown corpus data achieves 76% F-score.
364|Information Extraction with HMM Structures Learned by Stochastic Optimization|Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction -- the task of populating database slots with corresponding phrases from text documents. A remaining problem, however, is the selection of state-transition structure for the model. This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization. Our algorithm begins with a simple model and then performs hill-climbing in the space of possible structures by splitting states and gauging performance on a validation set. Experimental results show that this technique finds HMM models that almost always out-perform a fixed model, and have superior average performance across tasks.
365|The Use of Classifiers in Sequential Inference|We study the problem of combining the outcomes of several different  classifiers in a way that provides a coherent inference that satisfies some  constraints. In particular, we develop two general approaches for an important  subproblem - identifying phrase structure. The first is a Markovian  approach that extends standard HMMs to allow the use of a rich observation  structure and of general classifiers to model state-observation  dependencies. The second is an extension of constraint satisfaction formalisms.  We develop efficient combination algorithms under both models  and study them experimentally in the context of shallow parsing.  1 Introduction  In many situations it is necessary to make decisions that depend on the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints - the sequential nature of the data or other domain specific constraints. Consider, for example, the problem of chunking natural language sentences ...
366|Boosting Applied to Tagging and PP Attachment|Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.
367|Efficient Training of Conditional Random Fields|This thesis explores a number of parameter estimation techniques for conditional random fields, a recently introduced probabilistic model for labelling and segmenting sequential data. Theoretical and practical disadvantages of the training techniques reported in current literature on CRFs are discussed. We hypothesise that general numerical optimisation techniques result in improved performance over iterative scaling algorithms for training CRFs. Experiments run on a a subset of a well-known text chunking data set confirm that this is indeed the case. This is a highly promising result, indicating that such parameter estimation techniques make CRFs a practical and efficient choice for labelling sequential data, as well as a theoretically sound and principled probabilistic framework.
368|More Accurate Tests for the Statistical Significance of Result Differences|Sl,ai,isl,ica,1 significance, kest,ing of diflkn&#039;ences in values of metrics like recall, i)rccision mM batmined F-s(x)re is a ne(:cssm&#039;y tmrt of eml)iri(:a.l ua.t;ural bmguage 1)ro(;easing. Unfi)rtunat,ely we inertly used tests of&#039;ten ulnlerestimake i,he significm ce mM so a.re less likely to detect, difihrences l,hat exist between difM&#039;eni techniques. This 1111deresl;illla(;ioll comes from an independcn(;e  asSmnl)tion that is offten violated. Wc l)oint, out sonic ltse]&#039;Hl l.es(,s (,]mL do nol, make lhis assmnl)- tion, including contput;a, tionally--iltcnsive domizai,ion tests.
369|Dynamic Programming for Parsing and Estimation of Stochastic Unification-Based Grammars|Stochastic unification-based grammars (SUBGs) define exponential distributions over the parses generated by a unificationbased grammar (UBG). Existing algorithms for parsing and estimation require the enumeration of all of the parses of a string in order to determine the most likely one, or in order to calculate the statistics needed to estimate a grammar from a training corpus. This paper describes a graph-based dynamic programming algorithm for calculating these statistics from the packed UBG parse representations of Maxwell and Kaplan (1995) which does not require enumerating all parses. Like many graphical algorithms, the dynamic programming algorithm&#039;s complexity is worst-case exponential, but is often polynomial.
370|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
371|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
372|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
373|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
374|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
375|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
376|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
377|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
378|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
379|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
380|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
381|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
382|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
383|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
384|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
385|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
386|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
387|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
388|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
389|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
390|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
391|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
392|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
393|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
394|Using spin images for efficient object recognition in cluttered 3D scenes|We present a 3-D shape-based object recognition system for simultaneous recognition of multiple objects in scenes containing clutter and occlusion. Recognition is based on matching surfaces by matching points using the spin-image representation. The spin-image is a data level shape descriptor that is used to match surfaces represented as surface meshes. We present a compression scheme for spin-images that results in efficient multiple object recognition which we verify with results showing the simultaneous recognition of multiple objects from a library of 20 models. Furthermore, we demonstrate the robust performance of recognition in the presence of clutter and occlusion through analysis of recognition trials on 100 scenes. This research was performed at Carnegie Mellon University and was supported by the US Department Surface matching is a technique from 3-D computer vision that has many applications in the area of robotics and automation. Through surface matching, an object can be recognized in a scene by comparing a sensed surface to an object surface stored in memory. When the object surface is matched to the scene surface, an association is made between something known (the object) and
395|A Signal Processing Approach To Fair Surface Design|In this paper we describe a new tool for interactive free-form fair surface design. By generalizing classical discrete Fourier analysis to two-dimensional discrete surface signals -- functions defined on polyhedral surfaces of arbitrary topology --, we reduce the problem of surface smoothing, or fairing, to low-pass filtering. We describe a very simple surface signal low-pass filter algorithm that applies to surfaces of arbitrary topology. As opposed to other existing optimization-based fairing methods, which are computationally more expensive, this is a linear time and space complexity algorithm. With this algorithm, fairing very large surfaces, such as those obtained from volumetric medical data, becomes affordable. By combining this algorithm with surface subdivision methods we obtain a very effective fair surface design technique. We then extend the analysis, and modify the algorithm accordingly, to accommodate different types of constraints. Some constraints can be imposed without any modification of the algorithm, while others require the solution of a small associated linear system of equations. In particular, vertex location constraints, vertex normal constraints, and surface normal discontinuities across curves embedded in the surface, can be imposed with this technique.  CR Categories and Subject Descriptors: I.3.3 [Computer Graphics]: Picture/image generation - display algorithms; I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling - curve, surface, solid, and object representations;J.6[Com- puter Applications]: Computer-Aided Engineering - computeraided design  General Terms: Algorithms, Graphics.  1 
396|Iterative point matching for registration of free-form curves and surfaces| A heuristic method has been developed for registering two sets of 3-D curves obtained by using an edge-based stereo system, or two dense 3-D maps obtained by using a correlation-based stereo system. Geometric matching in general is a difficult unsolved problem in computer vision. Fortunately, in many practical applications, some a priori knowledge exists which considerably simplifies the problem. In visual navigation, for example, the motion between successive positions is usually approximately known. From this initial estimate, our algorithm computes observer motion with very good precision, which is required for environment modeling (e.g., building a Digital Elevation Map). Objects are represented by a set of 3-D points, which are considered as the samples of a surface. No constraint is imposed on the form of the objects. The proposed algorithm is based on iteratively matching points in one set to the closest points in the other. A statistical method based on the distance distribution is used to deal with outliers, occlusion, appearance and disappearance, which allows us to do subset-subset matching. A least-squares technique is used to estimate 3-D motion from the point correspondences, which reduces the average distance between points in the two sets. Both synthetic and real data have been used to test the algorithm, and the results show that it is efficient and robust, and yields an accurate motion estimate.
397|Spin-images: A representation for 3-d surface matching|surface registration, object modeling, scene clutter. Dedicated to Dorothy D. Funnell, a believer in higher education. Surface matching is the process that compares surfaces and decides whether they are similar. In three-dimensional (3-D) computer vision, surface matching plays a prominent role. Surface matching can be used for object recognition; by comparing two surfaces, an association between a known object and sensed data is established. By computing the 3-D transformation that aligns two surfaces, surface matching can also be used for surface registration. Surface matching is difficult because the coordinate system in which to compare two surfaces is undefined. The typical approach to surface matching is to transform the surfaces being compared into representations where comparison of surfaces is straightforward. Surface matching is further complicated by characteristics of sensed data, including clutter, occlusion and sensor noise. This thesis describes a data level representation of surfaces used for surface matching. In our representation, surface shape is described by a dense collection of oriented points, 3-D
398|Registration and Integration of Textured 3-D Data|In general, multiple views are required to create a complete 3-D model of an object or a multiroomed indoor scene. In this work, we address the problem of merging multiple textured 3-D data sets, each of which corresponding to a different view of a scene or object. There are two steps to the merging process: registration and integration. Registration is the process by which data sets are brought into alignment. To this end, we use a modified version of the Iterative Closest Point algorithm (ICP); our version, which we call color ICP, considers not only 3-D information, but color as well. This has shown to have resulted in improved performance. Once the 3-D data sets have been registered, we then integrate them to produce a seamless, composite 3-D textured model. Our approach to integration uses a 3-D occupancy grid to represent likelihood of spatial occupancy through voting. The occupancy grid representation allows the incorporation of sensor modeling. The surface of the merged model i...
399|COSMOS - A Representation Scheme for 3D Free-Form Objects|We address the problem of representing and recognizing 3D free-form objects when (a) the object viewpoint is arbitrary, (b) the objects may vary in shape and complexity, and (c) no restrictive assumptions are made about the types of surfaces on the object. We assume that a range image of a scene is available, containing a view of a rigid 3D object without occlusion. We propose a new and general surface representation scheme for recognizing objects with freeform (sculpted) surfaces. In this scheme, an object is described concisely in terms of maximal surface patches of constant shape index. The maximal patches that represent the object are mapped onto the unit sphere via their orientations, and aggregated via shape spectral functions. Properties such as surface area, curvedness and connectivity which are required to capture local and global information are also built into the representation. The scheme yields a meaningful and rich description useful for object recognition. A novel conce...
400|Representation and Recognition of FreeForm Surfaces|We introduce a new surface representation for recognizing curved objects. Our approach begins by representing an object by a discrete mesh of points built from range data or from a geometric model of the object. The mesh is computed from the data by deforming a standard shaped mesh, for example, an ellipsoid, until it fits the surface of the object. We define local regularity constraints that the mesh must satisfy. We then define a canonical mapping between the mesh describing the object and a standard spherical mesh. A surface curvature index that is pose-invariant is stored at every node of the mesh. We use this object representation for recognition by comparing the spherical model of a reference object with the model extracted from a new observed scene. We show how the similarity between reference model and observed data can be evaluated and we show how the pose of the reference object in the observed scene can be easily computed using this representation. We present results on real range images which show that this approach to modelling and recognizing three-dimensional objects has three main advantages: First, it is applicable to complex curved surfaces that cannot be handled by conventional techniques. Second, it reduces the recognition problem to the computation of similarity between spherical distributions; in particular, the recognition algorithm does not require any combinatorial search. Finally, even though it is based on a spherical mapping, the approach can handle occlusions and partial views.
401|Object Recognition Using Appearance-Based Parts and Relations|tapasQCaere.com The recognition of general three-dimensional objects in cluttered scenes is a challenging problem. In particular, the design of a good representation suitable to model large numbers of generic objects that is also robust to occlusion has been an stumbling block in achieving success. In this paper, we propose a representation using appearance-based parts and relations to overcome these problems. Appearance-based parts and relations are defined in terms of closed regions and the union of these regions, respectively. The regions are segmented using the MDL principle, and their appearance is obtained from collection of images and compactly represented by parametric manifolds in the two eigenspaces spanned by the parts and the relations. 1
402|Efficient Multiple Model Recognition in Cluttered 3-D Scenes|We present a 3-D shape-based object recognition system for  simultaneous recognition of multiple objects in scenes containing clutter and occlusion. Recognition is based on matching by matching points using the spin-image representation. The spin-image is a data level shape descriptor that is used to match surfaces represented as meshes. We present a compression scheme for images that results in multiple object recognition which we with results showing the simultaneous recognition of multiple objects from a library of 20 models. Furthermore, we demonstrate the robust performance of  recognition in the presence of clutter and occlusion through  analysis of recognition trials on 100 scenes.  1 
403|Control of Polygonal Mesh Resolution for 3-D Computer Vision|A common representation in 3-D computer vision is the polygonal surface mesh because meshes can model objects of arbitrary shape and are easily constructed from sensed 3-D data. The resolution of a surface mesh is the overall spacing between vertices that comprise the mesh. Because sensed 3-D points are often unevenly distributed, the resolution of a surface mesh is often poorly defined. We present an algorithm that transforms a mesh with an uneven spacing between vertices into a mesh with a more even spacing between vertices, thus improving its definition of resolution. In addition, we show how the algorithm can be used to control the resolution of surface meshes, making them amenable to multiresolution approaches in computer vision. The structure of our algorithm is modeled on iterative mesh simplification algorithms common in computer graphics; however, the individual steps in our algorithm are designed specifically to control mesh resolution. An even spacing between vertices is generated by applying a sequence of local edge operations that promote uniform edge lengths while preserving mesh shape. To account for polyhedral objects, we introduce an accurate shape change measure that permits edge operations along sharp creases. By locally bounding the total change in mesh shape, drastic changes in global shape are prevented. We show results from many 3-D sensing domains including computed tomography, range imaging, and digital elevation map construction.
404|Closest Point Search in High Dimensions|The problem of finding the closest point in highdimensional spaces is common in computational vision. Unfortunately, the complexity of most existing search algorithms, such as k-d tree and R-tree, grows exponentially with dimension, making them impractical for dimensionality above 15. In nearly all applications, the closest point is of interest only if it lies within a user specified distance ffl. We present a simple and practical algorithm to efficiently search for the nearest neighbor within Euclidean distance ffl. Our algorithm uses a projection search technique along with a novel data structure to dramatically improve performance in high dimensions. A complexity analysis is presented which can help determine ffl in structured problems. Benchmarks clearly show the superiority of the proposed algorithm for high dimensional search problems frequently encountered in machine vision, such as real-time object recognition.  1 Introduction  Searching for nearest neighbors continues to prove...
405|Particle swarm optimization| A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.
406|Cognitive load during problem solving: effects on learning|Considerable evidence indicates that domain specific knowledge in the form of schemes is the primary factor distinguishing experts from novices in problem-solving skill. Evidence that conventional problem-solving activity is not effective in schema acquisition is also accumulating. It is suggested that a major reason for the ineffectiveness of problem solving as a learning device, is that the cognitive processes required by the two activities overlap insufficiently, and that conventional problem solving in the form of means-ends analysis requires a relatively large amount of cognitive processing capacity which is consequently unavailable for schema acquisition. A computational model and experimental evidence provide support for this contention. Theoretical and practical implications are discussed. 
408|Transfer of Cognitive Skill|A framework for skill acquisition is proposed that includes two major stages in the development of a cognitive skill: a declarative stage in which facts about the skill domain are interpreted and a procedural stage in which the domain knowledge is directly embodied in procedures for performing the skill. This general framework has been instantiated in the ACT system in which facts are encoded in a propositional network and procedures are encoded as productions. Knowledge compilation is the process by which the skill transits from the declarative stage to the procedural stage. It consists of the subprocesses of composition, which collapses sequences of productions into single productions, and proceduralization, which embeds factual knowledge into productions. Once proceduralized, further learning processes operate on the skill to make the productions more selective in their range of applications. These processes include generalization, discrimination, and strengthening of productions. Comparisons are made to similar concepts from past learning theories. How these learning mechanisms apply to produce the power law speedup in processing time with practice is discussed. It requires at least 100 hours of learning and practice to acquire any significant cognitive skill to a reasonable degree of proficiency. For instance, after 100 hours a student learning to program a computer has achieved only a very modest facility in the skill. Learning one&#039;s primary language takes tens of thousands of hours. The psychology of human learning has been very thin in ideas about what happens to skills under the impact of this amount of learning—and for obvious reasons. This article presents a theory about the changes in the nature of a skill over such large time scales and about the basic learning processes that are responsible.
409|Icarus user’s manual|In this document, we introduce Icarus, a cognitive architecture for physical agents that utilizes hierarchical concepts and skills for inference, execution, and problem solving. We first review the assumptions typically made in work on cognitive architectures and explain how Icarus differs from earlier candidates. After this, we present the framework’s approach to conceptual inference, its mechanisms for teleoreactive execution, its processes for means-ends problem solving, and its techniques for learning new skills. In each case, we discuss the memories on which the processes rely, the structures they contain, and the manner in which they operate. In closing, we describe the commands available for creating Icarus programs, running them in simulated environments, and tracing their behavior.
410|A survey of general-purpose computation on graphics hardware|The rapid increase in the performance of graphics hardware, coupled with recent improvements in its programmability, have made graphics hardware acompelling platform for computationally demanding tasks in awide variety of application domains. In this report, we describe, summarize, and analyze the latest research in mapping general-purpose computation to graphics hardware. We begin with the technical motivations that underlie general-purpose computation on graphics processors (GPGPU) and describe the hardware and software developments that have led to the recent interest in this field. We then aim the main body of this report at two separate audiences. First, we describe the techniques used in mapping general-purpose computation to graphics hardware. We believe these techniques will be generally useful for researchers who plan to develop the next generation of GPGPU algorithms and techniques. Second, we survey and categorize the latest developments in general-purpose application development on graphics hardware.  
411|FFTW: An Adaptive Software Architecture For The FFT|FFT literature has been mostly concerned with minimizing the number of floating-point operations performed by an algorithm. Unfortunately, on present-day microprocessors this measure is far less important than it used to be, and interactions with the processor pipeline and the memory hierarchy have a larger impact on performance. Consequently, one must know the details of a computer architecture in order to design a fast algorithm. In this paper, we propose an adaptive FFT program that tunes the computation automatically for any particular hardware. We compared our program, called FFTW, with over 40 implementations of the FFT on 7 machines. Our tests show that FFTW&#039;s self-optimizing approach usually yields significantly better performance than all other publicly available software. FFTW also compares favorably with machine-specific, vendor-optimized libraries. 1. INTRODUCTION The discrete Fourier transform (DFT) is an important tool in many branches of science and engineering [1] and...
412|Modeling the Interaction of Light Between Diffuse Surfaces|A method is described which models the interaction of light between diffusely reflecting surfaces. Current light reflection models used in computer graphics do not account for the object-to-object reflection between diffuse surfaces, and thus incorrectly compute the global illumination effects. The new procedure, based on methods used in thermal engineering, includes the effects of diffuse light sources of finite area, as well as the &#034;color-bleeding&#034; effects which are caused by the diffuse reflections. A simple environment is used to illustrate these simulated effects and is presented with photographs of a physical model. The procedure is applicable to environments composed of ideal diffuse reflectors and can account for direct illumination from a variety of light sources. The resultant surface intensities are independent of observer position, and thus environments can be preprocessed for dynamic sequences.
413|Chromium: A Stream-Processing Framework for Interactive Rendering on Clusters|We describe Chromium, a system for manipulating streams of graphics API commands on clusters of workstations. Chromium&#039;s stream filters can be arranged to create sort-first and sort-last parallel graphics architectures that, in many cases, support the same applications while using only commodity graphics accelerators. In addition, these stream filters can be extended programmatically, allowing the user to customize the stream transformations performed by nodes in a cluster. Because our stream processing mechanism is completely general, any cluster-parallel rendering algorithm can be either implemented on top of or embedded in Chromium. In this paper, we give examples of real-world applications that use Chromium to achieve good scalability on clusters of workstations, and describe other potential uses of this stream processing technology. By completely abstracting the underlying graphics architecture, network topology, and API command processing semantics, we allow a variety of applications to run in different environments.
414|A progressive refinement approach to fast radiosity image generation|A reformulated radiosity algorithm is presented that produces initial images in time linear to the number of patches. The enormous memory costs of the radiosity algorithm are also elim-inated by computing form-factors on-the-fly. The technique is based on the approach of rendering by progressive refinement. The algorithm provides a useful solution almost immediately which progresses gracefully and continuously to the complete radiosity solution. In this way the competing demands of real-ism and interactivity are accommodated. The technique brings the use of radiosity for interactive rendering within reach and has implications for the use and development of current and future graphics workstations.
415|Fast Computation of Generalized Voronoi Diagrams Using Graphics Hardware|We present a new approach for computing generalized 2D and 3D Voronoi diagrams using interpolation-based polygon rasterization hardware. We compute a discrete Voronoi diagram by rendering a three dimensional distance mesh for each Voronoi site. The polygonal mesh is a bounded-error approximation of a (possibly) non-linear function of the distance between a site and a 2D planar grid of sample points. For each sample point, we compute the closest site and the distance to that site using polygon scan-conversion and the Z-buffer depth comparison. We construct distance meshes for points, line segments, polygons, polyhedra, curves, and curved surfaces in 2D and 3D. We generalize to weighted and farthest-site Voronoi diagrams, and present efficient techniques for computing the Voronoi boundaries, Voronoi neighbors, and the Delaunay triangulation of points. We also show how to adaptively refine the solution through a simple windowing operation. The algorithm has been implemented on SGI workstations and PCs using OpenGL, and applied to complex datasets. We demonstrate the application of our algorithm to fast motion planning in static and dynamic environments, selection in complex user-interfaces, and creation of dynamic mosaic effects.
416|Reflection from Layered Surfaces due to Subsurface Scattering|The reflection of light from most materials consists of two major terms: the specular and the diffuse. Specular reflection may be modeled from first principles by considering a rough surface consisting of perfect reflectors, or micro-facets. Diffuse reflection is generally considered to result from multiple scattering either from a rough surface or from within a layer near the surface. Accounting for diffuse reflection by Lambert&#039;s Cosine Law, as is universally done in computer graphics, is not a physical theory based on first principles. This paper presents
417|Brook for GPUs: Stream Computing on Graphics Hardware|In this paper, we present Brook for GPUs, a system for general-purpose computation on programmable graphics hardware. Brook extends C to include simple data-parallel constructs, enabling the use of the GPU as a streaming coprocessor. We present a compiler and runtime system that abstracts and virtualizes many aspects of graphics hardware. In addition, we present an analysis of the effectiveness of the GPU as a compute engine compared to the CPU, to determine when the GPU can outperform the CPU for a particular algorithm. We evaluate our system with five applications, the SAXPY and SGEMV BLAS operators, image segmentation, FFT, and ray tracing. For these applications, we demonstrate that our Brook implementations perform comparably to hand-written GPU code and up to seven times faster than their CPU counterparts.
418|GPUTeraSort: High Performance Graphics Coprocessor Sorting for Large Database Management|We present a new algorithm, GPUTeraSort, to sort billionrecord wide-key databases using a graphics processing unit (GPU) Our algorithm uses the data and task parallelism on the GPU to perform memory-intensive and computeintensive tasks while the CPU is used to perform I/O and resource management. We therefore exploit both the highbandwidth GPU memory interface and the lower-bandwidth CPU main memory interface and achieve higher memory bandwidth than purely CPU-based algorithms. GPUTera-Sort is a two-phase task pipeline: (1) read disk, build keys, sort using the GPU, generate runs, write disk, and (2) read, merge, write. It also pipelines disk transfers and achieves near-peak I/O performance. We have tested the performance of GPUTeraSort on billion-record files using the standard Sort benchmark. In practice, a 3 GHz Pentium IV PC with $265 NVIDIA 7800 GT GPU is significantly faster than optimized CPU-based algorithms on much faster processors, sorting 60GB for a penny; the best reported PennySort price-performance. These results suggest that a GPU co-processor can significantly improve performance on large data processing tasks. 1.
419|Problem-oriented software engineering|This paper introduces a formal conceptual framework for software development, based on a problem-oriented perspective that stretches from requirements engineering through to program code. In a software problem the goal is to develop a machine—that is, a computer executing the software to be developed—that will ensure satisfaction of the requirement in the problem world. We regard development steps as transformations by which problems are moved towards software solutions. Adequacy arguments are built as problem transformations are applied: adequacy arguments both justify proposed development steps and establish traceability relationships between problems and solutions. The framework takes the form of a sequent calculus. Although itself formal, it can accommodate both formal and informal steps in development. A number of transformations are presented, and illustrated by application to small examples.  
420|Interactive Order-Independent Transparency|this document is to enable OpenGL developers to implement this technique with NVIDIA OpenGL extensions and GeForce3 hardware. Since shadow mapping is integral to the technique a very basic introduction is provided, but the interested reader is encouraged to explore the referenced material for more detail
421|A Multigrid Solver for Boundary Value Problems Using Programmable Graphics Hardware|We present a method for using programmable graphics hardware to solve a variety of boundary value problems. The time-evolution of such problems is frequently governed by partial differential equations, which are used to describe a wide range of dynamic phenomena including heat transfer and fluid mechanics. The need to solve these equations efficiently arises in many areas of computational science. Finite difference methods are commonly used for solving partial differential equations; we show that this approach can be mapped onto a modern graphics processor. We demonstrate an implementation of the multigrid method, a fast and popular approach to solving boundary value problems, on two modern graphics architectures. Our initial tests with available hardware show speedups of roughly 15x compared to traditional software implementation. This work presents a novel use of computer hardware and raises the intriguing possibility that we can make the inexpensive power of modern commodity graphics hardware accessible to and useful for the simulation commuuity.
422|The Direct3D 10 system |We present a system architecture for the 4 th generation of PCclass programmable graphics processing units (GPUs). The new pipeline features significant additions and changes to the prior generation pipeline including a new programmable stage capable of generating additional primitives and streaming primitive data to memory, an expanded, common feature set for all of the programmable stages, generalizations to vertex and image memory resources, and new storage formats. We also describe structural modifications to the API, runtime, and shading language to complement the new pipeline. We motivate the design with descriptions of frequently encountered obstacles in current systems. Throughout the paper we present rationale behind prominent design choices and alternatives that were ultimately rejected, drawing on insights collected during a multi-year collaboration with application developers and hardware designers.
423|Fast computation of database operations using graphics processors|We present new algorithms for performing fast computation of several common database operations on commodity graphics processors. Specifically, we consider operations such as conjunctive selections, aggregations, and semi-linear queries, which are essential computational components of typical database, data warehousing, and data mining applications. While graphics processing units (GPUs) have been designed for fast display of geometric primitives, we utilize the inherent pipelining and parallelism, single instruction and multiple data (SIMD) capabilities, and vector processing functionality of GPUs, for evaluating boolean predicate combinations and semi-linear queries on attributes and executing database operations efficiently. Our algorithms take into account some of the limitations of the programming model of current GPUs and perform no data rearrangements. Our algorithms have been implemented on a programmable GPU (e.g. NVIDIA’s GeForce FX 5900) and applied to databases consisting of up to a million records. We have compared their performance with an optimized implementation of CPU-based algorithms. Our experiments indicate that the graphics processor available on commodity computer systems is an effective co-processor for performing database operations.
424|Physically-Based Visual Simulation on Graphics Hardware|In this paper, we present a method for real-time visual simulation of diverse dynamic phenomena using programmable graphics hardware. The simulations we implement use an extension of cellular automata known as the coupled map lattice (CML). CML represents the state of a dynamic system as continuous values on a discrete lattice. In our implementation we store the lattice values in a texture, and use pixel-level programming to implement simple next-state computations on lattice nodes and their neighbors. We apply these computations successively to produce interactive visual simulations of convection, reaction-diffusion, and boiling. We have built an interactive framework for building and experimenting with CML simulations running on graphics hardware, and have integrated them into interactive 3D graphics applications.
425|Understanding the Efficiency of GPU Algorithms for Matrix-Matrix Multiplication|Utilizing graphics hardware for general purpose numerical computations has become a topic of considerable  interest. The implementation of streaming algorithms, typified by highly parallel computations with little reuse  of input data, has been widely explored on GPUs. We relax the streaming model&#039;s constraint on input reuse and  perform an in-depth analysis of dense matrix-matrix multiplication, which reuses each element of input matrices  O(n) times. Its regular data access pattern and highly parallel computational requirements suggest matrix-matrix  multiplication as an obvious candidate for efficient evaluation on GPUs but, surprisingly we find even nearoptimal  GPU implementations are pronouncedly less efficient than current cache-aware CPU approaches. We find  the key cause of this inefficiency is that the GPU can fetch less data and yet execute more arithmetic operations  per clock than the CPU when both are operating out of their closest caches. The lack of high bandwidth access to  cached data will impair the performance of GPU implementations of any computation featuring significant input  reuse.
426|Lu-gpu: Efficient algorithms for solving dense linear systems on graphics hardware|We present a novel algorithm to solve dense linear systems using graphics processors (GPUs). We reduce matrix decomposition and row operations to a series of rasterization problems on the GPU. These include new techniques for streaming index pairs, swapping rows and columns and parallelizing the computation to utilize multiple vertex and fragment processors. We also use appropriate data representations to match the rasterization order and cache technology of graphics processors. We have implemented our algorithm on different GPUs and compared the performance with optimized CPU implementations. In particular, our implementation on a NVIDIA GeForce 7800 GPU outperforms a CPU-based ATLAS implementation. Moreover, our results show that our algorithm is cache and bandwidth efficient and scales well with the number of fragment processors within the GPU and the core GPU clock rate. We use our algorithm for fluid flow simulation and demonstrate that the commodity GPU is a useful co-processor for many scientific applications. 1
427|Sequential point trees|Figure 1: Continuous detail levels of a Buddha generated in vertex programs on the GPU. The colors denote the LOD level used and the bars describe the selected amount of points selected for the GPU (top row) and the average CPU load required for rendering (bottom row). In this paper we present sequential point trees, a data structure that allows adaptive rendering of point clouds completely on the graphics processor. Sequential point trees are based on a hierarchical point representation, but the hierarchical rendering traversal is replaced by sequential processing on the graphics processor, while the CPU is available for other tasks. Smooth transition to triangle rendering for optimized performance is integrated. We describe optimizations for backface culling and texture adaptive point selection. Finally, we discuss implementation issues and show results.
428|A memory model for scientific algorithms on graphics processors|We present a memory model to analyze and improve the performance of scientific algorithms on graphics processing units (GPUs). Our memory model is based on texturing hardware, which uses a 2D block-based array representation to perform the underlying computations. We incorporate many characteristics of GPU architectures including smaller cache sizes, 2D block representations, and use the 3C’s model to analyze the cache misses. Moreover, we present techniques to improve the performance of nested loops on GPUs. In order to demonstrate the effectiveness of our model, we highlight its performance on three memory-intensive scientific applications – sorting, fast Fourier transform and dense matrix-multiplication. In practice, our cache-efficient algorithms for these applications are able to achieve memory throughput of 30–50 GB/s on a NVIDIA 7900 GTX GPU. We also compare our results with prior GPU-based and CPU-based implementations on highend processors. In practice, we are able to achieve 2–5× performance improvement.
429|Radiosity on graphics hardware|Radiosity is a widely used technique for global illumination. Typically the computation is performed offline and the result is viewed interactively. We present a technique for computing radiosity, including an adaptive subdivision of the model, using graphics hardware. Since our goal is to run at interactive rates, we exploit the computational power and programmability of modern graphics hardware. Using our system on current hardware, we have been able to compute and display a radiosity solution for a 10,000 element scene in less than one second. Key words: Graphics Hardware, Global Illumination. 1
430|Fast and Simple 2D Geometric Proximity Queries Using Graphics Hardware|We present a new approach for computing generalized proximity information of arbitrary 2D objects using graphics hardware. Using multi-pass rendering techniques and accelerated distance computation, our algorithm performs proximity queries not only for detecting collisions, but also for computing intersections, separation distance, penetration depth, and contact points and normals. Our hybrid geometry and image-based approach balances computation between the CPU and graphics subsystems. Geometric object-space techniques coarsely localize potential intersection regions or closest features between two objects, and image-space techniques compute the low-level proximity information in these regions. Most of the proximity information is derived from a distance field computed using graphics hardware. We demonstrate the performance in collision response computation for rigid and deformable body dynamics simulations. Our approach provides proximity information at interactive rates for a variet...
431|GPU algorithms for radiosity and subsurface scattering|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
432|Fast and approximate stream mining of quantiles and frequencies using graphics processors|We present algorithms for fast quantile and frequency estimation in large data streams using graphics processors (GPUs). We exploit the high computation power and memory bandwidth of graphics processors and present a new sorting algorithm that performs ras-terization operations on the GPUs. We use sorting as the main computational component for histogram approximation and con-struction of -approximate quantile and frequency summaries. Our algorithms for numerical statistics computation on data streams are deterministic, applicable to xed or variable-sized sliding windows and use a limited memory footprint. We use GPU as a co-processor and minimize the data transmission between the CPU and GPU by taking into account the low bus bandwidth. We implemented our algorithms on a PC with a NVIDIA GeForce FX 6800 Ultra GPU and a 3:4 GHz Pentium IV CPU and applied them to large data streams consisting of more than 100 million values. We also compared the performance of our GPU-based algorithms with op-timized implementations of prior CPU-based algorithms. Overall, our results demonstrate that the graphics processors available on a commodity computer system are efcient stream-processor and useful co-processors for mining data streams.
433|Performance and accuracy of hardware-oriented native-, emulated- and mixed-precision solvers in FEM simulations |In a previous publication, we have examined the fundamental difference between computational precision and result accuracy in the context of the iterative solution of linear systems as they typically arise in the Finite Element discretization of Partial Differential Equations (PDEs) [1]. In particular, we evaluated mixed- and emulatedprecision schemes on commodity graphics processors (GPUs), which at that time only supported computations in single precision. With the advent of graphics cards that natively provide double precision, this report updates our previous results. We demonstrate that with new co-processor hardware supporting native double precision, such as NVIDIA’s G200 architecture, the situation does not change qualitatively for PDEs, and the previously introduced mixed precision schemes are still preferable to double precision alone. But the schemes achieve significant quantitative performance improvements with the more powerful hardware. In particular, we demonstrate that a Multigrid scheme can accurately solve a common test problem in Finite Element settings with one million unknowns in less than 0.1 seconds, which is truely outstanding performance. We support these conclusions by exploring the algorithmic design space enlarged by the availability of double precision directly in the hardware. 1 Introduction and
434|Detection of Collisions and Self-collisions Using Image-space Techniques|Image-space techniques have shown to be very efficient for collision detection in dynamic simulation and animation environments. This paper proposes a new image-space technique for efficient collision detection of arbitrarily shaped, water-tight objects. In contrast to existing approaches that do not consider self-collisions, our approach combines the image-space object representation with information on face orientation to overcome this limitation. While
435|Applications of Pixel Textures in Visualization and Realistic Image Synthesis|With fast 3D graphics becoming more and more available even on low end platforms, the focus in developing new graphics hardware is beginning to shift towards higher quality rendering and additional functionality instead of simply higher performance implementations of the traditional graphics pipeline. On this search for improved quality it is important to identify a powerful set of orthogonal features to be implemented in hardware, which can then be flexibly combined to form new algorithms.  Pixel textures are an OpenGL extension by Silicon Graphics that fits into this category. In this paper, we demonstrate the benefits of this extension by presenting several different algorithms exploiting its functionality to achieve high quality, high performance solutions for a variety of different applications from scientific visualization and realistic image synthesis. We conclude that pixel textures are a valuable, powerful feature that should become a standard in future graphics systems.   
436|Accelerating 3D convolution using graphics hardware|Many volume filtering operations used for image enhancement, data processing or feature detection can be written in terms of three-dimensional convolutions. It is not possible to yield interactive frame rates on todays hardware when applying such convolutions on volume data using software filter routines. As modern graph-ics workstations have the ability to render two-dimensional convo-luted images to the frame buffer, this feature can be used to accel-erate the process significantly. This way generic 3D convolution can be added as a powerful tool in interactive volume visualization toolkits.
437|GPU-ABiSort: Optimal parallel sorting on stream architectures|In this paper, we present a novel approach for parallel sorting on stream processing architectures. It is based on adaptive bitonic sorting. For sorting n values utilizing p stream processor units, this approach achieves the optimal time complexity O((n log n)/p). While this makes our approach competitive with common sequential sorting algorithms not only from a theoretical viewpoint, it is also very fast from a practical viewpoint. This is achieved by using efficient linear stream memory accesses and by combining the optimal time approach with algorithms optimized for small input sequences. We present an implementation on modern programmable graphics hardware (GPUs). On recent GPUs, our optimal parallel sorting approach has shown to be remarkably faster than sequential sorting on the CPU, and it is also faster than previous non-optimal sorting approaches on the GPU for sufficiently large input sequences. Because of the excellent scalability of our algorithm with the number of stream processor units p (up to n / log 2 n or even n / log n units, depending on the stream architecture), our approach profits heavily from the trend of increasing number of fragment processor units on GPUs, so that we can expect further speed improvement with upcoming GPU generations.
438|Towards Fast Non-Rigid Registration|A fast multiscale and multigrid method for the matching of images in 2D and 3D is presented. Especially in medical imaging this problem - denoted as the registration problem - is of fundamental importance in the handling of images from multiple image modalities or of image time series. The paper restricts to the simplest matching energy to be minimized, i.e., E[] =    R    jf 1   f2 j    , where f1 , f2 are the intensity maps of the two images to be matched and  is a deformation. The focus is on a robust and efficient solution strategy. Matching of
439|Interactive Time-Dependent Tone Mapping Using Programmable Graphics Hardware|Modern graphics architectures have replaced stages of the graphics pipeline with fully programmable modules. Therefore, it is now possible to perform fairly general computation on each vertex or fragment in a scene. In addition, the nature of the graphics pipeline makes substantial computational power available if the programs have a suitable structure. In this paper, we show that it is possible to cleanly map a state-of-the-art tone mapping algorithm to the pixel processor. This allows an interactive application to achieve higher levels of realism by rendering with physically based, unclamped lighting values and high dynamic range texture maps. We also show that the tone mapping operator can easily be extended to include a time-dependent model, which is crucial for interactive behavior. Finally, we describe the ways in which the graphics hardware limits our ability to compress dynamic range efficiently, and discuss modifications to the algorithm that could alleviate these problems.
440|Fast summed-area table generation and its applications|We introduce a technique to rapidly generate summed-area tables using graphics hardware. Summed area tables, originally introduced by Crow, provide a way to filter arbitrarily large rectangular regions of an image in a constant amount of time. Our algorithm for generating summed-area tables, similar to a technique used in scientific computing called recursive doubling, allows the generation of a summed-area table in O(log n) time. We also describe a technique to mitigate the precision requirements of summed-area tables. The ability to calculate and use summed-area tables at interactive rates enables numerous interesting rendering effects. We present several possible applications. First, the use of summed-area tables allows real-time rendering of interactive, glossy environmental reflections. Second, we present glossy planar reflections with varying blurriness dependent on a reflected object’s distance to the reflector. Third, we show a technique that uses a summed-area table to render glossy transparent objects. The final application demonstrates an interactive depth-of-field effect using summedarea tables. Categories and Subject Descriptors (according to ACM CCS): I.3.7 [Computer Graphics]: Three-Dimensional
441|STAPL: An adaptive, generic parallel C++ library| The Standard Template Adaptive Parallel Library (STAPL) is a parallel library designed as a superset of the ANSI C++ Standard Template Library (STL). It is sequentially consistent for functions with the same name, and executes on uni- or multi-processor systems that utilize shared or distributed memory. STAPL is implemented using simple parallel extensions of C++ that currently provide a SPMD model of parallelism, and supports nested parallelism. The library is intended to be general purpose, but emphasizes irregular programs to allow the exploitation of parallelism for applications which use dynamically linked data structures such as particle transport calculations, molecular dynamics, geometric modeling, and graph algorithms. STAPL provides several different algorithms for some library routines, and selects among them adaptively at runtime. STAPL can replace STL automatically by invoking a preprocessing translation phase. In the applications studied, the performance of translated code was within 5 % of the results obtained using STAPL directly. STAPL also provides functionality to allow the user to further optimize the code and achieve additional performance gains. We present results obtained using STAPL for a molecular dynamics code and a particle transport code.  
442|Hardware Acceleration in Commercial Databases: A Case Study of Spatial Operations|Traditional databases have focused on the issue  of reducing I/O cost as it is the bottleneck  in many operations. As databases become  increasingly accepted in areas such as Geographic  Information Systems (GIS) and Bioinformatics,  commercial DBMS need to support  data types for complex data such as spatial  geometries and protein structures. These  non-conventional data types and their associated  operations present new challenges. In  particular, the computational cost of some  spatial operations can be orders of magnitude  higher than the I/O cost. In order to improve  the performance of spatial query processing,  innovative solutions for reducing this  computational cost are beginning to emerge.
443|Nonlinear optimization framework for image-based modeling on programmable graphics hardware|Graphics hardware is undergoing a change from fixed-function pipelines to more programmable organizations that resemble general-purpose stream processors. In this paper, we show that certain general algorithms, not normally associated with computer graphics, can be mapped to such designs. Specifically, we cast nonlinear optimization as a data streaming process that is well matched to modern graphics processors. Our framework is particularly well suited for solving image-based modeling problems since it can be used to represent a large and diverse class of these problems using a common formulation. We successfully apply this approach to two distinct image-based modeling problems: light field mapping approximation and fitting the Lafortune model to spatial bidirectional reflectance distribution functions. Comparing the performance of the graphics hardware implementation to a CPU implementation, we show more than 5-fold improvement.
444|Generic Mesh Refinement on GPU|Many recent publications have shown that a large variety of computation involved in computer graphics can be  moved from the CPU to the GPU, by a clever use of vertex or fragment shaders. Nonetheless there is still one  kind of algorithms that is hard to translate from CPU to GPU: mesh refinement techniques. The main reason  for this, is that vertex shaders available on current graphics hardware do not allow the generation of additional  vertices on a mesh stored in graphics hardware. In this paper, we propose a general solution to generate mesh  refinement on GPU. The main idea is to define a generic refinement pattern that will be used to virtually create  additional inner vertices for a given polygon. These vertices are then translated according to some procedural  displacement map defining the underlying geometry (similarly, the normal vectors may be transformed according  to some procedural normal map). For illustration purpose, we use a tesselated triangular pattern, but many other  refinement patterns may be employed. To show its flexibility, the technique has been applied on a large variety  of refinement techniques: procedural displacement mapping, as well as more complex techniques such as curved  PN-triangles or ST-meshes.
445|A cache-efficient sorting algorithm for database and data mining computations using graphics processors|We present a fast sorting algorithm using graphics processors (GPUs) that adapts well to database and data mining applications. Our algorithm uses texture mapping and blending functionalities of GPUs to implement an efficient bitonic sorting network. We take into account the communication bandwidth overhead to the video memory on the GPUs and reduce the memory bandwidth requirements. We also present strategies to exploit the tile-based computational model of GPUs. Our new algorithm has a memoryefficient data access pattern and we describe an efficient instruction dispatch mechanism to improve the overall sorting performance. We have used our sorting algorithm to accelerate join-based queries and stream mining algorithms. Our results indicate up to an order of magnitude improvement over prior CPU-based and GPU-based sorting algorithms. 1
446|Computer vision signal processing on graphics processing units|In some sense, computer graphics and computer vision are inverses of one another. Special purpose computer vision hardware is rarely found in typical mass-produced personal computers, but graphics processing units (GPUs) found on most personal computers, often exceed (in number of transistors as well as in compute power) the capabilities of the Central Processing Unit (CPU). This paper shows speedups attained by using computer graphics hardware for implementation of computer vision algorithms by efficiently mapping mathematical operations of computer vision onto modern computer graphics architecture. As an example computer vision algorithm, we implement a real–time projective camera motion tracking routine on modern, GeForce FX class GPUs. Algorithms are implemented using OpenGL and the nVIDIA Cg fragment shaders. Trade–offs between computer vision requirements and GPU resources are discussed. Algorithm implementation is examined closely, and hardware bottlenecks are addressed to examine the performance of GPU architecture for computer vision. It is shown that significant speedups can be achieved, while leaving the CPU free for other signal processing tasks. Applications of our work include wearable, computer mediated reality systems that use both computer vision and computer graphics, and require realtime processing with low–latency and high throughput provided by modern GPUs. 1.
447|Fast and reliable collision culling using graphics hardware|Figure 1: Tree with falling leaves: In this scene, leaves fall from the tree and undergo non-rigid motion. They collide with other leaves and branches. The environment consists of more than 40K triangles and 150 leaves. Our algorithm, FAR, can compute all the collisions in about 35 msec per time step. We present a reliable culling algorithm that enables fast and accurate collision detection between triangulated models in a complex environment. Our algorithm performs fast visibility queries on the GPUs for eliminating a subset of primitives that are not in close proximity. To overcome the accuracy problems caused by the limited viewport resolution, we compute the Minkowski sum of each primitive with a sphere and perform reliable 2.5D overlap tests between the primitives. We are able to achieve more effective collision culling as compared to prior object-space culling algorithms. We integrate our culling algorithm with CULLIDE [8] and use it to perform reliable GPU-based collision queries at interactive rates on all types of models, including non-manifold geometry, deformable models, and breaking objects.
448|Solving the Euler Equations on Graphics Processing Units |Abstract. The paper describes how one can use commodity graphics cards (GPUs) as a high-performance parallel computer to simulate the dynamics of ideal gases in two and three spatial dimensions. The dynamics is described by the Euler equations, and numerical approximations are computed using state-of-the-art high-resolution finite-volume schemes. These schemes are based upon an explicit time discretisation and are therefore ideal candidates for parallel implementation. 1
449|An in-depth look at computer performance growth|Abstract — It is a common belief that computer performance growth is over 50 % annually, or that performance doubles every 18-20 months. By analyzing publicly available results from the SPEC integer (CINT) benchmark suites, we conclude that this was true between 1985 and 1996 – the early years of the RISC paradigm. During the last 7.5 years (1996-2004), however, performance growth has slowed down to 41%, with signs of a continuing decline. Meanwhile, clock frequency has improved with about 29 % annually. The improvement in clock frequency was enabled both by an annual device speed scaling of 20 % as well as by longer pipelines with a lower gate-depth in each stage. This paper takes a fresh look at – and tries to remove the confusion about – performance scaling that exists in the computer architecture community. I.
450|Fast Interpolated Cameras by combining a GPU based Plane Sweep with a Max-Flow Regularisation Algorithm|The paper presents a method for the high speed calculation of crude depth maps. Performance and applicability are illustrated for view interpolation based on two input video streams, but the algorithm is perfectly amenable to multi-camera environments. First a 
451|Kohonen Feature Mapping through Graphics Hardware|This work describes the utilization of the inherent parallelism of commonly available hardware graphics accelerators for the realization of the Kohonen feature map. The result is an essential reduction of computing time compared to standard software implementations.  Keywords. Kohonen feature map, computer graphics, hardware, OpenGL , frame buffer. 1 Introduction  The Kohonen feature map (KFM) [3] is a particular kind of an artificial neural network (ANN) model, which consists of one layer of n-dimensional units  (neurons). They are fully connected with the network input. Additionally, there exist lateral connections through which a topological structure is imposed. For the standard model, the topology is a regular two-dimensional map instantiated by connections between each unit and its direct neighbors. The KFM is used for unsupervised learning tasks [2]. Through n-dimensional training samples, the units organize in a way that they match the distribution of samples in their n-dimensi...
452|  A Relational Debugging Engine for the Graphics Pipeline |  We present a new, unified approach to debugging graphics software. We propose a representation of all graphics state over the course of program execution as a relational database, and produce a query-based framework for extracting, manipulating, and visualizing data from all stages of the graphics pipeline. Using an SQLbased query language, the programmer can establish functional relationships among all the data, linking OpenGL state to primitives to vertices to fragments to pixels. Based on the Chromium library, our approach requires no modification to or recompilation of the program to be debugged, and forms a superset of many existing techniques for debugging graphics software.
453|A graphics hardware accelerated algorithm for nearest neighbor search|Abstract. We present a GPU algorithm for the nearest neighbor search, an important database problem. The search is completely performed using the GPU: No further post-processing using the CPU is needed. Our experimental results, using large synthetic and real-world data sets, showed that our GPU algorithm is several times faster than its CPU version. 1
454|Toward real time fractal image compression using graphics hardware|Abstract. In this paper, we present a parallel fractal image compression using the programmable graphics hardware. The main problem of fractal compression is the very high computing time needed to encode images. Our implementation exploits SIMD architecture and inherent parallelism of recently graphic boards to speed-up baseline approach of fractal encoding. The results we present are achieved on cheap and widely available graphics boards. 1
455|Application of the Two-Sided Depth Test to CSG Rendering|Shadow mapping is a technique for doing real-time shadowing. Recent work has shown that shadow mapping hardware can be used as a second depth test in addition to the z-test. In this paper, we explore the computational power provided by this second depth test by examining the problem of rendering objects described as CSG (Constructive Solid Geometry) expressions. We provide an algorithm that asymptotically improves the number of rendering passes required to display a CSG object by a factor of n by exploiting the two-sided depth test. Interestingly, a matching lower bound can be proved demonstrating that our algorithm is optimal.
456|Hardware Based Wavelet Transformations|Abstract Many filtering and feature extraction algorithms usewavelet or related multiscale representations of volume data for edge detection and processing. Due tothe computational complexity of these approaches no interactive visualization of the extraction process ispossible nowadays. Using the hardware of modern graphics workstations for wavelet decomposition andreconstruction is a first important step for removing lags in the visualization cycle. 1 Introduction Feature extraction has been proven to be a usefulutility for segmentation and registration in volume visualization [6, 14]. Many edge detectionalgorithms used in this step employ wavelets or related basis functions for the internal represen-tation of the volume. Additionally, wavelets can be used for fast volume visualization [4] usingthe Fourier rendering approach [7, 13]. Wavelet decomposition and reconstruction isusually implemented by applying multiple convolution and down- / up-sampling steps to thevolume data. The convolution steps will not scale with new computer hardware as well aspure computational problems, as they are already mainly memory-bound. When using typ-ical tensor-product wavelets the complete volume data has to be accessed three times for eachwavelet filtering step.
457|Efficient 3D Audio Processing with the GPU|Introduction  Audio processing applications are among the most computeintensive and often rely on additional DSP resources for realtime performance. However, programmable audio DSPs are in general only available to product developers. Professional audio boards with multiple DSPs usually support specific effects and products while consumer &#034;game-audio&#034; hardware still only implements fixed-function pipelines which evolve at a rather slow pace.  The widespread availability and increasing processing power of GPUs could offer an alternative solution. GPU features, like multiply-accumulate instructions or multiple execution units, are similar to those of most DSPs [3]. Besides, 3D audio rendering applications require a significant number of geometric calculations, which are a perfect fit for the GPU. Our feasibility study investigates the use of GPUs for efficient audio processing.  GPU-accelerated audio rendering  We consider a combination of two simple operations commonly used for 3D audio
458|MANOCHA D.: Efficient relational database management using graphics processors|We present algorithms using graphics processing units (GPUs) to efficiently perform database management queries. Our algorithms use efficient data memory representations and storage models on GPUs to perform fast database computations. We present relational database algorithms that successfully exploit the high memory bandwidth and the inherent parallelism available in GPUs. We implement these algorithms on commodity GPUs and compare their performance with optimized CPU-based algorithms. We show that the GPUs can be used as a co-processor to accelerate many database and data mining queries. 1.
459|Wrapper Induction for Information Extraction|The Internet presents numerous sources of useful information---telephone directories, product catalogs, stock quotes, weather forecasts, etc. Recently, many systems have been built that automatically gather and manipulate such information on a user&#039;s behalf. However, these resources are usually formatted for use by people (e.g., the relevant content is embedded in HTML pages), so extracting their content is difficult. Wrappers are often used for this purpose. A wrapper is a procedure for extracting a particular resource&#039;s content. Unfortunately, hand-coding wrappers is tedious. We introduce wrapper induction, a technique for automatically constructing wrappers. Our techniques can be described in terms of three main contributions. First, we pose the problem of wrapper construction as one of inductive learn...
460|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
461|Learnability  and the Vapnik-Chervonenkis dimension| Valiant’s learnability model is extended to learning classes of concepts defined by regions in Euclidean space E”. The methods in this paper lead to a unified treatment of some of Valiant’s results, along with previous results on distribution-free convergence of certain pattern recognition algorithms. It is shown that the essential condition for distribution-free learnability is finiteness of the Vapnik-Chervonenkis dimension, a simple combinatorial parameter of the class of concepts to be learned. Using this parameter, the complexity and closure properties of learnable classes are analyzed, and the necessary and sufftcient conditions are provided for feasible learnability. 
462|The TSIMMIS Project: Integration of Heterogeneous Information Sources |The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center.
463|An Introduction to Software Agents|ion and delegation: Agents can be made extensible and composable in ways that common iconic interface objects cannot. Because we can &#034;communicate&#034; with them, they can share our goals, rather than simply process our commands. They can show us how to do things and tell us what went wrong (Miller and Neches 1987). . Flexibility and opportunism: Because they can be instructed at the level of 16 BRADSHAW goals and strategies, agents can find ways to &#034;work around&#034; unforeseen problems and exploit new opportunities as they help solve problems. . Task orientation: Agents can be designed to take the context of the person&#039;s tasks and situation into account as they present information and take action. . Adaptivity: Agents can use learning algorithms to continually improve their behavior by noticing recurrent patterns of actions and events. Toward Agent-Enabled System Architectures In the future, assistant agents at the user interface and resource-managing agents behind the scenes will increas...
464|A Scalable Comparison-Shopping Agent for the World-Wide Web|The Web is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following questions: (1) To what extent can an agent understand information published at Web sites? (2) Is the agent&#039;s understanding sufficient to provide genuinely useful assistance to users? (3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites? (4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using the ShopBot. ShopBot is a fullyimplemented, domain-independent comparison-shopping agent. Given the home pages of several on-line stores, ShopBot autonomously learns how to shop at those vendors. After its learning is com...
465|A Softbot-Based Interface to the Internet|this article, we focus on the ideas underlying the softbot-based interface.
466|Middle-Agents for the Internet|Like middle-men in physical commerce, middleagents  support the flow of information in electronic  commerce, assisting in locating and connecting the  ultimate information provider with the ultimate information  requester. Many different types of middleagents  will be useful in realistic, large, distributed,  open multi-agent problem solving systems. These  include matchmakers or yellow page agents that process  advertisements, blackboard agents that collect requests,  and brokers that process both. The behaviors  of each type of middle-agent have certain performance  characteristics---privacy, robustness, and  adaptiveness qualities---that are related to characteristics  of the external environment and of the agents  themselves. For example, while brokered systems are  more vulnerable to certain failures, they are also able  to cope more quickly with a rapidly fluctuating agent  workforce and meet certain privacy considerations.  This paper identifies a spectrum of middle-agents,  cha...
467|Inference of reversible languages|  A famdy of efficient algorithms for referring certain subclasses of the regular languages from fmtte posttwe samples is presented These subclasses are the k-reversible languages, for k = 0, 1, 2,.... For each k there is an algorithm for finding the smallest k-reversible language containing any fimte posluve sample. It ts shown how to use this algorithm to do correct identification m the ILmlt of the k-reversible languages from posmve data A reversible language is one that Is k-reverstble for some k _ _ 0. An efficient algonthrn is presented for mfernng reversible languages from posmve and negative examples, and it is shown that it leads to correct identification m the hmlt of the class of reversible languages. Numerous examples are gtven to illustrate the algorithms and their behavior
468|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
469|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
470|Semi-automatic Wrapper Generation for Internet Information Sources|To simplify the task of obtaining information from the vast number of information sources that are available on the World Wide Web (WWW), we are building tools to build information mediators for extracting and integrating data from multiple Web sources. In a mediator based approach, wrappers are built around individual information sources, that provide translation between the mediator query language and the individual source. We present an approach for semi-automatically generating wrappers for structured internet sources. The key idea is to exploit formatting information in Web pages from the source to hypothesize the underlying structure of a page. From this structure the system generates a wrapper that facilitates querying of a source and possibly integrating it with other sources. We demonstrate the ease with which we are able to build wrappers for a number of Web sources using our implemented wrapper generation toolkit.  1. Introduction  We are building information agents or media...
471|Kqml: A language and protocol for knowledge and information exchange|Abstract. This paper describes the design of and experimentation with the Knowledge Query and Manipulation Language (KQML), a new language and protocol for exchanging information and knowledge. This work is part a larger effort, the ARPA Knowledge Sharing Effort which is aimed at developing techniques and methodology for building large-scale knowledge bases which are sharable and reusable. KQML is both a message format and a message-handling protocol to support run-time knowledge sharing among agents. KQML can be used as a language for an application program to interact with an intelligent system or for two or more intelligent systems to share knowledge in support of cooperative problem solving. KQML focuses on an extensible set of performatives, which defines the permissible operations that agents may attempt on each other’s knowledge and goal stores. The performatives comprise a substrate on which to develop higher-level models of inter-agent interaction such as contract nets and negotiation. In addition, KQML provides a basic architecture for knowledge sharing through a special class of agent called communication facilitators which coordinate the interactions of other agents The ideas which underlie the evolving design of KQML are currently being explored through experimental prototype systems which are being used to support several testbeds in such areas as concurrent engineering, intelligent design and intelligent planning and scheduling.
472|The World Wide Web: quagmire or gold mine?|This article considers the question: is effective Web mining possible?
473|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
474|Intelligence without Robots (A Reply to Brooks)  (1993) |In his recent papers, entitled &#034;Intelligence without Representation and &#034;Intelligence without Reason,&#034; Brooks argues for studying complete agents in real-world environments and for mobile robots as the foundation for AI research. This article argues that, even if we seek to investigate complete agents in real-world environments, robotics is neither necessary nor sufficient as a basis for AI research. The article proposes real-world software environments, such as operating systems or databases, as a complementary substrate for intelligent-agents research, and considers the relative advantages of software environments as testbeds for AI. First, the cost, effort, and expertise necessary to develop and systematically experiment with software artifacts are relatively low. Second, software environments circumvent many thorny, but peripheral, research issues that are inescapable in physical environments. Brooks&#039;s mobile robots tug AI towards a bottom-up focus in which the mechanics of percept...
475|Building Softbots for UNIX (Preliminary Report)  (1992) |AI is moving away from &#034;toy tasks&#034; such as block stacking towards real-world problems. This trend is positive, but the amount of preliminary groundwork required to tackle a real-world task can be staggering, particularly when developing an integrated agent architecture. To address this problem, we advicate real-world software environments, such as operating systems or databases, as domains for agent research. The cost, effort, and expertise required to develop and systematically experiment with software agents is relatively low. Furthermore, software environments circumvent many thorny, but peripheral, research issues that are inescapable in other environments. Thus, software environments enable us to test agents ina real world yet focus on core AI research issues. To support this claim, we describe our project to develop UNIX softbots (software robots) -- intelligent agnets that interact with UNIX. Existing softbots accept a diverse set of high-level goals, generate and execute plans to achieve these goals in real time, and recover from errors when necessary.
476|Learning to Query the Web|The World Wide Web (WWW) is filled with &#034;resource directories&#034;---i.e., documents that collect together links to all known documents on a specific topic. Keeping resource directories up-to-date is difficult because of the rapid growth in online documents. We propose using machine learning methods to address this problem. In particular, we propose to treat a resource directory as a list of positive examples of an unknown concept, and then use machine learning methods to construct from these examples a definition of the unknown concept. If the learned definition is in the appropriate form, it can be translated into a query, or series of queries, for a WWW search engine. This query can be used at a later date to detect any new instances of the concept. We present experimental results with two implemented systems, and two learning methods. One system is interactive, and is implemented as an augmented WWW browser; the other is a batch system, which can collect and label documents without an...
477|Using Natural Language Processing for Identifying and Interpreting Tables in Plain Text|Characteristics of Tables 2.1 Table Denotations A simple table in some abstract sense denotes a relation among sets of values. 2 (We can think of this as corresponding to relations or views in relational database terms. See, for example, [Ull88]) In these terms, the underlying representation of a table is a set of n-tuples, where n is the number of domains or value sets in the table. The n-tuples of the table is that subset of the cross-product D 1 \Theta D 2 \Theta : : : \Theta Dn for which the relation holds. This is the canonical form of the table. For our nlp application, we need a relatively detailed ontology or world model of the objects of the sublanguage of construction. The classes of this world model we call world domains; they include 1 We thank Stephen McCarron of BICC. We acknowledge the support of the Department of Trade and Industry, the Engineering and Physical Sciences Research Council, and the BICC Group on the CISAU project (IED4/1/5818), and the Human Communi...
478|Towards Sophisticated Wrapping of Web-based Information Repositories|Access to on-line information via the Web is exploding. Index and retrieval engines already start to integrate a huge variety of heterogeneous repositories. However, the heterogeneity issue remains, both in terms of the search formats and the formats of the result pages. In this paper,
479|On Learning from Noisy and Incomplete Examples|We investigate learnability in the PAC model when the data used for learning, attributes and labels, is either corrupted or incomplete. In order to prove our main results, we define a new complexity measure on statistical query (SQ) learning algorithms. The view of an SQ algorithm is the maximumover all queries in the algorithm, of the number of input bits on which the query depends. We show that a restricted view SQ algorithm for a class is a general sufficient condition for learnability in both the models of attribute noise and covered (or missing) attributes. We further show that since the algorithms in question are statistical, they can also simultaneously tolerate classification noise. Classes for which these results hold, and can therefore be learned with simultaneous attribute noise and classification noise, include  k-DNF, k-term-DNF by DNF representations, conjunctions with few relevant variables, and over the uniform distribution, decision lists. These noise models are the fi...
480|Alternative isoform regulation in human tissue transcriptomes|Through alternative processing of pre-mRNAs, individual mammalian genes often produce multiple mRNA and protein isoforms that may have related, distinct or even opposing functions. Here we report an in-depth analysis of 15 diverse human tissue and cell line transcriptomes based on deep sequencing of cDNA fragments, yielding a digital inventory of gene and mRNA isoform expression. Analysis of mappings of sequence reads to exon-exon junctions indicated that 92-94% of human genes undergo alternative splicing (AS), ~86 % with a minor isoform frequency of 15% or more. Differences in isoform-specific read densities indicated that a majority of AS and of alternative cleavage and polyadenylation (APA) events vary between tissues, while variation between individuals was ~2- to 3-fold less common. Extreme or ‘switch-like ’ regulation of splicing between tissues was associated with increased sequence conservation in regulatory regions and with generation of full-length open reading frames. Patterns of AS and APA were strongly correlated across tissues, suggesting coordinated regulation of these processes, and sequence conservation of a subset of known regulatory motifs in both alternative introns and 3' UTRs suggested common involvement of specific factors in tissue-level regulation of both
481|Unusual Intron Conservation near Tissue-Regulated Exons Found by Splicing Microarrays|Alternative splicing contributes to both gene regulation and protein diversity. To discover broad relationships between regulation of alternative splicing and sequence conservation, we applied a systems approach, using oligonucleotide microarrays designed to capture splicing information across the mouse genome. In a set of 22 adult tissues, we observe differential expression of RNA containing at least two alternative splice junctions for about 40 % of the 6,216 alternative events we could detect. Statistical comparisons identify 171 cassette exons whose inclusion or skipping is different in brain relative to other tissues and another 28 exons whose splicing is different in muscle. A subset of these exons is associated with unusual blocks of intron sequence whose conservation in vertebrates rivals that of protein-coding exons. By focusing on sets of exons with similar regulatory patterns, we have identified new sequence motifs implicated in brain and muscle splicing regulation. Of note is a motif that is strikingly similar to the branchpoint consensus but is located downstream of the 59 splice site of exons included in muscle. Analysis of three paralogous membrane-associated guanylate kinase genes reveals that each contains a paralogous tissue-regulated exon with a similar tissue inclusion pattern. While the intron sequences flanking these exons remain highly conserved among mammalian orthologs, the paralogous flanking intron sequences have diverged considerably, suggesting unusually
482|Polypyrimidine tract binding protein modulates efficiency of polyadenylation|These include: This article cites 61 articles, 31 of which can be accessed free at:
483|CJ: Protein modularity of alternatively spliced exons is associated with tissue-specific regulation of alternative splicing |Recent comparative genomic analysis of alternative splicing has shown that protein modularity is an important criterion for functional alternative splicing events. Exons that are alternatively spliced in multiple organisms are much more likely to be an exact multiple of 3 nt in length, representing a class of ‘‘modular’ ’ exons that can be inserted or removed from the transcripts without affecting the rest of the protein. To understand the precise roles of these modular exons, in this paper we have analyzed microarray data for 3,126 alternatively spliced exons across ten mouse tissues generated by Pan and coworkers. We show that modular exons are strongly associated with tissue-specific regulation of alternative splicing. Exons that are alternatively spliced at uniformly high transcript inclusion levels or uniformly low levels show no preference for protein modularity. In contrast, alternatively spliced exons with dramatic changes of inclusion levels across mouse tissues (referred to as ‘‘tissue-switched’ ’ exons) are both strikingly biased to be modular and are strongly conserved between human and mouse. The analysis of different subsets of tissueswitched exons shows that the increased protein modularity cannot be explained by the overall exon inclusion level, but is specifically associated with tissue-switched alternative splicing. Citation: Xing Y, Lee CJ (2005) Protein modularity of alternatively spliced exons is associated with tissue-specific regulation of alternative splicing. PLoS Genet 1(3): e34.
484|High confidence visual recognition of persons by a test of statistical independence|Abstruct- A method for rapid visual recognition of personal identity is described, based on the failure of a statistical test of independence. The most unique phenotypic feature visible in a person’s face is the detailed texture of each eye’s iris: An estimate of its statistical complexity in a sample of the human population reveals variation corresponding to several hundred independent degrees-of-freedom. Morphogenetic randomness in the texture expressed phenotypically in the iris trabecular meshwork ensures that a test of statistical independence on two coded patterns originating from different eyes is passed almost certainly, whereas the same test is failed almost certainly when the compared codes originate from the same eye. The visible texture of a person’s iris in a real-time video image is encoded into a compact sequence of multi-scale quadrature 2-D Gabor wavelet coefficients, whose most-significant bits comprise a 256-byte “iris code. ” Statistical decision theory generates identification decisions from Exclusive-OR comparisons of complete iris codes at the rate of 4000 per second, including calculation of decision confidence levels. The distributions observed empirically in such comparisons imply a theoretical “cross-over ” error rate of one in 131000 when a decision criterion is adopted that would equalize the false accept and false reject error rates. In the typical recognition case, given the mean observed degree of iris code agreement, the decision confidence levels correspond formally to a conditional false accept probability of one in about lo”’. Index Terms- Image analysis, statistical pattern recognition, biometric identification, statistical decision theory, 2-D Gabor filters, wavelets, texture analysis, morphogenesis. I.
485|A theory for multiresolution signal decomposition : the wavelet representation|Abstract-Multiresolution representations are very effective for analyzing the information content of images. We study the properties of the operator which approximates a signal at a given resolution. We show that the difference of information between the approximation of a signal at the resolutions 2 ’ + ’ and 2jcan be extracted by decomposing this signal on a wavelet orthonormal basis of L*(R”). In LL(R), a wavelet orthonormal basis is a family of functions (  @ w (2’ ~-n)),,,“jEZt, which is built by dilating and translating a unique function t+r (xl. This decomposition defines an orthogonal multiresolution representation called a wavelet representation. It is computed with a pyramidal algorithm based on convolutions with quadrature mirror lilters. For images, the wavelet representation differentiates several spatial orientations. We study the application of this representation to data compression in image coding, texture discrimination and fractal analysis. Index Terms-Coding, fractals, multiresolution pyramids, quadrature mirror filters, texture discrimination, wavelet transform. I I.
486|Unsupervised texture segmentation using Gabor filters |We presenf a texture segmentation algorithm inspired by the multi-channel filtering theory for visual information processing in the early stages of human visual system. The channels are characterized by a bank of Gabor filters that nearly uniformly covers the spatial-frequency domain. We propose a systematic filter selection scheme which is based on reconstruction of the input image from the filtered images. Texture features are obtained by subjecting each (selected) filtered image to a nonlinear transformation and computing a measure of “energy ” in a window around each pixel. An unsupervised square-emr clustering algorithm is then used to integrate the feature images and produce a segmentation. A simple procedure to incorporate spatial adjacency information in the clustering process is also proposed. We report experiments on images with natural textures as well as artificial textures with identical 2nd- and 3rd-order statistics.
487|Texture discrimination by Gabor functions|Abstract. A 2D Gabor filter can be realized as a sinusoidal plane wave of some frequency and orienta-tion within a two dimensional Gaussian envelope. Its spatial extent, frequency and orientation preferences as well as bandwidths are easily controlled by the parameters used in generating the filters. However, there is an &#034;uncertainty relation &#034; associated with linear filters which limits the resolution simultaneously at-tainable in space and frequency. Daugman (1985) has determined that 2D Gabor filters are members of a class of functions achieving optimal joint resolution in the 2D space and 2D frequency domains. They have also been found to be a good model for two dimen-sional receptive fields of simple cells in the striate cortex (Jones 1985; Jones et al. 1985). The characteristic of optimal joint resolution in both space and frequency suggests that these filters are appropriate operators for tasks requiring simulta-neous measurement in these domains. Texture dis-crimination is such a task. Computer application of a set of Gabor filters to a variety of textures found to be preattentively discriminable produces results in which differently textured regions are distinguished by first-order differences in the values measured by the filters. This ability to reduce the statistical complexity dis-tinguishing differently textured region as well as the sensitivity of these filters to certain types of local features suggest that Gabor functions can act as detectors of certain &#034;texton &#034; types. The performance of the computer models suggests that cortical neurons with Gabor like receptive fields may be involved in preattentive texture discrimination. 1
488|Textured Image Segmentation Using Localized Receptive Fields|: We present an approach to texture analysis that uses spatially localized filters and cooperativecompetitive mechanisms for determining emergent boundaries. Gabor filters that closely resemble cortical receptive fields are used to activate columns of cells that selectively respond to localized frequency and orientation attributes of an image. For each cell column, a winner-take-all network that is moderated by the activities of neighboring columns is used to gradually segment the image. All the mechanisms used are biologically plausible and typically yield results that are superior to those reported previously for real images.  1. Texture-based Segmentation.  Both biological systems and computational vision models use texture for perceptual tasks such as segmentation of surfaces, classification of surface materials and computation of shape. An image texture can be interpreted as a pattern of image intensities projected from a surface of uniform surface radiance attributes. Several mod...
489|A survey of context-aware mobile computing research|Context-aware computing is a mobile computing paradigm in which applications can discover and take advantage of contextual information (such as user location, time of day, nearby people and devices, and user activity). Since it was proposed about a decade ago, many researchers have studied this topic and built several context-aware applications to demonstrate the usefulness of this new technology. Context-aware applications (or the system infrastructure to support them), however, have never been widely available to everyday users. In this survey of research on context-aware systems and applications, we looked in depth at the types of context used and models of context information, at systems that support collecting and disseminating context, and at applications that adapt to the changing context. Through this survey, it is clear that context-aware research is an old but rich area for research. The difficulties and possible solutions we outline serve as guidance for researchers hoping to make context-aware computing a reality. 1.
490|RADAR: an in-building RF-based user location and tracking system|The proliferation of mobile computing devices and local-area wireless networks has fostered a growing interest in location-aware systems and services. In this paper we present RADAR, a radio-frequency (RF) based system for locating and tracking users inside buildings. RADAR operates by recording and processing signal strength information at multiple base stations positioned to provide overlapping coverage in the area of interest. It employs techniques that combine empirical measurements with signal propagation modeling to enable location-aware services and applications. We present concrete experimental results that demonstrate the feasibility of using RADAR to estimate user location with a high degree of accuracy. 1
491|The Active Badge Location System|cation is the `pager system&#039;. In order to locate a person a signal is sent out by a central facility that addresses a particular receiver unit (beeper) and produces an audible signal. In addition, it may display a number to which the called-party should phone back (some systems allow a vocal message to be conveyed about the call-back number). It is then up to the recipient to use the conventional telephone system to call-back confirming the signal and determine the required action. Although useful in practice there are still circumstances where it is not ideal. For instance, if the called party does not reply the controller has no idea if they: 1) are in an area where the signal does not penetrate 2) have been completely out of the area for some time 3) have been too busy to reply or 4) have misheard or misread the call-back number. Moreover, in the case where there are a number of people who could respond to a crisis situation, it is not known which one is the nearest to the crisis an
492|ContextAware Computing Applications|This paper describes systems thatel:amine and re-actto an indi7Jidltal&#039;s changing context. Such systems can promote and mediate people&#039;s mleractlOns with de-Vices, computers, and other people, and they can help navigate unfamiliar places. We bel1eve that a lunded amount of information coveTIng a per&#039;son&#039;s proximale environment is most important for this form of com-puting since the interesting part of the world around us is what we can see, hear, and touch. In this paper we define context-aware computing, and describe four cal-egones of conteL·t-aware applications: proximate selec-tion, automatic contextual reconfiguratlOn, contexlual information and commands, and context-triggered ac-tions. fnstances of these application types ha11e been prototyped on the PARCTAB, a wireless, palm-sl.:ed computer. 1
493|Towards a better understanding of context and context-awareness|Abstract. The use of context is important in interactive applications. It is particularly important for applications where the user’s context is changing rapidly, such as in both handheld and ubiquitous computing. In order to better understand how we can use context and facilitate the building of context-aware applications, we need to more fully understand what constitutes a contextaware application and what context is. Towards this goal, we have surveyed existing work in context-aware computing. In this paper, we provide an overview of the results of this survey and, in particular, definitions and categories of context and context-aware. We conclude with recommendations for how this better understanding of context inform a framework for the development of context-aware applications. 1
494|The Context Toolkit: Aiding the Development of Context-Enabled Applications|Context-enabled applications are just emerging and promise richer interaction by taking environmental context into account. However, they are difficult to build due to their distributed nature and the use of unconventional sensors. The concepts of toolkits and widget libraries in graphical user interfaces has been tremendously successtil, allowing programmers to leverage off existing building blocks to build interactive systems more easily. We introduce the concept of context widgets that mediate between the environment and the application in the same way graphical widgets mediate between the user and the application. We illustrate the concept of context widgets with the beginnings of a widget library we have developed for sensing presence, identity and activity of people and things. We assess the success of our approach with two example context-enabled applications we have built and an existing application to which we have added contextsensing capabilities.
495|The Anatomy of a Context-Aware Application|We describe a platform for context-aware computing which enables applications to follow mobile users as they move around a building. The platform is particularly suitable for richly equipped, networked environments. The only item a user is required to carry is a small sensor tag, which identifies them to the system and locates them accurately in three dimensions. The platform builds a dynamic model of the environment using these location sensors and resource information gathered by telemetry software, and presents it in a form suitable for application programmers. Use of the platform is illustrated through a practical example, which allows a user&#039;s current working desktop to follow them as they move around the environment.
496|A New Location Technique for the Active Office|Configuration of the computing and communications systems found at home and in the workplace is a complex task that currently requires the  attention of the user. Recently, researchers have begun to examine computers that would autonomously change their functionality based on  observations of who or what was around them. By determining their context, using input from sensor systems distributed throughout the  environment, computing devices could personalize themselves to their current user, adapt their behavior according to their location, or react  to their surroundings. The authors present a novel sensor system, suitable for large-scale deployment in indoor environments, which allows the  locations of people and equipment to be accurately determined. We also describe some of the context-aware applications that might make use  of this fine-grained location information.
497|Agile Application-Aware Adaptation for Mobility|In this paper we show that application-aware adaptation, a collaborative partnership between the operating system and applications, offers the most general and effective approach to mobile information access. We describe the design of Odyssey, a prototype implementing this approach, and show how it supports concurrent execution of diverse mobile applications. We identify agility as a key attribute of adaptive systems, and describe how to quantify and measure it. We present the results of our evaluation of Odyssey, indicating performance improvements up to a factor of 5 on a benchmark of three applications concurrently using remote services over a network with highly variable bandwidth.  
498|The ParcTab Ubiquitous Computing Experiment|... This paper describes the Ubiquitous Computing philosophy, the PARCTAB system, user-interface issues for small devices, and our experience developing and testing a variety of mobile applications.
499|The Smart Floor: A Mechanism for Natural User Identification and Tracking|We have created a system for identifying people based on their footstep force profiles and have tested its accuracy against a large pool of footstep data. This floor system may be used to transparently identify users in their everyday living and working environments. We have created user footstep models based on footstep profile features and have been able to achieve a recognition rate of 93 % using this feature-based approach. We have also shown that the effect of footwear is negligible on recognition accuracy.
500|Location-aware information delivery with comMotion|This paper appears in the HUC 2000 Proceedings, pp.157-171, Springer-Verlag
501|Rapid Prototyping of Mobile Context-Aware Applications: The Cyberguide Case Study|We present the Cyberguide project, in which we are building prototypes of a mobile context-aware tour guide that provide information to a tourist based on knowledge of position and orientation. We describe features of existing Cyberguide prototypes and discuss research issues that have emerged in our context-aware applications development in a mobile environment. Keywords: Mobile computing applications, contextawareness, location-dependent applications, hand-held devices 1 Introduction  The project we report on in this paper, Cyberguide, has as its main focus the rapid prototyping of handheld mobile applications in order to assess the utility of context-awareness in mobile devices. The challenge we are addressing in Cyberguide is how to build mobile applications that make use of the context of the user. Initially, we are concerned with only a small part of the user&#039;s context, specifically location and orientation. The application which drives the development of Cyberguide is a that of ...
503|A System Architecture for Context-Aware Mobile Computing|Computer applications traditionally expect a static execution environment. However, this precondition is generally not possible for mobile systems, where the world around an application is constantly changing. This thesis explores how to support and also exploit the dynamic configurations and social settings characteristic of mobile systems. More specifically, it advances the following goals: (1) enabling seamless interaction across devices; (2) creating physical spaces that are responsive to users; and (3) and building applications that are aware of the context of their use. Examples of these goals are: continuing in your office a program started at home; using a PDA to control someone else&#039;s windowing UI; automatically canceling phone forwarding upon return to your office; having an airport overheaddisplay highlight the flight information viewers are likely to be interested in; easily locating and using the nearest printer or fax machine; and automatically turning off a PDA&#039;s audible e-mail notification when in a meeting.
504|The conference assistant: Combining context-awareness with wearable computing|We describe the Conference Assistant, a prototype mobile, context-aware application that assists conference attendees. We discuss the strong relationship between context-awareness and wearable computing and apply this relationship in the Conference Assistant. The application uses a wide variety of context and enhances user interactions with both the environment and other users. We describe how the application is used and the context-aware architecture on which it is based. 1.
505|Towards Situated Computing|Situated computing concerns the ability of computing devices to detect, interpret and respond to aspects of the user’s local environment. In this paper, we use our recent prototyping experience to identify a number of challenging issues that must be resolved in building wearable computers that support situated applications. The paper is organized into three areas: Sensing the local environment, interpreting sensor data, and realizing the value of situated applications. We conclude that while it is feasible to develop interesting prototypes, there remain many difficulties to overcome before robust systems may be widely deployed.
506|Supporting Location-Awareness in Open Distributed Systems|Mobile computers and communication devices are establishing themselves as ubiquitous features of daily life. This development is linked to tremendous growth in the number and sophistication of mobile and mobile-aware software applications. Increasingly, such applications need access to information about their own and other objects&#039; physical locations, a requirement known as location-awareness.
507|Experience with Disconnected Operation in a Mobile Computing Environment|In this paper we present qualitative and quantitative data on file access in a mobile computing environment. This information is based on actual usage experience with the Coda File System over a period of about two years. Our experience confirms the viability and effectiveness of disconnected operation. It also exposes certain deficiencies of the current implementation of Coda, and identifies new functionality that would enhance its usefulness for mobile computing. The paper concludes with a description of what we are doing to address these issues. This work was supported by the Advanced Research Projects Agency (Avionics Laboratory, Wright Research and Development Center, Aeronautical Systems Division(AFSC), U.S. Air Force, Wright-Patterson AFB under Contract F33615-90-C-1465, Arpa Order No. 7597), the National Science Foundation (Grant ECD 8907068), IBM, Digital Equipment Corporation, and Bellcore. The views and conclusions contained in this document are those of the authors and sho...
508|The Stick-e Note Architecture: extending the interface beyond the user|This paper proposes a redefinition of the human-computer interface, extending its boundaries to encompass interaction with the user’s physical environment. This extension to the interface enables computers to become aware of their context of use and intelligently adapt their activities and interface to suit their current circumstances. Context-awareness promises to greatly enhance user interfaces, but the complexity of capturing, representing and processing contextual data, presents a major obstacle to its further development. The Stick-e Note Architecture is proposed as a solution to this problem, offering a universal means of providing context-awareness through an easily understood metaphor based on the Post-It note. Keywords context-aware computing, stick-e note architecture, mobile computing, ubiquitous computing, situated information spaces.
509|MOBISAIC: An Information System for A Mobile Wireless Computing Environment|Mobisaic is a World Wide Web information system designed to serve users in a mobile wireless computing environment. Mobisaic extends the Web by allowing documents to both refer and react to potentially changing contextual information, such as current location in the wireless network. Mobisaic relies on clientside processing of HTML documents that support two new concepts: Dynamic Uniform Resource Locators (URLs) and Active Documents. A dynamic URL is one whose results depend upon the state of the user&#039;s mobile context at the time it is resolved. An active document is one that automatically updates its contents in response to changes in a user&#039;s mobile context. This paper describes the design of Mobisaic, the mechanism it uses for representing a user&#039;s mobile context, and the extensions made to the syntax and function of Uniform Resource Locators and HyperText Markup Language documents to support mobility. 
510|An indoor wireless system for personalized shopping assistance|By integrating wireless, video, speech and real-time data access technologies, a unique shopping assistant service can be created that personalizes the attention provided to a customer based on individual needs, without limiting his movement, or causing distractions for others in the shopping center. We have developed this idea into a service based on two products: a very high volume hand-held wireless communications device, the PSA (Personal Shopping Assistant), that the customer owns (or may be provided to a customer by the retailer), and a centralized server located in the shopping center to which the customer communicates using the PSA. The centralized server maintains the customer database, the store database and provides audio/visual responses to inquiries from tens to hundreds of customers in real-time over a small area wireless network. 1.
511|Customizing Mobile Applications|The dynamics of mobile systems require applications to intelligently adapt to changes in system configurations and to their environment. We describe a workplace in which users interact with a number of stationary and mobile systems through the course of a day. The relationship between systems and devices is constantly changing due to user mobility. We present a facility for mobile application customization called &#034;dynamic environment variables.&#034; The facility allows flexible sharing of customization contexts, supports short interactions with long term services, and provides efficient notification of environment changes to applications. A sample application built in PARC&#039;s mobile computing environment and initial performance evaluations are described. 1 Introduction Mobile computing differs from desktop computing because of the dynamic nature of system state. In our research lab mobile users interact with mobile computers as well as a world of stationary and embedded systems [8]. As use...
512|Keeping Up With The Changing Web|Our access to information today is unprecedented in history. However, information depreciates in  value as it gets older, and the problem of updating information to keep it current presents new design  challenges for information providers and consumers. These issues lead to novel concepts and results in  the context of the World Wide Web. We quantify what it means to for search engines to be \up-to-date&#034;  and estimate how often search engines must re-index the web to keep current with it changing pages and  structure.  Three weeks prior to the Soviet invasion of Czechoslovakia, Corona satellite imagery of the area showed no signs of imminent attack. By the time another round of imagery was available, it was too late to react; the invasion had already taken place. In a real sense, the information obtained by the satellite weeks earlier was no longer useful. The fact that information has a useful lifetime is well known in the intelligence community. On the other side of the Iron Curtain,...
514|Providing Architectural Support for Context-Aware applications|Context is an important, yet poorly utilized source of information in interactive computing. It is difficult to use because, unlike other forms of user input, there is no common, reusable way to handle context. Most context-aware applications have been built in an ad hoc manner. We discuss the requirements for dealing with context and present an architectural solution we have designed and implemented to help application designers build context-aware applications more easily. We illustrate the use of the architecture through a context-aware application that assists conference attendees.
515|Teleporting - Making Applications Mobile|The rapid emergence of mobile computers as a popular, and increasingly powerful, computing tool is presenting new challenges. This subject is already being widely addressed within the computing literature. A complementary and relatively unexplored notion of mobility is one in which application interfaces, rather than the computer on which the applications run, are able to move. The Teleporting System developed at the Olivetti Research Laboratory (ORL) is a tool for experiencing such `mobile applications&#039;. It operates within the X Window System  1  , and allows users to interact with their existing X applications at any X display within a building. The process of controlling the interface to the teleporting system is very simple. This simplicity comes from the use of an automatically maintained database of the location of equipment and people within the building. This paper describes the teleporting system, what it does, and how it is used. We outline some of the issues of making applic...
516|A System for Tracking and Recognizing Multiple People with Multiple Cameras|In this paper we present a robust real-time method for tracking and recognizing multiple people with multiple cameras. Our method uses both static and Pan-Tilt-Zoom (PTZ) cameras to provide visual attention. The PTZ camera system uses face recognition to register people in the scene and &#034;lock-on&#034; to those individuals. The static camera system provides a global view of the environment and is used to re-adjust the tracking of the system when the PTZ cameras lose their targets. The system works well even when people occlude one another. The underlying visual processes rely on color segmentation, movement tracking and shape information to locate target candidates. Color indexing and face recognition modules help register these candidates with the system. 1. Introduction  One of the goals of building an intelligent environment is to make it more aware of the user&#039;s presence so that the interface can seek out and serve the user [1]. This work addresses the ability of a system to determine th...
517|Scalable and Flexible Location-Based Services for Ubiquitous Information Access|Abstract. In mobile distributed environments applications often need to dynamically obtain information that is relevant to their current loca-tion. The current design of the Internet does not provide any conceptual models for addressing this issue. As a result, developing a system that requires this functionality becomes a challenging and costly task, lead-ing to individual solutions that only address the requirements of specic application scenarios. In this paper we propose a more generic approach, based on a scalable and 
exible concept of location-based services, and an architectural framework to support its application in the Internet envi-ronment. We describe a case study in which this architectural framework is used for developing a location-sensitive tourist guide. The realisation of this case study demonstrates the applicability of the framework, as well as the overall concept of location-based services, and highlights some of the issues involved. 1
518|Context-aware office assistant|This paper describes the design and implementation of the Office Assistant- an agent that interacts with visitors at the office door and manages the office owner’s schedule. We claim that rich context information about users is key to making a flexible and believable interaction. We also argue that natural face-to-face conversation is an appropriate metaphor for human-computer interaction.
519|Adaptive Support for a Mobile Museum Guide|The paper aims at supporting human activities with
520|Integration of location services in the open distributed office|There has recently been much interest in location systems which enable people and equipment to be tracked as they move within and across buildings  Thus far  such sys tems have been used in isolation with the result that  although there are often several sources of location information available at any one site  users have to consult each sys tem individually  Additionally  it is dicult for services requiring location information to take advantage of all these sources In this paper  we put forward the notion of a master location system to coordinate the location process so that available sources of information are used automatically in as ecient a manner as possible  We discuss a number of factors that are of concern to the design of such a system  and describe a particular implementation which we worked on as part of an oce automation project There has recently been much interest in location systems which enable people and equip ment to be tracked as they move within and across buildings  Perhaps one of the more
521|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
522|Why We Twitter: Understanding Microblogging Usage and Communities |Microblogging is a new form of communication in which users can describe their current status in short posts distributed by instant messages, mobile phones, email or the Web. Twitter, a popular microblogging tool has seen a lot of growth since it launched in October, 2006. In this paper, we present our observations of the microblogging phenomena by studying the topological and geographical properties of Twitter’s social network. We find that people use microblogging to talk about their daily activities and to seek or share information. Finally, we analyze the user intentions associated at a community level and show how users with similar intentions connect with each other.
524|Wide-area cooperative storage with CFS|The Cooperative File System (CFS) is a new peer-to-peer readonly storage system that provides provable guarantees for the efficiency, robustness, and load-balance of file storage and retrieval. CFS does this with a completely decentralized architecture that can scale to large systems. CFS servers provide a distributed hash table (DHash) for block storage. CFS clients interpret DHash blocks as a file system. DHash distributes and caches blocks at a fine granularity to achieve load balance, uses replication for robustness, and decreases latency with server selection. DHash finds blocks using the Chord location protocol, which operates in time logarithmic in the number of servers. CFS is implemented using the SFS file system toolkit and runs on Linux, OpenBSD, and FreeBSD. Experience on a globally deployed prototype shows that CFS delivers data to clients as fast as FTP. Controlled tests show that CFS is scalable: with 4,096 servers, looking up a block of data involves contacting only seven servers. The tests also demonstrate nearly perfect robustness and unimpaired performance even when as many as half the servers fail.
525|Chord: A Scalable Peer-to-Peer Lookup Service for Internet Applications|A fundamental problem that confronts peer-to-peer applications is to efficiently locate the node that stores a particular data item. This paper presents Chord, a distributed lookup protocol that addresses this problem. Chord provides support for just one operation: given a key, it maps the key onto a node. Data location can be easily implemented on top of Chord by associating a key with each data item, and storing the key/data item pair at the node to which the key maps. Chord adapts efficiently as nodes join and leave the system, and can answer queries even if the system is continuously changing. Results from theoretical analysis, simulations, and experiments show that Chord is scalable, with communication cost and the state maintained by each node scaling logarithmically with the number of Chord nodes.
526|A Scalable Content-Addressable Network|Hash tables – which map “keys ” onto “values” – are an essential building block in modern software systems. We believe a similar functionality would be equally valuable to large distributed systems. In this paper, we introduce the concept of a Content-Addressable Network (CAN) as a distributed infrastructure that provides hash table-like functionality on Internet-like scales. The CAN is scalable, fault-tolerant and completely self-organizing, and we demonstrate its scalability, robustness and low-latency properties through simulation. 
527|Pastry: Scalable, distributed object location and routing for large-scale peer-to-peer systems|This paper presents the design and evaluation of Pastry, a scalable, distributed object location and routing scheme for wide-area peer-to-peer applications. Pastry provides application-level routing and object location in a potentially very large overlay network of nodes connected via the Internet. It can be used to support a wide range of peer-to-peer applications like global data storage, global data sharing, and naming. An insert operation in Pastry stores an object at a user-defined number of diverse nodes within the Pastry network. A lookup operation reliably retrieves a copy of the requested object if one exists. Moreover, a lookup is usually routed to the node nearest the client issuing the lookup (by some measure of proximity), among the nodes storing the requested object. Pastry is completely decentralized, scalable, and self-configuring; it automatically adapts to the arrival, departure and failure of nodes. Experimental results obtained with a prototype implementation on a simulated network of 100,000 nodes confirm Pastry&#039;s scalability, its ability to self-configure and adapt to node failures, and its good network locality properties.
529|Tapestry: An infrastructure for fault-tolerant wide-area location and routing|In today’s chaotic network, data and services are mobile and replicated widely for availability, durability, and locality. Components within this infrastructure interact in rich and complex ways, greatly stressing traditional approaches to name service and routing. This paper explores an alternative to traditional approaches called Tapestry. Tapestry is an overlay location and routing infrastructure that provides location-independent routing of messages directly to the closest copy of an object or service using only point-to-point links and without centralized resources. The routing and directory information within this infrastructure is purely soft state and easily repaired. Tapestry is self-administering, faulttolerant, and resilient under load. This paper presents the architecture and algorithms of Tapestry and explores their advantages through a number of experiments. 1
530|Resilient Overlay Networks|A Resilient Overlay Network (RON) is an architecture that allows distributed Internet applications to detect and recover from path outages and periods of degraded performance within several seconds, improving over today’s wide-area routing protocols that take at least several minutes to recover. A RON is an application-layer overlay on top of the existing Internet routing substrate. The RON nodes monitor the functioning and quality of the Internet paths among themselves, and use this information to decide whether to route packets directly over the Internet or by way of other RON nodes, optimizing application-specific routing metrics. Results from two sets of measurements of a working RON deployed at sites scattered across the Internet demonstrate the benefits of our architecture. For instance, over a 64-hour sampling period in March 2001 across a twelve-node RON, there were 32 significant outages, each lasting over thirty minutes, over the 132 measured paths. RON’s routing mechanism was able to detect, recover, and route around all of them, in less than twenty seconds on average, showing that its methods for fault detection and recovery work well at discovering alternate paths in the Internet. Furthermore, RON was able to improve the loss rate, latency, or throughput perceived by data transfers; for example, about 5 % of the transfers doubled their TCP throughput and 5 % of our transfers saw their loss probability reduced by 0.05. We found that forwarding packets via at most one intermediate RON node is sufficient to overcome faults and improve performance in most cases. These improvements, particularly in the area of fault detection and recovery, demonstrate the benefits of moving some of the control over routing into the hands of end-systems. 
531|Oceanstore: An architecture for global-scale persistent storage|OceanStore is a utility infrastructure designed to span the globe and provide continuous access to persistent information. Since this infrastructure is comprised of untrusted servers, data is protected through redundancy and cryptographic techniques. To improve performance, data is allowed to be cached anywhere, anytime. Additionally, monitoring of usage patterns allows adaptation to regional outages and denial of service attacks; monitoring also enhances performance through pro-active movement of data. A prototype implementation is currently under development. 1
532|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
533|Summary cache: A scalable wide-area web cache sharing protocol|The sharing of caches among Web proxies is an important technique to reduce Web traffic and alleviate network bottlenecks. Nevertheless it is not widely deployed due to the overhead of existing protocols. In this paper we propose a new protocol called &#034;Summary Cache&#034;; each proxy keeps a summary of the URLs of cached documents of each participating proxy and checks these summaries for potential hits before sending any queries. Two factors contribute to the low overhead: the summaries are updated only periodically, and the summary representations are economical -- as low as 8 bits per entry. Using trace-driven simulations and a prototype implementation, we show that compared to the existing Internet Cache Protocol (ICP), Summary Cache reduces the number of inter-cache messages by a factor of 25 to 60, reduces the bandwidth consumption by over 50%, and eliminates between 30 % to 95 % of the CPU overhead, while at the same time maintaining almost the same hit ratio as ICP. Hence Summary Cache enables cache sharing among a large number of proxies.  
534|Storage management and caching in PAST, a large-scale, persistent peer-to-peer storage utility|This paper presents and evaluates the storage management and caching in PAST, a large-scale peer-to-peer persistent storage utility. PAST is based on a self-organizing, Internetbased overlay network of storage nodes that cooperatively route file queries, store multiple replicas of files, and cache additional copies of popular files. In the PAST system, storage nodes and files are each assigned uniformly distributed identifiers, and replicas of a file are stored at nodes whose identifier matches most closely the file’s identifier. This statistical assignment of files to storage nodes approximately balances the number of files stored on each node. However, non-uniform storage node capacities and file sizes require more explicit storage load balancing to permit graceful behavior under high global storage utilization; likewise, non-uniform popularity of files requires caching to minimize fetch distance and to balance the query load. We present and evaluate PAST, with an emphasis on its storage management and caching system. Extensive tracedriven experiments show that the system minimizes fetch distance, that it balances the query load for popular files, and that it displays graceful degradation of performance as the global storage utilization increases beyond 95%.  
535|Consistent hashing and random trees: Distributed caching protocols for relieving hot spots on the World Wide Web|We describe a family of caching protocols for distrib-uted networks that can be used to decrease or eliminate the occurrence of hot spots in the network. Our protocols are particularly designed for use with very large networks such as the Internet, where delays caused by hot spots can be severe, and where it is not feasible for every server to have complete information about the current state of the entire network. The protocols are easy to implement using existing network protocols such as TCP/IP, and require very little overhead. The protocols work with local control, make efficient use of existing resources, and scale gracefully as the network grows. Our caching protocols are based on a special kind of hashing that we call consistent hashing. Roughly speaking, a consistent hash function is one which changes minimally as the range of the function changes. Through the development of good consistent hash functions, we are able to develop caching protocols which do not require users to have a current or even consistent view of the network. We believe that consistent hash functions may eventually prove to be useful in other applications such as distributed name servers and/or quorum systems.  
536|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
537|A Hierarchical Internet Object Cache| This paper discusses the design andperformance  of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We believe that the conventional wisdom, that the benefits of hierarchical file caching do not merit the costs, warrants reconsideration in the Internet environment.  The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that the cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape&#039;s Netsite and NCSA 1.4 servers.  Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems.  
538|The Free Haven Project: Distributed Anonymous Storage Service|We present a design for a system of anonymous storage which resists the attempts of powerful adversaries to find or destroy any stored data. We enumerate distinct notions of anonymity for each party in the system, and suggest a way to classify anonymous systems based on the kinds of anonymity provided. Our design ensures the availability of each document for a publisher-specified lifetime. A reputation system provides server accountability by limiting the damage caused from misbehaving servers. We identify attacks and defenses against anonymous storage services, and close with a list of problems which are currently unsolved.
539|Separating key management from file system security|No secure network file system has ever grown to span the In-ternet. Existing systems all lack adequate key management for security at a global scale. Given the diversity of the In-ternet, any particular mechanism a file system employs to manage keys will fail to support many types of use. We propose separating key management from file system security, letting the world share a single global file system no matter how individuals manage keys. We present SFS, a se-cure file system that avoids internal key management. While other file systems need key management to map file names to encryption keys, SFS file names effectively contain public keys, making them self-certifying pathnames. Key manage-ment in SFS occurs outside of the file system, in whatever procedure users choose to generate file names. Self-certifying pathnames free SFS clients from any notion of administrative realm, making inter-realm file sharing triv-ial. They let users authenticate servers through a number of different techniques. The file namespace doubles as a key certification namespace, so that people can realize many key management schemes using only standard file utilities. Fi-nally, with self-certifying pathnames, people can bootstrap one key management mechanism using another. These prop-erties make SFS more versatile than any file system with built-in key management.  
540|Publius: A robust, tamper-evident, censorship-resistant, web publishing system|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
542|A Toolkit for User-Level File Systems|This paper describes a C toolkit for easily extending the Unix file system. The toolkit exposes the NFS interface, allowing new file systems to be implemented portably at user level. A number of programs have implemented portable, user-level file systems. However, they have been plagued by low-performance, deadlock, restrictions on file system structure, and the need to reboot after software errors. The toolkit makes it easy to avoid the vast majority of these problems. Moreover, the toolkit also supports user-level access to existing file systems through the NFS interface---a heretofore rarely employed technique. NFS gives software an asynchronous, low-level interface to the file system that can greatly benefit the performance, security, and scalability of certain applications. The toolkit uses a new asynchronous I/O library that makes it tractable to build large, event-driven programs that never block.
543|A Taste of Crispy Squid|Distributed proxy caches are in use throughout the world to reduce access latency and bandwidth demands for Internet object transfer. The CRISP project seeks to build more effective distributed Web caches by exploring alternatives to the hierarchical structure and multicast handling of probes common to the most popular distributed Web cache systems. CRISP caches are structured as a collective of autonomous Web proxy servers sharing their cache directories through a common mapping service that can be queried with at most one message exchange. Individual servers may be configured to replicate all or part of the global map in order to balance access cost, overhead and hit ratio, depending on the size and geographic dispersion of the collective cache. We have prototyped several CRISP cache structures in Crispy Squid, an extension to the Squid Internet Object Cache. We are evaluating these cache structures using Proxycizer, a full-featured package for replaying traces of observed request tr...
545|A Review of Current Routing Protocols for Ad-Hoc Mobile Wireless Networks   |An ad-hoc mobile network is a collection of mobile nodes that are dynamically and arbitrarily located in such a manner that the interconnections between nodes are capable of changing on a continual basis. In order to facilitate communication within the network, a routing protocol  is used to discover routes between nodes. The primary goal of such an ad-hoc network routing protocol is correct and efficient route establishment between a pair of nodes so that messages may  be delivered in a timely manner. Route construction should be done with a minimum of overhead  and bandwidth consumption. This paper examines routing protocols for ad-hoc networks and  evaluates these protocols based on a given set of parameters. The paper provides an overview of  eight different protocols by presenting their characteristics and functionality, and then provides a comparison and discussion of their respective merits and drawbacks.
546|Ad-hoc On-Demand Distance Vector Routing|An ad-hoc network is the cooperative engagement of a collection of mobile nodes without the required intervention of any centralized access point or existing infrastructure. In this paper we present Ad-hoc On Demand Distance Vector Routing (AODV), a novel algorithm for the operation of such ad-hoc networks. Each Mobile Host operates as a specialized router, and routes are obtained as needed (i.e., on-demand) with little or no reliance on periodic advertisements. Our new routing algorithm is quite suitable for a dynamic selfstarting network, as required by users wishing to utilize ad-hoc networks. AODV provides loop-free routes even while repairing broken links. Because the protocol does not require global periodic routing advertisements, the demand on the overall bandwidth available to the mobile nodes is substantially less than in those protocols that do necessitate such advertisements. Nevertheless we can still maintain most of the advantages of basic distance-vector routing mechanisms. We show that our algorithm scales to large populations of mobile nodes wishing to form ad-hoc networks. We also include an evaluation methodology and simulation results to verify the operation of our algorithm.
547|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
548|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
549|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
550|Cognitive networks|Abstract — This paper presents a definition and framework for a novel type of adaptive data network: the cognitive network. In a cognitive network, the collection of elements that make up the network observes network conditions and then, using prior knowledge gained from previous interactions with the network, plans, decides and acts on this information. Cognitive networks are different from other “intelligent ” communication technologies because these actions are taken with respect to the end-to-end goals of a data flow. In addition to the cognitive aspects of the network, a specification language is needed to translate the user’s end-to-end goals into a form understandable by the cognitive process. The cognitive network also depends on a Software Adaptable Network that has both an external interface accessible to the cognitive network and network status sensors. These devices are used to provide control and feedback. The paper concludes by presenting a simple case study to illustrate a cognitive network and its framework. I.
551|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
552|Power-Aware Routing in Mobile Ad Hoc Networks|In this paper we present a case for using new power-aware metrics for determining routes in wireless  ad hoc networks. We present five different metrics based on battery power consumption at nodes. We  show that using these metrics in a shortest-cost routing algorithm reduces the cost/packet of routing  packets by 5-30% over shortest-hop routing (this cost reduction is on top of a 40-70% reduction in energy  consumption obtained by using PAMAS, our MAC layer protocol). Furthermore, using these new metrics  ensures that the mean time to node failure is increased significantly. An interesting property of using  shortest-cost routing is that packet delays do not increase. Finally, we note that our new metrics can  be used in most traditional routing protocols for ad hoc networks.  1 
553|An Efficient Routing Protocol for Wireless Networks|We present
554|Routing In Clustered Multihop, Mobile Wireless Networks With Fading Channel|A clusterhead-token infrastructure for multihop, mobile wireless networks has been designed. Traditional routing algorithms in wireline networks are not feasible for mobile wireless environment due to the dynamic change in link connectivity. To gain better performance for clustered multihop, mobile wireless networks, routing must take into account radio channel access, code scheduling, and channel reservation. In this paper, we propose some heuristic routing schemes for clustered multihop, mobile wireless networks. A packet delay improvement up to fourfold has been observed in our simulations compared with shortest-pathscheme, making multimedia tra c viable. A radio channel model has been included to investigate the impact of channel fading on our protocols. To reduce the run time, a parallel simulator has been designed. Speedups of up to tenfold have been observed on a 16 processor SP/2.   
555|Signal Stability based Adaptive Routing (SSA) for Ad-Hoc Mobile Networks  (1997) |Unlike static networks, ad-hoc networks have no spatial hierarchy and suffer from frequent link failures which prevent mobile hosts from using traditional routing schemes. Under these conditions, mobile hosts must find routes to destinations without the use of designated routers and also must dynamically adapt the routes to the current link conditions. This paper proposes a distributed adaptive routing protocol for finding and maintaining stable routes based on signal strength and location stability in an ad-hoc network and presents an architecture for its implementation. 1 Introduction  Mobility is becoming increasingly important for users of computing systems. Technology has made possible wireless devices and smaller, less expensive, and more powerful computers. As a result users gain flexibility and the ability to maintain connectivity to their primary computer while roaming through a large area. The number of users with portable laptops and personal communications devices is increa...
556|Asynchronous Multimedia Multihop Wireless Networks+|Personal communications and mobile computing will require a wireless network infrastructure which is fast deployable, possibly multihop, and capable of multimedia service support. The first infrastructure of this type was the Packet Radio Network (PRNET), developed in the 70&#039;s to address the battlefield and disaster recovery communication requirements. PRNET was totally asynchronous and was based on a completely distributed architecture. It handled datagram traffic reasonably well, but did not offer efficient multimedia support. Recently, under the WAMIS and Glomo ARPA programs several mobile, multimedia, multihop (M  3  ) wireless network architectures have been developed, which assume some form of synchronous, time division infrastructure. The synchronous time frame leads to efficient multimedia support implementations. However, it introduces more complexity and is less robust in the face of mobility and channel fading. In this paper, we examine the impact of synchronization on wirel...
557|Adaptive Shared Tree Multicast in Mobile Wireless Networks|Shared Tree multicast is a well established concept used in several multicast protocols for wireline networks (eg. Core Base Tree, PIM sparse mode etc). In this paper, we extend the Shared Tree concept to wireless, mobile, multihop networks for applications ranging from ad hoc networking to disaster recovery and battlefield. The main challenge in wireless, mobile networks is the rapidly changing environment. We address this issue in our design by: (a) using &#034;soft state&#034;; (b) assigning different roles to nodes depending on their mobility (two level mobility model); (c) proposing an adaptive scheme which combines shared tree and source tree benefits. A detailed wireless simulation model is used to evaluate the proposed schemes and compare them with source based tree (as opposed to shared tree) multicast. Both uniform and 2-level mobility models are used in the comparison. The results show that shared tree protocols have low overhead and are very robust to mobility. In particular, the Ada...
558|Tree Multicast Strategies in Mobile, Multihop Wireless Networks|this paper, we extend the tree multicast concept to wireless, mobile, multihop networks for applications ranging from ad hoc networking to disaster recovery and battlefield. The main challenge in wireless, mobile networks is the rapidly changing environment. We address this issue in our design by: (a) using &#034;soft state&#034;; (b) assigning different roles to nodes depending on their mobility (2-level mobility model); (c) proposing an adaptive scheme which combines shared tree and per-source tree benefits, and; (d) dynamically relocating the shared tree Rendezvous Point (RP ). A detailed wireless simulation model is used to evaluate various multicast schemes. The results show that per-source trees perform better in heavy loads because of the more efficient traffic distribution; while shared trees are more robust to mobility and are more scalable to large network sizes. The adaptive tree multicast scheme, a hybrid between shared tree and per-source tree, combines the advantages of both and performs consistently well across all load and mobility scenarios. The main contributions of this study are: the use of a 2-level mobility model to improve the stability of the shared tree; the development of a hybrid, adaptive per-source and shared tree scheme, and; the dynamic relocation of the RP in the shared tree.
559|Loop-Free Internet Routing Using Hierarchical Routing Trees|We present a new hierarchical routing algorithm that combines the loop-free path-finding algorithm (LPA) with the area-based hierarchical routing scheme first proposed by McQuillan for distance-vector algorithms. The new algorithm, which we call the Hierarchical Information Path-based Routing (HIPR) algorithm, accommodates an arbitrary number of aggregation levels and can be viewed as a distributed version of Dijkstra&#039;s algorithm running over a hierarchical graph. HIPR is verified to be loop-free and correct. Simulations are used to show that HIPR is much more efficient than OSPF in terms of speed, communication and processing overhead required to converge to correct routing tables. HIPR constitutes the basis for future Internet routing protocols that are as simple as RIPv2, but with no looping and better performance than protocols based on link-states.  1. Introduction  Routing information maintained at each router has to be updated frequently to adapt to network dynamics. In a flat r...
560|A Fast and Elitist Multi-Objective Genetic Algorithm: NSGA-II|Multi-objective evolutionary algorithms which use non-dominated sorting and sharing have been mainly criticized for their (i) O(MN computational complexity (where M is the number of objectives and N is the population size), (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In this paper, we suggest a non-dominated sorting based multi-objective evolutionary algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which alleviates all the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN ) computational complexity is presented. Second, a selection operator is presented which creates a mating pool by combining the parent and child populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on a number of difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to PAES and SPEA - two other elitist multi-objective EAs which pay special attention towards creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems eciently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint non-linear problem, are compared with another constrained multi-objective optimizer and much better performance of NSGA-II is observed. Because of NSGA-II&#039;s low computational requirements, elitist approach, parameter-less niching approach, and simple constraint-handling strategy, NSGA-II should find increasing applications in the coming years.
561|Genetic Algorithms for Multiobjective Optimization: Formulation, Discussion and Generalization|The paper describes a rank-based fitness assignment method for Multiple Objective Genetic Algorithms (MOGAs). Conventional niche formation methods are extended to this class of multimodal problems and theory for setting the niche size is presented. The fitness assignment method is then modified to allow direct intervention of an external decision maker (DM). Finally, the MOGA is generalised further: the genetic algorithm is seen as the optimizing element of a multiobjective optimization loop, which also comprises the DM. It is the interaction between the two that leads to the determination of a satisfactory solution to the problem. Illustrative results of how the DM can interact with the genetic algorithm are presented. They also show the ability of the MOGA to uniformly sample regions of the trade-off surface.
562|Comparison of Multiobjective Evolutionary Algorithms: Empirical Results|In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.
563|Evolutionary Algorithms for Multiobjective Optimization|Multiple, often conflicting objectives arise naturally in most real-world optimization scenarios. As evolutionary algorithms possess several characteristics due to which they are well suited to this type of problem, evolution-based methods have been used for multiobjective optimization for more than a decade. Meanwhile evolutionary multiobjective optimization has become established as a separate subdiscipline combining the fields of evolutionary computation and classical multiple criteria decision making. In this paper, the basic principles of evolutionary multiobjective optimization are discussed from an algorithm design perspective. The focus is on the major issues such as fitness assignment, diversity preservation, and elitism in general rather than on particular algorithms. Different techniques to implement these strongly related concepts will be discussed, and further important aspects such as constraint handling and preference articulation are treated as well. Finally, two applications will presented and some recent trends in the field will be outlined.  
564|Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art|Solving optimization problems with multiple (often conflicting) objectives is, generally, a  very difficult goal. Evolutionary algorithms (EAs) were initially extended and applied during  the mid-eighties in an attempt to stochastically solve problems of this generic class. During  the past decade, a variety of multiobjective EA (MOEA) techniques have been proposed  and applied to many scientific and engineering applications. Our discussion&#039;s intent is  to rigorously define multiobjective optimization problems and certain related concepts,  present an MOEA classification scheme, and evaluate the variety of contemporary MOEAs.  Current MOEA theoretical developments are evaluated; specific topics addressed include  fitness functions, Pareto ranking, niching, fitness sharing, mating restriction, and secondary  populations. Since the development and application of MOEAs is a dynamic and rapidly  growing activity, we focus on key analytical insights based upon critical MOEA evaluation  of c...
565|A Niched Pareto Genetic Algorithm for Multiobjective Optimization|Many, if not most, optimization problems have multiple objectives. Historically, multiple objectives have been combined ad hoc to form a scalar objective function, usually through a linear combination (weighted sum) of the multiple attributes, or by turning objectives into constraints. The genetic algorithm (GA), however, is readily modified to deal with multiple objectives by incorporating the concept of Pareto domination in its selection operator, and applying a niching pressure to spread its population out along the Pareto optimal tradeoff surface. We introduce the Niched Pareto GA as an algorithm for finding the Pareto optimal set. We demonstrate its ability to find and maintain a diverse &#034;Pareto optimal population&#034; on two artificial problems and an open problem in hydrosystems.
566|Multiobjective Optimization and Multiple Constraint Handling with Evolutionary Algorithms-Part I: A Unified Formulation|In optimization, multiple objectives and constraints cannot be handled independently of the underlying optimizer. Requirements such as continuity and differentiability of the cost surface add yet another conflicting element to the decision process. While ``better&#039;&#039; solutions should be rated higher than ``worse&#039;&#039; ones, the resulting cost landscape must also comply with such requirements. Evolutionary algorithms (EAs), which have found application in many areas not amenable to optimization by other methods, possess many characteristics desirable in a multiobjective optimizer, most notably the concerted handling of multiple candidate solutions. However, EAs are essentially unconstrained search techniques which require the assignment of a scalar measure of quality, or fitness, to such candidate solutions. After reviewing current evolutionary approaches to multiobjective and constrained optimization, the paper proposes that fitness assignment be interpreted as, or at least related to, a multicriterion decision process. A suitable decision making framework based on goals and priorities is subsequently formulated in terms of a relational operator, characterized, and shown to encompass a number of simpler decision strategies. Finally, the ranking of an arbitrary number of candidates is considered. The effect of preference changes on the cost surface seen by an EA is illustrated graphically for a simple problem. The paper concludes with the formulation of a multiobjective genetic algorithm based on the proposed decision strategy. Niche formation techniques are used to promote diversity among preferable candidates, and progressive articulation of preferences is shown to be possible as long as the genetic algorithm can recover from abrupt changes in the cost landscape.
567|An Efficient Constraint Handling Method for Genetic Algorithms|Many real-world search and optimization problems involve inequality and/or equality constraints and are thus posed as constrained optimization problems. In trying to solve constrained optimization problems using genetic algorithms (GAs) or classical optimization methods, penalty function methods have been the most popular approach, because of their simplicity and ease of implementation. However, since the penalty function approach is generic and applicable to any type of constraint (linear or nonlinear), their performance is not always satisfactory. Thus, researchers have developed sophisticated penalty functions specific to the problem at hand and the search algorithm used for optimization. However, the most difficult aspect of the penalty function approach is to find appropriate penalty parameters needed to guide the search towards the constrained optimum. In this paper, GA&#039;s population-based approach and ability to make pair-wise comparison in tournament selection operator are explo...
568|Multiobjective Optimization Using Evolutionary Algorithms - A Comparative Case Study|. Since 1985 various evolutionary approaches to multiobjective optimization have been developed, capable of searching for multiple solutions concurrently in a single run. But the few comparative studies of different methods available to date are mostly qualitative and restricted to two approaches. In this paper an extensive, quantitative comparison is presented, applying four multiobjective evolutionary algorithms to an extended 0/1 knapsack problem.  1 Introduction  Many real-world problems involve simultaneous optimization of several incommensurable and often competing objectives. Usually, there is no single optimal solution, but rather a set of alternative solutions. These solutions are optimal in the wider sense that no other solutions in the search space are superior to them when all objectives are considered. They are known as Pareto-optimal solutions. Mathematically, the concept of Pareto-optimality can be defined as follows: Let us consider, without loss of generality, a multio...
569|Simulated Binary Crossover for Continuous Search Space|The success of binary-coded genetic algorithms (GAs) in problems having discrete search space largely depends on the coding used to represent the problem variables and on the crossover operator that propagates building-blocks from parent strings to children strings. In solving optimization problems having continuous search space, binary-coded GAs discretize the search space by using a coding of the problem variables in binary strings. However, the coding of real-valued variables in finite-length strings causes a number of difficulties---inability to achieve arbitrary precision in the obtained solution, fixed mapping of problem variables, inherent Hamming cliff problem associated with the binary coding, and processing of Holland&#039;s schemata in continuous search space. Although, a number of real-coded GAs are developed to solve optimization problems having a continuous search space, the search powers of these crossover operators are not adequate. In this paper, the search power of a cross...
570|Multi-Objective Genetic Algorithms: Problem Difficulties and Construction of Test Problems|In this paper, we study the problem features that may cause a multi-objective genetic  algorithm (GA) difficulty in converging to the true Pareto-optimal front. Identification  of such features helps us develop difficult test problems for multi-objective optimization.  Multi-objective test problems are constructed from single-objective optimization  problems, thereby allowing known difficult features of single-objective problems (such as  multi-modality, isolation, or deception) to be directly transferred to the corresponding  multi-objective problem. In addition, test problems having features specific to multiobjective  optimization are also constructed. More importantly, these difficult test problems  will enable researchers to test their algorithms for specific aspects of multi-objective  optimization.  Keywords  Genetic algorithms, multi-objective optimization, niching, pareto-optimality, problem difficulties,  test problems.  1 Introduction  After a decade since the pioneering wor...
571|Evolution Strategies for Vector Optimization|Evolution strategies --- a stochastic optimization method originally designed for single criterion problems --- have been modified in such a way that they can also tackle multiple criteria problems. Instead of computing only one efficient solution interactively, a decision maker can collect as many members of the Pareto set as needed before making up his mind. Apart from this feature one could also reflect upon the algorithm as a simple model of biological evolution. Following this idea one might emphasize the algorithm&#039;s capability of self--adapting its parameters. Furthermore, the effect of polyploid individuals corresponds in both `worlds&#039;. 1 Introduction  It has become increasingly obvious that the optimization under a single scalar--valued criterion --- often a monetary one --- fails to reflect the variety of aspects in a world getting more and more complex. Although V. Pareto [4] laid the mathematical foundations already about a hundred years ago the existing tools for multiple c...
572|On the Performance Assessment and Comparison of Stochastic Multiobjective Optimizers|Abstract. This work proposes a quantitative, non-parametric interpretation of statistical performance of stochastic multiobjective optimizers, including, but not limited to, genetic algorithms. It is shown that, according to this interpretation, typical performance can be defined in terms analogous to the notion of median for ordinal data, as can other measures analogous to other quantiles. Non-parametric statistical test procedures are then shown to be useful in deciding the relative performance of different multiobjective optimizers on a given problem. Illustrative experimental results are provided to support the discussion. 1
573|Understanding interactions among genetic algorithm parameters|Genetic algorithms (GAs) are multi-dimensional and stochastic search methods, involving complex interactions among their parameters. For last two decades, researchers have been trying to understand the mechanics of GA parameter interactions by using various techniques|careful `functional &#039; decomposition of parameter interactions, empirical studies, and Markov chain analysis. Although the complexities in these interactions are getting clearer with such analyses, it still remains an open question in the mind of a new-comer to the eld or to a GA-practitioner as to what values of GA parameters (such as population size, choice of GA operators, operator probabilities, and others) to use in an arbitrary problem. In this paper, we investigate the performance of simple tripartite GAs on a number of simple to complex test problems from a practical standpoint. Since in a real-world situation, the overall time to run a GA is more or less dominated by the time consumed by objective function evaluations, we compare di erent GAs for a xed number of function evaluations. Based on probability calculations and simulation results, it is observed that for solving simple problems (unimodal or small modality problems) the mutation operator plays an important role, although GAs with the crossover operator alone can also solve these problems. However, the two operators (when applied alone) have two di erent working zones for the population size. For complex problems involving massive multi-modality and misleadingness (deception), the crossover operator is the key search operator. Based on these studies, it is recommended that when in doubt, the use of the crossover operator with an adequate population size is a reliable approach.
574|Evolutionary Search under Partially Ordered Fitness Sets|The search for minimal elements in partially ordered sets is a generalization of the task of finding Pareto-optimal elements in multi-criteria optimization problems. Since there are usually many minimal elements within a partially ordered set, a population-based evolutionary search is, as a matter of principle, capable of finding several minimal elements in a single run and gains therefore a steadily increase of popularity. Here, we present an evolutionary algorithm which population converges with probability one to the set of minimal elements within a finite number of iterations.   
575|Protocols for self-organization of a wireless sensor network|We present a suite of algorithms for self-organization of wireless sensor networks, in which there is a scalably large number of mainly static nodes with highly constrained energy resources. The protocols further support slow mobility by a subset of the nodes, energy-efficient routing, and formation of ad hoc subnetworks for carrying out cooperative signal processing functions among a set of the nodes.
576|Max-Min D-Cluster Formation in Wireless Ad Hoc Networks|An ad hoc network may be logically represented as a set of clusters. The clusterheads form a d-hop dominating set. Each node is at most d hops from a clusterhead. Clusterheads form a virtual backbone and may be used to route packets for nodes in their cluster. Previous heuristics restricted themselves to 1-hop clusters. We show that the minimum d-hop dominating set problem is NP-complete. Then we present a heuristic to form d-clusters in a wireless ad hoc network. Nodes are assumed to have non-deterministic mobility pattern. Clusters are formed by diffusing node identities along the wireless links. When the heuristic terminates, a node either becomes a clusterhead, or is at most d wireless hops away from its clusterhead. The value of d is a parameter of the heuristic. The heuristic can be run either at regular intervals, or whenever the network configuration changes. One of the features of the heuristic is that it tends to re-elect existing clusterheads even when the network configurat...
577|Scalable routing strategies for ad hoc wireless networks| In this paper, we consider a large population of mobile stations that are interconnected by a multihop wireless network. The applications of this wireless infrastructure range from ad hoc networking (e.g., collaborative, distributed computing) to disaster recovery (e.g., fire, flood, earthquake), law enforcement (e.g., crowd control, search-and-rescue), and military (automated battlefield). Key characteristics of this system are the large number of users, their mobility, and the need to operate without the support of a fixed (wired or wireless) infrastructure. The last feature sets this system apart from existing cellular systems and in fact makes its design much more challenging. In this environment, we investigate routing strategies that scale well to large populations and can handle mobility. In addition, we address the need to support multimedia communications, with low latency requirements for interactive traffic and quality-of-service (QoS) support for real-time streams (voice/video). In the wireless routing area, several schemes have already been proposed and implemented (e.g., hierarchical routing, on-demand routing, etc.). We introduce two new schemes—fisheye state routing (FSR) and hierarchical state routing (HSR)—which offer some competitive advantages over the existing schemes. We compare the performance of existing and proposed schemes via simulation. 
578|Performance of a novel self-organization protocol for wireless ad hoc sensor networks|Abstract- The paper presents an ad-hoc architecture for wireless sensor networks and other wireless sys-tems similar to them. In this class of wireless system the physical resource at premium is energy. Band-width available to the system is in excess of system requirements. The approach to solve the problem of ad-hoc network formation here is to use available bandwidth in order to save energy. The method introduced solves the problem of connecting an ad-hoc network. This algorithm gives procedures for the joint formation of a time schedule (similar to a TDMA schedule) and activation of links therein for random network topologies.This self-organization method is energy-sensitive, distributed, scalable, and able to form a connected network rapidly. I.
579|Multicluster, Mobile, Multimedia Radio Network|A multi-cluster, multi-hop packet radio network architecture for wireless adaptive mobile information systems is presented...
580|Soft Handoffs in CDMA Mobile Systems|This article presents an overview of soft handoff, an idea which is becoming quite important because of its use in the IS-95 code-division multiple access (CDMA) cellular phone standard. The benefits and disadvantages of using soft handoff over hard handoff are discussed, with most results  drawn from the available literature. The two most well-known benefits are fade margin improvement and higher uplink capacity, while disadvantages  include increased downlink interference and more complex implementation. Handoff parameter optimization is extremely important, so  various studies on the trade-offs to be considered when selecting these parameters are surveyed, from both the link quality and resource  allocation perspectives. Finally, research directions and future trends are discussed.
581|Mobility Management in Hierarchical Multihop Mobile Wireless Networks |In this paper, we consider the mobility management in large, hierarchically organized multihop wireless networks. The examples of such networks range from battlefield networks, emergency disaster relief and law enforcement etc. We present a novel network addressing architecture to accommodate mobility using a “Home Agent ” concept akin to mobile IP. The performance of the mobility management scheme is investigated through simulations. I.
582|A Distributed Location System for the Active Office|Computer and commmunications systems continue to proliferate... This article describes the technology of a system for locating people and equipment, and the design of a distributed system service supporting access to that information. The application interfaces which are made possible by, or benefit from this facility are presented.
583|A Demonstrated Optical Tracker With Scalable Work Area for Head-Mounted Display Systems|An optoelectronic head-tracking system for head-mounted displays is described. The system features a scalable work area that currently measures 10&#039; x 12&#039;, a measurement update rate of 20-100 Hz with 20-60 ms of delay, and a resolution specification of 2 mm and 0.2 degrees. The sensors consist of four head-mounted imaging devices that view infrared lightemitting diodes (LEDs) mounted in a 10&#039; x 12&#039; grid of modular 2&#039; x 2&#039; suspended ceiling panels. Photogrammetric techniques allow the head&#039;s location to be expressed as a function of the known LED positions and their projected images on the sensors. The work area is scaled by simply adding panels to the ceiling&#039;s grid. Discontinuities that occurred when changing working sets of LEDs were reduced by carefully managing all error sources, including LED placement tolerances, and by adopting an overdetermined mathematical model for the computation of head position: space resecfion by collinearity. The working system was demonstrated in the Tomorrow&#039;s Realities gallery at the ACM SIGGRAPH &#039;91 conference.
584|Matrix: A Realtime Object Identification and  Registration Method for Augmented Reality|This paper introduces a new technique for producing augmented reality systems that simultaneously identify real world objects and estimate their coordinate systems. This method utilizes a 2D matrix marker, a square shaped barcode, which can identify a large number of objects. It also acts as a landmark to register information on real world images. As a result, it costs virtually nothing to produce and attach codes on various kinds of real world objects, because the matrix code are printable. We have developed an augmented reality system based on this method, and demonstrated several potential applications.
585|Teleporting in an X Window System Environment|Teleporting is the ability to redirect a windowing environment to different computer displays. This paper describes the implementation of a teleporting system developed at Olivetti Research Laboratory (ORL). We outline two particular features of the system that make it powerful. First, it operates with existing applications, which will run without any modification. Second, it incorporates sophisticated techniques of personnel and equipment location which make it simple to use. Teleporting may represent a development in attempts to achieve a ubiquitous, personalised computing environment for all. 1 Introduction  In the near future, communication networks will make it possible to access computing services from almost anywhere in the world. In addition, the number of computers readily available within an office is increasing. We would like to allow individuals to make use of such networked computing facilities as they move from place to place, whilst retaining the familiarity of their own...
586|Using collaborative filtering to weave an information tapestry|predicated on the belief that information filtering can be more effective when humans are involved in the filtering process. Tapestry was designed to support both content-based filtering and collaborative filtering, which entails people collaborating to help each other perform filtering by recording their reactions to documents they read. The reactions are called annotations; they can be accessed by other people’s filters. Tapestry is intended to handle any incoming stream of electronic documents and serves both as a mail filter and repository; its components are the indexer, document store, annotation store, filterer, little box, remailer, appraiser and reader/browser. Tapestry’s client/server architecture, its various components, and the Tapestry query language are described.
587|Browsing electronic mail: Experiences interfacing a mail system to a DBMS|Abstract: A database management system provides the ideal support for electronic mail applications. The Walnut mail system built at the Xerox Palo Alto Research Center was recently redesigned to take better advantage of its underlying database facilities. The ability to pose ad-hoc queries with a “fill-in-the-form” browser allows people to browse their mail quickly and effectively, while database access paths guarantee fast retrieval of stored information. Careful consideration of the systems ’ usage was reflected in both the database schema representation and the user-interface for browsing mail.
588|Detecting faces in images: A survey| Images containing faces are essential to intelligent vision-based human computer interaction, and research efforts in face processing include face recognition, face tracking, pose estimation, and expression recognition. However, many reported methods assume that the faces in an image or an image sequence have been identified and localized. To build fully automated systems that analyze the information contained in face images, robust and efficient face detection algorithms are required. Given a single image, the goal of face detection is to identify all image regions which contain a face regardless of its three-dimensional position, orientation, and the lighting conditions. Such a problem is challenging because faces are nonrigid and have a high degree of variability in size, shape, color, and texture. Numerous techniques have been developed to detect faces in a single image, and the purpose of this paper is to categorize and evaluate these algorithms. We also discuss relevant issues such as data collection, evaluation metrics, and benchmarking. After analyzing these algorithms and identifying their limitations, we conclude with several promising directions for future research.  
589|A computational approach to edge detection|Abstract-This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to- a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge. This detection scheme uses several elongated operators at each point, and the directional operator outputs are integrated with the gradient maximum detector. Index Terms-Edge detection, feature extraction, image processing, machine vision, multiscale image analysis. I.
590|Snakes: Active contour models|A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the cap-ture region surrounding a feature. Snakes provide a unified account of a number of visual problems, in-cluding detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.
591|Eigenfaces vs. Fisherfaces: Recognition Using Class Specific Linear Projection|We develop a face recognition algorithm which is insensitive to gross variation in lighting direction and facial expression. Taking a pattern classification approach, we consider each pixel in an image as a coordinate in a high-dimensional space. We take advantage of the observation that the images of a particular face, under varying  illumination but fixed pose, lie in a 3-D linear subspace of the high dimensional image space -- if the face is a Lambertian surface without shadowing. However, since faces are not truly Lambertian surfaces and do indeed produce self-shadowing,  images will deviate from this linear subspace. Rather than explicitly modeling  this deviation, we linearly project the image into a subspace in a manner which  discounts those regions of the face with large deviation. Our projection method is  based on Fisher&#039;s Linear Discriminant and produces well separated classes in a low-dimensional  subspace even under severe variation in lighting and facial expressions. The Eigenface
592|The cascade-correlation learning architecture|Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology, Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network.
593|Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm|learning Boolean functions, linear-threshold algorithms Abstract. Valiant (1984) and others have studied the problem of learning various classes of Boolean functions from examples. Here we discuss incremental learning of these functions. We consider a setting in which the learner responds to each example according to a current hypothesis. Then the learner updates the hypothesis, if necessary, based on the correct classification of the example. One natural measure of the quality of learning in this setting is the number of mistakes the learner makes. For suitable classes of functions, learning algorithms are available that make a bounded number of mistakes, with the bound independent of the number of examples seen by the learner. We present one such algorithm that learns disjunctive Boolean functions, along with variants for learning other classes of Boolean functions. The basic method can be expressed as a linear-threshold algorithm. A primary advantage of this algorithm is that the number of mistakes grows only logarithmically with the number of irrelevant attributes in the examples. At the same time, the algorithm is computationally efficient in both time and space. 1.
594|Probabilistic Visual Learning for Object Representation|We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and non-rigid objects such as hands.
596|PCA versus LDA|In the context of the appearance-based paradigm for object recognition, it is generally believed that algorithms based on LDA (Linear Discriminant Analysis) are superior to those based on PCA (Principal Components Analysis) . In this communication we show that this is not always the case. We present our case first by using intuitively plausible arguments and then by showing actual results on a face database. Our overall conclusion is that when the training dataset is small, PCA can outperform LDA, and also that PCA is less sensitive to different training datasets.  Keywords: face recognition, pattern recognition, principal components analysis, linear discriminant analysis, learning from undersampled distributions, small training datasets. 
597|Classifying Facial Actions|AbstractÐThe Facial Action Coding System (FACS) [23] is an objective method for quantifying facial movement in terms of component actions. This system is widely used in behavioral investigations of emotion, cognitive processes, and social interaction. The coding is presently performed by highly trained human experts. This paper explores and compares techniques for automatically recognizing facial actions in sequences of images. These techniques include analysis of facial motion through estimation of optical flow; holistic spatial analysis, such as principal component analysis, independent component analysis, local feature analysis, and linear discriminant analysis; and methods based on the outputs of local filters, such as Gabor wavelet representations and local principal components. Performance of these systems is compared to naive and expert human subjects. Best performances were obtained using the Gabor wavelet representation and the independent component representation, both of which achieved 96 percent accuracy for classifying 12 facial actions of the upper and lower face. The results provide converging evidence for the importance of using local filters, high spatial frequencies, and statistical independence for classifying facial actions.
598|The EM Algorithm for Mixtures of Factor Analyzers|Factor analysis, a statistical method for modeling the covariance structure of high dimensional data using a small number of latent variables, can be extended by allowing different local factor models in different regions of the input space. This results in a model which concurrently performs clustering and dimensionality reduction, and can be thought of as a reduced dimension mixture of Gaussians. We present an exact Expectation--Maximization algorithm for fitting the parameters of this mixture of factor analyzers. 1 Introduction  Clustering and dimensionality reduction have long been considered two of the fundamental problems in unsupervised learning (Duda &amp; Hart, 1973; Chapter 6). In clustering, the goal is to group data points by similarity between their features. Conversely, in dimensionality reduction, the goal is to group (or compress) features that are highly correlated. In this paper we present an EM learning algorithm for a method which combines one of the basic forms of dime...
599|Shape manifolds, Procrustean metrics, and complex projective spaces|2. Shape-spaces and shape-manifolds 82 3. Procrustes analysis, and the invariant (quotient) metric on I j.... 87 4. Shape-measures and shape-densities 93 5. The manifold carrying the shapes of triangles 96
600|Modeling the manifolds of images of handwritten digits|description length, density estimation.
601|A Context-Dependent Attention System for a Social Robot|This paper presents part of an on-going project  to integrate perception, attention, drives, emotions,  behavior arbitration, and expressive acts  for a robot designed to interact socially with  humans. We present the design of a visual attention  system based on a model of human visual  search behavior from Wolfe (1994). The  attention system integrates perceptions (motion  detection, color saliency, and face popouts)  with habituation e#ects and influences  from the robot&#039;s motivational and behavioral  state to create a context-dependent attention  activation map. This activation map is used to  direct eye movements and to satiate the drives  of the motivational system.
602|A Robust Visual Method for Assessing the Relative Performance of Edge-Detection Algorithms| A new method for evaluating edge detection algorithms is presented and applied to measure the relative performance of algorithms by Canny, Nalwa-Binford, Iverson-Zucker, Bergholm, and Rothwell. The basic measure of performance is a visual rating score which indicates the perceived quality of the edges for identifying an object. The process of evaluating edge detection algorithms with this performance measure requires the collection of a set of gray-scale images, optimizing the input parameters for each algorithm, conducting visual evaluation experiments and applying statistical analysis methods. The novel aspect of this work is the use of a visual task and real images of complex scenes in evaluating edge detectors. The method is appealing because, by definition, the results agree with visual evaluations of the edge images.  
603|A Transform for Multiscale Image Segmentation by Integrated Edge and Region Detection| This paper describes a new transform to extract image regions at all geometric and photometric scales. It is argued that linear approaches such as convolution and matching have the fundamental shortcoming that they require a priori models of region shape. The proposed transform avoids this limitation by letting the structure emerge, bottom-up, from interactions among pixels, in analogy with statistical mechanics and particle physics. The transform involves global computations on pairs of pixels followed by vector integration of the results, rather than scalar and local linear processing. An attraction force field is computed over the image in which pixels belonging to the same region are mutually attracted and the region is characterized by a convergent flow. It is shown that the kansform possesses properties that allow multiscale segmentation, or extraction of original, unblurred structure at all different geometric and photometric scales present in the image. This is in contrast with much of the previous work wherein multiscale structure is viewed as the smoothed structure in a multiscale decimation of image signal. Scale is an integral parameter of the force (computation, and the number and values of scale parameters associated with the image can be estimated automatically. Regions are detected at all, a priori unknown, scales resulting in automatic construction of a segmentation tree, in which each pixel is annotated with descriptions of all the regions it belongs to. Although some of the analytical properties of the transform are presented for piecewise constant images, it is shown that the results hold for more general images, e.g., those containing noise and shading. Thus the proposed method is intended as a solution to the problem of multiscale, integraled edge and region detection, or low-level image segmentation. Experimental results with synthetic and real images are given to demonstrate the properties and segmentation performance of the transform.
604|Finding Faces in Cluttered Scenes Using Random Labeled Graph Matching|An algorithm for locating quasi-frontal views of human faces in cluttered scenes is presented. The algorithm works by coupling a set of local feature detectors with a statistical model of the mutual distances between facial features; it is invariant with respect to translation, rotation (in the plane), and scale and can handle partial occlusions of the face. On a challenging database with complicated and varied backgrounds, the algorithm achieved a correct localization rate of 95% in images where the face appeared quasi-frontally. 1 Introduction The problem of face recognition has received considerable attention from the computer vision community, and a number of techniques have been proposed in the literature [3, 11, 12, 13, 14, 16, 17, 19]. However, in most of these studies the face was in a benign environment from which it could easily be extracted, or it was assumed to have been pre-segmented. For any of these recognition algorithms to work in a general setting, we need a system...
605|Multi-Modal Tracking of Faces for Video Communications|This paper describes a system which uses multiple visual processes to detect and track faces for video compression and transmission. The system is based on an architecture in which a supervisor selects and activates visual processes in cyclic manner. Control of visual processes is made possible by a confidence factor which accompanies each observation. Fusion of results into a unified estimation for tracking is made possible by estimating a covariance matrix with each observation. Visual processes for face tracking are described using blink detection, normalised color histogram matching, and cross correlation (SSD and NCC). Ensembles of visual processes are organised into processing states so as to provide robust tracking. Transition between states is determined by events detected by processes. The result of face detection is fed into recursive estimator (Kalman filter). The output from the estimator drives a PD controller for a pan/tilt/zoom camera. The resulting system provides robust and precise tracking which operates continuously at approximately 20 images per second on a 150 megahertz computer work-station. 1.
606|Automatic face identification system using flexible appearance models|We describe the use of flexible models for representing the shape and grey-level appearance of human faces. These models are controlled by a small number of parameters which can be used to code the overall appearance of a face for image compression and classification purposes. The model parameters control both inter-class and within-class variation. Discriminant analysis techniques are employed to enhance the effect of those parameters affecting inter-class variation, which are useful for classification. We have performed experiments on face coding and reconstruction and automatic face identification. Good recognition rates are obtained even when significant variation in lighting, expression and 3D viewpoint, is allowed. 
607|Face Localization via Shape Statistics|In this paper, a face localization system is proposed in which local detectors are coupled with a statistical model of the spatial arrangement of facial features to yield robust performance. The outputs from the local detectors are treated as candidate locations and constellations are formed from these. The effects of translation, rotation, and scale are eliminated by mapping to a set of shape variables. The constellations are then ranked according to the likelihood that the shape variables correspond to a face versus an alternative model. Incomplete constellations, which occur when some of the true features are missed, are handled in a principled way.  1 Introduction  The problem of face recognition has received considerable attention in the literature [11, 24, 21, 4, 19, 17, 22, 10]; however, in most of these studies, the faces were either embedded in a benign background or were assumed to have been pre-segmented. For any of these recognition algorithms to work in realworld applicati...
608|Finding Face Features|We describe a computer program which understands a greyscale image of a face well  enough to locate individual face features such as eyes and mouth. The program has two  distinct components: modules designed to locate particular face features, usually in a  restricted area; and the overall control strategy which activates modules on the basis of  the current solution state, and assesses and integrates the results of each module.
609|Joint Induction of Shape Features and Tree Classifiers|We introduce a very large family of binary features for two-dimensional shapes. The salient ones for separating particular shapes are determined by inductive learning during the construction of classi cation trees. There is a feature for every possible geometric arrangement of local topographic codes. The arrangements express coarse constraints on relative angles and distances among the code locations and are nearly invariant to substantial a ne and non-linear deformations. They are also partially ordered, which makes it possible to narrow the search for informative ones at each node of the tree. Di erent trees correspond to di erent aspects of shape. They are statistically weakly dependent due to randomization and are aggregated in a simple way. Adapting the algorithm to a shape family is then fully automatic once training samples are provided. As an illustration, we classify handwritten digits from the NIST database ? the error rate is:7%.
610|Face Detection With Information-Based Maximum Discrimination|In this paper we present a visual learning technique that maximizes the discrimination between positive and negative examples in a training set. We demonstrate our technique in the context of face detection with complex background without color or motion information, which has proven to be a challenging problem. We use a family of discrete Markov processes to model the face and background patterns and estimate the probability models using the data statistics. Then, we convert the learning process into an optimization, selecting the Markov process that optimizes the information-based discrimination between the two classes. The detection process is carried out by computing the likelihood ratio using the probability model obtained from the learning procedure. We show that because of the discrete nature of these models, the detection process is, by almost two orders of magnitude, less computationally expensive than neural network approaches. However, no improvement in terms of correct-answ...
611|Multi-Modal System for Locating Heads and Faces|We designed a modular system using a combination of shape analysis, color segmentation  and motion information for locating reliably heads  and faces of different sizes and orientations in  complex images. The first of the system&#039;s three channels does a shape analysis on gray-level images to determine the location of individual facial features as well as the outlines of heads. In the second channel the color space is analyzed with a clustering algorithm to find areas of skin colors. The color space  is first calibrated, using the results from the other channels. In the third channel motion information is extracted from frame differences. Head outlines are determined by analyzing the shapes of areas with large motion vectors. All three channels produce lists of shapes, each marking an area of the image where a facial feature or apart of the outline of a head may be present. Combinations of such shapes are  evaluated with n-gram searches to produce a list of likely head positions and the locations of facial features. We tested the system for tracking faces of people sitting in front of terminals and video phones and used it to track people entering through a doorway.
612|Rule-Based Face Detection in Frontal Views|Face detection is a key problem in building automated systems that perform face recognition. A very attractive approach for face detection is based on multiresolution images (also known as mosaic images). Motivated by the simplicity of this approach, a rule-based face detection algorithm in frontal views is developed that extends the work of G. Yang and T.S. Huang. The proposed algorithm has been applied to frontal views extracted from the European ACTS M2VTS database that contains the videosequences of 37 different persons. It has been found that the algorithm provides a correct facial candidate in all cases. However, the success rate of the detected facial features (e.g. eyebrows/eyes, nostrils/nose, and mouth) that validate the choice of a facial candidate is found to be 86.5 % under the most strict evaluation conditions. 1. INTRODUCTION  Face recognition has been an active research topic in computer vision for more than two decades. A critical survey of the literature on human and ...
613|Modelling Facial Colour and Identity with Gaussian Mixtures|An integrated system for the acquisition, normalisation and recognition of moving faces in dynamic scenes is introduced. Four face recognition tasks are defined and it is argued that modelling person-specific probability densities in a generic face space using mixture models provides a technique applicable to all four tasks. The use of Gaussian colour mixtures for face detection and tracking is also described. Results are presented using data from the integrated system.  Key words: Face recognition, Biometrics, Gaussian mixtures, Colour models.  1 Introduction  Face recognition in general and the recognition of moving people in natural scenes in particular, require a set of visual tasks to be performed robustly. These include (1) Acquisition: the detection and tracking of face-like image patches in a dynamic scene, (2) Normalisation: the segmentation, alignment and normalisation of the face images, and (3) Recognition: the representation and modelling of face images as identities, and ...
614|Performance Assessment through Bootstrap|A new performance evaluation paradigm for computer vision systems is proposed. In real situation, the complexity of the  input data and/or of the computational procedure can make traditional error propagation methods infeasible. The new approach  exploits a resampling technique recently introduced in statistics, the bootstrap. Distributions for the output variables are obtained by  perturbing the nuisance properties of the input, i.e., properties with no relevance for the output under ideal conditions. From these  bootstrap distributions, the confidence in the adequacy of the assumptions embedded into the computational procedure for the given  input is derived. As an example, the new paradigm is applied to the task of edge detection. The performance of several edge  detection methods is compared both for synthetic data and real images. The confidence in the output can be used to obtain an  edgemap independent of the gradient magnitude.
615|Fast Face Detection via Morphology-based Pre-processing|An efficient face detection algorithm which can detect multiple faces in cluttered  environment is proposed. The proposed system consists of three main steps. In the  first step, a morphology-based technique is devised to perform eye-analogue segmentation.  Morphological operations are applied to locate eye-analogue pixels in the original  image. Then, a labeling process is executed to generate the eye-analogue segments. In  the second step, the previously located eye-analogue segments are used as guides to  search for potential face regions. The last step of the proposed system is to perform  face verification. In this step, every face candidate obtained from the previous step is  normalized to a standard size. Then, each of these normalized potential face images  is fed into a trained backpropagation neural network for identification. After all the  true faces are identified, their corresponding poses are located based on the guidance of  optimizing a cost function. The proposed face...
616|Probabilistic Affine Invariants for Recognition|Under a weak perspective camera model, the image plane coordinates in different views of a planar object are related by an affine transformation. Because of this property, researchers have attempted to use affine invariants for recognition. However, there are two problems with this approach: (1) objects or object classes with inherent variability cannot be adequately treated using invariants; and (2) in practice the calculated affine invariants can be quite sensitive to errors in the image plane measurements. In this paper we use probability distributions to address both of these difficulties. Under the assumption that the feature positions of a planar object can be modeled using a jointly Gaussian density, we have derived the joint density over the corresponding set of affine coordinates. Even when the assumptions of a planar object and a weak perspective camera model do not strictly hold, the results are useful because deviations from the ideal can be treated as deformability in the ...
617|Mixtures of Eigenfeatures for Real-Time Structure from Texture|We describe a face modeling system which estimates complete facial structure and texture from a real-time video stream. The system begins with a face tracking algorithm which detects and stabilizes live facial images into a canonical 3D pose. The resulting canonical texture is then processed by a statistical model to filter imperfections and estimate unknown components such as missing pixels and underlying 3D structure. This statistical model is a soft mixture of eigenfeature selectors which span the 3D deformations and texture changes across a training set of laser scanned faces. An iterative algorithm is introduced for determining the dimensional partitioning of the eigenfeatures to maximize their generalization capability over a cross-validation set of data. The model&#039;s abilities to filter and estimate absent facial components are then demonstrated over incomplete 3D data. This ultimately allows the model to span known and regress unknown facial information from stabilized natural video sequences generated by a face tracking algorithm. The resulting continuous and dynamic estimation of the model&#039;s parameters over a video sequence generates a compact temporal description of the 3D deformations and texture changes of the face.  
618|Scale Invariant Face Detection Method using Higher-Order Local Autocorrelation Features extracted from Log-Polar Image|This paper proposes a scale invariant face detection method which combines higher-order local autocorrelation (HLA C) features extracted from a log-polar transformed image with Linear Discriminant Analysis for &#034;face&#034; and &#034;not face&#034; classification. Since HLAC features of log-polar image are sensitive to shifts of a face, we utilize this property and develop a face detection method. HLA C features extracted from a log-polar image become scale and rotation invariant because scalings and rotations of a face are expressed as shifts in a log-polar image (coordinate). By combining these features with the Linear Discriminant Analysis which is extended to treat &#034;face&#034; and &#034;not face&#034; classes, a scale invariant face detection system can be realized.
619|Detection of Human Faces Using Decision Trees|This paper proposes a novel algorithm for face detection using decision trees (DT) and shows its generality and feasibility using a data base consisting of 2,340 face images from the FERET data base (corresponding to 817 subjects and including 190 sets of duplicates) over a semi uniform background. The approach used for face detection involves three main stages, those of location, cropping, and post processing. The first stage finds a rough approximation for the possible location of the face box, the second stage will refine it, and the last stage decides whether a face is present in the image and if the answer is positive would normalize the face image. The algorithm does not require multiple (scale) templates and the accuracy achieved is 96%. Accuracy is based on the visual observation that the face box includes both eyes, nose, and mouth, and that the top side of the box is below the hairline. Experiments were also performed to assess the accuracy of the algorithm in rejecting image...
620|Information Theoretic View-Based and Modular Face Detection|This paper describes information theoretic methods for the determination of the optimal subset of pixels for the problem of face detection in complex backgrounds. A view-based method is described, which has limitations due to misalignments. This motivates the modular feature based method which minimizes the misalignment problem. Empirical comparisons between the viewbased, modular, and sum of squared difference methods are made using four databases from three universities. 1. Introduction  The face detection problem may be described as follows: Given a test image (any scanned in photograph or frame from a video camera), find the locations and size of every human face within the image. The problem of face detection differs from the problem of face recognition in that face detection has exactly two classifications: face or nonface, whereas face recognition usually has a number of classifications equal to the number of individuals. Face detection is important to a wide variety of areas wh...
621|Generalized likelihood ratio-based face detection and extraction of mouth features|isy.liu.se davoine,haibo,robert¥ Abstract. In this paper we describe a system to reliably localize the position of the speaker’s face and mouth in videophone sequences. A statistical scheme based on a subspace method is presented for detecting human faces under varying poses. We propose a new matching criterion based on the Generalized Likelihood Ratio. The criterion is optimized efficiently with respect to similarity, affine or perspective transform parameters using a coarse-to-fine search strategy combined with a simulated annealing algorithm. Moreover we propose to extract a vector of geometrical features (four points) on the outline of the mouth. The extraction consists in analyzing amplitude projections in the regions of the mouth. All the computations are performed on H263-coded frames, with a QCIF spatial resolution. To this end, we propose algorithms adapted to the poor quality of the images and suited to a further real-time application. 1
