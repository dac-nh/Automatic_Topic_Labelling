1|Revisiting the Hierarchical Data Model|this paper, we develop a framework for a modern hierarchical data model, substantially improved from the original version by taking advantage of the lessons learned in the relational database context. We argue that this new hierarchical data model has many benefits with respect to the ubiquitous flat relational data model.
2|Research Problems in Data Warehousing|The topic of data warehousing encompasses architectures, algorithms, and tools for bringing together selected data from multiple databases or other information sources into a single repository, called a data warehouse, suitable for direct querying or analysis. In recent years data warehousing has become a prominent buzzword in the database industry, but attention from the database research community has been limited. In this paper we motivate the concept of a data warehouse, we outline a general data warehousing architecture, and we propose a number of technical issues arising from the architecture that we believe are suitable topics for exploratory research.  1 Introduction  Providing integrated access to multiple, distributed, heterogeneous databases and other information sources has become one of the leading issues in database research and industry #6#. In the research community, most approaches to the data integration problem are based on the following very general two-step process...
3|Readings in object-oriented database systems|This paper summarizes the interface, implementation, and use of a server process that is used as a backend by an object-oriented database system. This server is responsible for managing objects on secondary storage, managing transactions, and implementing a simple form of trigger. We sketch the interface of this system and point out some of the more interesting implementation issues that were encountered in building it. Client processes communicate asynchronously with the server by message sending. The system is designed to be as efficient as possible since one of its clients is the GARDEN system, an object-oriented programming environment. GARDEN views both static and dynamic program pieces as objects. Our back-end server provides persistent and sharable storage for GARDEN. The paper includes an extended example of how GARDEN makes use of this resource. 1.
4|Focusing Search in Hierarchical Structures with Directory Sets|Keyword-based searches on the World Wide Web are often of limited use, because they return too many uninteresting matches. We propose here a novel mechanism that permits the user to specify directory sets to restrict the space of documents searched, and, at the same time, increase the speed of the search. We view the Web as a single, huge hierarchy, reflected in the structure of the URLs. We refer to each subtree in this hierarchy as a directory, and group semantically related documents from multiple directories into a directory set. Starting from a collection of pre-defined directory sets, a user can dynamically generate new directory sets by using operators in a directory set algebra, and focus keywordbased searches to documents that belong to a (pre-defined or dynamically generated) directory set. We design algorithms for efficiently evaluating expressions in the directory set algebra, and describe a technique for tightly integrating directory sets into keyword-based search engines....
5|Raster-based Spatiotemporal Hierarchical Data Model for Marine Fishery Management |A Raster-based Spatiotemporal Hierarchical Data Model (RSHDM) was designed for the nature of marine phenomena or marine fishery management. Based on the data model a spatiotemporal data warehouse for marine fishery management has been developed. The practice shows that the RSHDM can provide aggregation data for the fishery management on each level.
6|Hierarchical Data Model in Content-based Image Retrieval |Nowadays, we are living in the content-based visual information retrieval age. The users would like the query data with the description close to human beings and the resulting images should have the same semantics meanings with query image. Having resolved this problem, we used the high-level feature as regions to fill the gap between low-level features and semantics meanings of images. We used an Hierarchical Agglomerative Clustering algorithm [HAC] to segment images in database into the regions and to classify them into the clusters with their representative that we called the words of the images. The query data can be the whole image, some regions of query images, some cluster’s representatives in the image database or the concept related to some description text of the regions in the region’s clusters. Retrieving is performed on the hierarchical tree structure of the region’s clusters or hierarchical tree structure of the images. Our experiment results have shown that the performance of our system is better in the meanings of precision and recall than the traditional systems only based on using the whole image with the low-level features as query data and linear search or image retrieval system based on automated text annotation.
7|Image Indexing Using Color Correlograms|We define a new image feature called the color correlogram  and use it for image indexing and comparison. This feature distills the spatial correlation of colors, and is both effective and inexpensive for content-based image retrieval. The correlogramrobustly tolerates large changesin appearance and shape caused by changes in viewing positions, camera zooms, etc. Experimental evidence suggests that this new feature outperforms not only the traditional color histogram method but also the recently proposed histogram refinement methods for image indexing/retrieval.  
8|Region-based representations of image and video: Segmentation tools for multimedia services|This paper discusses region-based representations of image and  video that are useful for multimedia services such as those supported by the  MPEG-4 and MPEG-7 standards. Classical tools related to the generation of  the region-based representations are discussed. After a description of the main  processing steps and the corresponding choices in terms of feature spaces, decision  spaces, and decision algorithms, the state of the art in segmentation is  reviewed. Mainly tools useful in the context of the MPEG-4 and MPEG-7 standard  are discussed. The review is structured around the strategies used by the  algorithms (transition-based or homogeneity-based) and the decision spaces  (spatial, spatio-temporal and temporal).  The second part of the paper proposes a partition tree representation of images  and introduces a processing strategy that involves a similarity estimation  step followed by a partition creation step. This strategy tries to find a compromise  between what can be done in...
9|An Ontology Approach to Object-Based Image Retrieval|In this paper, an image retrieval methodology suited for search in large collections of heterogeneous images is presented. The proposed approach employs a fully unsupervised segmentation algorithm to divide images into regions. Low-level features describing the color, position, size and shape of the resulting regions are extracted and are automatically mapped to appropriate intermediatelevel descriptors forming a simple vocabulary termed object ontology. The object ontology is used to allow the qualitative definition of the high-level concepts the user queries for (semantic objects, each represented by a keyword) in a human-centered fashion. When querying, clearly irrelevant image regions are rejected using the intermediate-level descriptors; following that, a relevance feedback mechanism employing the low-level features is invoked to produce the final query results. The proposed approach bridges the gap between keyword-based approaches, which assume the existence of rich image captions or require manual evaluation and annotation of every image of the collection, and query-by-example approaches, which assume that the user queries for images similar to one that already is at his disposal.
10|SemQuery: Semantic Clustering and Querying on Heterogeneous Features for Visual data|The effectiveness of the content-based image retrieval can be enhanced using the heterogeneous features embedded in the images. However, since the features in texture, color, and shape are generated using different computation methods and thus may require different similarity measurements, the integration of the retrieval on heterogeneous features is a non-trivial task. In this paper, we present a semantics-based clustering and indexing approach, termed SemQuery,  to support visual queries on heterogeneous features of images. Using this approach, the database images are classified based on their heterogeneous features. Each semantic image cluster contains a set of subclusters that are represented by the heterogeneous features that the images contain. A database image is included into a feature subcluster only if the image contains all the features under the same cluster. We also design a multi-layer model to merge the results of basic queries on individual features. A visual query proc...
11|Region-Based Color Image Indexing and Retrieval|In this paper a region-based color image indexing and retrieval algorithm is presented. As a basis for the indexing, a novel K-Means segmentation algorithm is used, modified so as to take into account the coherence of the regions. A new color distance is also defined for this algorithm. Based on the extracted regions, characteristic features are estimated using color, texture and shape information. An important and unique aspect of the algorithm is that, in the context of similarity-based querying, the user is allowed to view the internal representation of the submitted image and the query results. Experimental results demonstrate the performance of the algorithm. The development of an intelligent image content-based search engine for the World Wide Web is also presented, as a direct application of the presented algorithm.  1. 
12|Hierarchical Models of Object Recognition in Cortex|The classical model of visual processing in cortex is a hierarchy of increasingly sophisticated  representations, extending in a natural way the model of simple to complex cells of Hubel and Wiesel.  Somewhat surprisingly, little quantitative modeling has been done in the last 15 years to explore the  biological feasibility of this class of models to explain higher level visual processing, such as object  recognition. We describe a new hierarchical model that accounts well for this complex visual task, is  consistent with several recent physiological experiments in inferotemporal cortex and makes testable  predictions. The model is based on a novel MAX-like operation on the inputs to certain cortical neurons  which may have a general role in cortical function.  
13|Neural Network-Based Face Detection| We present a neural network-based upright frontal face detection system. A retinally connected neural network examines small windows of an image and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We present a straightforward procedure for aligning positive face examples for training. To collect negative examples, we use a bootstrap algorithm, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting nonface training examples, which must be chosen to span the entire space of nonface images. Simple heuristics, such as using the fact that faces rarely overlap in images, can further improve the accuracy. Comparisons with several other state-of-the-art face detection systems are presented, showing that our system has comparable performance in terms of detection and false-positive rates. 
14|Competitive mechanisms subserve attention in macaque areas V2 and V4|It is well established that attention modulates visual processing in extrastriate cortex. However, the underlying neural mechanisms are unknown. A consistent observation is that attention has its greatest impact on neuronal responses when multiple stimuli appear together within a cell’s receptive field. One way to explain this is to assume that multiple stimuli activate competing populations of neurons and that attention biases this competition in favor of the attended stimulus. In the absence of competing stimuli, there is no competition to be resolved. Accordingly, attention has a more limited effect on the neuronal response to a single stimulus. To test this interpretation, we measured the responses of neurons in macaque areas V2 and V4 using a behavioral paradigm that allowed us to isolate automatic sensory processing mechanisms from attentional effects. First, we measured each cell’s response to a single
15|Learning Invariance From Transformation Sequences|Introduction  How can we consistently recognize objects when changes in the viewing angle, eye position, distance, size, orientation, relative position, or deformations of the object itself (e.g., of a newspaper or a gymnast) can change their retinal projections so significantly? The visual system must contain knowledge about such transformations in order to be able to generalize correctly. Part of this knowledge is probably determined genetically, but it is also likely that the visual system learns from its sensory experience, which contains plenty of examples of such transformations. Electrophysiological experiments suggest that the invariance properties of perception may be due to the receptive field characteristics of individual cells in the visual system. Complex cells in the primary visual cortex exhibit approximate invariance to position within a limited range (Hubel and Wiesel 1962), while cells in higher visual areas in the temporal cortex show more complex forms of invariance
17|Nonlinear Neural Networks: Principles, Mechanisms, and Architectures| An historical discussion is provided of the intellectual trends that caused nineteenth century interdisciplinary studies of physics and psychobiology by leading scientists such as Helmholtz, Maxwell, and Mach to splinter into separate twentieth-century scientific movements. The nonlinear, nonstationary, and nonlocal nature of behavioral and brain data are emphasized. Three sources of contemporary neural network research-the binary, linear, and continuous-nonlinear models-are noted. The remainder of the article describes results about continuous-nonlinear models: Many models of content-addressable memory are shown to be special cases of the Cohen-Grossberg model and global Liapunov function, including the additive, brain-state-in-a-box, McCulloch-Pitts, Boltzmann machine, Hartline-Ratliff-Millet; shunting, maskingfield, bidirectional associative memory, Volterra-Lotka, Gilpin-Ayala, and Eigen-Schuster models. A Liapunov functional method is described for proving global limit or oscillation theorems for nonlinear competitive systems when their decision schemes are globally consistent or inconsistent, respectively. The former case is illustrated by a model of a globally stable economic market, and the latter case is illustrated by a model of the voting paradox. Key properties of shunting competitive feedback networks are summarized, including the role of sigmoid signalling, automatic gain control, competitive choice and quantization, tunable filtering, total activity normalization, and noise suppression in pattern transformation and memory storage applications. Connections to models of competitive learning, vector quantization, and categorical perception are noted. Adaptive resonance
18|Visual properties of neurons in a polysensory area in superior temporal sulcus of the macaque|dorsal bank and fundus of the anterior por-tion of the superior temporal sulcus, an area we term the superior temporal polysensory area (STP). Five macaques were studied under anesthesia ( N20) and immobilization in repeated recording sessions. 2. Almost all of the neurons were visually responsive, and over half responded to more than one sensory modality; 21 % responded to visual and auditory stimuli, 17 % re-sponded to visual and somesthetic stimuli, 17 % were trimodal, and 41 % were exclu-sively visual. 3. Almost all the visual receptive fields extended into both visual half-fields, and the majority approached the size of the visual field of the monkey, including both monoc-ular crescents. Somesthetic receptive fields were also bilateral and usually included most of the body surface. 4. Virtually all neurons responded better to moving visual stimuli than to stationary visual stimuli, and almost half were sensitive to the direction of movement. Several classes of directional neurons were found, including a) neurons selective for a single direction of movement throughout their receptive field, b) neurons selective for directions of move-ment radially symmetric about the center of gaze, and c) neurons selective for movement in depth. 5. The majority of neurons (70%) had lit-tle or no preference for stimulus size, shape, orientation, or contrast. The minority (30%) responded best to particular stimuli. Some of these appeared to be selective for faces. 6. The properties of most STP neurons, such as large receptive fields, sensitivity to movement, insensitivity to form, and poly-modal responsiveness, suggest that STP is more involved in orientation and spatial functions than in pattern recognition.
19|SEEMORE: Combining Color, Shape, and Texture Histogramming in a Neurally Inspired Approach to Visual Object Recognition|this article.
20|Are cortical models really bound by the “Binding Problem|Address correspondence to T.P. The usual description of visual processing in cortex is an extension of the simple to complex hi-erarchy postulated by Hubel and Wiesel — a feedforward sequence of more and more complex and invariant features. The capability of this class of models to perform higher level visual processing such as viewpoint-invariant object recognition in cluttered scenes has been questioned in recent years by several researchers, who in turn proposed an alternative class of models based on the synchro-nization of large assemblies of cells, within and across cortical areas. The main implicit argument for this novel and controversial view was the assumption that hierarchical models cannot deal with the computational requirements of high level vision and suffer from the so-called “binding problem”. We review the present situation and discuss theoretical and experimental evidence showing that the perceived weaknesses of hierarchical models are not true. In particular, we show that recognition of multiple objects in cluttered scenes, arguably among the most difficult tasks in vision, can be done in a hierarchical feedforward model. 1
21|A Model of Invariant Object Recognition in the Visual System|Neurons in the ventral stream of the primate visual system exhibit responses to the images of objects which are invariant with respect to natural transformations such as translation, size, and view. Anatomical and neurophysiological evidence suggests that this is achieved through a series of hierarchical processing areas. In an attempt to elucidate the manner in which such representations are established, we have constructed a model of cortical visual processing which seeks to parallel many features of this system, specifically the multi-stage hierarchy with its topologically constrained convergent connectivity. Each stage is constructed as a competitive network utilising a modified Hebb-like learning rule, called the trace rule, which incorporates previous as well as current neuronal activity. The trace rule enables neurons to learn about whatever is invariant over short time periods (e.g. 0.5 s) in the representation of objects as the objects transform in the real world. The trace ru...
22|Neural models for part-whole hierarchies|We present a connectionist method for representing images that explicitly addresses their hierarchical nature. It blends data from neuroscience about whole-object viewpoint sensitive cells in inferotem-poral cortex8 and attentional basis- eld modulation in V43 with 5, 11 ideas about hierarchical descriptions based on microfeatures. The resulting model makes critical use of bottom-up and top-down pathways for analysis and synthesis. 6 We illustrate the model with a simple example of representing information about faces. 1 Hierarchical Models Images of objects constitute an important paradigm case of a representational hierarchy, inwhich `wholes&#039;, such as faces, consist of `parts&#039;, such aseyes, noses and mouths. The representation and manipulation of part-whole hierarchical information in xed hardwareisaheavy millstone around connectionist necks, and has
23|Just One View: Invariances in Inferotemporal Cell Tuning|In macaque inferotemporal cortex (IT), neurons have been found to respond selectively to complex shapes while showing broad tuning (&#034;invariance &#034;) with respect to stimulus transformations such as translation and scale changes and a limited tuning to rotation in depth. Training monkeys with novel, paperclip-like objects, Logothetis et al.  9  could investigate whether these invariance properties are due to experience with exhaustively many transformed instances of an object or if there are mechanisms that allow the cells to show response invariance also to previously unseen instances of that object. They found object-selective cells in anterior IT which exhibited limited invariance to various transformations after training with single object views. While previous models accounted for the tuning of the cells for rotations in depth and for their selectivity to a specific object relative to a population of distractor objects,  14,1  the model described here attempts to explain in a biologi...
24|Hierarchical Dirichlet processes|program. The authors wish to acknowledge helpful discussions with Lancelot James and Jim Pitman and the referees for useful comments. 1 We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, specifically one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessar-ily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of
25|Latent dirichlet allocation|We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.
26|Bayesian Density Estimation and Inference Using Mixtures|We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...
27|The Infinite Hidden Markov Model|We show that it is possible to extend hidden Markov models to have a countably infinite number of hidden states. By using the theory of Dirichlet processes we can implicitly integrate out the infinitely many transition parameters, leaving only three hyperparameters which can be learned from data. These three hyperparameters define a hierarchical Dirichlet process capable of capturing a rich set of transition dynamics. The three hyperparameters control the time scale of the dynamics, the sparsity of the underlying state-transition matrix, and the expected number of distinct hidden states in a finite sequence. In this framework it is also natural to allow the alphabet of emitted symbols to be infinite---consider, for example, symbols being possible words appearing in English text.
29|Gibbs Sampling Methods for Stick-Breaking Priors |... In this paper we present two general types of Gibbs samplers that can be used to fit posteriors of Bayesian hierarchical models based on stick-breaking priors. The first type of Gibbs sampler, referred to as a Polya urn Gibbs sampler, is a generalized version of a widely used Gibbs sampling method currently employed for Dirichlet process computing. This method applies to stick-breaking priors with a known P&#039;olya urn characterization; that is priors with an explicit and simple prediction rule. Our second method, the blocked Gibbs sampler, is based on a entirely different approach that works by directly sampling values from the posterior of the random measure. The blocked Gibbs sampler can be viewed as a more general approach as it works without requiring an explicit prediction rule. We find that the blocked Gibbs avoids some of the limitations seen with the Polya urn approach and should be simpler for non-experts to use. 
30|The two-parameter Poisson-Dirichlet distribution derived from a stable subordinator.|The two-parameter Poisson-Dirichlet distribution, denoted pd(ff; `),  is a distribution on the set of decreasing positive sequences with sum 1. The usual Poisson-Dirichlet distribution with a single parameter `,  introduced by Kingman, is pd(0; `). Known properties of pd(0; `), including the Markov chain description due to Vershik-Shmidt-Ignatov, are generalized to the two-parameter case. The size-biased random permutation of pd(ff; `) is a simple residual allocation model proposed by Engen in the context of species diversity, and rediscovered by Perman and the authors in the study of excursions of Brownian motion and Bessel processes. For 0 ! ff ! 1, pd(ff; 0) is the asymptotic distribution of ranked lengths of excursions of a Markov chain away from a state whose recurrence time distribution is in the domain of attraction of a stable law of index ff. Formulae in this case trace back to work of Darling, Lamperti and Wendel in the 1950&#039;s and 60&#039;s. The distribution of ranked lengths of e...
31|Hierarchical topic models and the nested Chinese restaurant process|We address the problem of learning topic hierarchies from data. The model selection problem in this domain is daunting—which of the large collection of possible trees to use? We take a Bayesian approach, generating an appropriate prior via a distribution on partitions that we refer to as the nested Chinese restaurant process. This nonparametric prior allows arbitrarily large branching factors and readily accommodates growing data collections. We build a hierarchical topic model by combining this prior with a likelihood that is based on a hierarchical variant of latent Dirichlet allocation. We illustrate our approach on simulated data and with an application to the modeling of NIPS abstracts. 1
32|The Infinite Gaussian Mixture Model|In a Bayesian mixture model it is not necessary a priori to limit the number of components to be finite. In this paper an infinite Gaussian mixture model is presented which neatly sidesteps the difficult problem of finding the &#034;right&#034; number of mixture components. Inference in the model is done using an efficient parameter-free Markov Chain that relies entirely on Gibbs sampling.
33|Combinatorial stochastic processes  |This is a collection of expository articles about various topics at the interface between enumerative combinatorics and stochastic processes. These articles expand on a course of lectures given at the École d’Été de Probabilités de St. Flour in July 2002. The articles are called ’chapters ’ and numbered according to the order of these chapters in a printed volume to appear in Springer Lecture Notes in Mathematics. Each chapter is fairly self-contained, so readers with adequate background can start reading any chapter, with occasional consultation of earlier chapters as necessary. Following this Chapter 0, there are 10 chapters, each divided into sections. Most sections conclude with some Exercises. Those for which I don’t know solutions are called Problems. Acknowledgments Much of the research reviewed here was done jointly with David Aldous. Much credit is due to him, especially for the big picture of continuum approximations to large combinatorial structures. Thanks also to my other collaborators in this work, especially Jean Bertoin, Michael Camarri, Steven
34|A Split-Merge Markov Chain Monte Carlo Procedure for the Dirichlet Process Mixture Model|. We propose a split-merge Markov chain algorithm to address the problem of inefficient sampling for conjugate Dirichlet process mixture models. Traditional Markov chain Monte Carlo methods for Bayesian mixture models, such as Gibbs sampling, can become trapped in isolated modes corresponding to an inappropriate clustering of data points. This article describes a Metropolis-Hastings procedure that can escape such local modes by splitting or merging mixture components. Our Metropolis-Hastings algorithm employs a new technique in which an appropriate proposal for splitting or merging components is obtained by using a restricted Gibbs sampling scan. We demonstrate empirically that our method outperforms the Gibbs sampler in situations where two or more components are similar in structure.  Key words: Dirichlet process mixture model, Markov chain Monte Carlo, Metropolis-Hastings algorithm, Gibbs sampler, split-merge updates  1 Introduction  Mixture models are often applied to density estim...
35|A hierarchical Bayesian language model based on Pitman–Yor processes|We propose a new hierarchical Bayesian n-gram model of natural languages. Our model makes use of a generalization of the commonly used Dirichlet distributions called Pitman-Yor processes which produce power-law distributions more closely resembling those in natural languages. We show that an approximation to the hierarchical Pitman-Yor language model recovers the exact formulation of interpolated Kneser-Ney, one of the best smoothing methods for n-gram language models. Experiments verify that our model gives cross entropy results superior to interpolated Kneser-Ney and comparable to modified Kneser-Ney. 1
36|Interpolating between types and tokens by estimating power-law generators|Standard statistical models of language fail to capture one of the most striking properties of natural languages: the power-law distribution in the frequencies of word tokens. We present a framework for developing statistical models that generically produce power-laws, augmenting standard generative models with an adaptor that produces the appropriate pattern of token frequencies. We show that taking a particular stochastic process – the Pitman-Yor process – as an adaptor justifies the appearance of type frequencies in formal analyses of natural language, and improves the performance of a model for unsupervised learning of morphology. 1
37|Modelling heterogeneity with and without the Dirichlet process|We investigate the relationships between Dirichlet process (DP) based models and allocation models for a variable number of components, based on exchangeable distributions. It is shown that the DP partition distribution is a limiting case of a Dirichlet± multinomial allocation model. Comparisons of posterior performance of DP and allocation models are made in the Bayesian paradigm and illustrated in the context of univariate mixture models. It is shown in particular that the unbalancedness of the allocation distribution, present in the prior DP model, persists a posteriori. Exploiting the model connections, a new MCMC sampler for general DP based models is introduced, which uses split/merge moves in a reversible jump framework. Performance of this new sampler relative to that of some traditional samplers for DP processes is then explored.
38|A Hierarchical Internet Object Cache| This paper discusses the design andperformance  of a hierarchical proxy-cache designed to make Internet information systems scale better. The design was motivated by our earlier trace-driven simulation study of Internet traffic. We believe that the conventional wisdom, that the benefits of hierarchical file caching do not merit the costs, warrants reconsideration in the Internet environment.  The cache implementation supports a highly concurrent stream of requests. We present performance measurements that show that the cache outperforms other popular Internet cache implementations by an order of magnitude under concurrent load. These measurements indicate that hierarchy does not measurably increase access latency. Our software can also be configured as a Web-server accelerator; we present data that our httpd-accelerator is ten times faster than Netscape&#039;s Netsite and NCSA 1.4 servers.  Finally, we relate our experience fitting the cache into the increasingly complex and operational world of Internet information systems, including issues related to security, transparency to cache-unaware clients, and the role of file systems in support of ubiquitous wide-area information systems.  
39|Scale and performance in a distributed file system|The Andrew File System is a location-transparent distributed tile system that will eventually span more than 5000 workstations at Carnegie Mellon University. Large scale affects performance and complicates system operation. In this paper we present observations of a prototype implementation, motivate changes in the areas of cache validation, server process structure, name translation, and low-level storage representation, and quantitatively demonstrate Andrew’s ability to scale gracefully. We establish the importance of whole-file transfer and caching in Andrew by comparing its performance with that of Sun Microsystem’s NFS tile system. We also show how the aggregation of files into volumes improves the operability of the system.
40|The Case for Geographical Push-Caching|Most existing wide-area caching schemes are client initiated. Decisions on when and where to cache information are made without the benefit of the server&#039;s global knowledge of the situation. We believe that the server should play a role in making these caching decisions, and we propose  geographical push-caching as a way of bringing the server back into the loop. The World Wide Web is an excellent example of a wide-area system that will benefit from geographical push-caching, and we present an architecture that allows a Web server to autonomously replicate HTML pages. 1 Introduction  The World-Wide Web [1] operates for the most part as a cache-less distributed system. When two neighboring clients retrieve a document from the same server, the document is sent twice. This is inefficient, especially considering the ease with which Web browsers allow users to transfer large multimedia documents. To combat this problem, some Web browsers have begun to add local client caches. These prevent ...
41|The Harvest Information Discovery and Access System|It is increasingly difficult to make effective use of Internet information, given the rapid growth in data volume, user base, and data diversity. In this paper we introduce Harvest, a system that provides a scalable, customizable architecture for gathering, indexing, caching, replicating, and accessing Internet information.  
42|Alex -- a global filesystem|The Alex filesystem provides users and applications transparent read access to files in Internet anonymous FTP sites. Today there are thousands of anonymous FTP sites with a total of a few million files and roughly a terabyte of data. The standard approach to accessing these files involves logging in to the remote machine. This means that an application can not access remote files and that users do not have any of their aliases or local tools available when connected to a remote site. Users who want to use an application on a remote file must first manually make a local copy of the file. Not only is this inconvenient, it creates two more problems. First, there is no mechanism for automatically updating this local copy when the remote file changes. The users must keep track of where they get their files from and check to see if there are updates, and then fetch these. Second, many different users at the same site may have made copies of the same remote file, thus wasting disk space. Alex addresses the problems with the above approach while maintaining compatibility with the existing FTP protocol so that the large collection of currently available files can be accessed. To get reasonable performance, long term file caching must be used. Thus consistency must be addressed. Traditional solutions to the cache consistency problem do not work in the Internet FTP domain: callbacks are not an
43|Multi-level Caching in Distributed File Systems or Your cache ain’t nuthin’ but trash|We are investigating the potential for intermediate file servers to address scaling problems in increasingly large distributed file systems. To this end, we have run trace-driven simulations based on data from DEC-SRC and our own data collection to determine the potential of caching-only intermediate servers. The degree of sharing among clients is central to the effectiveness of an intermediate server. This turns out to be quite low in the traces available to us. All told, fewer than 10 % of block accesses are to files shared by more than one file system client. Trace-driven simulation shows that even with an infinite cache at the intermediate, cache hit rates are disappointingly low. For client caches as small as 20 MB, we observe hit rates less than 19%. As client cache sizes increase, the hit rate at the intermediate approaches the degree of sharing among all clients. On the other hand, the intermediate does appear to be effective in reducing the peak load presented to upstream file servers.
44|Demand-based Document Dissemination for the World-Wide Web|We analyzed the logs of our departmental HTTP server
45|Imagenet: A large-scale hierarchical image database|The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a largescale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond. 1.
46|Distinctive Image Features from Scale-Invariant Keypoints|This paper presents a method for extracting distinctive invariant features from  images, which can be used to perform reliable matching between different images  of an object or scene. The features are invariant to image scale and rotation,  and are shown to provide robust matching across a a substantial range  of affine distortion, addition of noise, change in 3D viewpoint, and change in  illumination. The features are highly distinctive, in the sense that a single feature  can be correctly matched with high probability against a large database of  features from many images. This paper also describes an approach to using  these features for object recognition. The recognition proceeds by matching  individual features to a database of features from known objects using a fast  nearest-neighbor algorithm, followed by a Hough transform to identify clusters  belonging to a single object, and finally performing verification through leastsquares  solution for consistent pose parameters. This approach to recognition  can robustly identify objects among clutter and occlusion while achieving near  real-time performance.
47|Scalable Recognition with a Vocabulary Tree|A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CD&#039;s. The scheme
48|Labeling Images with a Computer Game|We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people  play the game they help determine the contents of images  by providing meaningful labels for them. If the game is  played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the  Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users  block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don&#039;t work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.
49|LabelMe: A Database and Web-Based Tool for Image Annotation|  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.
50|The 2005 pascal visual object classes challenge|Abstract. The PASCAL Visual Object Classes Challenge ran from February to March 2005. The goal of the challenge was to recognize objects from a number of visual object classes in realistic scenes (i.e. not pre-segmented objects). Four object classes were selected: motorbikes, bicycles, cars and people. Twelve teams entered the challenge. In this chapter we provide details of the datasets, algorithms used by the teams, evaluation criteria, and results achieved. 1
51|Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments |Abstract — Face recognition has benefitted greatly from the many databases that have been produced to study it. Most of these databases have been created under controlled conditions to facilitate the study of specific parameters on the face recognition problem. These parameters include such variables as position, pose, lighting, expression, background, camera quality, occlusion, age, and gender. While there are many applications for face recognition technology in which one can control the parameters of image acquisition, there are also many applications in which the practitioner has little or no control over such parameters. This database is provided as an aid in studying the latter, unconstrained, face recognition problem. The database represents an initial attempt to provide a set of labeled face photographs spanning the range of conditions typically encountered by people in their everyday lives. The database exhibits “natural ” variability in pose, lighting, focus, resolution, facial expression, age, gender, race, accessories, make-up, occlusions, background, and photographic quality. Despite this variability, the images in the database are presented in a simple and consistent format for maximum ease of use. In addition to describing the details of the database and its acquisition, we provide specific experimental paradigms for which the database is suitable. This is done in an effort to make research performed with the database as consistent and comparable as possible. I.
52|TextonBoost: Joint Appearance, Shape and Context Modeling for Multi-Class Object . . .|This paper proposes a new approach to learning a discriminative  model of object classes, incorporating appearance, shape and context  information efficiently. The learned model is used for automatic  visual recognition and semantic segmentation of photographs. Our discriminative  model exploits novel features, based on textons, which jointly  model shape and texture. Unary classification and feature selection is  achieved using shared boosting to give an efficient classifier which can  be applied to a large number of classes. Accurate image segmentation is  achieved by incorporating these classifiers in a conditional random field. Efficient training
54|One-shot learning of object categories| Learning visual models of object categories notoriously requires hundreds or thousands of training examples. We show that it is possible to learn much information about a category from just one, or a handful, of images. The key insight is that, rather than learning from scratch, one can take advantage of knowledge coming from previously learned categories, no matter how different these categories might be. We explore a Bayesian implementation of this idea. Object categories are represented by probabilistic models. Prior knowledge is represented as a probability density function on the parameters of these models. The posterior model for an object category is obtained by updating the prior in the light of one or more observations. We test a simple implementation of our algorithm on a database of 101 diverse object categories. We compare category models learned by an implementation of our Bayesian approach to models learned from by Maximum Likelihood (ML) and Maximum A Posteriori (MAP) methods. We find that on a database of more than 100 categories, the Bayesian approach produces informative models when the number of training examples is too small for other methods to operate successfully.
55|Learning object categories from google’s image search|Current approaches to object category recognition require datasets of training images to be manually prepared, with varying degrees of supervision. We present an approach that can learn an object category from just its name, by uti-lizing the raw output of image search engines available on the Internet. We develop a new model, TSI-pLSA, which extends pLSA (as applied to visual words) to include spa-tial information in a translation and scale invariant man-ner. Our approach can handle the high intra-class vari-ability and large proportion of unrelated images returned by search engines. We evaluate the models on standard test sets, showing performance competitive with existing meth-ods trained on hand prepared datasets. 1.
56|In Defense of Nearest-Neighbor Based Image Classification |State-of-the-art image classification methods require an intensive learning/training stage (using SVM, Boosting, etc.) In contrast, non-parametric Nearest-Neighbor (NN) based image classifiers require no training time and have other favorable properties. However, the large performance gap between these two families of approaches rendered NNbased image classifiers useless. We claim that the effectiveness of non-parametric NNbased image classification has been considerably undervalued. We argue that two practices commonly used in image classification methods, have led to the inferior performance of NN-based image classifiers: (i) Quantization of local image descriptors (used to generate “bags-of-words”, codebooks). (ii) Computation of ‘Image-to-Image ’ distance, instead of ‘Image-to-Class ’ distance. We propose a trivial NN-based classifier – NBNN, (Naive-Bayes Nearest-Neighbor), which employs NNdistances in the space of the local image descriptors (and not in the space of images). NBNN computes direct ‘Imageto-Class’ distances without descriptor quantization. We further show that under the Naive-Bayes assumption, the theoretically optimal image classifier can be accurately approximated by NBNN. Although NBNN is extremely simple, efficient, and requires no learning/training phase, its performance ranks among the top leading learning-based image classifiers. Empirical comparisons are shown on several challenging databases (Caltech-101,Caltech-256 and Graz-01). 1.
57|Semantic hierarchies for visual object recognition|In this paper we propose to use lexical semantic networks to extend the state-of-the-art object recognition techniques. We use the semantics of image labels to integrate prior knowledge about inter-class relationships into the visual appearance learning. We show how to build and train a semantic hierarchy of discriminative classifiers and how to use it to perform object detection. We evaluate how our approach influences the classification accuracy and speed on the PASCAL VOC challenge 2006 dataset, a set of challenging real-world images. We also demonstrate additional features that become available to object recognition due to the extension with semantic inference tools—we can classify high-level categories, such as animals, and we can train part detectors, for example a window detector, by pure inference in the semantic network. 1.
58|Optimol: automatic online picture collection via incremental model learning|A well-built dataset is a necessary starting point for ad-vanced computer vision research. It plays a crucial role in evaluation and provides a continuous challenge to state-of-the-art algorithms. Dataset collection is, however, a te-dious and time-consuming task. This paper presents a novel automatic dataset collecting and model learning approach that uses object recognition techniques in an incremental method. The goal of this work is to use the tremendous re-sources of the web to learn robust object category models in order to detect and search for objects in real-world cluttered scenes. It mimics the human learning process of iteratively accumulating model knowledge and image examples. We adapt a non-parametric graphical model and propose an incremental learning framework. Our algorithm is capa-ble of automatically collecting much larger object category datasets for 22 randomly selected classes from the Caltech 101 dataset. Furthermore, we offer not only more images in each object category dataset, but also a robust object model and meaningful image annotation. Our experiments show that OPTIMOL is capable of collecting image datasets that are superior to Caltech 101 and LabelMe. 1.
59|Exploiting Object Hierarchy: Combining Models from Different Category Levels|We investigated the computational properties of natural object hierarchy in the context of constellation object class models, and its utility for object class recognition. We first observed an interesting computational property of the object hierarchy: comparing the recognition rate when using models of objects at different levels, the higher more inclusive levels (e.g., Closed-Frame Vehicles or Vehicles) exhibit higher recall but lower precision when compared with the class specific level (e.g., bus). These inherent differences suggest that combining object classifiers from different hierarchical levels into a single classifier may improve classification, as it appears like these models capture different aspects of the object. We describe a method to combine these classifiers, and analyze the conditions under which improvement can be guaranteed. When given a small sample of a new object class, we describe a method to transfer knowledge across the tree hierarchy, between related objects. Finally, we describe extensive experiments using object hierarchies obtained from publicly available datasets, and show that the combined classifiers significantly improve recognition results. 1.
60|Constructing category hierarchies for visual recognition|Abstract. Class hierarchies are commonly used to reduce the complexity of the classification problem. This is crucial when dealing with a large number of categories. In this work, we evaluate class hierarchies currently constructed for visual recognition. We show that top-down as well as bottom-up approaches, which are commonly used to automatically construct hierarchies, incorporate assumptions about the separability of classes. Those assumptions do not hold for visual recognition of a large number of object categories. We therefore propose a modification which is appropriate for most top-down approaches. It allows to construct class hierarchies that postpone decisions in the presence of uncertainty and thus provide higher recognition accuracy. We also compare our method to a one-against-all approach and show how to control the speed-foraccuracy trade-off with our method. For the experimental evaluation, we use the Caltech-256 visual object classes dataset and compare to stateof-the-art methods. 1
62|Towards scalable dataset construction: An active learning approach|Abstract. As computer vision research considers more object categories and greater variation within object categories, it is clear that larger and more exhaustive datasets are necessary. However, the process of collecting such datasets is laborious and monotonous. We consider the setting in which many images have been automatically collected for a visual category (typically by automatic internet search), and we must separate relevant images from noise. We present a discriminative learning process which employs active, online learning to quickly classify many images with minimal user input. The principle advantage of this work over previous endeavors is its scalability. We demonstrate precision which is often superior to the state-of-the-art, with scalability which exceeds previous work. 1
63|The Cornetto Database: Architecture and User-Scenarios |We outline the architecture of a semantic database for Dutch, developed in the Cornetto project, and its possible usage in language technology and information access applications. The database combines the Dutch part of EuroWordNet with the Referentie Bestand Nederlands. The resulting database is also aligned with the English WordNet and with a formal ontology. As such it represents a unique database with rich semantic and combinatoric information. There are many ways in which this knowledge can be exploited. In this paper we give an overview of possible application areas in order to inspire future research based on the Cornetto database.
64|WordNet for italian and its use for lexical discrimination|email  fartalejmagninijstrappagirstitcit Abstract  We present a prototype of the Italian version of WordNet a general computational lexical resource  Some relevant extensions are discussed to make it usable for parsing  in particular we add verbal selec tional restrictions to make lexical discrimination eective  Italian Word Net has been coupled with a parser and a number of experiments have been performed to individuate the methodology with the best tradeo between disambiguation rate and precision  Results conrm intuitive hy pothesis on the role of selectional restrictions and show evidences for a WordNetlike organization of lexical senses
65|The quadtree and related hierarchical data structures|A tutorial survey is presented of the quadtree and related hierarchical data structures. They are based on the principle of recursive decomposition. The emphasis is on the representation of data used in applications in image processing, computer graphics, geographic information systems, and robotics. There is a greater emphasis on region data (i.e., two-dimensional shapes) and to a lesser extent on point, curvilinear, and threedimensional data. A number of operations in which such data structures find use are examined in greater detail.
66|An algorithm for finding best matches in logarithmic expected time|An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional-to 1ogN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods.
67|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
68|Optimal search in planar subdivisions|Abstract. A planar subdivision is any partition of the plane into (possibly unbounded) polygonal regions. The subdivision search problem is the following: given a subdivision S with n line segments and a query point P, determine which region of S contains P. We present a practical algorithm for subdivision search that achieves the same (optimal) worst case complexity bounds as the significantly more complex algorithm of Lipton and Tarjan, namely O (log n) search time with O (n) storage. Our subdivision search structure can be constructed in linear time from the subdivision representation used in many applications. Key words, computational geometry, analysis of algorithms, point location, planar graphs, hierarchical search
69|Picture segmentation by a tree traversal algorithm|ABSTRACT. In the past, picture segmentation has been performed by merging small primitive regions or by recursively splitting the whole picture. This paper combines the two approaches with significant increase in processing speed while maintaining small memory requirements. The data structure is described in detail and examples of implementations are given.
70|Tactile Recognition and Localization Using Object Models: The Case of Polyhedra on a Plane|This paper discusses how data from multiple tactile sensors may be used to identify and locate one object, from among a set of known objects. We use only local information from sensors: (1) the position of contact points, and (2) ranges of  surface normals at the contact points. The recognition and localization process is structured as the development and pruning of a tree of consistent hypotheses about pairings between contact points and object surfaces. In this paper, we deal with polyhedral objects constrained to lie on a known plane, i.e., having three&#039; degrees of positioning freedom relative to the sensors.
71|Map-Guided Feature Extraction from Aerial . Imagery|In this paper we discuss the use of map descriptions to guide the extraction of man-made and natural features from aerial imagery. An approach to image analysis using a region-based segmentation system is described. This segmentation system has been used to search a database of images that are in correspondence with a geodetic map to find occurrences of known buildings, roads, and natural features. The map predicts the approximate appearance and position of a feature in an image. The map also predicts the area of uncertainty caused by errors in the image to map correspondence. The segmentation process then searches for image regions that satisfy 2-dimensional shape and intensity criteria. If no initial region is found, the process attempts to merge together those regions that may satisfy these criteria. Several detailed examples of the segmentation process are given 1.
72|Approximate Pattern Matching in a Pattern Database System|This paper is also concerned with the complexity of picture operations  using pyramid-like data structures In particular, we are interested in  discovering that two binary pictures (normalized for size, rotation, and  position) &#034;almost&#034; match Our motivation for studying this problem is the  design of error-tolerant pattern database systems. Too often, in pattern  analysis, matching algorithms are proposed without regard to the global  organization of the representations of the models they are to match  Consequently, the algorithms are only practical for small databases  This paper will discuss the matching problem along with the design of a pattern database system. Section 2 contains definitions. Section 3  describes both depth-first and breadth-first approximate matching  algorithms. Section 4 contains a probabilistic analysis of approximate  matching both with and without pyramids. Section 5 describes the  organization of the pattern database. Finally, Section 6 contains  conclusions. 2. Definitions  Definition: A binary image, I, is a 2nx2 n array of O&#039;s and 1&#039;s
73|Distributed hierarchical processing in the primate cerebral cortex|In recent years, many new cortical areas have been identified in the macaque monkey. The number of identified connections between areas has increased even more dramatically. We report here on (1) a summary of the layout of cortical areas associated with vision and with other modalities, (2) a computerized database for storing and representing large amounts of information on connectivity patterns, and (3) the application of these data to the analysis of hierarchical organization of the cerebral cortex. Our analysis concentrates on the visual system, which includes 25 neocortical areas that are predominantly or exclusively visual in function, plus an additional 7 areas that we regard as visual-association areas on the basis of their extensive visual inputs. A total of 305 connections among these 32 visual and
74|Stimulus-selective properties of inferior temporal neurons in the macaque|Previous studies have reported that some neurons in the inferior temporal (IT) cortex respond selectively to highly specific complex objects. In the present study, we conducted the first systematic survey of the responses of IT neurons to both simple stimuli, such as edges and bars, and highly complex stimuli, such as models of flowers, snakes, hands, and faces. If a neuron responded to any of these stimuli, we attempted to isolate the critical stimulus features underlying the response. We found that many of the responsive neurons responded well to virtually every stimulus tested. The remaining, stimulus-selective cells were often selective along the dimensions of shape, color, or texture of a stimulus, and this selectivity was maintained throughout a large receptive field. Although most IT neurons do not appear to be “detectors ” for complex objects, we did find a separate population of cells that responded selectively to faces. The responses of these cells were dependent on the configuration of specific face features, and their selectivity was maintained over changes in stimulus size and position. A particularly high incidence of such cells was found deep in the superior temporal sulcus. These results indicate that there may be specialized mechanisms for the analysis of faces in IT cortex. Inferior temporal (IT) cortex plays a role in visual processing several steps beyond that of the primary visual cortex. Removal
75|A double-labeling investigation of the afferent connectivity to cortical areas V1 and V2 of the macaque monkey|The afferent connectivity of areas Vl and V2 was investigated using the fluorescent dyes fast blue and diamidino yellow. Simultaneous injection of each dye in retinotopically corresponding regions of these areas gave rise to two afferent populations of labeled neurons in subcortical and cortical structures which project to both areas. These two populations showed a variable degree of overlap in their spatial distribution. Neurons labeled by both dyes (double-labeled neurons) which, therefore, project to both areas, were found in substantial numbers in these overlap zones. When the injections were made in non-retinotopically corresponding regions in the two areas, both populations of labeled cells overlapped extensively in the cortex but not in subcortical structures, suggesting that the laws governing the topography of these
76|Hierarchically Classifying Documents Using Very Few Words|The proliferation of topic hierarchies for text documents has resulted in a need for tools that automatically classify new documents within such hierarchies. Existing classification schemes which ignore the hierarchical structure and treat the topics as separate classes are often inadequate in text classification where the there is a large number of classes and a huge number of relevant features needed to distinguish between them. We propose an approach that utilizes the hierarchical topic structure to decompose the classification task into a set of simpler problems, one at each node in the classification tree. As we show, each of these smaller problems can be solved accurately by focusing only on a very small set of features, those relevant to the task at hand. This set of relevant features varies widely throughout the hierarchy, so that, while the overall relevant feature set may be large, each classifier only examines a small subset. The use of reduced feature sets allows us to util...
77|Knowledge acquisition via incremental conceptual clustering|hill climbing Abstract. Conceptual clustering is an important way of summarizing and explaining data. However, the recent formulation of this paradigm has allowed little exploration of conceptual clustering as a means of improving performance. Furthermore, previous work in conceptual clustering has not explicitly dealt with constraints imposed by real world environments. This article presents COBWEB, a conceptual clustering system that organizes data so as to maximize inference ability. Additionally, COBWEB is incremental and computationally economical, and thus can be flexibly applied in a variety of domains. 1.
78|Toward optimal feature selection|In this paper, we examine a method for feature subset selection based on Information Theory. Initially, a framework for de ning the theoretically optimal, but computationally intractable, method for feature subset selection is presented. We show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features. In particular, this will be the case for both irrelevant and redundant features. We then give an e cient algorithm for feature selection which computes an approximation to the optimal feature selection criterion. The conditions under which the approximate algorithm is successful are examined. Empirical results are given on a number of data sets, showing that the algorithm e ectively handles datasets with a very large number of features.
79|OHSUMED: An interactive retrieval evaluation and new large test collection for research|A series of information retrieval experiments was earned out with a computer installed in a medical practice setting for relatively inexperienced physician end-users. Using a commercial MEDLINE product based on the vector space model, these physicians searched just as effectively as more experienced searchers using Boolean searching. The results of this experiment were subsequently used to create anew large medical test collection, which was used in experiments with the SMART ~trieval system to obtain baseline performance data as well as compare SMART with the other searchers. 1
80|Learning Bayesian Networks With Local Structure|.  We examine a novel addition to the known methods for learning Bayesian networks from data that improves the quality of the learned networks. Our approach explicitly represents and learns the local structure in the conditional probability distributions (CPDs) that quantify these networks. This increases the space of possible models, enabling the representation of CPDs with a variable number of parameters. The resulting learning procedure induces models that better emulate the interactions present in the data. We describe the theoretical foundations and practical aspects of learning local structures and provide an empirical evaluation of the proposed learning procedure. This evaluation indicates that learning curves characterizing this procedure converge faster, in the number of training instances, than those of the standard procedure, which ignores the local structure of the CPDs. Our results also show that networks learned with local structures tend to be more complex (in terms of a...
81|Learning Bayesian Networks is NP-Complete|Algorithms for learning Bayesian networks from data havetwo components: a scoring metric and a  search procedure. The scoring metric computes a score reflecting the goodness-of-fit of the structure  to the data. The search procedure tries to identify network structures with high scores. Heckerman  et al. (1995) introduce a Bayesian metric, called the BDe metric, that computes the relative posterior  probabilityofanetwork structure given data. In this paper, we show that the search problem of  identifying a Bayesian network---among those where each node has at most K parents---that has a  relative posterior probability greater than a given constant is NP-complete, when the BDe metric  is used.  12.1 
82|Learning Limited Dependence Bayesian Classifiers|We present a framework for characterizing Bayesian classification methods. This framework can be thought of as a spectrum of allowable dependence in a given probabilistic model with the Naive Bayes algorithm at the most restrictive end and the learning of full Bayesian networks at the most general extreme. While much work has been carried out along the two ends of this spectrum, there has been surprising little done along the middle. We analyze the assumptions made as one moves along this spectrum and show the tradeoffs between model accuracy and learning speed which become critical to consider in a variety of data mining domains. We then present a general induction algorithm that allows for traversal of this spectrum depending on the available computational power for carrying out induction and show its application in a number of domains with different properties. Introduction  Recently, work in Bayesian methods for classification has grown enormously (Cooper &amp; Herskovits 1992) (Buntin...
83|Building Classifiers using Bayesian Networks|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state of the art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we examine and evaluate approaches for inducing classifiers from data, based on recent results in the theory of learning Bayesian networks. Bayesian networks are factored representations of probability distributions that generalize the naive Bayes classifier and explicitly represent statements about independence. Among these approacheswe single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness which are characteristic of naive Bayes. We experimentally tested these approaches using benchmark problems from...
84|Searching for Dependencies in Bayesian Classifiers|Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data. 23.1 Introduction The Bayesian classifier (Duda
85|Efficient Learning of Selective Bayesian Network Classifiers|In this paper, we present a computationally efficient method for inducing  selective Bayesian network classifiers. Our approach is to use informationtheoretic metrics to efficiently select a subset of attributes from which to learn the classifier. We explore three conditional, information-theoretic metrics that are extensions of metrics used extensively in decision tree learning, namely Quinlan&#039;s gain and gain ratio metrics and Mantaras&#039;s distance metric. We experimentally show that the algorithms based on gain ratio and distance metric learn selective Bayesian networks that have predictive accuracies as good as or better than those learned by existing selective Bayesian network induction approaches (K2-AS), but at a significantly lower computational cost. We prove that the subset-selection phase of these information-based algorithms has polynomial complexity as compared to the worst-case exponential time complexity of the corresponding phase in K2-AS. We also compare the performance o...
86|Hierarchical model-based motion estimation| This paper describes a hierarchical estimation framework for the computation of diverse representations of motion information. The key features of the resulting framework (or family of algorithms) a,re a global model that constrains the overall structure of the motion estimated, a local rnodel that is used in the estimation process, and a coa,rse-fine refinement strategy. Four specific motion models: affine flow, planar surface flow, rigid body motion, and general optical flow, are described along with their application to specific examples.
87|Determining Optical Flow|Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image. 
88|OBB-Tree: A hierarchical structure for rapid interference detection| We present a data structure and an algorithm for efficient and exact interference detection amongst complex models undergoing rigid motion. The algorithm is applicable to all general polygonal and curved models. It pre-computes a hierarchical representation of models using tight-fitting oriented bounding box trees. At runtime, the algorithm traverses the tree and tests for overlaps between oriented bounding boxes based on a new separating axis theorem, which takes less than 200 operations in practice. It has been implemented and we compare its performance with other hierarchical data structures. In particular, it can accurately detect all the contacts between large complex geometries composed of hundreds of thousands of polygons at interactive rates, almost one order of magnitude faster than earlier methods.
89|The Quickhull algorithm for convex hulls|The convex hull of a set of points is the smallest convex set that contains the points. This article presents a practical convex hull algorithm that combines the two-dimensional Quickhull Algorithm with the general-dimension Beneath-Beyond Algorithm. It is similar to the randomized, incremental algorithms for convex hull and Delaunay triangulation. We provide empirical evidence that the algorithm runs faster when the input contains nonextreme points and that it uses less memory. Computational geometry algorithms have traditionally assumed that input sets are well behaved. When an algorithm is implemented with floating-point arithmetic, this assumption can lead to serious errors. We briefly describe a solution to this problem when computing the convex hull in two, three, or four dimensions. The output is a set of “thick ” facets that contain all possible exact convex hulls of the input. A variation is effective in five or more dimensions.
91|A fast procedure for computing the distance between complex objects in three space|Abstract-An efficient and reliable algorithm for computing the Euclidean distance between a pair of convex sets in Rm is described. Extensive numerical experience with a broad family of polytopes in R3 shows that the computational cost is approximately linear in the total number of vertices specifying the two polytopes. The algorithm has special features which makes its application in a variety of robotics problems attractive. These are discussed and an example of collision detection is given. I.
92|I-COLLIDE: An interactive and exact collision detection system for large-scale environments|We present an exact and interactive collision detection system, I-COLLIDE, for large-scale environments. Such environments are characterized by the number of objects undergoing rigid motion and the complexity of the mod-els. The algorithm does not assume the objects ’ motions can be expressed as a closed form function of time. The collision detection system is general and can be easily in-terfaced with a variety of applications. The algorithm uses a two-level approach based on pruning multiple-object pairs using bounding boxes and performing exact collision detection between selected pairs of polyhedral models. We demonstrate the performance of the system in walkthrough and simulation environments consisting of a large number of moving objects. In particular, the system takes less than l/20 of a second to determine all the collisions and contacts in an environment consisting of more than a 1000 moving polytopes, each consisting of more than 50 faces on an HP-9000/750. 1
93|Efficient Distance Computation between Non-Convex Objects|This paper describes an efficient algorithm for computing the distance between non-convex objects. Objects are modeled as the union of a set of convex components. From this model we construct a hierarchical bounding representation based on spheres. The distance between objects is determined by computing the distance between pairs of convex components using preexisting techniques. The key to efficiency is a simple search routine that uses the bounding representation to ignore most of the possible pairs of components. The efficiency can further be improved by accepting a relative error in the returned result. Several empirical trials are presented to examine the performance of the algorithm.  1. Introduction  Computing the distance between objects is a common problem in robotics. Using a mathematical model of two objects, we find a point on each object such that the distance between the points is minimized. If one object is a robot and the other object is the union of all the obstacles i...
94|Collision Detection for Interactive Graphics Applications|Collision detection and response are important for interactive graphics applications such as vehicle simulators and virtual reality. Unfortunately, previous collision-detection algorithms are too slow for interactive use. This paper presents a new algorithm for rigid or articulated objects that meets performance goals through a form of time-critical computing. The algorithm supports progressive refinement, detecting collisions between successively tighter approximations to object surfaces as the application allows it more processing time. The algorithm uses simple four-dimensional geometry to approximate motion, and hierarchies of spheres to approximate threedimensional surfaces at multiple resolutions. In a sample application, the algorithm allows interactive performance that is not possible with a good previous algorithm. In particular, the new algorithm provides acceptable accuracy while maintaining a steady and high frame rate, which in some cases improves on the previous algorithm...
95|Improved Computational Methods for Ray Tracing|This paper describes algorithmic procedures that have been implemented to reduce the computational expense of producing ray-traced images. The selection of bounding volumes is examined to reduce the computational cost of the ray-intersection test. The use of object coherence, which relies on a hierarchical description of the environment, is then presented. Finally, since the building of the ray-intersection trees is such a large portion of the computation, a method using image coherence is described. This visible-surface preprocessing method, which is dependent upon the creation of an &amp;quot;item buffer, &amp;quot; takes advantage of a priori image information. Examples that indicate the efficiency of these techniques for a variety of representative environments are presented.
97|Incremental algorithms for collision detection between solid models|solid models
98|Interval Methods for Multi-Point Collisions between Time-Dependent Curved Surfaces|We present an efficient and robust algorithm for finding points of collision between time-dependent parametric and implicit surfaces. The algorithm detects simultaneous collisions at multiple points of contact. When the regions of contact form curves or surfaces, it returns a finite set of points uniformly distributed over each contact region.  Collisions can be computed for a very general class of surfaces: those for which inclusion functions can be constructed. Included in this set are the familiar kinds of surfaces and time behaviors encountered in computer graphics.  We use a new interval approach for constrained minimization to detect collisions, and a tangency condition to reduce the dimensionality of the search space. These approaches make interval methods practical for multi-point collisions between complex surfaces. An interval Newton method based on the solution of the interval linear equation is used to speed convergence to the collision time and location. This method is mor...
99|Intersection of Convex Objects in Two and Three Dimensions|One of the basic geometric operations involves determining whether a pair of convex objects intersect. This problem is well understood in a model of computation in which the objects are given as input and their intersection is returned as output. For many applications, however, it may be assumed that the objects already exist within the computer and that the only output desired is a single piece of data giving a common point if the objects intersect or reporting no intersection if they are disjoint. For this problem, none of the previous lower bounds are valid and algorithms are proposed requiring sublinear time for their solution in two and three dimensions.
100|Collision Detection and Analysis in a Physically Based Simulation|We consider the geometric support in detecting and analyzing collisions and contact between arbitrarily shaped polyhedral objects for a physically based simulation. The contact detection is formulated as a static collision-detection problem in three-dimensional space. We address both robustness and efficiency of the problem, and show how both can be achieved by using the brep-index data structure. 1 Introduction  A computer simulation of physical systems that is based on rigid-body dynamics and that involves objects with arbitrary shapes requires the services of a geometric modeling system. The geometric modeling system is used initially to establish mass properties and to formulate constraints for the system&#039;s dynamics. When simulating the motion of objects in the presence of obstacles with possible collisions and prolonged contact, the geometric modeling system is used throughout the simulation. The dynamics modeler must inquire at each time step whether two bodies are about to colli...
101|Hierarchical phrase-based translation|We present a statistical machine translation model that uses hierarchical phrases—phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a parallel text without any syntactic annotations. Thus it can be seen as combining fundamental ideas from both syntax-based translation and phrase-based translation. We describe our system’s training and decoding methods in detail, and evaluate it for translation speed and translation accuracy. Using BLEU as a metric of translation accuracy, we find that our system performs significantly better than the Alignment Template System, a state-of-the-art phrasebased system. 1.
102|BLEU: a Method for Automatic Evaluation of Machine Translation|Human evaluations of machine translation  are extensive but expensive. Human evaluations  can take months to finish and involve  human labor that can not be reused.
103|Statistical phrase-based translation|We propose a new phrase-based translation model and decoding algorithm that enables us to evaluate and compare several, previously proposed phrase-based translation models. Within our framework, we carry out a large number of experiments to understand better and explain why phrase-based models outperform word-based models. Our empirical results, which hold for all examined language pairs, suggest that the highest levels of performance can be obtained through relatively simple means: heuristic learning of phrase translations from word-based alignments and lexical weighting of phrase translations. Surprisingly, learning phrases longer than three words and learning phrases from high-accuracy wordlevel alignment models does not have a strong impact on performance. Learning only syntactically motivated phrases degrades the performance of our systems. 1
104|Improved Statistical Alignment Models|In this paper, we present and compare various single-word based alignment models for statistical machine translation. We discuss the five IBM alignment models, the Hidden-Markov alignment model, smoothing techniques and various modifications.
106|Discriminative Training and Maximum Entropy Models for Statistical Machine Translation|We present a framework for statistical  machine translation of natural languages  based on direct maximum entropy models,  which contains the widely used source  -channel approach as a special case. All  knowledge sources are treated as feature  functions, which depend on the source  language sentence, the target language  sentence and possible hidden variables.
107|The Alignment Template Approach to Statistical Machine Translation|A phrase-based statistical machine translation approach — the alignment template approach — is described. This translation approach allows for general many-to-many relations between words. Thereby, the context of words is taken into account in the translation model, and local changes in word order from source to target language can be learned explicitly. The model is described using a log-linear modeling approach, which is a generalization of the often used source–channel approach. Thereby, the model is easier to extend than classical statistical machine translation systems. We describe in detail the process for learning phrasal translations, the feature functions used, and the search algorithm. The evaluation of this approach is performed on three different tasks. For the German–English speech Verbmobil task, we analyze the effect of various system components. On the French–English Canadian Hansards task, the alignment template system obtains significantly better results than a single-word-based translation model. In the Chinese–English 2002 National Institute of Standards and Technology (NIST) machine translation evaluation it yields statistically significantly better NIST scores than all competing research and commercial translation systems.  
108|Statistical Decision-Tree Models for Parsing|Syntactic natural language parsers have  shown themselves to be inadequate for processing  highly-ambiguous large-vocabulary  text, as is evidenced by their poor per-  formance on domains like the Wall Street  Journal, and by the movement away  from parsing-based approaches to textprocessing  in general. In this paper, I describe  SPATTER, a statistical parser based  on decision-tree learning techniques which  constructs a complete parse for every sentence  and achieves accuracy rates far better  than any published result. This work  is based on the following premises: (1)  grammars are too complex and detailed to  develop manually for most interesting domains;  (2) parsing models must rely heavily  on lexical and contextual information  to analyze sentences accurately; and (3)  existing n-gram modeling techniques are  inadequate for parsing models. In experiments  comparing SPATTER with IBM&#039;s  computer manuals parser, SPATTER significantly  outperforms the grammar-based  parser. Evaluating SPATTER against the  Penn Treebank Wall Street Journal corpus  using the PARSEVAL measures, SPATTER  achieves 86% precision, 86% recall,  and 1.3 crossing brackets per sentence for  sentences of 40 words or less, and 91% precision,  90% recall, and 0.5 crossing brackets  for sentences between 10 and 20 words in  length.
109|A Syntax-based Statistical Translation Model|We present a syntax-based statistical  translation model. Our model transforms  a source-language parse tree  into a target-language string by applying  stochastic operations at each node.  These operations capture linguistic differences  such as word order and case  marking. Model parameters are estimated  in polynomial time using an EM  algorithm. The model produces word  alignments that are better than those  produced by IBM Model 5.  1 
110|A Phrase-Based, Joint Probability Model for Statistical Machine Translation|We present a joint probability model for statistical machine translation, which automatically learns word and phrase equivalents from bilingual corpora. Translations produced with parameters estimated using the joint model are more accurate than translations produced using IBM Model 4.
111|Better k-best parsing|We discuss the relevance of k-best parsing to recent applications in natural language processing, and develop efficient algorithms for k-best trees in the framework of hypergraph parsing. To demonstrate the efficiency, scalability and accuracy of these algorithms, we present experiments on Bikel’s implementation of Collins ’ lexicalized PCFG model, and on Chiang’s CFG-based decoder for hierarchical phrase-based translation. We show in particular how the improved output of our algorithms has the potential to improve results from parse reranking systems and other applications. 1
112|Principles and implementation of deductive parsing|We present a system for generating parsers based directly on the metaphor of parsing as deduction. Parsing algorithms can be represented directly as deduction systems, and a single deduction engine can interpret such deduction systems so as to implement the corresponding parser. The method generalizes easily to parsers for augmented phrase structure formalisms, such as definiteclause grammars and other logic grammar formalisms, and has been used for rapid prototyping of parsing algorithms for a variety of formalisms including variants of tree-adjoining grammars, categorial grammars, and lexicalized context-free grammars.
113|Dependency treelet translation: Syntactically informed phrasal SMT|We describe a novel approach to statistical machine translation that combines syntactic information in the source language with recent advances in phrasal translation. This method requires a source-language dependency parser, target language word segmentation and an unsupervised word alignment component. We align a parallel corpus, project the source dependency parse onto the target sentence, extract dependency treelet translation pairs, and train a tree-based ordering model. We describe an efficient decoder and show that using these treebased models in combination with conventional SMT models provides a promising approach that incorporates the power of phrasal SMT with the linguistic generality available in a parser. 1.
114|Clause restructuring for statistical machine translation|We describe a method for incorporating syntactic information in statistical machine translation systems. The first step of the method is to parse the source language string that is being translated. The second step is to apply a series of transformations to the parse tree, effectively reordering the surface string on the source language side of the translation system. The goal of this step is to recover an underlying word order that is closer to the target language word-order than the original string. The reordering approach is applied as a pre-processing step in both the training and decoding phases of a phrase-based statistical MT system. We describe experiments on translation from German to English, showing an improvement from 25.2 % Bleu score for a baseline system to 26.8 % Bleu score for the system with reordering, a statistically significant improvement.
115|A Polynomial-Time Algorithm for Statistical Machine Translation|We introduce a polynomial-time algorithm  for statistical machine translation. This  algorithm can be used in place of the  expensive, slow best-first search strategies  in current statistical translation architectures.
116|Semiring Parsing|this paper is that all five of these commonly computed  quantities can be described as elements of complete semirings (Kuich 1997). The relationship between grammars and semirings was discovered by Chomsky and Schtitzenberger (1963), and for parsing with the CKY algorithm, dates back to Teitelbaum (1973). A complete semiring is a set of values over which a multiplicative operator and a commutative additive operator have been defined, and for which infinite  summations are defined. For parsing algorithms satisfying certain conditions, the multiplicative and additive operations of any complete semiring can be used in place of/x and , and correct values will be returned. We will give a simple normal form for describing parsers, then precisely define complete semirings, and the conditions for correctness
117|Machine Translation Using Probabilistic Synchronous Dependency Insertion Grammars|Syntax-based statistical machine translation (MT) aims at applying statistical models to structured data. In this paper, we present a syntax-based statistical machine translation system based on a probabilistic synchronous dependency insertion grammar. Synchronous dependency insertion grammars are a version of synchronous grammars defined on dependency trees. We first introduce our approach to inducing such a grammar from parallel corpora. Second, we describe the graphical model for the machine translation task, which can also be viewed as a stochastic tree-to-tree transducer. We introduce a polynomial time decoding algorithm for the model. We evaluate the outputs of our MT system using the NIST and Bleu automatic MT evaluation software. The result shows that our system outperforms the baseline system based on the IBM models in both translation speed and quality.  
118|Learning Dependency Translation Models as Collections of Finite State Head Transducers|The paper defines weighted head transducers,finite-state machines that perform middle-out string transduction. These transducers are strictly more expressive than the special case of standard leftto-right finite-state transducers. Dependency transduction models are then defined as collections of weighted head transducers that are applied hierarchically. A dynamic programming search algorithm is described for finding the optimal transduction of an input string with respect to a dependency transduction model. A method for automatically training a dependency transduction model from a set of input-output example strings is presented. The method first searches for hierarchical alignments of the training examples guided by correlation statistics, and then constructs the transitions of head transducers that are consistent with these alignments. Experimental results are given for applying the training method to translation from English to Spanish and Japanese. 1.
120|The hiero machine translation system: Extensions, evaluation, and analysis|Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1
121|Translating with non-contiguous phrases|This paper presents a phrase-based statistical machine translation method, based on non-contiguous phrases, i.e. phrases with gaps. A method for producing such phrases from a word-aligned corpora is proposed. A statistical translation model is also presented that deals such phrases, as well as a training method based on the maximization of translation accuracy, as measured with the NIST evaluation metric. Translations are produced by means of a beam-search decoder. Experimental results are presented, that demonstrate how the proposed method allows to better generalize from the training data. 1
122|MT for minority languages using elicitation-based learning of syntactic transfer rules|Abstract. The AVENUE project contains a run-time machine translation program that is surrounded by pre- and post-run-time modules. The post-run-time module selects among translation alternatives. The pre-run-time modules are concerned with elicitation of data and automatic learning of transfer rules in order to facilitate the development of machine translation between a language with extensive resources for natural language processing and a language with few resources for natural language processing. This paper describes the run-time transfer-based machine translation system as well as two of the pre-run-time modules: elicitation of data from the minority language and automated learning of transfer rules from the elicited data. Key words: elicitation, rule learning, syntactic transfer rules, minority languages 1.
123|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
124|Randomized Algorithms|Randomized algorithms, once viewed as a tool in computational number theory, have by now found widespread application. Growth has been fueled by the two major benefits of randomization: simplicity and speed. For many applications a randomized algorithm is the fastest algorithm available, or the simplest, or both. A randomized algorithm is an algorithm that uses random numbers to influence the choices it makes in the course of its computation. Thus its behavior (typically quantified as running time or quality of output) varies from
125|The space complexity of approximating the frequency moments|The frequency moments of a sequence containing mi elements of type i, for 1 = i = n, are the numbers Fk = ?n i=1 mki. We consider the space complexity of randomized algorithms that approximate the numbers Fk, when the elements of the sequence are given one by one and cannot be stored. Surprisingly, it turns out that the numbers F0, F1 and F2 can be approximated in logarithmic space, whereas the approximation of Fk for k = 6 requires n?(1) space. Applications to data bases are mentioned as well. 
126|Multiparty Communication Complexity|A given Boolean function has its input distributed among many parties. The aim is to determine which parties to tMk to and what information to exchange with each of them in order to evaluate the function while minimizing the total communication. This paper shows that it is possible to obtain the Boolean answer deterministically with only a polynomial increase in communication with respect to the information lower bound given by the nondeterministic communication complexity of the function.
127|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
128|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
129|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
130|Mining high-speed data streams|Categories and Subject ?????????? ? ?¨?????????????????????????¦???¦??????????¡¤?? ¡  ? ¡????????????????¦¡¤????§?£????
131|Online Aggregation|Aggregation in traditional database systems is performed in batch mode: a query is submitted, the system processes a large volume of data over a long period of time, and, eventually, the final answer is returned. This archaic approach is frustrating to users and has been abandoned in most other areas of computing. In this paper we propose a new online aggregation interface that permits users to both observe the progress of their aggregation queries and control execution on the fly. After outlining usability and performance requirements for a system supporting online aggregation, we present a suite of techniques that extend a database system to meet these requirements. These include methods for returning the output in random order, for providing control over the relative rate at which different aggregates are computed, and for computing running confidence intervals. Finally, we report on an initial implementation of online aggregation in postgres. 1 Introduction  Aggregation is an incre...
132|Efficient Filtering of XML Documents for Selective Dissemination of Information|Information Dissemination applications are gaining increasing  popularity due to dramatic improvements in  communications bandwidth and ubiquity. The sheer  volume of data available necessitates the use of selective  approaches to dissemination in order to avoid overwhelming  users with unnecessaryinformation. Existing  mechanisms for selective dissemination typically rely  on simple keyword matching or &#034;bag of words&#034; information  retrieval techniques. The advent of XML as a  standard for information exchangeand the development  of query languages for XML data enables the development  of more sophisticated filtering mechanisms that  take structure information into account. We have developed  several index organizations and search algorithms  for performing efficient filtering of XML documents for  large-scale information dissemination systems. In this  paper we describe these techniques and examine their  performance across a range of document, workload, and  scale scenarios.  1 
133|External Memory Algorithms and Data Structures|Data sets in large applications are often too massive to fit completely inside the computer&#039;s internal memory. The resulting input/output communication (or I/O) between fast internal memory and slower external memory (such as disks) can be a major performance bottleneck. In this paper, we survey the state of the art in the design and analysis of external memory algorithms and data structures (which are sometimes referred to as &#034;EM&#034; or &#034;I/O&#034; or &#034;out-of-core&#034; algorithms and data structures). EM algorithms and data structures are often designed and analyzed using the parallel disk model (PDM). The three machine-independent measures of performance in PDM are the number of I/O operations, the CPU time, and the amount of disk space. PDM allows for multiple disks (or disk arrays) and parallel CPUs, and it can be generalized to handle tertiary storage and hierarchical memory. We discuss several important paradigms for how to solve batched and online problems efficiently in external memory. Programming tools and environments are available for simplifying the programming task. The TPIE system (Transparent Parallel I/O programming Environment) is both easy to use and efficient in terms of execution speed. We report on some experiments using TPIE in the domain of spatial databases. The newly developed EM algorithms and data structures that incorporate the paradigms we discuss are significantly faster than methods currently used in practice.  
134|Random sampling with a reservoir|We introduce fast algorithms for selecting a random sample of n records without replacement from a pool of N records, where the value of N is unknown beforehand. The main result of the paper is the design and analysis of Algorithm Z; it does the sampling in one pass using constant space and in O(n(1 + log(N/n))) expected time, which is optimum, up to a constant factor. Several optimizations are studied that collectively improve the speed of the naive version of the algorithm by an order of magnitude. We give an efficient Pascal-like implementation that incorporates these modifications and that is suitable for general use. Theoretical and empirical results indicate that Algorithm Z outperforms current methods by a significant margin.
135|Stable Distributions, Pseudorandom Generators, Embeddings and Data Stream Computation|In this paper we show several results obtained by combining the use of stable distributions with pseudorandom generators for bounded space. In particular: ffl we show how to maintain (using only O(log n=ffl  2  ) words of storage) a sketch C(p) of a point p 2 l  n  1 under dynamic updates of its coordinates, such that given sketches C(p) and C(q) one can estimate jp \Gamma qj 1 up to a factor of (1 + ffl) with large probability. This solves the main open problem of [10].  ffl we obtain another sketch function C  0  which maps l  n  1 into a normed space l  m  1 (as opposed to C), such that m = m(n) is much smaller than n; to our knowledge this is the first dimensionality reduction lemma for l 1 norm  ffl we give an explicit embedding of l  n  2 into l  n  O(log n) 1  with distortion (1 + 1=n  \Theta(1)  ) and a nonconstructive embedding of l  n  2 into l  O(n)  1 with distortion  (1 + ffl) such that the embedding can be represented using only O(n log  2  n) bits (as opposed to at least...
136|Mining time-changing data streams|Most statistical and machine-learning algorithms assume that the data is a random sample drawn from a station-ary distribution. Unfortunately, most of the large databases available for mining today violate this assumption. They were gathered over months or years, and the underlying pro-cesses generating them changed during this time, sometimes radically. Although a number of algorithms have been pro-posed for learning time-changing concepts, they generally do not scale well to very large databases. In this paper we propose an efficient algorithm for mining decision trees from continuously-changing data streams, based on the ultra-fast VFDT decision tree learner. This algorithm, called CVFDT, stays current while making the most of old data by growing an alternative subtree whenever an old one becomes ques-tionable, and replacing the old with the new when the new becomes more accurate. CVFDT learns a model which is similar in accuracy to the one that would be learned by reapplying VFDT to a moving window of examples every time a new example arrives, but with O(1) complexity per example, as opposed to O(w), where w is the size of the window. Experiments on a set of large time-changing data streams demonstrate the utility of this approach.
137|Continuous Queries over Data Streams|In many recent applications, data may take the form of continuous data streams, rather than finite stored data sets. Several aspects of data management need to be re-considered in the presence of data streams, offering a new research direction for the database community. In this pa-per we focus primarily on the problem of query processing, specifically on how to define and evaluate continuous queries over data streams. We address semantic issues as well as efficiency concerns. Our main contributions are threefold. First, we specify a general and flexible architecture for query processing in the presence of data streams. Second, we use our basic architecture as a tool to clarify alternative semantics and processing techniques for continuous queries. The architecture also captures most previous work on continuous queries and data streams, as
139|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
140|Maintaining Stream Statistics over Sliding Windows (Extended Abstract)  (2002) |Mayur Datar  Aristides Gionis  y  Piotr Indyk  z  Rajeev Motwani  x  Abstract  We consider the problem of maintaining aggregates and statistics over data streams, with respect to the last N data elements seen so far. We refer to this model as the sliding window model. We consider the following basic problem: Given a stream of bits, maintain a count of the number of 1&#039;s in the last N elements seen from the stream. We show that using O(  1  ffl log  2  N) bits of memory, we can estimate the number of 1&#039;s to within a factor of 1 + ffl. We also give a matching lower bound of \Omega\Gamma  1  ffl log  2  N) memory bits for any deterministic or randomized algorithms. We extend our scheme to maintain the sum of the last N positive integers. We provide matching upper and lower bounds for this more general problem as well. We apply our techniques to obtain efficient algorithms for the Lp norms (for p 2 [1; 2]) of vectors under the sliding window model. Using the algorithm for the basic counting problem, one can adapt many other techniques to work for the sliding window model, with a multiplicative overhead of O(  1  ffl log N) in memory and a 1 + ffl factor loss in accuracy. These include maintaining approximate histograms, hash tables, and statistics or aggregates such as sum and averages.
141|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
142|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
143|Trajectory Sampling for Direct Traffic Observation|Traffic measurement is a critical component for the control and engineering of communication networks. We argue that traffic measurement should make it possible to obtain the spatial flow of traffic through the domain, i.e., the paths followed by packets between any ingress and egress point of the domain. Most resource allocation and capacity planning tasks can benefit from such information. Also, traffic measurements should be obtained without a routing model and without knowledge of network state. This allows the traffic measurement process to be resilient to network failures and state uncertainty.  We propose a method that allows the direct inference of traffic flows through a domain by observing the trajectories of a subset of all packets traversing the network. The key advantages of the method are that (i) it does not rely on routing state, (ii) its implementation cost is small, and (iii) the measurement reporting traffic is modest and can be controlled precisely. The key idea of the method is to sample packets based on a hash function computed over the packet content. Using the same hash function will yield the same sample set of packets in the entire domain, and enables us to reconstruct packet trajectories.  I. 
144|  Wavelet-Based Histograms for Selectivity Estimation |Query optimization is an integral part of relational database management systems. One important task in query optimization is selectivity estimation, that is, given a query P, we need to estimate the fraction of records in the database that satisfy P. Many commercial database systems maintain histograms to approximate the frequency distribution of values in the attributes of relations. In this paper, we present a technique based upon a multiresolution wavelet decomposition for building histograms on the underlying data distributions, with applications to databases, statistics, and simulation. Histograms built on the cumulative data values give very good approximations with limited space usage. We give fast algorithms for constructing histograms and using
145|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
146|Surfing wavelets on streams: One-pass summaries for approximate aggregate queries|Abstract We present techniques for computing small spacerepresentations of massive data streams. These are inspired by traditional wavelet-based approx-imations that consist of specific linear projections of the underlying data. We present general&amp;quot;sketch &amp;quot; based methods for capturing various linear projections of the data and use them to pro-vide pointwise and rangesum estimation of data streams. These methods use small amounts ofspace and per-item time while streaming through the data, and provide accurate representation asour experiments with real data streams show.
147|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
148|Space-Efficient Online Computation of Quantile Summaries|An &amp;epsilon;-approximate quantile summary of a sequence of N elements is a data structure that can answer quantile queries about the sequence to within a precision of &amp;epsilon;N . We present a new online...
149| Approximate Computation of Multidimensional Aggregates of Sparse Data Using Wavelets |Computing multidimensional aggregates in high dimensions is a performance bottleneck for many OLAP applications. Obtaining the exact answer to an aggregation query can be prohibitively expensive in terms of time and/or storage space in a data warehouse environment. It is advantageous to have fast, approximate answers to OLAP aggregation queries. In this paper, we present anovel method that provides approximate answers to high-dimensional OLAP aggregation queries in massive sparse data sets in a time-efficient and space-efficient manner. We construct a compact data cube, which is an approximate and space-efficient representation of the underlying multidimensional array, based upon a multiresolution wavelet decomposition. In the on-line phase, each aggregation query can generally be answered using the compact data cube in one I/O or a small number of I/Os, depending upon the desired accuracy. We present two I/O-efficient algorithms to construct the compact data cube for the important case of sparse high-dimensional arrays, which often arise in practice. The traditional histogram methods are infeasible for the massive high-dimensional data sets in OLAP applications. Previously developed wavelet techniques are efficient only for dense data. Our on-line query processing algorithm is very fast and capable of refining answers as the user demands more accuracy. Experiments on real data show that our method provides significantly more accurate results for typical OLAP aggregation queries than other efficient approximation techniques such as random sampling.
150|Computing on Data Streams|In this paper we study the space requirement of algorithms that make  only one (or a small number of) pass(es) over the input data. We study such  algorithms under a model of data streams that we introduce here. We give  a number of upper and lower bounds for problems stemming from queryprocessing,  invoking in the process tools from the area of communication  complexity.
151|Processing Complex Aggregate Queries over Data Streams|Recent years have witnessed an increasing interest in designing algorithms for querying and analyzing streaming data (i.e., data that is seen only once in a fixed order) with only limited memory. Providing (perhaps approximate) answers to queries over such continuous data streams is a crucial requirement for many application environments; examples include large telecom and IP network installations where performance data from different parts of the network needs to be continuously collected and analyzed.
152|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
153|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
154|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
155|Reductions in Streaming Algorithms, with an Application to Counting Triangles in Graphs |We introduce reductions in the streaming model as a tool in the design of streaming algorithms. We develop
156|Computing iceberg queries efficiently|Many applications compute aggregate functions...
157|Data-Streams and Histograms|Histograms have been used widely to capture data distribution, to represent the data by a small number of step functions. Dynamic programming algorithms which provide optimal construction of these histograms exist, albeit running in quadratic time and linear space. In this paper we provide linear time construction of 1 + epsilon approximation of optimal histograms, running in polylogarithmic space. Our results extend to the context of data-streams, and in fact generalize to give 1 + epsilon approximation of several problems in data-streams which require partitioning the index set into intervals. The only assumptions required are that the cost of an interval is monotonic under inclusion (larger interval has larger cost) and that the cost can be computed or approximated in small space. This exhibits a nice class of problems for which we can have near optimal data-stream algorithms.
158|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
159|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
160|Making Views Self-Maintainable for Data Warehousing|A data warehouse stores materialized views over data from one or more sources in order to provide fast access to the integrated data, regardless of the availability of the data sources. Warehouse views need to be maintained inresponse to changes to the base data in the sources. Except for very simple views, maintaining a warehouse view requires access to data that is not available in the view itself. Hence, to maintain the view, one either has to query the data sources or store auxiliary data in the warehouse. We show that by using key and referential integrity constraints, we often can maintain a select-project-join view without going to the data sources or replicating the base relations in their entirety in the warehouse. We derive a set of auxiliary views such that the warehouse view and the auxiliary views together are self-maintainable|they can be maintained without going to the data sources or replicating all base data. In addition, our technique can be applied to simplify traditional materialized view maintenance by exploiting key and referential integrity constraints. 1
161|Approximate Medians and other Quantiles in One Pass and with Limited Memory|We present new algorithms for computing approximate quantiles of large datasets in a single pass. The approximation guarantees are explicit, and apply without regard to the value distribution or the arrival distributions of the dataset. The main memory requirements are smaller than those reported earlier by an order of magnitude.  We also discuss methods that couple the approximation algorithms with random sampling to further reduce memory requirements. With sampling, the approximation guarantees are explicit but probabilistic, i.e., they apply with respect to a (user controlled) confidence parameter.  We present the algorithms, their theoretical analysis and simulation results.  1 Introduction  This article studies the problem of computing order statistics  of large sequences of online or disk-resident data using as little main memory as possible. We focus on computing  quantiles, which are elements at specific positions in the sorted order of the input. The OE-quantile, for OE 2 [0; ...
162|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
163|Sampling From a Moving Window Over Streaming Data|We introduce the problem of sampling from a moving window of recent items from a data stream and develop the &#034;chain-sample&#034; and &#034;priority-sample&#034; algorithms for this problem.
164|Tracking join and self-join sizes in limited storage|This paper presents algorithms for tracking (approximate) join and self-join sizes in limited storage, in the presence of insertions and deletions to the data set(s). Such algorithms detect changes in join and self-join sizes without an expensive recomputation from the base data, and without the large space overhead required to maintain such sizes exactly. Query optimizers rely on fast, high-quality estimates of join sizes in order to select between various join plans, and estimates of self-join sizes are used to indicate the degree of skew in the data. For self-joins, we considertwo approaches proposed in [Alon, Matias, and Szegedy. The Space Complexity of Approximating the Frequency Moments. JCSS, vol. 58, 1999, p.137-147], which we denote tug-of-war and sample-count. Wepresent fast algorithms for implementing these approaches, and extensions to handle deletions as well as insertions. We also report on the rst experimental study of the two approaches, on a range of synthetic and real-world data sets. Our study shows that tug-of-war provides more accurate estimates for a given storage limit than sample-count, which in turn is far more accurate than a standard sampling-based approach. For example, tug-of-war needed only 4{256 memory words, depending on the data set, in order to estimate the self-join size
165|Data Cube Approximation and Histograms via Wavelets (Extended Abstract)  (1998) |) Jeffrey Scott Vitter   Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  jsv@cs.duke.edu Min Wang  y Center for Geometric Computing and Department of Computer Science Duke University Durham, NC 27708--0129 USA  minw@cs.duke.edu Bala Iyer  Database Technology Institute IBM Santa Teresa Laboratory P.O. Box 49023 San Jose, CA 95161 USA  balaiyer@vnet.ibm.com Abstract  There has recently been an explosion of interest in the analysis of data in data warehouses in the field of On-Line Analytical Processing (OLAP). Data warehouses can be extremely large, yet obtaining quick answers to queries is important. In many situations, obtaining the exact answer to an OLAP query is prohibitively expensive in terms of time and/or storage space. It can be advantageous to have fast, approximate answers to queries. In this paper, we present an I/O-efficient technique based upon a multiresolution wavelet decomposition that yields an approximate a...
166|The design and implementation of a sequence database system|This paper discusses the design and implementation of SEQ, a database system with support for sequence data. SEQ models a sequence as an ordered collection of records, and supports a declarative sequence query language based on an algebra of query operators, thereby permitting algebraic query optimization and evaluation. SEQ has been built as a component of the PREDATOR database system that provides support for relational and other kinds of complex data as well. There are three distinct contributions made in this paper. (1) We describe the specification of sequence queries using the SEQUIN query language. (2) We quantitatively demonstrate the importance of various storage and optimization techniques by studying their effect on performance. (3) We present a novel nested design paradigm used in PREDATOR to combine sequence ‘and relational data.
167|Random sampling techniques for space efficient online computation of order statistics of large datasets|In a recent paper [MRL98], we had described a general framework for single pass approximate quantile nding algorithms. This framework included several known algorithms as special cases. We had identi ed a new algorithm, within the framework, which had a signi cantly smaller requirement for main memory than other known algorithms. In this paper, we address two issues left open in our earlier paper. First, all known and space e cient algorithms for approximate quantile nding require advance knowledge of the length of the input sequence. Many important database applications employing quantiles cannot provide this information. In this paper, we present anovel non-uniform random sampling scheme and an extension of our framework. Together, they form the basis of a new algorithm which computes approximate quantiles without knowing the input sequence length. Second, if the desired quantile is an extreme value (e.g., within the top 1 % of the elements), the space requirements of currently known algorithms are overly pessimistic. We provide a simple algorithm which estimates extreme values using less space than required by the earlier more general technique for computing all quantiles. Our principal observation here is that random sampling is quanti ably better when estimating extreme values than is the case with the median.  
168|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
169|Congressional samples for approximate answering of group-by queries|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex decision support queries using precomputed summary statistics, such as samples. Decision support queries routinely segment the data into groups and then aggregate the information in each group (group-by queries). Depending on the data, there can be a wide disparity between the number of data items in each group. As a result, approximate answers based on uniform random samples of the data can result in poor accuracy for groups with very few data items, since such groups will be represented in the sample by very few (often zero) tuples. In this paper, we propose a general class of techniques for obtaining fast, highly-accurate answers for group-by queries. These techniques rely on precomputed non-uniform (biased) samples of the data. In particular, we proposecongressional samples, ahybrid union of uniform and biased samples. Given a xed amount of space, congressional samples seek to maximize the accuracy for all possible group-by queries on a set of columns. We present a one pass algorithm for constructing a congressional sample and use this technique to also incrementally maintain the sample up-to-date without accessing the base relation. We also evaluate query rewriting strategies for providing approximate answers from congressional samples. Finally, we conduct an extensive set of experiments on the TPC-D database, which demonstrates the e cacy of the techniques proposed. 1
170|Histogram-Based Approximation of Set-Valued Query Answers|Answering queries approximately has recently  been proposed as a way to reduce query response  times in on-line decision support systems,  when the precise answer is not necessary  or early feedback is helpful. Most of the  work in this area uses sampling-based techniques  and handles aggregate queries, ignoring  queries that return relations as answers. In  this paper, we extend the scope of approximate  query answering to general queries. We propose  a novel and intuitive error measure for  quantifying the error in an approximate query  answer, which can be a multiset in general.
171|Data Integration using Self-Maintainable Views|.  In this paper we de#ne the concept of self-maintainable views  # these are views that can be maintained using only the contents of  the view and the database modi#cations, without accessing any of the  underlying databases. We derive tight conditions under which several  types of select-project-join are self-maintainable upon insertions, deletions  and updates. Self-Maintainability is a desirable property for e#-  ciently maintaining large views in applications where fast response and  high availability are important. One example of suchanenvironment  is data warehousing wherein views are used for integrating data from  multiple databases.  1 Introduction  Most large organizations have related data in distinct databases. Many of these databases may be legacy systems, or systems separated for organizational reasons like funding and ownership. Integrating data from such distinct databases is a pressing business need. A common approach for integration is to de#ne an integrated view and...
172|Monitoring XML Data on the Web|We consider the monitoring of a flow of incoming documents. More precisely, we present here the monitoring used in a very large warehouse built from XML documents found on the web. The flow of documents consists in XML pages (that are warehoused) and HTML pages (that are not). Our contributions are the following:  ffl a subscription language which specifies the monitoring of pages when fetched, the periodical evaluation of continuous queries and the production of XML reports. ffl the description of the architecture of the system we implemented that makes it possible to monitor a flow of millions of pages per day with millions of subscriptions on a single PC, and scales up by using more machines. ffl a new algorithm for processing alerts that can be used in a wider context. We support
174|Expiring data in a warehouse|Data warehouses collect data into materi-alized views for analysis. After some time, some of the data may no longer be needed or may not be of interest. In this pa-per, we handle this by expiring or remov-ing unneeded materialized view tuples. A framework supporting such expiration is presented. Within it, a user or adminis-trator can declaratively request expirations and can specify what type of modifications are expected from external sources. The lat-ter can significantly increase the amount of data that can be expired. We present effi-cient algorithms for determining what data can be expired (data not needed for main-tenance of other views), taking into account the types of updates that may occur. 1
176|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
177|SEQ: A Model for Sequence Databases|This paper presents the model which is the basis for a system to manage various kinds of sequence data. The model separates the data from the ordering information, and includes operators based on two distinct abstractions of a sequence. The main contributions of the model are: (a) it can deal with different types of sequence data, (b) it supports an expressive range of sequence queries, (c) it draws from many of the diverse existing approaches to modeling sequence data. 1
178|Sampling Algorithms: Lower Bounds and Applications (Extended Abstract)  (2001) |]  Ziv Bar-Yossef  y  Computer Science Division  U. C. Berkeley  Berkeley, CA 94720  zivi@cs.berkeley.edu  Ravi Kumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  ravi@almaden.ibm.com  D. Sivakumar  IBM Almaden  650 Harry Road  San Jose, CA 95120  siva@almaden.ibm.com  ABSTRACT  We develop a framework to study probabilistic sampling algorithms that approximate general functions of the form f : A  n  ! B, where A and B are arbitrary sets. Our goal is to obtain lower bounds on the query complexity of functions, namely the number of input variables x i that any sampling algorithm needs to query to approximate f(x1 ; : : : ; xn ).  We define two quantitative properties of functions --- the block sensitivity and the minimum Hellinger distance --- that give us techniques to prove lower bounds on the query complexity. These techniques are quite general, easy to use, yet powerful enough to yield tight results. Our applications include the mean and higher statistical moments, the median and other selection functions, and the frequency moments, where we obtain lower bounds that are close to the corresponding upper bounds.  We also point out some connections between sampling and streaming algorithms and lossy compression schemes.  1. 
179|Approximating a Data Stream for Querying and Estimation: Algorithms and Performance Evaluation|Obtaining fast and good quality approximations to data distributions is a problem of central interest to database management. A variety of popular database applications including, approximate querying, similarity searching and data mining in most application domains, rely on such good quality approximations. Histogram based approximation is a very popular method in database theory and practice to succinctly represent a data distribution in a space efficient manner. In this paper, we place the problem of histogram construction into perspective and we generalize it by raising the requirement of a finite data set and/or known data set size. We consider the case of an infinite data set on which data arrive continuously forming an infinite data stream. In this context, we present the first single pass algorithms capable of constructing histograms of provable good quality. We present algorithms for the fixed window variant of the basic histogram construction problem, supporting incremental maintenance of the histograms. The proposed algorithms trade accuracy for speed and allow for a graceful tradeoff between the two, based on application requirements. In the case of approximate queries on infinite data streams, we present a detailed experimental evaluation comparing our algorithms with other applicable techniques using real data sets, demonstrating the superiority of our proposal. 1
180|A robust, optimization-based approach for approximate answering of aggregate queries|The ability to approximately answer aggregation queries accurately and efficiently is of great benefit for decision support and data mining tools. In contrast to previous sampling-based studies, we treat the problem as an optimization problem whose goal is to minimize the error in answering queries in the given workload. A key novelty of our approach is that we can tailor the choice of samples to be robust even for workloads that are “similar ” but not necessarily identical to the given workload. Finally, our techniques recognize the importance of taking into account the variance in the data distribution in a principled manner. We show how our solution can be implemented on a database system, and present results of extensive experiments on Microsoft SQL Server 2000 that demonstrate the superior quality of our method compared to previous work. 1
181|Online Dynamic Reordering for Interactive Data Processing|Abstract We present a pipelining, dynamically user-controllable reorder operator, for use in data-intensive applications. Allowing the user to reorder the data delivery on the fly increases the interactivity in several contexts such as online aggregation and large-scale spreadsheets; it allows the user to control the processing of data by dynamically specifying preferences for different data items based on prior feedback, so that data of interest is prioritized for early processing.
183|On Sampling and Relational Operators|A major bottleneck in implementing sampling as a primitive relational operation is the inefficiency of sampling the output of a query. We highlight the primary difficulties, summarize the results of some recent work in this area, and indicate directions for future work.   
184|Hierarchical mixtures of experts and the EM algorithm|We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM’s). Learning is treated as a max-imum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parame-ters of the architecture. We also develop an on-line learning algorithm in which the pa-rameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain. 
185|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
186|Multivariate adaptive regression splines|A new method is presented for flexible regression modeling of high dimensional data. The model takes the form of an expansion in product spline basis functions, where the number of basis functions as well as the parameters associated with each one (product degree and knot locations) are automatically determined by the data. This procedure is motivated by the recursive partitioning approach to regression and shares its attractive properties. Unlike recursive partitioning, however, this method produces continuous models with continuous derivatives. It has more power and flexibility to model relationships that are nearly additive or involve interactions in at most a few variables. In addition, the model can be represented in a form that separately identifies the additive contributions and those associated with the different multivariable interactions.
187|Hierarchies of adaptive experts|In this paper we present a neural network architecture that discovers a recursive decomposition of its input space. Based on a generalization of the modular architecture of Jacobs, Jordan, Nowlan, and Hinton (1991), the architecture uses competition among networks to recursively split the input space into nested regions and to learn separate associative mappings within each region. The learning algorithm is shown to perform gradient ascent in a log likelihood function that captures the architecture&#039;s hierarchical structure. 1
188|Limma: linear models for microarray data|This free open-source software implements academic research by the authors and co-workers. If you use it, please support the project by citing the appropriate journal articles listed in Section 2.1.Contents
190|Normalization for cDNA microarray data: a robust composite method addressing single and multiple slide systematic variation|There are many sources of systematic variation in cDNA microarray experiments which affect the measured gene expression levels (e.g. differences in labeling efficiency between the two fluorescent dyes). The term normalization refers to the process of removing such variation. A constant adjustment is often used to force the distribution of the intensity log ratios to have a median of zero for each slide. However, such global normalization approaches are not adequate in situations where dye biases can depend on spot overall intensity and/or spatial location within the array. This article proposes normalization methods that are based on robust local regression and account for intensity and spatial dependence in dye biases for different types of cDNA microarray experiments. The selection of appropriate controls for normalization is discussed and a novel set of controls (microarray sample pool, MSP) is introduced to aid in intensity-dependent normalization. Lastly, to allow for comparisons of expression levels across slides, a robust method based on maximum likelihood estimation is proposed to adjust for scale differences among slides.
191|Normalization of cDNA microarray data|Normalization means to adjust microarray data for effects which arise from variation in the technology rather than from biological differences between the RNA samples or between the printed probes. This article describes normalization methods based on the fact that dye balance typically varies with spot intensity and with spatial position on the array. Print-tip loess normalization provides a well-tested general purpose normalization method which has given good results on a wide range of arrays. The method may be refined by using quality weights for individual spots. The method is best combined with diagnostic plots of the data which display the spatial and intensity trends. When diagnostic plots show that biases still remain in the data after normalization, further normalization steps such as plate-order normalization or scalenormalization between the arrays may be undertaken. Composite normalization may be used when control spots are available which are known to be not differentially expressed. Variations on loess normalization include global loess normalization and 2D normalization. Detailed commands are given to implement the normalization techniques using freely available software. 1
192|Use of within-array replicate spots for assessing differential expression in microarray experiments|Motivation. Spotted arrays are often printed with probes in duplicate or triplicate, but current methods for assessing differential expression are not able to make full use of the resulting information. Usual practice is to average the duplicate or triplicate results for each probe before assessing differential expression. This loses valuable information about gene-wise variability. Results. A method is proposed for extracting more information from within-array replicate spots in microarray experiments by estimating the strength of the correlation between them. The method involves fitting separate linear models to the expression data for each gene but with a common value for the between-replicate correlation. The method greatly improves the precision with which the genewise variances are estimated and thereby improves inference methods designed to identify differentially expressed genes. The method may be combined with empirical Bayes methods for moderating the genewise variances between genes. The method is validated using data from a microarray experiment involving calibration and ratio control spots in conjunction with spiked-in RNA. Comparing results for calibration and ratio control spots shows that the common correlation method results in substantially better discrimination of differentially expressed genes from those which are not. The spike-in experiment also confirms that the results may be further improved by empirical Bayes smoothing of the variances when the sample size is small. Availability. The methodology is implemented in the limma software package for R, available from the CRAN repository
193|Identifying differentially expressed genes using false discovery rate controlling procedures|Motivation: DNA microarrays have recently been used for the purpose of monitoring expression levels of thousands of genes simultaneously and identifying those genes that are differentially expressed. The probability that a false identification (type I error) is committed can increase sharply when the number of tested genes gets large. Correlation between the test statistics attributed to gene co-regulation and dependency in the measurement errors of the gene expression levels further complicates the problem. In this paper we address this very large multiplicity problem by adopting the false discovery rate (FDR) controlling approach. In order to address the dependency problem, we present three resampling-based FDR controlling procedures, that account for the test statistics distribution, and compare their performance to that of the naïve application of the linear step-up procedure in Benjamini and Hochberg (1995). The procedures are studied using simulated microarray data, and their performance is examined relative to their ease of implementation. Results: Comparative simulation analysis shows that all four FDR controlling procedures control the FDR at the desired level, and retain substantially more power then the family-wise error rate controlling procedures. In terms of power, using resampling of the marginal distribution of each test statistics substantially improves the performance over the naïve one. The highest power is achieved, at the expense of a more sophisticated algorithm, by the resampling-based procedures that resample the joint distribution of the test statistics and estimate the level of FDR control.
194|Assessing Gene Significance from cDNA Microarray Expression Data via Mixed Models|The determination of a list of differentially expressed genes is a basic objective in many cDNA microarray experiments. We present a statistical approach that allows direct control over the percentage of false positives in such a list and, under certain reasonable assumptions, improves on existing methods with respect to the percentage of false negatives. The method accommodates a wide variety of experimental designs and can simultaneously assess significant differences between multiple types of biological samples. Two interconnected mixed linear models are central to the method and provide a flexible means to properly account for variability both across and within genes. The mixed model also provides a convenient framework for evaluating the statistical power of any particular experimental design and thus enables a researcher to a priori select an appropriate number of replicates. We also suggest some basic graphics for visualizing lists of significant genes. Analyses of published experiments studying human cancer and yeast cells illustrate the results.
195|Statistical Issues in cDNA Microarray Data Analysis|This article summarizes some of the issues involved and provides a brief review of the analysis tools which are available to researchers to deal with them. Any microarray experiment involves a number of distinct stages. Firstly there is the design of the experiment. The researchers must decide which genes are to be printed on the arrays, which sources of RNA are to be hybridized to the arrays and on how many arrays the hybridizations will be replicated. Secondly, after hybridization, there follows a number of data-cleaning steps or `low-level analysis&#039; of the microarray data. The microarray images must be processed to acquire red and green foreground and background intensities for each spot. The acquired red/green ratios must be normalized to adjust for dye-bias and for any systematic variation other than that due to the differences between the RNA samples being studied. Thirdly, the normalized ratios are analyzed by various graphical and numerical means to select differentially expressed genes or to find groups of genes whose expression profiles can reliably classify the different RNA sources into meaningful groups. The sections of this article correspond roughly to the various analysis steps. The following notation will be used throughout the article. The foreground red and green intensities will be written Pp and 9p for each spot. The background intensities will be Pf and 9f . The background-corrected intensities will be P and 9 where usually P Pp Pf 0 # and 9 9p 9f 0 # . The log-differential expression ratio will be vyq # E P 9 0 for each spot. Finally, the log-intensity of the spot will be   vyq 3 P9 0 , a measure of the overall brightness of the spot. (The letter E is a mnemonic for minus as vyq vyq E P 9 0 # while 3 is a mnemonic for add as #vyq vyq #...
196|The Subread aligner: fast, accurate and scalable read mapping by seed-and-vote. Nucleic Acids Res 2013;41:e108 |Read alignment is an ongoing challenge for the analysis of data from sequencing technologies. This article proposes an elegantly simple multi-seed strategy, called seed-and-vote, for mapping reads to a reference genome. The new strategy chooses the mapped genomic location for the read directly from the seeds. It uses a relatively large number of short seeds (called subreads) extracted from each read and allows all the seeds to vote on the optimal location. When the read length is &lt;160bp, overlapping subreads are used. More con-ventional alignment algorithms are then used to fill in detailed mismatch and indel information between the subreads that make up the winning voting block. The strategy is fast because the overall genomic location has already been chosen before the detailed alignment is done. It is sensitive because no individual subread is required to map exactly, nor are individual subreads constrained to map close by other subreads. It is accurate because the final location must be supported by several dif-ferent subreads. The strategy extends easily to find exon junctions, by locating reads that contain sets of subreads mapping to different exons of the same gene. It scales up efficiently for longer reads.
197|CARMAweb: comprehensive R- and Bioconductor-based web service for microarray data analysis|web service for microarray data analysis
198|Optimizing the noise versus bias trade-off for Illumina whole genome expression BeadChips. Nucleic acids research. 2010; 38:e204. [PubMed: 20929874 |Five strategies for pre-processing intensities from Illumina expression BeadChips are assessed from the point of view of precision and bias. The strategies include a popular variance stabilizing transformation and model-based background cor-rections that either use or ignore the control probes. Four calibration data sets are used to evaluate precision, bias and false discovery rate (FDR). The original algorithms are shown to have operating characteristics that are not easily com-parable. Some tend to minimize noise while others minimize bias. Each original algorithm is shown to have an innate intensity offset, by which unlogged intensities are bounded away from zero, and the size of this offset determines its position on the noise–bias spectrum. By adding extra offsets, a continuum of related algorithms with different noise–bias trade-offs is generated, allowing direct comparison of the performance of the strategies on equivalent terms. Adding a positive offset is shown to decrease the FDR of each original algo-rithm. The potential of each strategy to generate an algorithm with an optimal noise–bias trade-off is explored by finding the offset that minimizes its FDR. The use of control probes as part of the back-ground correction and normalization strategy is shown to achieve the lowest FDR for a given bias.
199|Estimating the proportion of microarray probes expressed in an RNA sample. Nucleic acids research 38|A fundamental question in microarray analysis is the estimation of the number of expressed probes in different RNA samples. Negative control probes available in the latest microarray platforms, such as Illumina whole genome expression BeadChips, provide a unique opportunity to estimate the number of expressed probes without setting a threshold. A novel algorithm was proposed in this study to estimate the number of expressed probes in an RNA sample by utilizing these negative controls to measure background noise. The perfor-mance of the algorithm was demonstrated by comparing different generations of Illumina BeadChips, comparing the set of probes targeting well-characterized RefSeq NM transcripts with other probes on the array and comparing pure samples with heterogenous samples. Furthermore, hematopoietic stem cells were found to have a larger transcriptome than progenitor cells. Aire knockout medullary thymic epithelial cells were shown to have significantly less expressed probes than matched wild-type cells.
200|arrayMagic: twocolour cDNA microarray quality control and preprocessing|Summary: arrayMagic is a software package for quality control and preprocessing of two-colour cDNA microarray data. The automated analysis pipeline comprises data import, normalization, replica merging, quality diagnostics and data export. The script-based processing combines reproducibility and flexibility at high-throughput and provides quality-assured and preprocessed microarray data to high-level follow-up analysis. Availability: The R package arrayMagic is available with BSD license at
201|doi:10.1093/nar/gkp366 Pomelo II: finding differentially expressed genes|Pomelo II
202|GK: Individual channel analysis of two-colour microarray data|The traditional approach to the analysis of data from two-colour spotted microarrays is to compute the log-ratio of the expression values for each spot (Chen et al, 1997). The log-ratios are then treated as the responses in any statistical analysis of the data (Yang and Speed, 2003; Smyth, 2004). Relatively few papers have analysed spotted microarrays in terms of the separate red and green log-intensities (Kerr et al, 2000; Jin et al, 2001; Wolfinger et al, 2001). The second and third of these papers popularised a mixed model approach in which each spot is treated as a randomised block of size two. A number of papers starting with Yang et al (2001) have summarised red and green channel intensities in terms of M-values (log-ratios) and A-values (spot log-intensities) for the purposes of graphical displays and normalisation. This paper demonstrates that the usefulness of this partition arises in good part from the fact that the M and A-values for a given spot are approximately independent even though the individual intensities are highly correlated. This paper reformulates the mixed model approach in terms of the M and A-values. This approach not only presents an efficient algorithm for estimating the mixed model but also elucidates the difference between the traditional log-ratio based approach and the analysis of
203|Hierarchical correctness proofs for distributed algorithms|Abstract: We introduce the input-output automaton, a simple but powerful model of computation in asynchronous distributed networks. With this model we are able to construct modular, hierarchical correctness proofs for distributed algorithms. We de ne this model, and give aninteresting example of how itcan be used to construct such proofs. 1
204|Statecharts: A Visual Formalism For Complex Systems|We present a broad extension of the conventional formalism of state machines and state diagrams, that is relevant to the specification and design of complex discrete-event systems, such as multi-computer real-time systems, communication protocols and digital control units. Our diagrams, which we call statecharts, extend conventional state-transition diagrams with essentially three olements, dealing, respectively, with the notions of hierarchy, concurrency and communication. These transform the language of state diagrams into a highly structured&#039; and economical description language. Statecharts are thus compact and expressive--small diagrams can express complex behavior--as well as compositional and modular. When coupled with the capabilities of computerized graphics, statecharts enable viewing the description at different levels of detail, and make even very large specifications manageable and comprehensible. In fact, we intend to demonstrate here that statecharts counter many of the objections raised against conventional state diagrams, and thus appear to render specification by diagrams an attractive and plausible approach. Statecharts can be used either as a stand-alone behavioral description or as part of a more general design methodology that deals also with the system&#039;s other aspects, such as functional decomposition and data-flow specification. We also discuss some practical experience that was gained over the last three years in applying the statechart formalism to the specification of a particularly complex system.
205|Constructing Two-Writer Atomic Registers|In this paper, we construct a 2-writer, n-reader atomic memory register from two l-writer, (n + l)-reader atomic memory registers. There are no restrictions on the size of the constructed register. The simulation requires only a single extra bit per real register, and can survive the failure of any set of readers and writers. This construction is a part of a systematic investigation of register simulations, by several researchers.  
206|ON THE CORRECTNESS OF ORPHAN ELIMINATION ALGORITHMS|In a distributed system, node crashes and network delays can result in orphaned computations:  computations that are still running but whose results are no longer needed. Several algorithms have been  proposed to detect and eliminate such computations before they can see inconsistent states of the shared,  concurrently accessed data. In this paper we analyze two orphan elimination algorithms that have been  proposed for nested transaction systems. We describe the algorithms formally, and present complete detailed proofs of correctness. Our proofs are remarkably simple, and show that the fundamental  concepts underlying the two algorithms are quite similar. In addition, we show formally that the  algorithms can be used in combination with any correct concurrency control technique, thus providing  formal justification for the informal claims made by the algorithms&#039; designers. Our results are a  significant advance over earlier work in the area, in which it was extremely difficult to state and prove  comparable results.
207|Proving Boolean Combinations of Deterministic Properties|This paper gives a method for proving that a program satisfies a temporal property that has been specified in terms of Buchi automata. The method permits extraction of proof obligations for a property formulated as the Boolean combination of properties, each of which is specified by a deterministic Buchi automaton, directly from the individual automata. The proof obligations can be formulated as Hoare triples. The method is proved sound and relatively complete. A simple example illustrates applica- tion of the method.
208|Nested Transactions and Read/Write Locking|We give a clear yet rigorous correctness proof for Moss&#039;s algorithm for managing data in a nested transaction system. The algorithm, which is the basis of concurrency control and recovery in the Argus system, uses read- and write-locks and a stack of versions of each object to ensure the serializability and recoverability of transactions accessing the data. Our proof extends earlier work on exclusive locking to prove that Moss&#039;s algorithm generates serially correct executions in the presence of concurrency and transaction aborts. The key contribution is the identification of a simple property of cead operations, called transparency, that permits shared locks to be used for read operations.
209|A bayesian hierarchical model for learning natural scene categories|We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes. 1.
210|Object Recognition from Local Scale-Invariant Features |An object recognition system has been developed that uses a new class of local image features. The features are invariant to image scaling, translation, and rotation, and partially invariant to illumination changes and affine or 3D projection. These features share similar properties with neurons in inferior temporal cortex that are used for object recognition in primate vision. Features are efficiently detected through a staged filtering approach that identifies stable points in scale space. Image keys are created that allow for local geometric deformations by representing blurred image gradients in multiple orientation planes and at multiple scales. The keys are used as input to a nearest-neighbor indexing method that identifies candidate object matches. Final verification of each match is achieved by finding a low-residual least-squares solution for the unknown model parameters. Experimental results show that robust object recognition can be achieved in cluttered partially-occluded images with a computation time of under 2 seconds.  
211|Bayesian Data Analysis|I actually own a copy of Harold Jeffreys’s Theory of Probability but have only read small bits of it, most recently over a decade ago to confirm that, indeed, Jeffreys was not too proud to use a classical chi-squared p-value when he wanted to check the misfit of a model to data (Gelman, Meng and Stern, 2006). I do, however, feel that it is important to understand where our probability models come from, and I welcome the opportunity to use the present article by Robert, Chopin and Rousseau as a platform for further discussion of foundational issues. 2 In this brief discussion I will argue the following: (1) in thinking about prior distributions, we should go beyond Jeffreys’s principles and move toward weakly informative priors; (2) it is natural for those of us who work in social and computational sciences to favor complex models, contra Jeffreys’s preference for simplicity; and (3) a key generalization of Jeffreys’s ideas is to explicitly include model checking in the process of data analysis.
212|Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope|In this paper, we propose a computational model of the recognition of real world scenes that bypasses the segmentation and the processing of individual objects or regions. The procedure is based on a very low dimensional representation of the scene, that we term the Spatial Envelope. We propose a set of perceptual dimensions (naturalness, openness, roughness, expansion, ruggedness) that represent the dominant spatial structure of a scene. Then, we show that these dimensions may be reliably estimated using spectral and coarsely localized information. The model generates a multidimensional space in which scenes sharing membership in semantic categories (e.g., streets, highways, coasts) are projected closed together. The performance of the spatial envelope model shows that specific information about object shape or identity is not a requirement for scene categorization and that modeling a holistic representation of the scene informs about its probable semantic category.
213|A Parametric Texture Model based on Joint Statistics of Complex Wavelet Coefficients|We present a universal statistical model for texture images in the context of an overcomplete complex wavelet transform. The model is parameterized by a set of statistics computed on pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. We develop an efficient algorithm for synthesizing random images subject to these constraints, by iteratively projecting onto the set of images satisfying each constraint, and we use this to test the perceptual validity of the model. In particular, we demonstrate the necessity of subgroups of the parameter set by showing examples of texture synthesis that fail when those parameters are removed from the set. We also demonstrate the power of our model by successfully synthesizing examples drawn from a diverse collection of artificial and natural textures.
214|Sharing Features: Efficient Boosting Procedures for Multiclass Object Detection|We consider the problem of detecting a large number of different object classes in cluttered scenes. Traditional approaches require applying a battery of different classifiers to the image, which can be slow and require much training data. We present a multi-class boosting procedure (joint boosting) that reduces both the computational and sample complexity, by finding common features that can be shared across the classes. The detectors for each class are trained jointly, rather than independently. For a given performance level, the total number of features required is observed to scale approximately logarithmically with the number of classes. In addition, we find that the features selected by independently trained classifiers are often specific to the class, whereas the features selected by the jointly trained classifiers are more generic features, such as lines and edges.
215|Indoor-outdoor image classification|We show how high-level scene properties can be inferred from classification of low-level image features, specifically for the indoor-outdoor scene retrieval problem. We systematically studied the features: (1) histograms in the Ohta color space (2) multiresolution, simultaneous autoregressive model parameters (3) coefficients of a shift-invariant DCT. We demonstrate that performance is improved by computing features on subblocks, classifying these subblocks, and then combining these results in a way reminiscent of &#034;stacking.&#034; State of the art single-feature methods are shown to result in about 75-86 % performance, while the new method results in 90.3 % correct classification, when evaluated on a diverse database of over 1300 consumer images provided by Kodak.
216|Image classification for content-based indexing| Grouping images into (semantically) meaningful categories using low-level visual features is a challenging and important problem in content-based image retrieval. Using binary Bayesian classifiers, we attempt to capture high-level concepts from low-level image features under the constraint that the test image does belong to one of the classes. Specifically, we consider the hierarchical classification of vacation images; at the highest level, images are classified as indoor or outdoor; outdoor images are further classified as city or landscape; finally, a subset of landscape images is classified into sunset, forest, and mountain classes. We demonstrate that a small vector quantizer (whose optimal size is selected using a modified MDL criterion) can be used to model the class-conditional densities of the features, required by the Bayesian methodology. The classifiers have been designed and evaluated on a database of 6931 vacation photographs. Our system achieved a classification accuracy of 90.5 % for indoor/outdoor, 95.3 % for city/landscape, 96.6 % for sunset/forest &amp; mountain, and 96 % for forest/mountain classification problems. We further develop a learning method to incrementally train the classifiers as additional data become available. We also show preliminary results for feature reduction using clustering techniques. Our goal is to combine multiple two-class classifiers into a single hierarchical classifier. 
217|Texture classification: Are filter banks necessary |We question the role that large scale filter banks have traditionally played in texture classification. It is demonstrated that textures can be classified using the joint distribution of intensity values over extremely compact neighbourhoods (starting from as small as 3 × 3 pixels square), and that this outperforms classification using filter banks with large support. We develop a novel texton based representation which is suited to modelling this joint neighbourhood distribution for MRFs. The representation is learnt from training images, and then used to classify novel images (with unknown viewpoint and lighting) into texture classes. The power of the method is demonstrated by classifying over 2800 images of all 61 textures present in the Columbia-Utrecht database. The classification performance surpasses that of recent state-of-the-art filter bank based classifiers such as Leung &amp; Malik [IJCV 01], Cula &amp; Dana [CVPR 01], and Varma &amp; Zisserman [ECCV 02]. 1
218|Texture Orientation for Sorting Photos &#034;at a Glance&#034;|We investigate a measure of &#034;dominant perceived orientation &#034; that has recently been developed to match the output of a human study involving 40 subjects. The results of this measure are compared with humans analyzing seven &#034;teaser&#034; images to test its effectiveness for finding perceptually dominant orientations. The use of low-level orientation is then applied to a &#034;quick search&#034; problem important in image database applications. Since both pigeons and humans are able to perform coarse classification of certain kinds of scenes, e.g., city from country, without taking time or brainpower to solve the image understanding problem, we conjecture that the collective behavior of low-level textural features such as orientation may be doing most of the work. We demonstrate a simple test of global multiscale orientation for quickly searching a database of vacation photos for likely &#034;city/suburb&#034; shots. The orientation features achieve agreement with human classification in 91 out of 98 of the sce...
219|A semantic typicality measure for natural scene categorization|Abstract. We propose an approach to categorize real-world natural scenes based on a semantic typicality measure. The proposed typicality measure allows to grade the similarity of an image with respect to a scene category. We argue that such a graded decision is appropriate and justified both from a human’s perspective as well as from the image-content point of view. The method combines bottom-up information of local semantic concepts with the typical semantic content
220|Bayes Factors|In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P -values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology and psychology.
221|Discrete Multivariate Analysis: Theory and Practice|the collaboration of Richard J. Light and Frederick Mosteller.
222|Games and decisions|Agency
224|Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)  (1995) |It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented. Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them. It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis...
225|Prior distributions for variance parameters in hierarchical models|Various noninformative prior distributions have been suggested for scale parameters in hierarchical models. We construct a new folded-noncentral-t family of conditionally conjugate priors for hierarchical standard deviation parameters, and then consider noninformative and weakly informative priors in this family. We use an example to illustrate serious problems with the inverse-gamma family of “noninformative ” prior distributions. We suggest instead to use a uniform prior on the hierarchical standard deviation, using the half-t family when the number of groups is small and in other settings where a weakly informative prior is desired.
226|Model selection and accounting for model uncertainty in graphical models using Occam&#039;s window|We consider the problem of model selection and accounting for model uncertainty in high-dimensional contingency tables, motivated by expert system applications. The approach most used currently is a stepwise strategy guided by tests based on approximate asymptotic P-values leading to the selection of a single model; inference is then conditional on the selected model. The sampling properties of such a strategy are complex, and the failure to take account of model uncertainty leads to underestimation of uncertainty about quantities of interest. In principle, a panacea is provided by the standard Bayesian formalism which averages the posterior distributions of the quantity of interest under each of the models, weighted by their posterior model probabilities. Furthermore, this approach is optimal in the sense of maximising predictive ability. However, this has not been used in practice because computing the posterior model probabilities is hard and the number of models is very large (often greater than 1011). We argue that the standard Bayesian formalism is unsatisfactory and we propose an alternative Bayesian approach that, we contend, takes full account of the true model uncertainty byaveraging overamuch smaller set of models. An efficient search algorithm is developed for nding these models. We consider two classes of graphical models that arise in expert systems: the recursive causal models and the decomposable
227|Posterior Predictive Assessment of Model Fitness Via Realized Discrepancies|Abstract: This paper considers Bayesian counterparts of the classical tests for goodness of fit and their use in judging the fit of a single Bayesian model to the observed data. We focus on posterior predictive assessment, in a framework that also includes conditioning on auxiliary statistics. The Bayesian formulation facilitates the construction and calculation of a meaningful reference distribution not only for any (classical) statistic, but also for any parameter-dependent “statistic ” or discrepancy. The latter allows us to propose the realized discrepancy assessment of model fitness, which directly measures the true discrepancy between data and the posited model, for any aspect of the model which we want to explore. The computation required for the realized discrepancy assessment is a straightforward byproduct of the posterior simulation used for the original Bayesian analysis. We illustrate with three applied examples. The first example, which serves mainly to motivate the work, illustrates the difficulty of classical tests in assessing the fitness of a Poisson model to a positron emission tomography image that is constrained to be nonnegative. The second and third examples illustrate the details of the posterior predictive approach in two problems: estimation in a model with inequality constraints on the parameters, and estimation in a mixture model. In all three examples, standard test statistics (either a ? 2 or a likelihood ratio) are not pivotal: the difficulty is not just how to compute the reference distribution for the test, but that in the classical framework no such distribution exists, independent of the unknown model parameters. Key words and phrases: Bayesian p-value, ? 2 test, discrepancy, graphical assessment, mixture model, model criticism, posterior predictive p-value, prior predictive
228|Sampling and Bayes inference in scientific modeling and robustness. (with discussion  (1980) |t-
229|Information-theoretic asymptotics of Bayes methods| In the absence of knowledge of the true density function, Bayesian models take the joint density function for a sequence of n random variables to be an average of densities with respect to a prior. We examine the relative entropy distance D,, between the true density and the Bayesian density and show that the asymptotic distance is (d/2Xlogn)+ c, where d is the dimension of the parameter vector. Therefore, the relative entropy rate D,,/n converges to zero at rate (logn)/n. The constant c, which we explicitly identify, depends only on the prior density function and the Fisher information matrix evaluated at the true parameter value. Consequences are given for density estima-tion, universal data compression, composite hypothesis testing, and stock-market portfolio selection.  
230|Bayes factors and marginal distributions in invariant situations|SUMMARY. In Bayesian analysis with a “minimal ” data set and common noninformative priors, the (formal) marginal density of the data is surprisingly often independent of the error distribution. This results in great simplifications in certain model selection methodologies; for instance, the Intrinsic Bayes Factor for models with this property reduces simply to the Bayes factor with respect to the noninformative priors. The basic result holds for comparison of models which are invariant with respect to the same group structure. Indeed the condi-tion reduces to a condition on the distributions of the common maximal invariant. In these situations, the marginal density of a “minimal ” data set is typically available in closed form, regardless of the error distribution. This provides very useful expressions for computation of Intrinsic Bayes Factors in more general settings. The conditions for the results to hold are explored in some detail for nonnormal linear models and various transformations thereof. 1.
231|Formal Rules for Selecting Prior Distributions: A Review and Annotated Bibliography|Subjectivism has become the dominant philosophical foundation for Bayesian inference. Yet, in practice, most Bayesian analyses are performed with so-called &#034;noninformative&#034; priors, that is, priors constructed by some formal rule. We review the plethora of techniques for constructing such priors, and discuss some of the practical and philosophical issues that arise when they are used. We give special emphasis to Jeffreys&#039;s rules and discuss the evolution of his point of view about the interpretation of priors, away from unique representation of ignorance toward the notion that they should be chosen by convention. We conclude that the problems raised by the research on priors chosen by formal rules are serious and may not be dismissed lightly; when sample sizes are small (relative to the number of parameters being estimated) it is dangerous to put faith in any &#034;default&#034; solution; but when asymptotics take over, Jeffreys&#039;s rules and their variants remain reasonable choices. We also provi...
232|Bernstein Von Mises Theorem for linear functionals of the density|In this paper, we study the asymptotic posterior distribution of linear functionals of the density. In particular, we give general conditions to obtain a semiparametric version of the Bernstein-Von Mises theorem. We then apply this general result to nonparametric priors based on infinite dimensional exponential families. As a byproduct, we also derive adaptive nonparametric rates of concentration of the posterior distributions under these families of priors on the class of Sobolev and Besov spaces.
233|A Bayesian approach to the selection and testing of mixture models|Abstract: An important aspect of mixture modeling is the selection of the number of mixture components. In this paper, we discuss the Bayes factor as a selection tool. The discussion will focus on two aspects: computation of the Bayes factor and prior sensitivity. For the computation, we propose a variant of Chib’s estimator that accounts for the non-identifiability of the mixture components. To reduce the prior sensitivity of the Bayes factor, we propose to extend the model with a hyperprior. We further discuss the use of posterior predictive checks for examining the fit of the model. The ideas are illustrated by means of a psychiatric diagnosis example.
234|Computational methods for Bayesian model choice|In this note, we shortly survey some recent approaches on the approximation of the Bayes factor used in Bayesian hypothesis testing and in Bayesian model choice. In particular, we reassess importance sampling, harmonic mean sampling, and nested sampling from a unified perspective.
235|Bayesian Inference on Mixtures of Distributions|This survey covers state-of-the-art Bayesian techniques for the estimation of mixtures. It complements the earlier Marin et al. (2005) by studying new types of distributions, the multinomial, latent class and t distributions. It also exhibits closed form solutions for Bayesian inference in some discrete setups. At last, it sheds a new light on the computation of Bayes factors via the approximation of Chib (1995). 
236|Nonasymptotic bounds for Bayesian order identification with application to mixtures|The efficiency of two Bayesian order estimators is studied. By using nonparametric techniques, we prove new underestimation and overestimation bounds. The results apply to various models, including mixture models. In this case, the errors are shown to be O(e -an) and O((log n) b /  v n) (a,b&gt; 0), respectively. 1. Introduction. Order
237|The Elimination of Nuisance Parameters|We review the Bayesian approach to the problem of the elimination of nuisance parameters from a statistical model. Many Bayesian statisticians feel that the framework of Bayesian statistics is so clear and simple that the elimination of nuisance parameters should not be considered a problem: one has simply to compute the marginal posterior distribution of the parameter of interest. However we will argue that this exercise need not be so simple from a practical perspective. The paper is divided in two main parts: the first deals with regular parametric models whereas the second will focus on non regular problem, including the so-called Neyman and Scott’s class of models and semiparametric models where the nuisance parameter lies in an infinite dimensional space. Finally we relate the issues of the elimination of nuisance parameters to other, apparently different, problems. Occasionally, we will mention non Bayesian treatment of nuisance parameters, mainly for comparative analyses.
238|Implementing data cubes efficiently|Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query. 1
239|Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals|Abstract. Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, crosstabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.
240|Query evaluation techniques for large databases|Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today’s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.
241|Multi-table joins through bitmapped join indices|This technical note shows how to combine some well-known techniques to create a method that will efficiently execute common multi-table joins. We concentrate on a commonly occurring type of join known as a star-join, although the method presented will generalize to any type of multi-table join. A star-join consists of a central detail table with large cardinality, such as an orders table (where an order row contains a single purchase) with foreign keys that join to descriptive tables, such as customers, products, and (sales) agents. The method presented in this note uses join indices with compressed bitmap representations, which allow predicates restricting columns of descriptive tables to determine an answer set (or foundsef) in the central detail table; the method uses different predicates on different descriptive tables in combination to restrict the detail table through compressed bitmap representations of join indices, and easily completes the join of the fully restricted detail table rows back to the descriptive tables. We outline realistic examples where the combination of these techniques yields substantial performance improvements over alternative, more traditional query evaluation plans. 1.
242|Aggregate-Query Processing in Data Warehousing Environments|In this paper we introduce generalized projections (GPs), an extension of duplicate eliminating projections, that capture aggregations, groupbys, duplicate-eliminating projections (distinct), and duplicate-preserving projections in a common unified framework. Using GPs we extend well known and simple algorithms for SQL queries that use distinct projections to derive algorithms for queries using aggregations like sum, max, min, count, and avg. We develop powerful query rewrite rules for aggregate queries that unify and extend rewrite rules previously known in the literature. We then illustrate the power of our approach by solving a very practical and important problem in data warehousing: how to answer an aggregate query on base tables using materialized aggregate views (summary tables).
243|Including Group-By in Query Optimization|In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fas...
244|Eager Aggregation and Lazy Aggregation|Efficient processing of aggregation queries is essential for decision support applications. This paper describes a class of query transformations, called eager aggregation and lazy aggregation, that allows a query optimizer to move group-by operations up and down the query tree. Eager aggregation partially pushes a group-by past a join. After a group-by is partially pushed down, we still need to perform the original group-by in the upper query block. Eager aggregation reduces the number of input rows to the join and thus may result in a better overall plan. The reverse transformation, lazy aggregation, pulls a group-by above a join and combines two group-by operations into one. This transformation is typically of interest when an aggregation query references a grouped view (a view containing a group-by). Experimental results show that the technique is very beneficial for queries in the TPC-D benchmark. 1 Introduction Aggregation is widely used in decision support systems. All queries...
245|Querying Semi-Structured Data|
246|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
247|DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases|In semistructured databases there is no schema fixed  in advance. To provide the benefits of a schema in  such environments, we introduce DataGuides:  concise and accurate structural summaries of  semistructured databases. DataGuides serve as  dynamic schemas, generated from the database; they  are useful for browsing database structure,  formulating queries, storing information such as  statistics and sample values, and enabling query  optimization. This paper presents the theoretical  foundations of DataGuides along with an algorithm  for their creation and an overview of incremental  maintenance. We provide performance results based  on our implementation of DataGuides in the Lore  DBMS for semistructured data. We also describe the  use of DataGuides in Lore, both in the user interface  to enable structure browsing and query formulation,  and as a means of guiding the query processor and  optimizing query execution.
248|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
249|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
250|A Query Language and Optimization Techniques for Unstructured Data|A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility in data representation  What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or &#034;vertical&#034; dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the &#034;horizontal&#034; dimension of UnQL.  
251|Lore: A database management system for semistructured data|Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources. 
252|Maintenance of Materialized Views: Problems, Techniques, and Applications|In this paper we motivate and describe materialized views, their applications, and the problems  and techniques for their maintenance. We present a taxonomy of view maintenanceproblems  basedupon the class of views considered, upon the resources used to maintain the view, upon the types of modi#cations to the base data that areconsidered during maintenance, and whether the technique works for all instances of databases and modi#cations. We describe some of the view maintenancetechniques proposed in the literature in terms of our taxonomy. Finally, we consider new and promising application domains that are likely to drive work in materialized  views and view maintenance.  1 Introduction  What is a view? A view is a derived relation de#ned in terms of base #stored# relations. A view thus de#nes a function from a set of base tables to a derived table; this function is typically recomputed every time the view is referenced.  What is a materialized view? A view can be materialized by storin...
253|Objects and Views|Object-oriented databases have been introduced primarily to ease the development of database applications. However, the difficulties encountered when, for instance, trying to restructure data or integrate databases demonstrate that the models being used still lack flexibility. We claim that the natural way to overcome these shortcomings is to introduce a sophisticated view mechanism. This paper presents such a mechanism, one which allows a programmer to restructure the class hierarchy and modify the behavior and structure of objects. The mechanism allows a programmer to specify attribute values implicitly, rather than storing them. It also allows him to introduce new classes into the class hierarchy. These virtual classes are populated by selecting existing objects from other classes and by creating new objects. Fixing the identify of new objects during database updates introduces subtle issues into view design. Our presentation, mostly informal, leans on a number of illustrative examples meant to emphasize the simplicity of our mechanism. 1
254|Object fusion in mediator systems|One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are &amp;quot;imported &amp;quot; into the mediator.
255|Regular Path Queries with Constraints|The evaluation of path expression queries on semistructured data in a distributed asynchronous environment is considered. The focus is on the use of local information expressed in the form of path constraints in the optimization of path expression queries. In particular, decidability and complexity results on the implication problem for path constraints are established.
256|Supporting Views in Object-Oriented Databases|  Relational database systems provide a powerful abstraction mechanism: any query, since it returns a relation, can be used to define a view, that becomes a derived (or virtual) relation. Views are defined by a statement such as &#034;define view &lt;name&gt; as &lt;query&gt;.&#034; Views can be used to tailor the global database schema.  . Query formulation is simplified, if frequent subexpressions are predefined.  . Application programs are insulated from changes to the underlying schema.  . Information can be restructured to better suit an application programs&#039; requirements.  . Derived information is kept consistent with base data.  . Access restrictions (for authorization) can be enforced by hiding data.  In contrast to base relations, views are typically not stored permanently, but rather computed on demand. Queries to view relations are modified by query substitution so as to operate on the underlying b
257|MultiView: A Methodology for Supporting Multiple Views in Object-Oriented Databases|A view in object-oriented databases (OODB) corresponds to virtual schema graph with possibly restructured generalization and decomposition hierarchies. We propose a methodology, called MultiView, for supporting multiple such view schemata. MultiView represents a simple yet powerful approach achieved by breaking view specification into independent tasks: class derivation, global schema integration, view class selection, and view schema generation. Novel features of MultiView include an object algebra for class customization; an algorithm for the integration of virtual classes into the global schema; a view definition language for view class selection, and the automatic generation of a view class hierarchy. In addition, we present algorithms that verify the closure property of a view and, if found to be incomplete, transform it into a closed, yet minimal, view. Lastly, we introduce the fundamental concept of view independence and show  MultiView to be view independent.  
258|The GMAP: a versatile tool for physical data independence|.&lt;F3.733e+05&gt; Physical data independence is touted as a central feature of modern database systems. It allows users to frame queries in terms of the logical structure of the data, letting a query processor automatically translate them into optimal plans that access physical storage structures. Both relational and object-oriented systems, however, force users to frame their queries in terms of a logical schema that is directly tied to physical structures. We present an approach that eliminates this dependence. All storage structures are defined in a declarative language based on relational algebra as functions of a logical schema. We present an algorithm, integrated with a conventional query optimizer, that translates queries over this logical schema into plans that access the storage structures. We also show how to compile update requests into plans that update all relevant storage structures consistently and optimally. Finally, we report on experiments with a prototype implementation ...
259|Virtual Schemas and Bases|We propose the notions of virtual schemas and virtual bases as a coherent way of integrating various features in OODB views. A virtual schema is defined based on some existing (real) schema. A virtual base is obtained when a (real) base is attached to a virtual schema. We study the consequences of this simple assumption. In particular, we observe the differences between a real schema and a virtual one. We also consider an extension (that we call generic schemas) where it is necessary to specify several real bases to attach data to a virtual schema. We show how the flexibility provided by virtual schemas can be used to cope with various dynamic features of database systems. 1 Introduction Views are intended to increase the flexibility of database systems and their definition in the object-oriented database (OODB) context comes as a natural extension of the original paradigm. The yet relatively young research on this topic has introduced a large variety of indispensable new features. H...
260|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
261|From Structured Documents to Novel Query Facilities|Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB&#039;s and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval. Although motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion. 1 Introduction  Structured documents are central to a wide class of applications such as software engineering, libraries, technical documentation, etc. They are often stored in file systems and document access tools are somewhat limited. We believe that (object-oriented) d...
262|Finding Regular Simple Paths In Graph Databases|We consider the following problem: given a labelled directed graph G and a regular expression R, find all pairs of nodes connected by a simple path such that the concatenation of the labels along the path satisfies R. The problem is motivated by the observation that many recursive queries in relational databases can be expressed in this form, and by the implementation of a query language, G+ , based on this observation. We show that the problem is in general intractable, but present an algorithm than runs in polynomial time in the size of the graph when the regular expression and the graph are free of conflicts. We also present a class of languages whose expressions can always be evaluated in time polynomial in the size of both the graph and the expression, and characterize syntactically the expressions for such languages. 
263|MedMaker: A Mediation System Based on Declarative Specifications|Mediators are used for integration of heterogeneous information sources. We present a system for declaratively specifying mediators. It is targeted for integration of sources with unstructured or semi-structured data and/or sources with changing schemas. We illustrate the main features of the Mediator Specification Language (MSL), show how they facilitate integration, and describe the implementation of the system that interprets the MSL specifications. 
264|Representative Objects: Concise Representations of Semistructured Hierarchical Data|In this paper we introduce the representative object, which uncovers the inherent schema(s) in semistructured, hierarchical data sources and provides a concise description of the structure of the data. Semistructured data, unlike data stored in typical relational or object-oriented databases, does not have fixed schema that is known in advance and stored separately from the data. Withthe rapid growth of the World Wide Web, semistructured hierarchical data sources are becoming widely available to the casual user. The lack of external schema information currently makes browsing and querying these data sources inefficient at best, and impossible at worst. We show how representative objects make schema discovery efficient and facilitate the generation of meaningful queries over the data. 1.
266|Evaluating Queries with Generalized Path Expressions|In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm. In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.  1 Introduction  In the past few years there has been a growing interest in query languages featuring generalized path expressions (GPE) [BRG88, KKS92, CACS94, QRS  +  95]. With these languages, one may issue queries on data without exact knowledge of its structure. A GPE queries data and structure at the same time. Although very useful for standard database applications, these languages are vital for new applications dedicated, for insta...
267|Integrating a Structured-Text Retrieval System with an Object-Oriented Database System|We describe the integration of a structured-text retrieval system (TextMachine) into an  object-oriented database system (OpenODB). Our approach is a light-weight one, using the external  function capability of the database system to encapsulate the text retrieval system as an  external information source. Yet, we are able to provide a tight integration in the query language  and processing; the user can access the text retrieval system using a standard database query  language. The efficient and effective retrieval of structured text performed by the text retrieval  system is seamlessly combined with the rich modeling and general-purpose querying capabilities  of the database system, resulting in an integrated system with querying power beyond those of  the underlying systems. The integrated system also provides uniform access to textual data in  the text retrieval system and structured data in the database system, thereby achieving information  fusion. We discuss the design and imple...
268|Biclustering of Expression Data|An efficient node-deletion algorithm is introduced to find submatrices...
269|The NP-completeness column: an ongoing guide|This is the nineteenth edition of a (usually) quarterly column that covers new developments in the theory of NP-completeness. The presentation is modeled on that used by M. R. Garey and myself in our book &#034;Computers and Intractability: A Guide to the Theory of NP-Completeness,&#034; W. H. Freeman &amp; Co., New York, 1979 (hereinafter referred to as &#034;[G&amp;J]&#034;; previous columns will be referred to by their dates). A background equivalent to that provided by [G&amp;J] is assumed, and, when appropriate, cross-references will be given to that book and the list of problems (NP-complete and harder) presented there. Readers who have results they would like mentioned (NP-hardness, PSPACE-hardness, polynomial-time-solvability, etc.) or open problems they would like publicized, should
270|An Algorithm for Clustering cDNAs for Gene Expression Analysis|We have developed a novel algorithm for cluster analysis that is based on graph theoretic techniques. A similarity graph is defined and clusters in that graph correspond to highly connected subgraphs. A polynomial algorithm to compute them efficiently is presented. Our algorithm produces a clustering with some provably good properties.  The application that motivated this study was gene expression analysis, where a collection of cDNAs must be clustered based on their oligonucleotide fingerprints. The algorithm has been tested intensively on simulated libraries and was shown to outperform extant methods. It demonstrated robustness to high noise levels. In a blind test on real cDNA fingerprint data the algorithm obtained very good results. Utilizing the results of the algorithm would have saved over 70% of the cDNA sequencing cost on that data set.  1 Introduction  Cluster analysis seeks grouping of data elements into subsets, so that elements in the same subset are in some sense more cl...
271|Contentment in graph theory: covering graphs with cliques|Fundamental questions posed by Boole in 1868 on the theory of sets have in recent years been translated to problems in graph theory. The major problems that this paper deals with are determining the minimum number of complete subgraphs of graph G which include all of the edges of G, and determining the minimum number of complete bipartite subgraphs which cover G. The two problems are of a very similar nature. Determining whether there is a projective plane of order p is a special case of the former problem. The latter problem has a natural translation into matrix theory which yields tight upper and lower bounds. An elementary proof is given for Graham&#039;s theorem. Two non-obvious classes are given for which the above problems are easily handled; however, this author doubts that these classes can be extended significantly. Two new problems are shown in this
272|Systematic Management and Analysis of Yeast Gene Expression Data|roarrays, Serial Analysis of Gene Expression (SAGE), and other techniques (Velculescu et al. 1995; Lockhart et al. 1996; DeRisi et al. 1997), has led to the rapid accumulation of large expression data sets and the development of the field of functional genomics. Functional genomics has been contrasted as having a systematic, genome-wide approach to the collection and analysis of biological data compared with more traditional methods, which focus in depth on particular genes, proteins, or pathways (Hieter and Boguski 1997). But despite rapid strides, capped by a string of successful studies (see Table 1), functional genomics has yet to develop a highly integrated system of tools and methods such those used in sequence analysis and structural genomics (Hieter and Boguski 1997); for that, we must await development of the three following components: general databases, data standards, and integrated general-purpose analysis tools. A comparison of the status of these fields with respect to t
273|An introduction to hidden Markov models|The basic theory of Markov chains has been known to
274|A View Of The Em Algorithm That Justifies Incremental, Sparse, And Other Variants|. The EM algorithm performs maximum likelihood estimation for data in which some variables are unobserved. We present a function that resembles negative free energy and show that the M step maximizes this function with respect to the model parameters and the E step maximizes it with respect to the distribution over the unobserved variables. From this perspective, it is easy to justify an incremental variant of the EM algorithm in which the distribution for only one of the unobserved variables is recalculated in each E step. This variant is shown empirically to give faster convergence in a mixture estimation problem. A variant of the algorithm that exploits sparse conditional distributions is also described, and a wide range of other variant algorithms are also seen to be possible. 1. Introduction The Expectation-Maximization (EM) algorithm finds maximum likelihood parameter estimates in problems where some variables were unobserved. Special cases of the algorithm date back several dec...
275|Probabilistic Inference Using Markov Chain Monte Carlo Methods|Probabilistic inference is an attractive approach to uncertain reasoning and empirical learning in artificial intelligence. Computational difficulties arise, however, because probabilistic models with the necessary realism and flexibility lead to complex distributions over high-dimensional spaces. Related problems in other fields have been tackled using Monte Carlo methods based on sampling using Markov chains, providing a rich array of techniques that can be applied to problems in artificial intelligence. The &#034;Metropolis algorithm&#034; has been used to solve difficult problems in statistical physics for over forty years, and, in the last few years, the related method of &#034;Gibbs sampling&#034; has been applied to problems of statistical inference. Concurrently, an alternative method for solving problems in statistical physics by means of dynamical simulation has been developed as well, and has recently been unified with the Metropolis algorithm to produce the &#034;hybrid Monte Carlo&#034; method. In computer science, Markov chain sampling is the basis of the heuristic optimization technique of &#034;simulated annealing&#034;, and has recently been used in randomized algorithms for approximate counting of large sets. In this review, I outline the role of probabilistic inference in artificial intelligence, present the theory of Markov chains, and describe various Markov chain Monte Carlo algorithms, along with a number of supporting techniques. I try to present a comprehensive picture of the range of methods that have been developed, including techniques from the varied literature that have not yet seen wide application in artificial intelligence, but which appear relevant. As illustrative examples, I use the problems of probabilistic inference in expert systems, discovery of latent classes from data, and Bayesian learning for neural networks.
276|Hidden Markov models in computational biology: applications to protein modeling|Hidden.Markov Models (HMMs) are applied t.0 the problems of statistical modeling, database searching and multiple sequence alignment of protein families and protein domains. These methods are demonstrated the on globin family, the protein kinase catalytic domain, and the EF-hand calcium binding motif. In each case the parameters of an HMM are estimated from a training set of unaligned sequences. After the HMM is built, it is used to obtain a multiple alignment of all the training sequences. It is also used to search the. SWISS-PROT 22 database for other sequences. that are members of the given protein family, or contain the given domain. The Hi &#034; produces multiple alignments of good quality that agree closely with the alignments produced by programs that incorporate threedimensional structural information. When employed in discrimination tests (by examining how closely the sequences in a database fit the globin, kinase and EF-hand HMMs), the &#039;\ HMM is able to distinguish members of these families from non-members with a high degree of accuracy. Both the HMM and PROFILESEARCH (a technique used to search for relationships between a protein sequence and multiply aligned sequences) perform better in these tests than PROSITE (a dictionary of sites and patterns in proteins). The HMM appecvs to have a slight advantage over PROFILESEARCH in terms of lower rates of false
277|A Tutorial on Learning Bayesian Networks|We examine a graphical representation of uncertain knowledge called a Bayesian network. The representation is easy to construct and interpret, yet has formal probabilistic semantics making it suitable for statistical manipulation. We show how we can use the representation to learn new knowledge by combining domain knowledge with statistical data. 1 Introduction  Many techniques for learning rely heavily on data. In contrast, the knowledge encoded in expert systems usually comes solely from an expert. In this paper, we examine a knowledge representation, called a Bayesian network, that lets us have the best of both worlds. Namely, the representation allows us to learn new knowledge by combining expert domain knowledge and statistical data. A Bayesian network is a graphical representation of uncertain knowledge that most people find easy to construct and interpret. In addition, the representation has formal probabilistic semantics, making it suitable for statistical manipulation (Howard,...
278|Probabilistic independence networks for hidden Markov probability models|Graphical techniques for modeling the dependencies of random variables have been explored in a variety of different areas including statistics, statistical physics, artificial intelligence, speech recognition, image processing, and genetics. Formalisms for manipulating these models have been developed relatively independently in these research communities. In this paper we explore hidden Markov models (HMMs) and related structures within the general framework of probabilistic independence networks (PINs). The paper contains a self-contained review of the basic principles of PINs. It is shown that the well-known forward-backward (F-B) and Viterbi algorithms for HMMs are special cases of more general inference algorithms for arbitrary PINs. Furthermore, the existence of inference and estimation algorithms for more general graphical models provides a set of analysis tools for HMM practitioners who wish to explore a richer class of HMM structures. Examples of relatively complex models to handle sensor fusion and coarticulation in speech recognition are introduced and treated within the graphical model framework to illustrate the advantages of the general approach. 
279|Stochastic simulation algorithms for dynamic probabilistic networks|Stochastic simulation algorithms such as likelihood weighting often give fast, accurate approximations to posterior probabilities in probabilistic networks, and are the methods of choice for very large networks. Unfortunately, the special characteristics of dynamic probabilistic networks (DPNs), which are used to represent stochastic temporal processes, mean that standard simulation algorithms perform very poorly. In essence, the simulation trials diverge further and further from reality as the process is observed over time. In this paper, we present simulation algorithms that use the evidence observed at each time step to push the set of trials back towards reality. The first algorithm, &amp;quot;evidence reversal &amp;quot; (ER) restructures each time slice of the DPN so that the evidence nodes for the slice become ancestors of the state variables. The second algorithm, called &amp;quot;survival of the fittest &amp;quot; sampling (SOF), &amp;quot;repopulates &amp;quot; the set of trials at each time step using a stochastic reproduction rate weighted by the likelihood of the evidence according to each trial. We compare the performance of each algorithm with likelihood weighting on the original network, and also investigate the benefits of combining the ER and SOF methods. The ER/SOF combination appears to maintain bounded error independent of the number of time steps in the simulation.
280|Hidden Markov Model Induction by Bayesian Model Merging|This paper describes a technique for learning both the number of states and the topology of Hidden Markov Models from examples. The induction process starts with the most specific model consistent with the training data and generalizes by successively merging states. Both the choice of states to merge and the stopping criterion are guided by the Bayesian posterior probability. We compare our algorithm with the Baum-Welch method of estimating fixed-size models, and find that it can induce minimal HMMs from data in cases where fixed estimation does not converge or requires redundant parameters to converge.  
281|Mean Field Theory for Sigmoid Belief Networks|We develop a mean field theory for sigmoid belief networks based on ideas from statistical mechanics.
282|Autoencoders, Minimum Description Length and Helmholtz Free Energy|An autoencoder network uses a set of recognition weights to convert an input vector into a code vector. It then uses a set of generative weights to convert the code vector into an approximate reconstruction of the input vector. We derive an objective function for training autoencoders based on the Minimum Description Length (MDL) principle. The aim is to minimize the information required to describe both the code vector and the reconstruction error. We show that this information is minimized by choosing code vectors stochastically according to a Boltzmann distribution, where the generative weights de ne the energy of each possible code vector given the input vector. Unfortunately, if the code vectors use distributed representations, it is exponentially expensive to compute this Boltzmann distribution because it involves all possible code vectors. We show that the recognition weights of an autoencoder can be used to compute an approximation to the Boltzmann distribution and...
283|An Input Output HMM Architecture|We introduce a recurrent architecture having a modular structure  and we formulate a training procedure based on the EM algorithm.  The resulting model has similarities to hidden Markov models, but  supports recurrent networks processing style and allows to exploit  the supervised learning paradigm while using maximum likelihood estimation.
284|Exploiting Tractable Substructures in Intractable Networks|We develop a refined mean field approximation for inference and learning in probabilistic neural networks. Our mean field theory, unlike most, does not assume that the units behave as independent degrees of freedom; instead, it exploits in a principled way the existence of large substructures that are computationally tractable. To illustrate the advantages of this framework, we show how to incorporate weak higher order interactions into a first-order hidden Markov model, treating the corrections (but not the first order structure) within mean field theory. 1 INTRODUCTION  Learning the parameters in a probabilistic neural network may be viewed as a problem in statistical estimation. In networks with sparse connectivity (e.g. trees and chains), there exist efficient algorithms for the exact probabilistic calculations that support inference and learning. In general, however, these calculations are intractable, and approximations are required. Mean field theory provides a framework for app...
285|Multiple Viewpoint Systems for Music Prediction|This paper examines the prediction and generation of music using a multiple viewpoint system, a collection of independent views of the musical surface each of which models a specific type of musical phenomena. Both the general style and a particular piece are modeled using dual short-term and long-term theories, and the model is created using machine learning techniques on a corpus of musical examples. The models are
286|A hierarchical dirichlet language model|We discuss a hierarchical probabilistic model whose predictions are similar to those of the popular language modelling procedure known as &#039;smoothing&#039;. A number of interesting differences from smoothing emerge. The insights gained from a probabilistic view of this problem point towards new directions for language modelling. The ideas of this paper are also applicable to other problems such as the modelling of triphomes in speech, and DNA and protein sequences in molecular biology. The new algorithm is compared with smoothing on a two million word corpus. The methods prove to be about equally accurate, with the hierarchical model using fewer computational resources. 1
287|Ensemble Learning for Hidden Markov Models|The standard method for training Hidden Markov Models optimizes a point estimate of the model parameters. This estimate, which can be viewed as the maximum of a posterior probability density over the model parameters, may be susceptible to overfitting, and contains no indication of parameter uncertainty. Also, this maximummay be unrepresentative of the posterior probability distribution. In this paper we study a method in which we optimize an ensemble which approximates the entire posterior probability distribution. The ensemble learning algorithm requires the same resources as the traditional Baum--Welch algorithm. The traditional training algorithm for hidden Markov models is an expectation-- maximization (EM) algorithm (Dempster et al. 1977) known as the Baum--Welch algorithm. It is a maximum likelihood method, or, with a simple modification, a penalized maximum likelihood method, which can be viewed as maximizing a posterior probability density over the model parameters. Recently, ...
288|Mixtures of Controllers for Jump Linear and Non-linear Plants|We describe an extension to the Mixture of Experts architecture for modelling and controlling dynamical systems which exhibit multiple modes of behavior. This extension is based on a Markov process model, and suggests a recurrent network for gating a set of linear or non-linear controllers. The new architecture is demonstrated to be capable of learning effective control strategies for jump linear and non-linear plants with multiple modes of behavior. 1 Introduction  Many stationary dynamic systems exhibit significantly different behaviors under different operating conditions. To control such complex systems it is computationally more efficient to decompose the problem into smaller subtasks, with different control strategies for different operating points. When detailed information about the plant is available, gain scheduling has proven a successful method for designing a global control (Shamma and Athans, 1992). The system is partitioned by choosing several operating points and a line...
289|Boltzmann Chains and Hidden Markov Models|We propose a statistical mechanical framework for the modeling of discrete time series. Maximum likelihood estimation is done via Boltzmann learning in one-dimensional networks with tied weights. We call these networks Boltzmann chains and show that they contain hidden Markov models (HMMs) as a special case. Our framework also motivates new architectures that address particular shortcomings of HMMs. We look at two such architectures: parallel chains that model feature sets with disparate time scales, and looped networks that model long-term dependencies between hidden states. For these networks, we show how to implement the Boltzmann learning rule exactly, in polynomial time, without resort to simulated or mean-field annealing. The necessary computations are done by exact decimation procedures from statistical mechanics. 1 INTRODUCTION AND SUMMARY  Statistical models of discrete time series have a wide range of applications, most notably to problems in speech recognition (Juang &amp; Rabin...
290|Factorial Learning and the EM Algorithm|Many real world learning problems are best characterized by an  interaction of multiple independent causes or factors. Discovering  such causal structure from the data is the focus of this paper. Based on 
291|Hidden Markov decision trees|We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly and the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Bach chorales. Accepted for oral presentation at NIPS*96. 1 Introduction  Decision trees are regression or classification models that are based on a nested decomposition of the input space. An input vector x is classified recursively by a set of &#034;decisions&#034; at the nonterminal nodes of a tree, resulting in the choice of a terminal node at which an output...
292|Learning fine motion by Markov mixtures of experts|Compliantcontrol is a standard method for performing fine manipulation tasks, like grasping and assembly,  but it requires estimation of the state of contact between the robot arm and the objects involved. Here  we present a method to learn a model of the movement from measured data. The method requires little  or no prior knowledge and the resulting model explicitly estimates the state of contact . The current state  of contact is viewed as the hidden state variable of a discrete HMM. The control dependent transition  probabilities between states are modeled as parametrized functions of the measurement. We showthat  their parameters can be estimated from measurements concurrently with the estimation of the parameters  of the movementineach state of contact . The learning algorithm is a variant of the EM procedure. The  E step is computed exactly# solving the M step exactly would require solving a set of coupled nonlinear  algebraic equations in the parameters. Instead, gradient ascent is used to produce an increase in likelihood.
293|Atmospheric Modeling, Data Assimilation and Predictability|Numerical weather prediction (NWP) now provides major guidance in our daily weather forecast. The accuracy of NWP models has improved steadily since the first successful experiment made by Charney, Fj!rtoft and von Neuman (1950). During the past 50 years, a large number of technical papers and reports have been devoted to NWP, but the number of textbooks dealing with the subject has been very small, the latest being the 1980 book by Haltiner &amp; Williams, which was dedicated to descriptions of the atmospheric dynamics and numerical methods for atmospheric modeling. However, in the intervening years much impressive progress has been made in all aspects of NWP, including the success in model initialization and ensemble forecasts. Eugenia Kalnay’s recent book covers for the first time in the long history of NWP, not only methods for numerical modeling, but also the important related areas of data assimilation and predictability. It incorporates all aspects of environmental computer modeling including an historical overview of NWP, equations of motion and their approximations, a modern description of the methods to determine the initial conditions using weather observations and a clear discussion of chaos in dynamic systems and how these concepts can be
294|A hierarchical phrase-based model for statistical machine translation|We present a statistical phrase-based translation model that uses hierarchical phrases— phrases that contain subphrases. The model is formally a synchronous context-free grammar but is learned from a bitext without any syntactic information. Thus it can be seen as a shift to the formal machinery of syntaxbased translation systems without any linguistic commitment. In our experiments using BLEU as a metric, the hierarchical phrasebased model achieves a relative improvement of 7.5 % over Pharaoh, a state-of-the-art phrase-based system.  
296|An Empirical Study of Smoothing Techniques for Language Modeling|We present an extensive empirical comparison of several smoothing techniques in the domain of language modeling, including those described by Jelinek and Mercer (1980), Katz (1987), and Church and Gale (1991). We investigate for the first time how factors such as training data size, corpus (e.g., Brown versus Wall Street Journal), and n-gram order (bigram versus trigram) affect the relative performance of these methods, which we measure through the cross-entropy of test data. In addition, we introduce two novel smoothing techniques, one a variation of Jelinek-Mercer smoothing and one a very simple linear interpolation technique, both of which outperform existing methods. 1
297|Improvements in Phrase-Based Statistical Machine Translation|In statistical machine translation, the currently  best performing systems are based in some way  on phrases or word groups. We describe the  baseline phrase-based translation system and  various refinements. We describe a highly efficient  monotone search algorithm with a complexity  linear in the input sentence length. We  present translation results for three tasks: Verbmobil,  Xerox and the Canadian Hansards. For  the Xerox task, it takes less than 7 seconds to  translate the whole test set consisting of more  than 10K words. The translation results for  the Xerox and Canadian Hansards task are very  promising. The system even outperforms the  alignment template system.
298|Data networks|a b s t r a c t In this paper we illustrate the core technologies at the basis of the European SPADnet project (www. spadnet.eu), and present the corresponding first results. SPADnet is aimed at a new generation of MRI-compatible, scalable large area image sensors, based on CMOS technology, that are networked to perform gamma-ray detection and coincidence to be used primarily in (Time-of-Flight) Positron Emission Tomography (PET). The project innovates in several areas of PET systems, from optical coupling to single-photon sensor architectures, from intelligent ring networks to reconstruction algorithms. In addition, SPADnet introduced the first computational model enabling study of the full chain from gamma photons to network coincidence detection through scintillation events, optical coupling, etc. &amp; 2013 Elsevier B.V. All rights reserved. 1.
299|Modeling annotated data|We consider the problem of modeling annotated data—data with multiple types where the instance of one type (such as a caption) serves as a description of the other type (such as an image). We describe three hierarchical probabilistic mixture models that are aimed at such data, culminating in the Corr-LDA model, a latent variable model that is effective at modeling the joint distribution of both types and the conditional distribution of the annotation given the primary type. We take an empirical Bayes approach to finding parameter estimates and conduct experiments in held-out likelihood, automatic annotation, and text-based image retrieval using the Corel database of images and captions.  
301|A Language Modeling Approach to Information Retrieval|Models of document indexing and document retrieval have been extensively studied. The integration of these two classes of models has been the goal of several researchers but it is a very difficult problem. We argue that much of the reason for this is the lack of an adequate indexing model. This suggests that perhaps a better indexing model would help solve the problem. However, we feel that making unwarranted parametric assumptions will not lead to better retrieval performance. Furthermore, making prior assumptions about the similarity of documents is not warranted either. Instead, we propose an approach to retrieval based on probabilistic language modeling. We estimate models for each document individually. Our approach to modeling is non-parametric and integrates document indexing and document retrieval into a single model. One advantage of our approach is that collection statistics which are used heuristically in many other retrieval models are an integral part of our model. We have...
303|Matching words and pictures|We present a new approach for modeling multi-modal data sets, focusing on the specific case of segmented images with associated text. Learning the joint distribution of image regions and words has many applications. We consider in detail predicting words associated with whole images (auto-annotation) and corresponding to particular image regions (region naming). Auto-annotation might help organize and access large collections of images. Region naming is a model of object recognition as a process of translating image regions to words, much as one might translate from one language to another. Learning the relationships between image regions and semantic correlates (words) is an interesting example of multi-modal data mining, particularly because it is typically hard to apply data mining techniques to collections of images. We develop a number of models for the joint distribution of image regions and words, including several which explicitly learn the correspondence between regions and words. We study multi-modal and correspondence extensions to Hofmann’s hierarchical clustering/aspect model, a translation model adapted from statistical machine translation (Brown et al.), and a multi-modal extension to mixture of latent Dirichlet allocation
304|A Variational Bayesian Framework for Graphical Models|This paper presents a novel practical framework for Bayesian model  averaging and model selection in probabilistic graphical models.  Our approach approximates full posterior distributions over model  parameters and structures, as well as latent variables, in an analytical  manner. These posteriors fall out of a free-form optimization  procedure, which naturally incorporates conjugate priors. Unlike  in large sample approximations, the posteriors are generally nonGaussian  and no Hessian needs to be computed. Predictive quantities  are obtained analytically. The resulting algorithm generalizes  the standard Expectation Maximization algorithm, and its convergence  is guaranteed. We demonstrate that this approach can be  applied to a large class of models in several domains, including  mixture models and source separation.  1 Introduction  A standard method to learn a graphical model  1  from data is maximum likelihood (ML). Given a training dataset, ML estimates a single optimal value f...
305|A probabilistic framework for semantic video indexing, filtering, and retrival|Abstract—Semantic filtering and retrieval of multimedia con-tent is crucial for efficient use of the multimedia data repositories. Video query by semantic keywords is one of the most difficult prob-lems in multimedia data retrieval. The difficulty lies in the mapping between low-level video representation and high-level semantics. We therefore formulate the multimedia content access problem as a multimedia pattern recognition problem. We propose a proba-bilistic framework for semantic video indexing, which can support filtering and retrieval and facilitate efficient content-based access. To map low-level features to high-level semantics we propose prob-abilistic multimedia objects (multijects). Examples of multijects in movies include explosion, mountain, beach, outdoor, music etc. Se-mantic concepts in videos interact and to model this interaction ex-plicitly, we propose a network of multijects (multinet). Using prob-abilistic models for six site multijects, rocks, sky, snow, water-body, forestry/greenery and outdoor and using a Bayesian belief network as the multinet we demonstrate the application of this framework to semantic indexing. We demonstrate how detection performance can be significantly improved using the multinet to take intercon-ceptual relationships into account. We also show how the multinet can fuse heterogeneous features to support detection based on in-ference and reasoning. Index Terms—Bayesian belief networks, hidden Markov models, likelihood ratio test, multimedia understanding, probabilistic graphical networks, ROC curves, semantic video indexing, query
306|Image Information Retrieval: An Overview of Current Research|This paper provides an overview of current research in image information retrieval and provides an outline of areas for future research. The approach is broad and interdisciplinary and focuses on three aspects of image research (IR): text-based retrieval, content-based retrieval, and user interactions with image information retrieval systems. The review concludes with a call for image retrieval evaluation studies similar to TREC.  Keywords: Information Science, Image Retrieval, CBIR, Introduction  Interest in image retrieval has increased in large part due to the rapid growth of the World Wide Web. According to a recent study (Lawrence &amp; Giles, 1999) there are 180 million images on the publicly indexable Web, a total amount of image data of about 3Tb [terabytes], and an astounding one million or more digital images are being produced every day (Jain, 93). The need to find a desired image from a collection is shared by many groups, including journalists, engineers, historians, designers...
307|A model of multimedia information retrieval|Abstract. Research on multimedia information retrieval (MIR) has recently witnessed a booming interest. A prominent feature of this research trend is its simultaneous but independent materialization within several fields of computer science. The resulting richness of paradigms, methods and systems may, on the long run, result in a fragmentation of efforts and slow down progress. The primary goal of this study is to promote an integration of methods and techniques for MIR by contributing a conceptual model that encompasses in a unified and coherent perspective the many efforts that are being produced under the label of MIR. The model offers a retrieval capability that spans two media, text and images, but also several dimensions: form, content and structure. In this way, it reconciles similarity-based methods with semantics-based ones, providing the guidelines for the design of systems that are able to provide a generalized multimedia retrieval service, in which the existing forms of retrieval not only coexist, but can be combined in any desired manner. The model is formulated in terms of a fuzzy description logic, which plays a twofold role: (1) it directly models semantics-based retrieval, and (2) it offers an ideal framework for the integration of the multimedia and multidimensional aspects of retrieval mentioned above. The model also accounts for relevance feedback in both text and image retrieval, integrating known techniques for taking into account user judgments. The implementation of
308|&#034;Name That Song!&#034;: A Probabilistic Approach to Querying on Music and Text|We present a novel, flexible statistical approach for modelling music and  text jointly. The approach is based on multi-modal mixture models and  maximum a posteriori estimation. The learned models can be used to  browse databases with documents containing music and text, to search  for music using queries consisting of music and text (lyrics and other  contextual information), to annotate text documents with music, and to  automatically recommend or identify similar songs.
309|Data Security|The rising abuse of computers and increasing threat to personal privacy through data banks have stimulated much interest m the techmcal safeguards for data. There are four kinds of safeguards, each related to but distract from the others. Access controls regulate which users may enter the system and subsequently whmh data sets an active user may read or wrote. Flow controls regulate the dissemination of values among the data sets accessible to a user. Inference controls protect statistical databases by preventing questioners from deducing confidential information by posing carefully designed sequences of statistical queries and correlating the responses. Statlstmal data banks are much less secure than most people beheve. Data encryption attempts to prevent unauthorized disclosure of confidential information in transit or m storage. This paper describes the general nature of controls of each type, the kinds of problems they can and cannot solve, and their inherent limitations and weaknesses. The paper is intended for a general audience with little background in the area.
311|New Directions in Cryptography|Two kinds of contemporary developments in cryptography are examined. Widening applications of teleprocessing have given rise to a need for new types of cryptographic systems, which minimize the need for secure key distribution channels and supply the equivalent of a written signature. This paper suggests ways to solve these currently open problems. It also discusses how the theories of communication and computation are beginning to provide the tools to solve cryptographic problems of long standing.
312|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
313|A Lattice Model of Secure Information Flow|This paper investigates mechanisms that guarantee secure information flow in a computer system. These mechanisms are examined within a mathematical framework suitable for formulating the requirements of secure information flow among security classes. The central component of the model is a lattice structure derived from the security classes and justified by the semantics of information flow. The lattice properties permit concise formulations of the security requirements of different existing systems and facilitate the construction of mechanisms that enforce security. The model provides a unifying view of all systems that restrict information flow, enables a classification of them according to security objectives, and suggests some new approaches. It also leads to the construction of automatic program certification mechanisms for verifying the secure flow of information through a program.
314|A Note on the Confinement Problem|This not explores the problem of confining a program during its execution so that it cannot transmit information to any other program except its caller. A set of examples attempts to stake out the boundaries of the problem. Necessary conditions for a solution are stated and informally justified.
315|Certification of Programs for Secure Information Flow|This paper presents a certification mechanism for verifying the secure flow of information through a program. Because it exploits the properties of a lattice structure among security classes, the procedure is sufficiently simple that it can easily be included in the analysis phase of most existing compilers. Appropriate semantics are presented and proved correct. An important application is the confinement problem: The mechanism can prove that a program cannot cause supposedly nonconfidential results to depend on confidential input data.
316|Password Security: A Case History|This paper describes the history of the design of the password security scheme on a  remotely accessed time-sharing system. The present design was the result of countering  observed attempts to penetrate the system. The result is a compromise between extreme  security and ease of use.
317|A hardware architecture for implementing protection rings|Protection of computations and information is an important aspect of a computer utility. In a system which usessegmentation as a memory addressing scheme, protection can be achieved in part by associating concentric rings of decreasing access privilege with a computation. This paper describes hardware processor mechanisms for implementing these rings of protection. The mechanisms allow cross-ring calls and subsequent returns to occur without trapping to the supervisor. Automatic hardware validation of referencesacross ring boundaries is also performed. Thus, a call by a user procedure to a protected subsystem (including the the supervisor) is identical to a call to a companion user procedure. The mechanisms of passing and referencing arguments are the same in both cases as well.
318|The tracker: a threat to statistical database security|The query programs of certain databases report raw statistics for query sets, which are groups of records specified implicitly by a characteristic formula. The raw statistics include query set size and sums of powers of values in the query set. Many users and designers believe that the individual records will remain confidential as long as query programs refuse to report the statistics of query sets which are too small. It is shown that the compromise of small query sets can in fact almost always be accomplished with the help of characteristic formulas called trackers. Schlorer’s individual tracker is reviewed, it is derived from known characteristics of a given individual and permits deducing additional characteristics he may have. The general tracker is introduced: It permits calculating statistics for arbitrary query sets, without requiring preknowledge of anything in the database. General trackers always exist if there are enough distinguishable classes of individuals in the database, in which case the trackers have a simple form. Almost all databases have a general tracker, and general trackers are almost always easy to find. Security is not guaranteed by the lack of a general tracker.
319|Operating Systems|Introduction Early operating systems were control programs a few thousand bytes long that scheduled jobs, drove peripheral devices, and kept track of system usage for billing purposes. Modern operating systems are much larger, ranging from hundreds of thousands of bytes for personal computers (e.g., MS/DOS, Xenix) to tens of millions of bytes for mainframes (e.g., Honeywell&#039;s Multics, IBM&#039;s MVS, AT
320|Third generation computer systems|The common features of third generation operating systems are surveyed from a general view, with emphasis on the common abstractions that constitute at least the basis for a &amp;quot;theory &amp;quot; of operating systems. Properties of specific systems are not discussed except where examples are useful. The technical aspects of issues and concepts are stressed, the nontechnical aspects mentioned only briefly. A perfunctory knowledge of third generation systems is presumed. Key words and phrases: multiprogramming systems, operating systems, supervisory systems, time-sharing systems, programming, storage allocation, memory allocation, processes, concurrency, parallelism, resource allocation, protection CR categories: 1.3, 4.0, 4.30, 6.20 It has been the custom to divide the era of electronic computing into &amp;quot;generations&amp;quot; whose approximate dates are:
321|From data mining to knowledge discovery in databases|¦ Data mining and knowledge discovery in databases have been attracting a significant amount of research, industry, and media attention of late. What is all the excitement about? This article provides an overview of this emerging field, clarifying how data mining and knowledge discovery in databases are related both to each other and to related fields, such as machine learning, statistics, and databases. The article mentions particular real-world applications, specific data-mining techniques, challenges involved in real-world applications of knowledge discovery, and current and future research directions in the field. Across a wide variety of fields, data are
323|The Merge/Purge Problem for Large Databases|Many commercial organizations routinely gather large numbers of databases for various marketing and business analysis functions. The task is to correlate information from different databases by identifying distinct individuals that appear in a number of different databases typically in an inconsistent and often incorrect fashion. The problem we study here is the task of merging data from multiple sources in as efficient manner as possible, while maximizing the accuracy of the result. We call this the merge/purge problem. In this paper we detail the sorted neighborhood method that is used by some to solve merge/purge and present experimental results that demonstrates this approach may work well in practice but at great expense. An alternative method based upon clustering is also presented with a comparative evaluation to the sorted neighborhood method. We show a means of improving the accuracy of the results based upon a multi-pass approach that succeeds by computing the Transitive Clos...
324|Unexpectedness as a Measure of Interestingness in Knowledge Discovery|Organizations are taking advantage of &#034;data-mining&#034; techniques to leverage the vast amounts of data captured as they process routine transactions. Data-mining is the process of discovering hidden structure or patterns in data. However several of the pattern discovery methods in datamining systems have the drawbacks that they discover too many obvious or irrelevant patterns and that they do not leverage to a full extent valuable prior domain knowledge that managers have. This research addresses these drawbacks by developing ways to generate interesting patterns by incorporating managers&#039; prior knowledge in the process of searching for patterns in data. Specifically we focus on providing methods that generate unexpected patterns with respect to managerial intuition by eliciting managers&#039; beliefs about the domain and using these beliefs to seed the search for unexpected patterns in data. Our approach should lead to the development of decision support systems that provide managers with mor...
325|The interestingness of deviations|gps~gte.com, matheus~gte.com One of the moet promising areas in Knowledge Discovery in Databases is the automatic analysis of changes and deviations. Several systems have recently been developed for this task. Suc ~ of these systems hinges on their ability to identify s few important and relevant deviations among the multitude of potentially interesting events:  ~ In this paper we argue that related deviations should be grouped togetherin a finding and that the interestingness of a finding is the estimated benefit from a poesible ~tion connected to it. We discuss methods for determining the estimated benefit from the impact of the deviations and the success probability of an action. Our analysis is done in the context of the Key Findings Reporter (KEFIIt), a system for discovering and explaining ~key findings &#034; in large relational databases, currently being applied to the analysis of healthcare information.
326|The World Wide Web: quagmire or gold mine?|This article considers the question: is effective Web mining possible?
327|Discovering Informative Patterns and Data Cleaning|We present a method for discovering informative patterns from data. With this method, large databases can be reduced to only a few representative data entries. Our framework also encompasses methods for cleaning databases containing corrupted data. Both on-line and off-line algorithms are proposed and experimentally checked on databases of handwritten images. The generality of the framework makes it an attractive candidate for new applications in knowledge discovery.  Keywords: knowledge discovery, machine learning, informative patterns, data cleaning, information gain.  4.1 
328|Selecting Among Rules Induced from a Hurricane Database|can achieve orders of magnitude reduction in the volume of data For example, we applied a commercial tool (IXLtin) to a 1,819 record tropical storm database, yielding 161 rules. However, the human comprehension goals of Knowledge Discovery in Databases may require still more orders of magnitude. We present a rule refinement strategy, partly implemented in a Prolog program, that operationalizes &#034;interestingness &#034; into performance, simplicity, novelty, and significance. Applying the strategy to the induced rulebase yielded 10 &#034;genuinely interesting &#034; rules. I. PURPOSE OF THE STUDY At The Travelers Insurance Company, we are involved in applying statistics and artificial intelligence techniques to the solution of business problems. This work is part of an investigation into applications for Natural Hazards Research Services. The purpose of this study is not to deyelop a hurricane model or predictor. It is, rather, to assess the utility of rule induction technology and our particular rule refinement strategy. The object task of the study is to develop rules that
329|KDD for Science Data Analysis: Issues and Examples|Tile analysis of tile massive data sets collected by scientific instruments demands automation as a pre-requisite to analysis. There is an urgent need to cre-ate an intermediate level at which scientists can oper-ate effectively; isolating them from the massive sizes and harnessing human analysis capabilities to focus on tasks in which machines do not even renmtely ap-proach humans--namely, creative data analysis, the-ory and hypothesis formation, and drawing insights into underlying phe,mmena. We give an overview of the main issues in the exploitation of scientific data.sets, present five c,~se studies where KDD tools play important and enabling roles, and conclude with fi,ture challenges for data mining and KDD techniques in science data analysis. 1
330| 	 Active Data Mining   |We introduce an active data mining paradigm that combines the recent work in data mining with the rich literature on active database systems. In this paradigm, data is continuously mined at a desired frequency. As rules are discovered, they are added to a rulebase, and if they already exist, the history of the statistical parameters associated with the rules is updated. When the history starts exhibiting certain trends, specified as shape queries in the user-speci ed triggers, the triggers are red and appropriate actions are initiated. To be able to specify shape queries, we describe the constructs for de ning shapes, and discuss how the shape predicates are used in a query construct to retrieve rules whose histories exhibit the desired trends. We describe how this query capability is integrated into a trigger system to realize an active mining system. The system presented here has been validated using two sets of customer data.
331|Fast Spatio-Temporal Data Mining of Large Geophysical Datasets|The important scientific challenge of understanding global climate change is one that clearly requires the application of knowledge discovery and datamining techniques on a massive scale. Advances in parallel supercomputing technology, enabling high-resolution modeling, as well as in sensor technology, allowing data capture on an unprecedented scale, conspire to overwhelm present-day analysis approaches. We present here early experiences with a prototype exploratory data analysis environment, CONQUEST, designed to provide content-based access to such massive scientific datasets. CONQUEST (CONtent-based Querying in Space and Time) employs a combination of workstations and massively parallel processors (MPP&#039;s) to mine geophysical datasets possessing a prominent temporal component. It is designed to enable complex multi-modal interactive querying and knowledge discovery, while simultaneously coping with the extraordinary computational demands posed by the scope of the datasets involved. A...
332|Predicting Equity Returns from Securities Data|Our experiments with capital markets data suggest that the domain can be effectively modeled by classification rules induced from available historical data for the purpose of making gainful predictions for equityinvestments. New classification techniques developed at IBM Research, including minimal rule generation (R-MINI) and contextual feature analysis, seem robust enough for consistently extracting useful information from noisy domains such as financial markets. We will briefly introduce the rationale for our minimal rule generation technique, and the motivation for the use of contextual information in analyzing features. We will then describe our experience from several experiments with the S&amp;P 500 data, illustrating the general methodology, and the results of correlations and simulated managed investment based on classification rules generated by R-MINI. Wewillsketchhow the rules for classifications can be effectively used for numerical prediction, and eventually to an investment ...
333|Graphical Models for Discovering Knowledge|There are many different ways of representing knowledge, and for each of these ways there are many different discovery algorithms. How can we compare different representations? How can we mix, match and merge representations and algorithms on new problems with their own unique requirements? This chapter introduces probabilistic modeling as a philosophy for addressing these questions and presents graphical models for representing probabilistic models. Probabilistic graphical models are a unified qualitative and quantitative framework for representing and reasoning with probabilities and independencies. 4.1 Introduction Perhaps one common element of the discovery systems described in this and previous books on knowledge discovery is that they are all different. Since the class of discovery problems is a challenging one, we cannot write a single program to address all of knowledge discovery. The KEFIR discovery system applied to health care by Matheus, Piatetsky-Shapiro, and McNeill (199...
334|An Overview of Issues in Developing Industrial Data Mining and Knowledge Discovery Applications|This paper surveys the growing number of indu5 trial applications of data mining and knowledge discovery. We look at the existing tools, describe some representative applications, and discuss the major issues and problems for building and deploying successful applications and their adoption by business users. Finally, we examine how to assess the potential of a knowledge discovery application. 1
335|Analyzing the Benefits of Domain Knowledge in Substructure Discovery|Discovering repetitive, interesting, and functional substructures in a structural  database improves the ability to interpret and compress the data. However, scientists  working with a database in their area of expertise often search for a predetermined  type of structure, or for structures exhibiting characteristics specic to the domain.  This paper presents methods for guiding the discovery process with domain-specic  knowledge. In this paper, the Subdue discovery system is used to evaluate the bene-  ts of using domain knowledge. The domain knowledge is incorporated into Subdue  following a single general methodology to guide the discovery process. Results show  using domain-specic knowledge improves the search for substructures which are useful  to the domain, and leads to greater compression of the data. To illustrate these bene-  ts, examples and experiments from the domain of computer programming, computer  aided design circuit, and a series of articially-generated domains...
336|A discriminatively trained, multiscale, deformable part model|This paper describes a discriminatively trained, multiscale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose. 1.
337|Histograms of Oriented Gradients for Human Detection|We study the question of feature sets for robust visual object recognition, adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of Histograms of Oriented Gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds. 1
338|Making Large-Scale SVM Learning Practical|Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, off-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains.  
339|Object class recognition by unsupervised scale-invariant learning|We present a method to learn and recognize object class models from unlabeled and unsegmented cluttered scenes in a scale invariant manner. Objects are modeled as flexible constellations of parts. A probabilistic representation is used for all aspects of the object: shape, appearance, occlusion and relative scale. An entropy-based feature detector is used to select regions and their scale within the image. In learning the parameters of the scale-invariant object model are estimated. This is done using expectation-maximization in a maximum-likelihood setting. In recognition, this model is used in a Bayesian manner to classify images. The flexible nature of the model is demonstrated by excellent results over a range of datasets including geometrically constrained classes (e.g. faces, cars) and flexible objects (such as animals). 1.
340|Pictorial Structures for Object Recognition|In this paper we present a statistical framework for modeling the appearance of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to model an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We use these models to address the problem of detecting an object in an image as well as the problem of learning an object model from training examples, and present efficient algorithms for both these problems. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.
341|Local features and kernels for classification of texture and object categories: a comprehensive study|Recently, methods based on local image features have shown promise for texture and object recognition tasks. This paper presents a large-scale evaluation of an approach that represents images as distributions (signatures or histograms) of features extracted from a sparse set of keypoint locations and learns a Support Vector Machine classifier with kernels based on two effective measures for comparing distributions, the Earth Mover’s Distance and the ? 2 distance. We first evaluate the performance of our approach with different keypoint detectors and descriptors, as well as different kernels and classifiers. We then conduct a comparative evaluation with several state-of-the-art recognition methods on four texture and five object databases. On most of these databases, our implementation exceeds the best reported results and achieves comparable performance on the rest. Finally, we investigate the influence of background correlations on recognition performance via extensive tests on the PASCAL database, for which ground-truth object localization information is available. Our experiments demonstrate that image representations based on distributions of local features are surprisingly effective for classification of texture and object images under challenging real-world conditions, including significant intra-class variations and substantial background clutter.
342|Spatial priors for part-based recognition using statistical models|We present a class of statistical models for part-based object recognition that are explicitly parameterized according to the degree of spatial structure they can represent. These models provide a way of relating different spatial priors that have been used for recognizing generic classes of objects, including joint Gaussian models and tree-structured models. By providing explicit control over the degree of spatial structure, our models make it possible to study the extent to which additional spatial constraints among parts are actually helpful in detection and localization, and to consider the tradeoff in representational power and computational cost. We consider these questions for object classes that have substantial geometric structure, such as airplanes, faces and motorbikes, using datasets employed by other researchers to facilitate evaluation. We find that for these classes of objects, a relatively small amount of spatial structure in the model can provide statistically indistinguishable recognition performance from more powerful models, and at a substantially lower computational cost. 1.
343|Distance transforms of sampled functions|This paper provides linear-time algorithms for solving a class of minimization problems in-volving a cost function with both local and spatial terms. These problems can be viewed as a generalization of classical distance transforms of binary images, where the binary image is replaced by an arbitrary sampled function. Alternatively they can be viewed in terms of the minimum convolution of two functions, which is an important operation in grayscale mor-phology. A useful consequence of our techniques is a simple, fast method for computing the Euclidean distance transform of a binary image. The methods are also applicable to Viterbi decoding, belief propagation and optimal control. 1
344|A probabilistic approach to object recognition using local photometry and global geometry|Abstract. Many object classes, including human faces, can be modeled as a set of characteristic parts arranged in a variable spatial con guration. We introduce a simpli ed model of a deformable object class and derive the optimal detector for this model. However, the optimal detector is not realizable except under special circumstances (independent part positions). A cousin of the optimal detector is developed which uses \soft &#034; part detectors with a probabilistic description of the spatial arrangement of the parts. Spatial arrangements are modeled probabilistically using shape statistics to achieve invariance to translation, rotation, and scaling. Improved recognition performance over methods based on \hard &#034; part detectors is demonstrated for the problem of face detection in cluttered scenes. 1
345|Object Detection Using the Statistics of Parts| In this paper we describe a trainable object detector and its instantiations for detecting faces and cars at any size, location, and pose. To cope with variation in object orientation, the detector uses multiple classifiers, each spanning a different range of orientation. Each of these classifiers determines whether the object is present at a specified size within a fixed-size image window. To find the object at any location and size, these classifiers scan the image exhaustively. Each classifier is based on the statistics of localized parts. Each part is a transform from a subset of wavelet coefficients to a discrete set of values. Such parts are designed to capture various combinations of locality in space, frequency, and orientation. In building each classifier, we gathered the class-conditional statistics of these part values from representative samples of object and non-object images. We trained each classifier to minimize classification error on the training set by using Adaboost with Confidence-Weighted Predictions (Shapire and Singer, 1999). In detection, each classifier computes the part values within the image window and looks up their associated classconditional probabilities. The classifier then makes a decision by applying a likelihood ratio test. For efficiency, the classifier evaluates this likelihood ratio in stages. At each stage, the classifier compares the partial likelihood ratio to a threshold and makes a decision about whether to cease evaluation—labeling the input as non-object—or to continue further evaluation. The detector orders these stages of evaluation from a low-resolution to a high-resolution search of the image. Our trainable object detector achieves reliable and efficient detection of human faces and passenger cars with out-of-plane rotation.
346|Probabilistic Methods  for Finding People|Finding people in pictures presents a particularly difficult object recognition problem. We show how to find people by finding candidate body segments, and then constructing assemblies of segments that are consistent with the constraints on the appearance of a person that result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to inspect every group, due to the huge combinatorial complexity. We propose two
347|Pop: Patchwork of parts models for object recognition|We formulate a deformable template model for objects with a clearly defined mechanism for parameter estimation. A separate model is estimated for each class, and classification is likelihood based- no discrmination boundaries are learned. Nonethe-less high classification rates are achieved with small training samples. The data models are defined on binary oriented edge features that are highly robust to photometric vari-ation and small local deformations. The deformation of an object is defined in terms of locations of a moderate number reference points. Each reference point is associated with a part- a probability map assigning a probability for each edge type at each pixel in a window. The likelihood of the edge data on the entire image conditional on the deformation is described as a patchwork of parts (POP) model- the edges are assumed conditionally independent, and the marginal at each pixel is obtained by a patchwork operation: averaging the marginal probabilities contributed by each part covering the pixel. Object classes are modeled as mixtures of POP models that are discovered se-quentially as more class data is observed. Experiments are presented on the MNIST database, hundreds of deformed LATEX shapes, reading zipcodes, and face detection. 1
348|The generalized A* architecture|We consider the problem of computing a lightest derivation of a global structure using a set of weighted rules. A large variety of inference problems in AI can be formulated in this framework. We generalize A * search and heuristics derived from abstractions to a broad class of lightest derivation problems. We also describe a new algorithm that searches for lightest derivations using a hierarchy of abstractions. Our generalization of A * gives a new algorithm for searching AND/OR graphs in a bottom-up fashion. We discuss how the algorithms described here provide a general architecture for addressing the pipeline problem — the problem of passing information back and forth between various stages of processing in a perceptual system. We consider examples in computer vision and natural language processing. We apply the hierarchical search algorithm to the problem of estimating the boundaries of convex objects in grayscale images and compare it to other search methods. A second set of experiments demonstrate the use of a new compositional model for finding salient curves in images. 1.
349|A discriminative framework for modeling object classes|Here we explore a discriminative learning method on underlying generative models for the purpose of discriminating between object categories. Visual recognition algorithms learn models from a set of training examples. Generative models learn their representations by considering data from a single class. Generative models are popular in computer vision for many reasons, including their ability to elegantly incorporate prior knowledge and to handle correspondences between object parts and detected features. However, generative models are often inferior to discriminative models during classification tasks. We study a discriminative approach to learning object categories which maintains the representational power of generative learning, but trains the generative models in a discriminative manner. The discriminatively trained models perform better during classification tasks as a result of selecting discriminative sets of features. We conclude by proposing a multiclass object recognition system which initially trains object classes in a generative manner, identifies subsets of similar classes with high confusion, and finally trains models for these subsets in a discriminative manner to realize gains in classification performance. 1
350|Training Deformable Models for Localization|We present a new method for training deformable models. Assume that we have training images where part locations have been labeled. Typically, one fits a model by maximizing the likelihood of the part labels. Alternatively, one could fit a model such that, when the model is run on the training images, it finds the parts. We do this by maximizing the conditional likelihood of the training data. We formulate model-learning as parameter estimation in a conditional random field (CRF). Initializing parameters with their maximum likelihood estimates, we reach the global optimum by gradient ascent. We present a learning algorithm that searches exhaustively over all part locations in an image without relying on feature detectors. This provides millions of examples of training data, and seems to avoid over-fitting issues known with CRFs. Results for part localization are relatively scarce in the community. We present results on three established datasets; Caltech motorbikes [8], USC people [19], and Weizmann horses [3]. In the Caltech set we significantly outperform the state-of-the-art [6]. For the challenging people dataset, we present results that are comparable to [19], but are obtained using a significantly more generic model (devoid of a face or skin detector). Our model is general enough to find other articulated objects; we use it to recover poses of horses in the challenging Weizmann database. 1.
351|Using segmentation to verify object hypotheses|We present an approach for object recognition that combines detection and segmentation within a efficient hypothesize/test framework. Scanning-window template classifiers are the current state-of-the-art for many object classes such as faces, cars, and pedestrians. Such approaches, though quite successful, can be hindered by their lack of explicit encoding of object shape/structure – one might, for example, find faces in trees. We adopt the following strategy; we first use these systems as attention mechanisms, generating many possible object locations by tuning them for low missed-detections and high false-positives. At each hypothesized detection, we compute a local figure-ground segmentation using a window of slightly larger extent than that used by the classifier. This segmentation task is guided by top-down knowledge. We learn offline from training data those segmentations that are consistent with true positives. We then prune away those hypotheses with bad segmentations. We show this strategy leads to significant improvements (10-20%) over established approaches such as ViolaJones and DalalTriggs on a variety of benchmark datasets including the PASCAL challenge, LabelMe, and the INRIAPerson dataset. 1.
352|Semantic hierarchies for recognizing objects and parts|This paper describes the construction and use of a novel representation for the recognition of objects and their parts, the semantic hierarchy. Its advantages include improved classification performance, accurate detection and localization of object parts and sub-parts, and explicitly identifying the different appearances of each object part. The semantic hierarchy algorithm starts by constructing a minimal feature hierarchy and proceeds by adding semantically equivalent representatives to each node, using the entire hierarchy as a context for determining the identity and locations of added features. Part detection is obtained by a bottom-up top-down cycle. Unlike previous approaches, the semantic hierarchy learns to represent the set of possible appearances of object parts at all levels, and their statistical dependencies. The algorithm is fully automatic and is shown experimentally to substantially improve the recognition of objects and their parts. 1.
355|Regularization and variable selection via the Elastic Net|Summary. We propose the elastic net, a new regularization and variable selection method. Real world data and a simulation study show that the elastic net often outperforms the lasso, while enjoying a similar sparsity of representation. In addition, the elastic net encourages a grouping effect, where strongly correlated predictors tend to be in or out of the model together.The elastic net is particularly useful when the number of predictors (p) is much bigger than the number of observations (n). By contrast, the lasso is not a very satisfactory variable selection method in the p n case. An algorithm called LARS-EN is proposed for computing elastic net regularization paths efficiently, much like algorithm LARS does for the lasso.
356|On Model Selection Consistency of Lasso|Sparsity or parsimony of statistical models is crucial for their proper interpretations, as  in sciences and social sciences. Model selection is a commonly used method to find such  models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani,  1996) is now being used as a computationally feasible alternative to model selection.
357|Leave-One-Out Support Vector Machines|We present a new learning algorithm for pattern recognition inspired by a recent upper bound on leave--one--out error [ Jaakkola and Haussler, 1999 ] proved for Support Vector Machines (SVMs) [ Vapnik, 1995; 1998 ] . The new approach directly minimizes the expression given by the bound in an attempt to minimize leave--one--out error. This gives a convex optimization problem which constructs a sparse linear classifier in feature space using the kernel technique. As such the algorithm possesses many of the same properties as SVMs. The main novelty of the algorithm is that apart from the choice of kernel, it is parameterless -- the selection of the number of training errors is inherent in the algorithm and not chosen by an extra free parameter as in SVMs. First experiments using the method on benchmark datasets from the UCI repository show results similar to SVMs which have been tuned to have the best choice of parameter.  1 Introduction  Support Vector Machines (SVMs), motivated by minim...
358|Sparse Principal Component Analysis|Principal component analysis (PCA) is widely used in data processing and dimensionality  reduction. However, PCA su#ers from the fact that each principal component is a linear combination  of all the original variables, thus it is often di#cult to interpret the results. We introduce  a new method called sparse principal component analysis (SPCA) using the lasso (elastic net)  to produce modified principal components with sparse loadings. We show that PCA can be  formulated as a regression-type optimization problem, then sparse loadings are obtained by imposing  the lasso (elastic net) constraint on the regression coe#cients. E#cient algorithms are  proposed to realize SPCA for both regular multivariate data and gene expression arrays. We  also give a new formula to compute the total variance of modified principal components. As  illustrations, SPCA is applied to real and simulated data, and the results are encouraging.
359|Boosting with early stopping: convergence and consistency|Abstract Boosting is one of the most significant advances in machine learning for classification and regression. In its original and computationally flexible version, boosting seeks to minimize empirically a loss function in a greedy fashion. The resulted estimator takes an additive function form and is built iteratively by applying a base estimator (or learner) to updated samples depending on the previous iterations. An unusual regularization technique, early stopping, is employed based on CV or a test set. This paper studies numerical convergence, consistency, and statistical rates of convergence of boosting with early stopping, when it is carried out over the linear span of a family of basis functions. For general loss functions, we prove the convergence of boosting&#039;s greedy optimization to the infinimum of the loss function over the linear span. Using the numerical convergence result, we find early stopping strategies under which boosting is shown to be consistent based on iid samples, and we obtain bounds on the rates of convergence for boosting estimators. Simulation studies are also presented to illustrate the relevance of our theoretical results for providing insights to practical aspects of boosting. As a side product, these results also reveal the importance of restricting the greedy search step sizes, as known in practice through the works of Friedman and others. Moreover, our results lead to a rigorous proof that for a linearly separable problem, AdaBoost with ffl! 0 stepsize becomes an L1-margin maximizer when left to run to convergence. 1 Introduction In this paper we consider boosting algorithms for classification and regression. These algorithms present one of the major progresses in machine learning. In their original version, the computational aspect is explicitly specified as part of the estimator/algorithm. That is, the empirical minimization of an appropriate loss function is carried out in a greedy fashion, which means that at each step, a basis function that leads to the largest reduction of empirical risk is added into the estimator. This specification distinguishes boosting from other statistical procedures which are defined by an empirical minimization of a loss function without the numerical optimization details.
360|Empty alternation|Abstract. We introduce the notion of empty alternation by investigating alternating automata which are restricted to empty their storage except for a logarithmically space-bounded tape before making an alternating transition. In particular, we consider the cases when the depth of alternation is bounded by a constant or a polylogarithmic function. In this way we get new characterizations of the classes AC k, SAC k and P using a push-down store and new characterizations of the class T P 2 using Turing tapes. 1
362|Initial Conditions and Moment Restrictions in Dynamic Panel Data Models|Estimation of the dynamic error components model is considered using two alternative linear estimators that are designed to improve the properties of the standard firstdifferenced GMM estimator. Both estimators require restrictions on the initial conditions process. Asymptotic efficiency comparisons and Monte Carlo simulations for the simple AR(1) model demonstrate the dramatic improvement in performance of the proposed estimators compared to the usual first-differenced GMM estimator, and compared to non-linear GMM. The importance of these results is illustrated in an application to the estimation of a labour demand model using company panel data.
363|Estimation and Inference in Econometrics|The astonishing increase in computer performance over the past two decades has made it possible for economists to base many statistical inferences on simulated, or bootstrap, distributions rather than on distributions obtained from asymptotic theory. In this paper, I review some of the basic ideas of bootstrap inference. The paper discusses Monte Carlo tests, several types of bootstrap test, and bootstrap confidence intervals. Although bootstrapping often works well, it does not do so in every case.
364|Using Geographic Variation in College Proximity to Estimate the Return to Schooling|Although schooling and earnings are highly correlated, social scientists have argued for decades over the causal effect of education. A convincing analysis of the causal link between education and earnings requires an exogenous source of variation in education outcomes. This paper explores the use of college proximity as an exogenous determinant of schooling. An examination of the NLS Young Men Cohort reveals that men who grew up in local labor markets with a nearby college have significantly higher education and significantly higher earnings than other men. The education and earnings gains are concentrated among men with poorlyeducated parents -- men who would otherwise stop schooling at relatively low levels. When college proximity is taken as an exogenous determinant of schooling the implied instrumental variables estimates of the return to schooling are 25-60% higher than conventional ordinary least squares estimates.
365|Learning by Doing and Learning from Others: Human Capital and Technical Change in Agriculture|University of PennsylvaniaHousehold-level panel data from a nationally representative sample of rural Indian households describing the adoption and profitability of high-yielding seed varieties (HYVs) associated with the Green Revolution are used to test the implications of a model incorporating learning-by-doing and learning spillovers. The estimates indicate that: (i) imperfect knowledge about the management of the new seeds was a significant barrier to adoption; (ii) this barrier diminished as farmer experience with the new technologies increased; (iii) own experience and neighbors &#039; experience with HYV significantly increased HYV profitability; (iv) farmers do not fully incorporate the village returns to learning in making adoption decisions 1 I.
366|Finishing High School and Starting College: Do Catholic Schools Make a Difference? Quarterly|In this paper, we consider two measures of the relative effectiveness of public and Catholic schools: finishing high school and starting college. These measures are potentially more important indicators of school quality than standardized test scores in light of the economic consequences of obtaining more education. Single-equation estimates suggest that for the typical student, attending a Catholic high school raises the probability of finishing high school or entering a four-year college by thirteen percentage points. In bivariate probit models we find almost no evidence that our single-equation estimates are subject to selection bias. I.
367|Estimating the Labor Market Impact of Voluntary Military Service Using Social Security Data on Military Applicants,” Econometrica 66:2|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
368|Measuring Positive Externalities from Unobservable Victim Precaution: An Empirical Analysis of Lojack.” Quarterly|Lojack is a hidden radio-transmitter device used for retrieving stolen vehicles. Because there is no external indication that Lojack has been installed, it does not directly affect the likelihood that a protected car will be stolen. There may, however, be positive externalities due to general deterrence. We find that the availability of Lojack is associated with a sharp fall in auto theft. Rates of other crime do not change appreciably. At least historically, the marginal social benefit of an additional unit of Lojack has been fifteen times greater than the marginal social cost in high crime areas. Those who install Lojack, however, obtain less than 10 percent of the total social benefits, leading to underprovision by the market. I.
369|Hedging Winner&#039;s Curse with Multiple Bids: Evidence from the Portuguese Treasury Bill Auction|Auctions of government securities typically permit bidders to enter multiple price-quantity bids. Despite the widespread adoption of this institutional feature and its use by bidders, the motivations behind its use and its e ects on auction outcomes are not well understood theoretically and have been little explored empirically. Using bidding data from treasury bill auctions in Portugal, this paper examines how bidders use multiple bids to hedge against winner&#039;s curse. The data show that, ceteris paribus, a bidder submits a greater number of bids and disperses prices on these bids more widely when there is a greater potential for winner&#039;s curse. In particular, both these measures of bid-spreading increase with the volatility of market interest rates and the expected number of participating well-informed bidders.
370|Pensions and Retirement: Evidence from Union Army Veterans. The Quarterly|I investigate the factors that fostered rising retirement rates prior to social security and most private-sector pensions by estimating the income effect of the first major pension program in the United Sates, that covering Union Army veterans. The elasticity of nonparticipation with respect to Union Army pension income was 0.73. The findings suggest that secularly rising income explains a substantial part of increased retirement rates. Comparisons with elasticities of nonparticipation with respect to social security income suggest that the elasticity of labor force nonpartici-pation may have decreased with time, perhaps because of the increasing attractive-ness of leisure. I. RETIREMENT SINCE THE TURN OF THE CENTURY Increasing numbers of men have permanently abandoned the labor force at ever younger ages during the twentieth century. In 1880 78 percent of men 65 years of age or older were in the labor force and in 1900 65 percent, whereas in 1930 the figure had dropped to 58 percent. But by 1980 the figure was less than 25 percent [Moen 1987; cf. Ransom and Sutch 1986]. Among men aged 55-64 and 45-64, labor force participation rates were 86 and
371|Privacy-Preserving Data Mining|A fruitful direction for future data mining research will be the development of techniques that incorporate privacy concerns. Specifically, we address the following question. Since the primary task in data mining is the development of models about aggregated data, can we develop accurate models without access to precise information in individual data records? We consider the concrete case of building a decision-tree classifier from tredning data in which the values of individual records have been perturbed. The resulting data records look very different from the original records and the distribution of data values is also very different from the original distribution. While it is not possible to accurately estimate original values in individual data records, we propose a-novel reconstruction procedure to accurately estimate the distribution of original data values. By using these reconstructed distributions, we are able to build classifiers whose accuracy is comparable to the accuracy of classifiers built with the original data. 
372|Mathematical Methods of Statistics|Optimised flows and bottlenecks in granular fabric
373|Security-control methods for statistical databases: a comparative study|This paper considers the problem of providing security to statistical databases against disclosure of confidential information. Security-control methods suggested in the literature are classified into four general approaches: conceptual, query restriction, data perturbation, and output perturbation. Criteria for evaluating the performance of the various security-control methods are identified. Security-control methods that are based on each of the four approaches are discussed, together with their performance with respect to the identified evaluation criteria. A detailed comparative analysis of the most promising methods for protecting dynamic-online statistical databases is also presented. To date no single security-control method prevents both exact and partial disclosures. There are, however, a few perturbation-based methods that prevent exact disclosure and enable the database administrator to exercise “statistical disclosure control. ” Some of these methods, however introduce bias into query responses or suffer from the O/l query-set-size problem (i.e., partial disclosure is possible in case of null query set or a query set of size 1). We recommend directing future research efforts toward developing new methods that prevent exact disclosure and provide statistical-disclosure control, while at the same time do not suffer from the bias problem and the O/l query-set-size problem. Furthermore, efforts directed toward developing a bias-correction mechanism and solving the general problem of small query-set-size would help salvage a few of the current perturbation-based methods.
374|SPRINT: A scalable parallel classifier for data mining|Classification is an important data mining problem. Although classification is a well-studied problem, most of the current classi-fication algorithms require that all or a por-tion of the the entire dataset remain perma-nently in memory. This limits their suitability for mining over large databases. We present a new decision-tree-based classification algo-rithm, called SPRINT that removes all of the memory restrictions, and is fast and scalable. The algorithm has also been designed to be easily parallelized, allowing many processors to work together to build a single consistent model. This parallelization, also presented here, exhibits excellent scalability as well. The combination of these characteristics makes the proposed algorithm an ideal tool for data min-ing. 1
375|SLIQ: A Fast Scalable Classifier for Data Mining|. Classification is an important problem in the emerging field of data mining. Although classification has been studied extensively in the past, most of the classification algorithms are designed only for memory-resident data, thus limiting their suitability for data mining large data sets. This paper discusses issues in building a scalable classifier and presents the design of SLIQ  1  , a new classifier. SLIQ is a decision tree classifier that can handle both numeric and categorical attributes. It uses a novel pre-sorting technique in the tree-growth phase. This sorting procedure is integrated with a breadth-first tree growing strategy to enable classification of disk-resident datasets. SLIQ also uses a new tree-pruning algorithm that is inexpensive, and results in compact and accurate trees. The combination of these techniques enables SLIQ to scale for large data sets and classify data sets irrespective of the number of classes, attributes, and examples (records), thus making it an ...
376|Beyond Concern: Understanding Net Users&#039;Attitudes About Online Privacy|Introduction  Over the past decade, numerous surveys conducted around the world have found consistently high levels of concern about privacy. The more recent studies have found that this concern is as prevalent in the online environment as it is for physical-world interactions. For example, Westin (Harris 1998) found 81% of Net users are concerned about threats to their privacy while online. While many studies have measured the magnitude of privacy concerns, it is still critical to study the concern in detail, especially for the online environment. As Hine and Eve (1998) point out: Despite this wide range of interests in privacy as a topic, we have little idea of the ways in which people in their ordinary lives conceive of privacy and their reactions to the collection and use of personal information (Hine and Eve 1998, 253) With this study, we have tried to better understand the nature of online privacy concerns; we look beyond the fact that people are concerned and attempt to 
377|Privacy-enhancing technologies for the Internet  |The increased use of the Internet for everyday activities is bringing new threats to personal privacy. This paper gives an overview of existing and potential privacyenhancing technologies for the Internet, as well as motivation and challenges for future work in this field. 
378|Secure databases: Protection against user influence|Users may be able to compromise databases by asking a series of questions and then inferring new information from the answers. The complexity of protecting a database against this technique is discussed here.
379|Security and Privacy Implications of Data Mining|Data mining enables us to discover information we do not expect to find in databases. This can be a security/privacy issue: If we make information available, are we perhaps giving out more than we bargained for? This position paper discusses possible problems and solutions, and outlines ideas for further research in this area.  1 Introduction  Database technology provides a number of advantages. Data mining is one of these; using automated tools to analyze corporate data can help find ways to increase efficiency of an organization.  Another advantage of database technology is information sharing (including sharing with other organizations). For example, publicly accessible corporate telephone books can decrease the need for telephone operators (offloading this task to the caller...) Sharing need not be completely public - making inventory information available to suppliers can help a retail operation to avoid shortages, and can lower the supplier&#039;s cost (thus allowing the retailer to n...
380|Secure statistical database with random sample queries|A new inference control, called random sample queries, is proposed for safeguarding confidential data in on-line statistical databases. The random sample queries control deals directly with the basic principle of compromise by making it impossible for a questioner to control precisely the formation of query sets. Queries for relative frequencies and averages are computed using random samples drawn from the query sets. The sampling strategy permits the release of accurate and timely statistics and can be implemented at very low cost. Analysis shows the relative error in the statistics decreases as the query set size increases; in contrast, the effort required to compromise increases with the query set size due to large absolute errors. Experiments performed on a simulated database support the analysis.
381|Privacy Critics: UI Components to Safeguard Users&#039; Privacy|Creating usable systems to protect online privacy is an inherently difficult problem. Privacy critics are semiautonomous agents that help people protect their online privacy by offering suggestions and warnings. Two sample critics are presented. KEYWORDS: privacy, World Wide Web, critics, agent architectures, CSCW, collaboration, P3P.
382|A data distortion by probability distribution|This paper introduces data distortion by probability distribution, a probability distortion that involves three steps. The first step is to identify the underlying density function of the original series and to estimate the parameters of this density function. The second step is to generate a series of data from the estimated density function. And the final step is to map and replace the generated series for the original one. Because it is replaced by the distorted data set, probability distortion guards the privacy of an individual belonging to the original data set. At the same time, the probability distorted series provides asymptotically the same statistical properties as those of the original series, since both are under the same distribution. Unlike conventional point distortion, probability distortion is difficult to compromise by repeated queries, and provides a maximum exposure for statistical analysis.
383|Privacy Interfaces for Information Management|To facilitate the sharing of information using modern communication networks, users must be able to decide on a  privacy policy---what information to conceal, what to reveal, and to whom. We describe the evolution of privacy interfaces---the user interfaces for specifying privacy policies ---in COLLABCLIO, a system for sharing web browsing histories. Our experience has shown us that privacy policies ought to be treated as first-class objects: policy objects should have an intensional representation, and privacy interfaces should support direct manipulation of these objects. We also show how these conclusions apply to a variety of domains such as file systems, email, and telephony.  Keywords  Privacy, user interfaces, direct manipulation, WWW, information retrieval, intensional/extensional set representations.   INTRODUCTION  It is commonplace that modern communication networks should support the sharing of information while protecting people&#039;s privacy. To this end networks provide mech...
384|Recovering Information from Summary Data|Data is often stored in summarized form, as a histogram of aggregates (COUNTs, SUMs, or AVeraGes) over specified ranges. We study how to estimate the original detail data from the stored summary. We formulate this task as an inverse problem,  specifying a well-defined cost function that has to be optimized under constraints. We show that our formulation includes the uniformity and independence assumptions as a special case, and that it can achieve better reconstruction results if we maximize the smoothness as opposed to the uniformity. In our experiments on real and synthetic datasets, the proposed method almost consistently outperforms its competitor, improving the root-mean-square error by up to 20 per cent for stock price data, and up to 90 per cent for smoother data sets. Finally, we show how to apply this theory to a variety of database problems that involve partial information, such as OLAP, data warehousing and histograms in query optimization.
385|An Analytic Approach to Statistical Databases|Abstract. In the commonly adopted data models (as ins entity-relationship data model 111, for example) an attribute is a mapping between an en-tity set or a relationship set and a value set. The intension of a mapping property is given im-plicitly or explicitly in the data models, but the extension can be generally represented by the set I&lt;entity,value&gt;), as in the relational model. We propose an alternative data model for statisti cal databases, in which an attribute is represen-ted by its analytic properties (the distribution function of the values of the attribute). These analytic properties are described by a set of pa-rameters,which we call the canonica2 coefficients of the attribute. The canonical coefficients can be used to solve the usual statistical queries with no access to the data. In particular, we pre sent: 1) the methods for computing and updating the canonical coefficients, 2) the use of the ca-nonical coefficients for solving the main statis-tical queries, also in distributed statistical database environments. Besides, an application of such parameters to the query decomposition in distributed database environments is discussed.
386|Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition|This paper presents a new approach to hierarchical reinforcement learning based on decomposing  the target Markov decision process (MDP) into a hierarchy of smaller MDPs  and decomposing the value function of the target MDP into an additive combination of the  value functions of the smaller MDPs. The decomposition, known as the MAXQ decomposition,  has both a procedural semantics---as a subroutine hierarchy---and a declarative  semantics---as a representation of the value function of a hierarchical policy. MAXQ unifies  and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and  Dayan and Hinton. It is based on the assumption that the programmer can identify useful  subgoals and define subtasks that achieve these subgoals. By defining such subgoals, the  programmer constrains the set of policies that need to be considered during reinforcement  learning. The MAXQ value function decomposition can represent the value function of any  policy that is consisten...
389|On-Line Q-Learning Using Connectionist Systems|Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete finite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of different algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Difference algorithm (Sutton 1988), including a new algorithm (Modified Connectionist Q-Learning), and Q() (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each t...
390|Prioritized sweeping: Reinforcement learning with less data and less time|We present a new algorithm, Prioritized Sweeping, for e cient prediction and control of stochas-tic Markov systems. Incremental learning methods such asTemporal Di erencing and Q-learning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of di erent stochastic optimal control prob-lems. It successfully solves large state-space real time problems with which other methods have di culty. 1 1
391|O-Plan: the Open Planning Architecture|O-Plan is an AI planner based on previous experience with the Nonlin planner and its derivatives. Nonlin and other similar planning systems had limited control architectures and were only partially successful at limiting their search spaces. O-Plan is a design and implementation of a more flexible system aimed at supporting planning research and development, opening up new planning methods and supporting strong search control heuristics. O-Plan takes an engineering approach to the construction of an efficient domain independent planning system which includes a mixture of AI and numerical techniques from Operations Research. The main contributions of the work are centred around the control of search within the OPlan planning framework, and this paper outlines the search control heuristics employed within the planner. These involve the use of condition typing, time and resource constraints and domain constraints to allow knowledge about an application domain to be used to prune the searc...
392|Motivated Reinforcement Learning|The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson &amp; Balleine, into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.
393|Residual Algorithms: Reinforcement Learning with Function Approximation|A number of reinforcement learning algorithms have been developed that are guaranteed to converge to the optimal solution when used with lookup tables. It is shown, however, that these algorithms can easily become unstable when implemented directly with a general function-approximation system, such as a sigmoidal multilayer perceptron, a radial-basisfunction system, a memory-based learning system, or even a linear function-approximation system. A new class of algorithms, residual gradient algorithms, is proposed, which perform gradient descent on the mean squared Bellman residual, guaranteeing convergence. It is shown, however, that they may learn very slowly in some cases. A larger class of algorithms, residual  algorithms, is proposed that has the guaranteed convergence of the residual gradient algorithms, yet can retain the fast learning speed of direct algorithms. In fact, both direct and residual gradient algorithms are shown to be special cases of residual algorithms, and it is s...
394|Reinforcement learning with hierarchies of machines|We present a new approach to reinforcement learning in which the policies considered by the learning process are constrained by hierarchies of partially specified machines. This allows for the use of prior knowledge to reduce the search space and provides a framework in which knowledge can be transferred across problems and in which component solutions can be recombined to solve larger and more complicated problems. Our approach can be seen as providing a link between reinforcement learning and “behavior-based ” or “teleo-reactive ” approaches to control. We present provably convergent algorithms for problem-solving and learning with hierarchical machines and demonstrate their effectiveness on a problem with several thousand states. 1
395|Convergence of Stochastic Iterative Dynamic Programming Algorithms|Increasing attention has recently been paid to algorithms based on  dynamic programming (DP) due to the suitability of DP for learning  problems involving control. In stochastic environments where  the system being controlled is only incompletely known, however,  a unifying theoretical account of the behavior of these methods has  been missing. In this paper we relate DP-based learning algorithms  to powerful techniques of stochastic approximation via a new convergence  theorem, enabling us to establish a class of convergent  algorithms to which both TD() and Q-learning belong.  1 
396|Exploiting structure in policy construction|Markov decision processes (MDPs) have recently been applied to the problem of modeling decisiontheoretic planning. While traditional methods for solving MDPs are often practical for small states spaces, their effectiveness for large AI planning problems is questionable. We present an algorithm, called structured policy iteration (SPI), that constructs optimal policies without explicit enumeration of the state space. The algorithm retains the fundamental computational steps of the commonly used modified policy iteration algorithm, but exploitsthe variable and propositionalindependencies reflected in a temporal Bayesian network representation of MDPs. The principles behind SPI can be applied to any structured representation of stochastic actions, policies and value functions, and the algorithm itself can be used in conjunction with recent approximation methods. 1
397|Convergence Results for Single-Step On-Policy Reinforcement-Learning Algorithms|An important application of reinforcement learning (RL) is to finite-state control problems and one of the most difficult problems in learning for control is balancing the exploration/exploitation tradeoff. Existing theoretical results for RL give very little guidance on reasonable ways to perform exploration. In this paper, we examine the convergence of single-step on-policy RL algorithms for control. On-policy algorithms cannot separate exploration from learning and therefore must confront the exploration problem directly. We prove convergence results for several related on-policy algorithms with both decaying exploration and persistent exploration. We also provide examples of exploration strategies that can be followed during learning that result in convergence to both optimal values and optimal policies.
398|The MAXQ Method for Hierarchical Reinforcement Learning|This paper presents a new approach to hierarchical reinforcement learning based on the MAXQ decomposition of the value function. The MAXQ decomposition has both a procedural semantics---as a subroutine hierarchy---and a declarative semantics---as a representation of the value function of a hierarchical policy. MAXQ unifies and extends previous work on hierarchical reinforcement learning by Singh, Kaelbling, and Dayan and Hinton. Conditions under which the MAXQ decomposition can represent the optimal value function are derived. The paper defines a hierarchical Q learning algorithm, proves its convergence, and shows experimentally that it can learn much faster than ordinary &#034;flat&#034; Q learning. Finally, the paper discusses some interesting issues that arise in hierarchical reinforcement learning including the hierarchical credit assignment problem and non-hierarchical execution of the MAXQ hierarchy. 1 Introduction  Hierarchical approaches to reinforcement learning (RL) problems promise ma...
399|Hierarchical solution of Markov decision processes using macro-actions|actions, or macro-actions, in the solution of Markov decision processes. Unlike current models that combine both primitive actions and macro-actions and leave the state space unchanged, we propose a hierarchical model (using an abstract MDP) that works with macro-actions only, and that significantly reduces the size of the state space. This is achieved by treating macroactions as local policies that act in certain regions MDP to those at the boundaries of regions. The abstract MDP approximates the original and can be solved more efficiently. We discuss several ways in which macro-actions can be generated to ensure good solution quality. Finally, we consider ways in which macro-actions can be reused to solve multiple, related MDPs; and we show that this can justify the computational overhead of macro-action generation. 1
400|Decomposition Techniques for Planning in Stochastic Domains|This paper is concerned with modeling planning problems involving uncertainty as discrete-time, finite-state stochastic automata. Solving planning problems is reduced to computing policies for Markov decision processes. Classical methods for solving Markov decision processes cannot cope with the size of the state spaces for typical problems encountered in practice. As an alternative, we investigate methods that decompose global planning problems into a number of local problems, solve the local problems separately, and then combine the local solutions to generate a global solution. We present algorithms that decompose planning problems into smaller problems given an arbitrary partition of the state space. The local problems are interpreted as Markov decision processes and solutions to the local problems are interpreted as policies restricted to the subsets of the state space defined by the partition. One algorithm relies on constructing and solving an abstract version of the original de...
401|Hierarchical Control and Learning for Markov Decision Processes| This dissertation investigates the use of hierarchy and problem decomposition as a means of solving large, stochastic, sequential decision problems. These problems are framed as Markov decision problems (MDPs). The new technical content of this dissertation begins with a discussion of the concept of temporal abstraction. Temporal abstraction is shown to be equivalent to the transformation of a policy defined over a region of an MDP to an action in a semi-Markov decision problem (SMDP). Several algorithms are presented for performing this transformation efficiently. This dissertation introduces the HAM method for generating hierarchical, temporally abstract actions. This method permits the partial specification of abstract actions in a way that corresponds to an abstract plan or strategy. Abstr...
402|Learning Abstraction Hierarchies for Problem Solving|The use of abstraction in problem solving is an  effective approach to reducing search, but finding  good abstractions is a difficult problem, even for  people. This paper identifies a criterion for selecting  useful abstractions, describes a tractable  algorithm for generating them, and empirically  demonstrates that the abstractions reduce search.  The abstraction learner, called  alpine,isinte-  grated with the prodigy problem solver [Minton  et al., 1989b, Carbonell et al.,1991] and has been  tested on large problem sets in multiple domains.  
403|Multi-time Models for Temporally Abstract Planning|Planning  Doina Precup, Richard S. Sutton  University of Massachusetts  Amherst, MA 01003  fdprecupjrichg@cs.umass.edu  Abstract  Planning and learning at multiple levels of temporal abstraction is a key  problem for artificial intelligence. In this paper we summarize an approach  to this problem based on the mathematical framework of Markov  decision processes and reinforcement learning. Current model-based reinforcement  learning is based on one-step models that cannot represent  common-sense higher-level actions, such as going to lunch, grasping an  object, or flying to Denver. This paper generalizes prior work on temporally  abstract models [Sutton, 1995] and extends it from the prediction  setting to include actions, control, and planning. We introduce a more  general form of temporally abstract model, the multi-time model, and establish  its suitability for planning and learning by virtue of its relationship  to the Bellman equations. This paper summarizes the theoretical framewo...
404|TD Models: Modeling the World at a Mixture of Time Scales|Temporal-difference (TD) learning can be used not just to predict  rewards, as is commonly done in reinforcement learning, but also to  predict states, i.e., to learn a model of the world&#039;s dynamics. We  present theory and algorithms for intermixing TD models of the world  at different levels of temporal abstraction within a single structure.  Such multi-scale TD models can be used in model-based reinforcementlearning  architectures and dynamic programming methods in place of  conventional Markov models. This enables planning at higher and varied  levels of abstraction, and, as such, may prove useful in formulating  methods for hierarchical or multi-level planning and reinforcement  learning. In this paper we treat only the prediction problem---that of  learning a model and value function for the case of fixed agent behavior.  Within this context, we establish the theoretical foundations of  multi-scale models and derive TD algorithms for learning them. Two  small computational experim...
405|Between MDPs and semi-MDPs: Learning, planning, and representing knowledge at multiple temporal scales|Learning, planning, and representing knowledge at multiple levels of temporal abstraction are key challenges for AI. In this paper we develop an approach to these problems based on the mathematical framework of reinforcement learning and Markov decision processes (MDPs). We extend the usual notion of action to include options—whole courses of behavior that may be temporally extended, stochastic, and contingent on events. Examples of options include picking up an object, going to lunch, and traveling to a distant city, as well as primitive actions such as muscle twitches and joint torques. Options may be given a priori, learned by experience, or both. They may be used interchangeably with actions in a variety of planning and learning methods. The theory of semi-Markov decision processes (SMDPs) can be applied to model the consequences of options and as a basis for planning and learning methods using them. In this paper we develop these connections, building on prior work by Bradtke and Duff (1995), Parr (in prep.) and others. Our main novel results concern the interface between the MDP and SMDP levels of analysis. We show how a set of options can be altered by changing only their termination conditions
406|Flexible Decomposition Algorithms for Weakly Coupled Markov Decision Problems|This paper presents two new approaches to decomposing and solving large Markov decision problems (MDPs), a partial decoupling method and a complete decoupling method. In these approaches, a large, stochastic decision problem is divided into smaller pieces. The first approach builds a cache of policies for each part of the problem independently, and then combines the pieces in a separate, light-weight step. A second approach also divides the problem into smaller pieces, but information is communicatedbetween the different problem pieces, allowing intelligent decisions to be made about which piece requires the most attention. Both approaches can be used to find optimal policies or approximately optimal policies with provable bounds. These algorithms also provide a framework for the efficient transfer of knowledge across problems that share similar structure. 1 Introduction  The Markov Decision Problem (MDP) framework provides a formal framework for modeling a large variety of stochastic,...
407|Approximating value trees in structured dynamic programming|We propose and examine a method of approximate dynamic programming for Markov decision processes based on structured problem representations. We assume an MDP is represented using a dynamic Bayesian network, and construct value functions using decision trees as our function representation. The size of the representation is kept within acceptable limits by pruning these value trees so that leaves represent possible ranges of values, thus approximating the value functions produced during optimization. We propose a method for detecting convergence,prove errors bounds on the resulting approximately optimal value functions and policies, and describe some preliminary experimental results. 1
408|Multi-Value-Functions: Efficient Automatic Action Hierarchies for Multiple Goal MDPs|If you have planned to achieve one particular goal in a stochastic delayed rewards problem and then someone asks about a different goal what should you do? What if you need to be ready to quickly supply an answer for any possible goal? This paper shows that by using a new kind of automatically generated abstract action hierarchy that with  N states, preparing for all of N possible goals can be much much cheaper than N times the work of preparing for one goal. In goal-based Markov Decision Problems, it is usual to generate a policy ß(x),  mapping states to actions, and a value function  J(x), mapping states to an estimate of minimum expected cost-to-goal, starting at x. In this paper we will use the terminology that a multi-policy  ß  ?  (x; y) (for all state-pairs (x; y)) maps a state x  to the first action it should take in order to reach y  with expected minimum cost and a multi-valuefunction  J  ?  (x; y) is a definition of this minimum cost. Building these objects quickly and with ...
409|Hierarchical Explanation-Based Reinforcement Learning|Explanation-Based Reinforcement Learning (EBRL) was introduced by Dietterich and Flann as a way of combining the ability of Reinforcement Learning (RL) to learn optimal plans with the generalization ability of Explanation-Based Learning (EBL) (Dietterich &amp; Flann, 1995). We extend this work to domains where the agent must order and achieve a sequence of subgoals in an optimal fashion. Hierarchical EBRL can effectively learn optimal policies in some of these sequential task domains even when the subgoals weakly interact with each other. We also show that when a planner that can achieve the individual subgoals is available, our method converges even faster. 1 Introduction  Reinforcement Learning (RL) has emerged as the method of choice for building autonomous agents that improve their performance with experience. One obstacle to scaling this approach to large problems is the lack of a robust and justifiable method to generalize from one experience to another. Dietterich and Flann (Dietter...
410|Module Based Reinforcement Learning for a Real Robot|. The behaviour of reinforcement learning (RL) algorithms is best understood in completely observable, finite state- and action-space, discrete-time controlled Markov-chains. Robot-learning domains, on the other hand, are inherently infinite both in time and space, and moreover they are only partially observable. In this article we suggest a systematic method whose motivation comes from the desire to transform the task-to-be-solved into a finite-state, discrete-time, &#034;approximately&#034; Markovian task, which is completely observable too. The key idea is to break up the problem into subtasks and design controllers for each of the subtasks. Then operating conditions are attached to the controllers (together the controllers and their operating conditions which are called modules) and possible additional features are designed to facilitate observability. A new discrete time-counter is introduced at the &#034;module-level&#034; that clicks only when a change in the value of one of the features is observe...
411|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
412|Data structure diagrams|Successful communication of ideas has been and will continue to be a limiting factor in man&#039;s endeavors to survive and to better his life. The invention of algebra, essentially a graphic technique for communicating truths with respect to classes of arithmetic statements, broke the bond that slowed the development of mathematics. Whereas &#034;12+ 13=25 &#039; &#039; and &#034;3+7 = 10 &#034; and &#034;14+(-2)  = 12&#034; are arithmetic statements, &#034;a+b=c &#039; &#039; is an algebraic statement. In particular, it is an algebraic statement controlling an entire class of arithmetic statements such as those listed. Data Structure Diagrams The Data Structure Diagram is also a graphic technique. It is based on a type of notation dealing with classes--specifically, with classes of entities and the classes of sets that relate them. For example, individual people and automobiles
413|Conditional random fields: Probabilistic models for segmenting and labeling sequence data|We present conditional random fields, a framework for building probabilistic models to segment and label sequence data. Conditional random fields offer several advantages over hidden Markov models and stochastic grammars for such tasks, including the ability to relax strong independence assumptions made in those models. Conditional random fields also avoid a fundamental limitation of maximum entropy Markov models (MEMMs) and other discriminative Markov models based on directed graphical models, which can be biased towards states with few successor states. We present iterative parameter estimation algorithms for conditional random fields and compare the performance of the resulting models to HMMs and MEMMs on synthetic and natural-language data. 1.
415|Gradient-based learning applied to document recognition|Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradientbased learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of two dimensional (2-D) shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN’s), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank check is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day.
416|A Maximum Entropy approach to Natural Language Processing|The concept of maximum entropy can be traced back along multiple threads to Biblical times. Only recently, however, have computers become powerful enough to permit the widescale application of this concept to real world problems in statistical estimation and pattern recognition. In this paper we describe a method for statistical modeling based on maximum entropy. We present a maximum-likelihood approach for automatically constructing maximum entropy models and describe how to implement this approach efficiently, using as examples several problems in natural language processing.  
418|Transformation-Based Error-Driven Learning and Natural Language Processing: A Case Study in Part-of-Speech Tagging|this paper, we will describe a simple rule-based approach to automated learning of linguistic knowledge. This approach has been shown for a number of tasks to capture information in a clearer and more direct fashion without a compromise in performance. We present a detailed case study of this learning method applied to part of speech tagging
419|Inducing Features of Random Fields|We present a technique for constructing random fields from a set of training samples. The learning paradigm builds increasingly complex fields by allowing potential functions, or features, that are supported by increasingly large subgraphs. Each feature has a weight that is trained by minimizing the Kullback-Leibler divergence between the model and the empirical distribution of the training data. A greedy algorithm determines how features are incrementally added to the field and an iterative scaling algorithm is used to estimate the optimal values of the weights. The random field models and techniques introduced in this paper differ from those common to much of the computer vision literature in that the underlying random fields are non-Markovian and have a large number of parameters that must be estimated. Relations to other learning approaches, including decision trees, are given. As a demonstration of the method, we describe its application to the problem of automatic word classifica...
420|A Maximum Entropy Model for Part-Of-Speech Tagging|This paper presents a statistical model which trains from a corpus annotated with Part-OfSpeech tags and assigns them to previously unseen text with state-of-the-art accuracy(96.6%). The model can be classified as a Maximum Entropy model and simultaneously uses many contextual &#034;features&#034; to predict the POS tag. Furthermore, this paper demonstrates the use of specialized features to model difficult tagging decisions, discusses the corpus consistency problems discovered during the implementation of these features, and proposes a training strategy that mitigates these problems.
421|Maximum entropy markov models for information extraction and segmentation|Hidden Markov models (HMMs) are a powerful probabilistic tool for modeling sequential data, and have been applied with success to many text-related tasks, such as part-of-speech tagging, text segmentation and information extraction. In these cases, the observations are usually modeled as multinomial distributions over a discrete vocabulary, and the HMM parameters are set to maximize the likelihood of the observations. This paper presents a new Markovian sequence model, closely related to HMMs, that allows observations to be represented as arbitrary overlapping features (such as word, capitalization, formatting, part-of-speech), and defines the conditional probability of state sequences given observation sequences. It does this by using the maximum entropy framework to fit a set of exponential models that represent the probability of a state given an observation and the previous state. We present positive experimental results on the segmentation of FAQ’s.  
422|Finite-State Transducers in Language and Speech Processing|Finite-state machines have been used in various domains of natural language processing. We consider here the use of a type of transducers that supports very efficient programs: sequential transducers. We recall classical theorems and give new ones characterizing sequential string-tostring transducers. Transducers that output weights also play an important role in language and speech processing. We give a specific study of string-to-weight transducers, including algorithms for determinizing and minimizing these transducers very efficiently, and characterizations of the transducers admitting determinization and the corresponding algorithms. Some applications of these algorithms in speech recognition are described and illustrated. 1.
423|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
424|Learning to resolve natural language ambiguities: A unified approach|distinct semanticonceptsuch as interest rate and has interest in Math are conflated in ordinary text. We analyze a few of the commonly used statistics based The surrounding context- word associations and syn-and machine learning algorithms for natural language tactic patterns in this case- are sufflcicnt to identify disambiguation tasks and observe tha they can bc recast as learning linear separators in the feature space. the correct form. Each of the methods makes a priori assumptions, which Many of these arc important stand-alone problems it employs, given the data, when searching for its hy- but even more important is thei role in many applicapothesis. Nevertheless, as we show, it searches a space tions including speech recognition, machine translation, that is as rich as the space of all linear separators. information extraction and intelligent human-machine We use this to build an argument for a data driven interaction. Most of the ambiguity resolution problems approach which merely searches for a good linear sepa- are at the lower level of the natural language inferences rator in the feature space, without further assumptions chain; a wide range and a large number of ambigui-
425|Information Extraction with HMM Structures Learned by Stochastic Optimization|Recent research has demonstrated the strong performance of hidden Markov models applied to information extraction -- the task of populating database slots with corresponding phrases from text documents. A remaining problem, however, is the selection of state-transition structure for the model. This paper demonstrates that extraction accuracy strongly depends on the selection of structure, and presents an algorithm for automatically finding good structures by stochastic optimization. Our algorithm begins with a simple model and then performs hill-climbing in the space of possible structures by splitting states and gauging performance on a validation set. Experimental results show that this technique finds HMM models that almost always out-perform a fixed model, and have superior average performance across tasks.
426|Boosting Applied to Tagging and PP Attachment|Boosting is a machine learning algorithm that is not well known in computational linguistics. We apply it to part-of-speech tagging and prepositional phrase attachment. Performance is very encouraging. We also show how to improve data quality by using boosting to identify annotation errors.
427|Minimization Algorithms for Sequential Transducers|We present general algorithms for minimizing sequential finite-state transducers that output strings or numbers. The algorithms are shown to be efficient since in the case of acyclic transducers and for output strings they operate in O(S+|E|+|V|+(|E|-|V|+|F|)x(|Pmax|+1)) steps, where S is the sum of the lengths of all output labels of the resulting transducer, E the set of transitions of the given transducer, V the set of its states, F the set of final states, and Pmax one of the longest of the longest common prefixes of the output paths leaving each state of the transducer. The algorithms apply to a larger class of transducers which includes subsequential transducers.
428|A Whole Sentence Maximum Entropy Language Model|We introduce a new kind of language model, which models whole sentences or utterances directly using the Maximum Entropy paradigm. The new model is conceptually simpler, and more naturally suited to modeling whole-sentence phenomena, than the conditional ME models proposed to date. By avoiding the chain rule, the model treats each sentence or utterance as a &#034;bag of features&#034;, where features are arbitrary computable properties of the sentence. The model is unnormalizable, but this does not interfere with training (done via sampling) or with use. Using the model is computationally straightforward. The main computational cost of training the model is in generating sample sentences from a Gibbs distribution. Interestingly, this cost has different dependencies, and is potentially lower, than in the comparable conditional ME model.  1 Motivation  Conventional statistical language models estimate the probability of an sentence s by using the chain rule to decompose it into a product of condit...
429|Equivalence of Linear Boltzmann Chains and Hidden Markov Models|Several authors have studied the relationship between hidden Markov models and `Boltzmann chains&#039; with a linear or `time-sliced&#039; architecture. Boltzmann chains model sequences of states by defining statestate transition energies instead of probabilities. In this note I demonstrate that, under the simple condition that the state sequence has a mandatory end state, the probability distribution assigned by a strictly linear Boltzmann chain is identical to that assigned by a hidden Markov model.  Several authors have made a link between hidden Markov models for time series and energy-based models (Luttrell 1989, Williams 1990, Saul and Jordan 1995). Saul and Jordan (1995) discuss a linear Boltzmann chain model with state-state transition energies A ii  0  (going from state i to state i  0  ) and symbol emission energies B ij , under which the probability of an entire state fi l ; j l g  L  1 given the length of the sequence L, is: P (fi l ; j l g  L  1 j\Pi; A; B; L; HBC ) = 1  Z(\Pi; A; B...
430|Bootstrap Methods in Econometrics: Theory and Numerical Performance|1.
431|Dynamic topic models|Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly
433|Sparse Gaussian processes using pseudo-inputs|We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M « N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N) training cost and O(M 2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime. 1
434|Discovering object categories in image collections|Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].  
435|Integrating topics and syntax|Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 1
436|A Generalized Mean Field Algorithm for Variational Inference in Exponential Families|We present a class of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on...
437|Collaborative Filtering: A Machine Learning Perspective|Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modi  cations of one or more standard machine learning methods for classifi cation, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.
438|Applying Discrete PCA in Data Analysis|Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.  
439|The authorrecipienttopic model for topic and role discovery in social networks: Experiments with Enron and academic email|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities. The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient—steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher’s email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people’s roles. 1
441|Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles|To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a lineartime distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained ’ by a relatively small number of structurally simple common behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations. 1
442|Inductive-Data-Type Systems|In a previous work (&#034;Abstract Data Type Systems&#034;, TCS 173(2), 1997), the leI two authors presented a combined lmbined made of a (strongl normal3zG9 alrmal rewrite system and a typed  #-calA#Ik  enriched by pattern-matching definitions folnitio a certain format,calat the &#034;General Schema&#034;, whichgeneral39I theusual recursor definitions fornatural numbers and simil9 &#034;basic inductive types&#034;. This combined lmbined was shown to bestrongl normalIk39f The purpose of this paper is toreformul33 and extend theGeneral Schema in order to make it easil extensibl3 to capture a more general cler of inductive types, cals, &#034;strictly positive&#034;, and to ease the strong normalgAg9Ik proof of theresulGGg system. Thisresul provides a computation model for the combination of anal&#034;DAfGI specification language based on abstract data types and of astrongl typed functional language with strictly positive inductive types.  
444|ORDERINGS FOR TERM-REWRITING SYSTEMS|  Methods of proving that a term-rewriting system terminates are presented. They are based on the intuitive notion of &#039;simplification orderings&#039;. orderings in which any term that is syntactically simpler than another is smaller than the other. M a consequence of Kruskal&#039;s Tree Theorem, any nonterminating system must be self-embedding in the sense that it allows for the derivation of some term from a simpler one; thus termination is guaranteed jf every rule in the system as a reduction in some simplification ordering. Most 01 the orderings that have been used for proving tennination are indeed simplication orderings; using this notion often allows for much easier proofs. A particularly useful class of simplification orderings, the &#039;recursive path orderings&#039;, is defined. Examples of the use of simplification orderings in termination proofs are given.
445|PROVING TERMINATION WITH MULTISET ORDERINGS  |A common tool for proving the termination of programs is the well-founded set, a set ordered in such a way as to admit no infinite descending sequences. The basic approach is to find a termination function that maps the values of the program variables into some well-founded set, such that the value of the termination function is continually reduced throughout the computation. All too often, the termination functions required are difficult to find and are of a complexity out of proportion to the program under consideration. However, by providing more sophisticated well-founded sets, the corresponding termination functions can be simplified. Given a well-founded set S, we consider multisets over S, &#034;sets&#034; that admit multiple occurrences of elements taken from S. We define an ordering on all finite multisets over S that is induced by the given ordering on S. This multiset ordering is shown to be well-founded. The value of the multiset ordering is that it permits the use of relatively simple and intuitive termination functions in otherwise difficult termination proofs. In particular, we apply the multiset ordering to prove the termination of production systems, programs defined in terms of sets of rewriting rules.
446|Natural termination |Abstract. We generalize the various path orderings and the conditions under which they work, and describe an implementation of this general ordering. We look at methods for proving termination of orthogonal systems and give a new solution to a problem of Zantema&#039;s. 1
447|1987], `Termination of rewriting systems by polynomial interpretations and its implementation&#039;, Science of Computer Programming |Abstract. This paper describes the actual implementation in the rewrite rule laboratory REVE of an elementary procedure that checks inequalities between polynomials and is used for proving termination of rewriting systems, especially in the more difficult case of associative-commutative rewriting systems, for which a complete characterization is given. 1. The origin of the problem Termination is central in programming and in particular in term-rewriting systems, the latter being both a theoretical and a practical basis for functional and logic languages. Indeed the problem is not only a key for ensuring that a program and its procedures eventually produce the expected result, it is also important in concur rent programming where liveness results rely on termination of the components. Term-rewriting systems are also used for proving equational theorems and are a basic tool for checking specifications of abstract data types. Again, the termination problem is crucial in the implementation of the Kn~h-Bendix algorithm, which tests the local confluence and needs the termination to be able to infer the total confluence. Termination is also necessary to direct equations properly. Until now, methods based on recursive path ordering were satisfactory [8,19], but when we recently ran experiments on transformation of FP programs [2], we were faced with a problem that the recursive path ordering could not handle. The problem, motivated by a simple example of code optimization, is just Associativity + Endomorphism: (XI * x2)  * X3 = XI * (x2 * x 3), f(x I * x2)  = f(xJ * f(x 2). The variables are functions,  * is the composition and f is a mapcar-like operator. In order to optimize the program, the user wants to decrease the number of uses * This work was supported by the Greco de Programmation.
448|Completion for rewriting modulo a congruence|Abstract. We present completion methods for rewriting modulo a congruence, generalizing previous methods by Peterson and Stickel (1981) and Jouannaud and Kirchner (1986). We formalize our methods as equational inference systems and describe techniques for reasoning about such systems. 1.
449|Solving Goals in Equational Languages|Solving equations in equational Horn-clause theories is a programming paradigm that combines logic programming and functional programming in a clean manner. Languages like EQLOG, SLOG and RITE, express programs as conditional rewrite rules and goals as equations to be solved. Procedures for completion of conditional equational theories, in a manner akin to that of Knuth and Bendix for unconditional theories, also require methods for solving equations appearing in conditions. Rewrite-based logic-programming uses (conditional) narrowing to solve equational goals. Recently a different, topdown equation solving procedure was proposed for unconditional rewrite systems. In this paper, we express equational goal solving using conditional rules. Some refinements are described: the notion of operator derivability is used to prune useless paths in the search tree and our use of oriented goals eliminates some redundant paths leading to non-normalized solutions. Our goal-directed method can also be extended to handle conditional systems. I. Equational Programming Several proposed programming languages use conditional equations as a means of combining the main features of logic programming and functional programming; such languages include RITE [Dershowitz-Plaisted-85], SLOG [Fribourg-85], and EQLOG [Goguen-Meseguer-86]. In this paradigm, a program is a set of rules, that is, directed (conditional) equations, and a goal is the question whether an equation s = t has a solution in the equational theory presented by the program. Computing consists of finding values (substitutions) for the variables in s and t for which the equality holds. Efficient methods of solving equations are therefore very important. So, too, is the ability to detect that equations are unsatisfiable.
450|Canonical Conditional Rewrite Systems |Conditional equations have been studied for their use in the specification of abstract data types and as a computational paradigm that combines logic and function programming in a clean way. In this paper we examine different formulations of conditional equations as rewrite systems, compare their expressive power and give sufficient conditions for rewrite systems to have the &#034;confluence &#034; property. We then examine a restriction of these systems using a &#034;decreasing&#034; ordering. With this restriction, most of the basic notions (like rewriting and computing normal forms) are decidable, the &#034;critical pair&#034; lemma holds, and some formulations preserve canonicity. 
451|Model-Based Clustering, Discriminant Analysis, and Density Estimation|Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as \How many clusters are there?&#034;, &#034;Which clustering method should be used?&#034; and \How should outliers be handled?&#034;. We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a...
454|Regularized discriminant analysis|Linear and quadratic discriminant analysis are considered in the small sample high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved. Submitted to Journal of the American Statistical Association
456|Discriminant Analysis by Gaussian Mixtures|Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA---our new techniques inherit this feature. We are able to control the within-class spread of the subclass centers relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.  Keywords: Classification, Pattern Recognition, Clustering, Nonparametric, Penalized. 1 Introduction  In the generic classification or discrimination problem, the outcome of interest  G falls into J unordered classes, which for convenience we denote by the set J =  f1; 2; 3; \Delta \Delta \Delta Jg. We wish to build a rule for pred...
457|Practical Bayesian Density Estimation Using Mixtures Of Normals|this paper, we propose some solutions to these problems. Our goal is to come up with a simple, practical method for estimating the density. This is an interesting problem in its own right, as well as a first step towards solving other inference problems, such as providing more flexible distributions in hierarchical models. To see why the posterior is improper under the usual reference prior, we write the model in the following way. Let Z = (Z 1 ; : : : ; Z n ) and X = (X 1 ; : : : ; X n ). The Z
459|Detecting Features in  Spatial Point Processes with . . .|We consider the problem of detecting features in spatial point processes in the presence of substantial clutter. One example is the detection of mine elds using reconnaissance aircraft images that erroneously identify many objects that are not mines. Another is the detection of seismic faults on the basis of earthquake catalogs: earthquakes tend to be clustered close to the faults, but there are many that are farther away. Our solution uses model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Very nonlinear features are represented by several highly linear multivariate normal densities, giving a piecewise linear representation. The model is estimated in two stages. In the rst stage, hierarchical model-based clustering is used to provide a rst estimate of the features. In the second stage, this clustering is re ned using the EM algorithm. The number of features is found using an approximation to the posterior probability of each number of features. For the minefield
460|MCLUST: Software for Model-based Cluster Analysis|MCLUST is a software package for cluster analysis written in Fortran and interfaced to the S-PLUS commercial software package1. It implements parameterized Gaussian hierarchical clustering algorithms [16, 1, 7] and the EM algorithm for parameterized Gaussian mixture models [5, 13, 3, 14] with the possible addition of a Poisson noise term. MCLUST also includes functions that combine hierarchical clustering, EM and the Bayesian Information Criterion (BIC) in a comprehensive clustering strategy [4, 8]. Methods of this type have shown promise in a number of practical applications, including character recognition [16], tissue segmentation [1], mine eld and seismic fault detection [4], identi cation of textile aws from images [2], and classi cation of astronomical data [3, 15]. Aweb page with related links can be found at
461|Algorithms for model-based Gaussian hierarchical clustering|1 Funded by the O ce of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-
462|Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition|Friedman (1989) has proposed a regularization technique (RDA) of discriminant analysis in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between linear and quadratic discriminant analysis. In this paper, we propose an alternative approach to design classi cation rules which have also a median position between linear and quadratic discriminant analysis. Our approach is based on the reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k?Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA.
463|Scaling EM (Expectation-Maximization) Clustering to Large Databases  (1999) |Practical statistical clustering algorithms typically center upon an iterative refinement optimization procedure to compute a locally optimal clustering solution that maximizes the fit to data. These algorithms typically require many database scans to converge, and within each scan they require the access to every record in the data table. For large databases, the scans become prohibitively expensive. We present a scalable implementation of the Expectation-Maximization (EM) algorithm. The database community has focused on distance-based clustering schemes and methods have been developed to cluster either numerical or categorical data. Unlike distancebased algorithms (such as K-Means), EM constructs proper statistical models of the underlying data source and naturally generalizes to cluster databases containing both discrete-valued and continuous-valued data. The scalable method is based on a decomposition of the basic statistics the algorithm needs: identifying regions of the data that...
464|Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates|We apply the idea of averaging ensembles of estimators to probability density estimation.
465|Non Parametric Maximum Likelihood Estimation of Features in . . .|This paper addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs for example in the detection of a mine field from aerial observations. A Maximum Likelihood Estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoï tessellation. The methodology is tested on simulations and compared to a model-based clustering technique.
466|Inference in model-based cluster analysis|A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are biased, the shape matrix has to be specified by the user, prior group probabilities are assumed to be equal, the method for choosing the number of groups is based on a crude approximation, and no formal way of choosing between the various possible models is included. Here, we propose a new approach which overcomes all these difficulties. It consists of exact Bayesian inference via Gibbs sampling, and the calculation of Bayes factors (for choosing the model and the number of groups) from the output using the Laplace-Metropolis estimator. It works well in several real and simulated examples.  
467|Principal curve clustering with noise|was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape. This is useful for detecting curvilinear features in spatial point patterns, with or without background noise. Applications of this include the detection of curvilinear mine elds from reconnaissance images, some of the points in which represent false detections, and the detection of seismic faults from earthquake catalogs. Our algorithm for principal curve clustering is in two steps: the rst is hierarchical and agglomerative (HPCC), and the second consists of iterative relocation based on the Classi cation EM algorithm (CEM-PCC). HPCC is used to combine potential feature clusters, while CEM-PCC re nes the results and deals with background noise. It is importanttohave a good starting point for the algorithm: this can be found manually or automatically using, for example, nearest neighbor clutter removal or model-based clustering. We choose the number of features and the amount of smoothing simultaneously using approximate Bayes
468|Fitting mixtures of Kent distributions to aid in joint set identification|When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation–maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data.
469|From Massive Data Sets to Science Catalogs: Applications and Challenges|With hardware advances in scientific instruments and data gathering techniques comes the  inevitable flood of data that can render traditional approaches to science data analysis severely  inadequate. The traditional approach of manual and exhaustive analysis of a data set is no longer  feasible for many tasks ranging from remote sensing, astronomy, and atmospherics to medicine,  molecular biology, and biochemistry. In this paper we present our views as practitioners engaged  in building computational systems to help scientists deal with large data sets. We focus on  what we view as challenges and shortcomings of the current state-of-the-art in data analysis in  view of the massive data sets that are still awaiting analysis. The presentation is grounded in  applications in astronomy, planetary sciences, solar physics, and atmospherics that are currently  driving much of our work at JPL.  keywords: science data analysis, limitations of current methods, challenges for massive data sets, ...
470|On understanding types, data abstraction, and polymorphism|Our objective is to understand the notion of type in programming languages, present a model of typed, polymorphic programming languages that reflects recent research in type theory, and examine the relevance of recent research to the design of practical programming languages. Object-oriented languages provide both a framework and a motivation for exploring the interaction among the concepts of type, data abstraction, and polymorphism, since they extend the notion of type to data abstraction and since type inheritance is an important form of polymorphism. We develop a ?-calculus-based model for type systems that allows us to explore these interactions in a simple setting, unencumbered by complexities of production programming languages. The evolution of languages from untyped universes to monomorphic and then polymorphic type systems is reviewed. Mechanisms for polymorphism such as overloading, coercion, subtyping, and parameterization are examined. A unifying framework for polymorphic type systems is developed in terms of the typed ?-calculus augmented to include binding of types by quantification as well as binding of values by abstraction. The typed ?-calculus is augmented by universal quantification to model generic functions with type parameters, existential quantification and packaging (information hiding) to model abstract data types, and
471|Principal type-schemes for functional programs|the copies are not made or distributed for direct commercial advantage, the ACM copyright notice and the title of its publication and date appear, and notice is given
472|Galileo: a strongly typed, interactive conceptual language|Galileo, a programming language for database applications, is presented. Galileo is a strongly typed, interactive programming language designed specifically to support Semantic Data Model features (classification, aggregation and specialization) as well as abstraction mechanisms of modern programming languages (types, abstract types and modularization). The main contributions of Galileo are: a) the proposal of a flexible type system to model database structure and semantic integrity constraints; b) the inclusion of type hierarchies to support the specialization abstraction mechanism of Semantic Data Models. c) the proposal of a modularization mechanism to structure data and operations into interrelated units; d) the integration of the abstraction mechanisms into an expression based language that allows an interactive use of the database without resorting to a new stand alone query language. Galileo will be used in the immediate future as a tool for database design and, in the long term, as a high level interface for DBMSs. 
473|Hope: An Experimental Applicative Language|An applicative language called HOPE is described and discussed. The underlying goal of the design and implementation effort was to produce a very simple programming language which encourages the construction of clear and manipulable programs. HOPE does not include an assignment statement; this is felt to be an important simplification. The user may freely define his own data types, without the need to devise a complicated encoding in terms of low-level types. The language is very strongly typed, and as implemented it incorporates a typechecker which handles polymorphic types and overloaded operators. Functions are defined by a set of recursion equations; the left-hand side of each equation includes a pattern used to determine which equation to use for a given argument. The availability of arbitrary higher-order types allows functions to be defined which &#039;package&#039; recursion. Lazily-evaluated lists are provided, allowing the use of infinite lists which could be used to provide interactive input/output and concurrency.
474|Privacy Preserving Data Mining|In this paper we address the issue of privacy preserving data mining. Specifically, we consider a  scenario in which two parties owning confidential databases wish to run a data mining algorithm on  the union of their databases, without revealing any unnecessary information. Our work is motivated  by the need to both protect privileged information and enable its use for research or other purposes. The
475|Public-key cryptosystems based on composite degree residuosity classes| This paper investigates a novel computational problem, namely the Composite Residuosity Class Problem, and its applications to public-key cryptography. We propose a new trapdoor mechanism and derive from this technique three encryption schemes: a trapdoor permutation and two homomorphic probabilistic encryption schemes computationally comparable to RSA. Our cryptosystems, based on usual modular arithmetics, are provably secure under appropriate assumptions in the standard model.  
476|A randomized protocol for signing contracts|Two parties, A and B, want to sign a contract C over a communication network. To do so, they must “simultaneously” exchange their commitments to C. Since simultaneous exchange is usually impossible in practice, protocols are needed to approximate simultaneity by exchanging partial commitments in piece by piece manner. During such a protocol, one party or another may have a slight advantage; a “fair” protocol keeps this advantage within acceptable limits. We present a new protocol that is fair in the sense that, at any stage in its execution, the conditional probability that one party cannot commit both parties to the contract given that the other party can, is close to zero. This is true even if A and B have vastly different computing powers, and is proved under very weak cryptographic assumptions. Our protocol has the following additional properties: 4 during the procedure the parties exchange probadilistic options for committing both parties to the contract; the protocol never terminates in an asymmetric situation where party A knows that party B is committed to the contract while he is not; the protocol makes use of a weak form of a third party (judge). If both A and B are honest, the judge will never be called upon. Otherwise, the judge rules by performing a simple computation. No bookkeeping is required of the judge. 
477|Multiparty unconditionally secure protocols|Under the assumption that each pair of participants em communieatc secretly, we show that any reasonable multiparty protwol can be achieved if at least Q of the Participants am honest. The secrecy achieved is unconditional, It does not rely on any assumption about computational intractability. 1.
478|Security and Composition of Multi-party Cryptographic Protocols|We present general definitions of security for multi-party cryptographic protocols, with focus  on the task of evaluating a probabilistic function of the parties&#039; inputs. We show that, with  respect to these definitions, security is preserved under a natural composition operation.  The definitions follow the general paradigm of known definitions; yet some substantial modifications  and simplifications are introduced. The composition operation is the natural `subroutine  substitution&#039; operation, formalized by Micali and Rogaway.  We consider several standard settings for multi-party protocols, including the cases of eavesdropping,  Byzantine, non-adaptive and adaptive adversaries, as well as the information-theoretic  and the computational models. In particular, in the computational model we provide the first  definition of security of protocols that is shown to be preserved under composition.  
479|Efficient generation of shared RSA keys|We describe efficient techniques for a number of parties to jointly generate an RSA key. At the end of the protocol an RSA modulus N = pq is publicly known. None of the parties know the factorization of N. In addition a public encryption exponent is publicly known and each party holds a share of the private exponent that enables threshold decryption. Our protocols are efficient in computation and communication. All results are presented in the honest but curious settings (passive adversary).
480|Secure Multi-Party Computation|Contents  1 Introduction and Preliminaries 4 1.1 A Tentative Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.1 Overview of the Definitions : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.1.2 Overview of the Known Results : : : : : : : : : : : : : : : : : : : : : : : : : : 5 1.1.3 Aims and nature of the current manuscript : : : : : : : : : : : : : : : : : : : 6 1.1.4 Organization of this manuscript : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2 Preliminaries (also tentative) : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.1 Computational complexity : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 6 1.2.2 Two-party and multi-party protocols : : : : : : : : : : : : : : : : : : : : : : : 10 1.2.3 Strong Proofs of Knowledge : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2 General Two-Party Computation 13 2.1.1 The semi-honest model : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 
481|Two Party RSA Key Generation|. We present a protocol for two parties to generate an RSA  key in a distributed manner. At the end of the protocol the public key: a  modulus N = PQ, and an encryption exponent e are known to both parties.  Individually, neither party obtains information about the decryption  key d and the prime factors of N : P and Q. However, d is shared among  the parties so that threshold decryption is possible.  1 Introduction  We show how two parties can jointly generate RSA public and private keys. Following the execution of our protocol each party learns the public key: N = PQ and e, but does not know the factorization of N or the decryption exponent d. The exponent d is shared among the two players in such a way that joint decryption of cipher-texts is possible.  Generation of RSA keys in a private, distributed manner figures prominently in several cryptographic protocols. An example is threshold cryptography, see [12] for a survey. In a threshold RSA signature scheme there are k parties who ...
482|Oblivious Polynomial Evaluation|Oblivious polynomial evaluation is a protocol involving two parties, a sender whose input is a polynomial P, and a receiver whose input is a value a. At the end of the protocol the receiver learns P (a) and the sender learns nothing. We describe efficient constructions for this protocol, which are based on new intractability assumptions that are closely related to noisy polynomial reconstruction. Oblivious polynomial evaluation can be used as a primitive in many applications. We describe several such applications, including protocols for private comparison of data, for mutually authenticated key exchange based on (possibly weak) passwords, and for anonymous coupons. 1
483|Data Integration: A Theoretical Perspective|Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
484|A Survey of Approaches to Automatic Schema Matching|Schema matching is a basic problem in many database application domains, such as data integration, E-business, data warehousing, and semantic query processing. In current implementations, schema matching is typically performed manually, which has significant limitations. On the other hand, previous research papers have proposed many techniques to achieve a partial automation of the match operation for specific application domains. We present a taxonomy that covers many of these existing approaches, and we describe the approaches in some detail. In particular, we distinguish between schema-level and instance-level, element-level and structure-level, and language-based and constraint-based matchers. Based on our classification we review some previous match implementations thereby indicating which part of the solution space they cover. We intend our taxonomy and review of past work to be useful when comparing different approaches to schema matching, when developing a new match algorithm, and when implementing a schema matching component.
485|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
486|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
487|Information integration using logical views|A number of ideas concerning information-integration tools can be thought of as constructing answers to queries using views that represent the capabilities of information sources. We review the formal basis of these techniques, which are closely related to containment algorithms for conjunctive queries and/or Datalog programs. Then we compare the approaches taken by AT&amp;T Labs&#039; &#034;Information Manifold&#034; and the Stanford &#034;Tsimmis&#034; project in these terms.
488|The TSIMMIS Approach to Mediation: Data Models and Languages|TSIMMIS -- The Stanford-IBM Manager of Multiple Information Sources -- is a system for integrating information. It o ers a data model and a common query language that are designed to support the combining of information from many different sources. It also o ers tools for generating automatically the components that are needed to build systems for integrating information. In this paper we shall discuss the principal architectural features and their rationale. 
489|Query Answering in Inconsistent Databases|In this chapter, we summarize the research on querying inconsistent databases we have been conducting over the last five years. The formal framework we have used is based on two concepts: repair and consistent query answer. We describe different approaches to the issue of computing consistent query answers: query transformation, logic programming, inference in annotated logics, and specialized algorithms. We also characterize the computational complexity of this problem. Finally, we discuss related research in artificial intelligence, databases, and logic programming.
490|Index Structures for Path Expressions|In recent years there has been an increased interest in managing data which does not conform to traditional data models, like the relational or object oriented model. The reasons for this non-conformance are diverse. One one hand, data may not conform to such models at the physical level: it may be stored in data exchange formats, fetched from the Internet, or stored as structured les. One the other hand, it may not conform at the logical level: data may have missing attributes, some attributes may be of di erent types in di erent data items, there may be heterogeneous collections, or the data may be simply specified by a schema which is too complex or changes too often to be described easily as a traditional schema. The term semistructured data has been used to refer to such data. The data model proposed for this kind of data consists of an edge-labeled graph, in which nodes correspond to objects and edges to attributes or values. Figure 1 illustrates a semistructured database providing information about a city. Relational databases are traditionally queried with associative queries, retrieving tuples based on the value of some attributes. To answer such queries efciently, database management systems support indexes for translating attribute values into tuple ids (e.g. B-trees or hash tables). In object-oriented databases, path queries replace the simpler associative queries. Several data structures have been proposed for answering path queries e ciently: e.g., access support relations 14] and path indexes 4]. In the case of semistructured data, queries are even more complex, because they may contain generalized path expressions 1, 7, 8, 16]. The additional exibility is needed in order to traverse data whose structure is irregular, or partially unknown to the user.
491|Complexity of answering queries using materialized views|WC study the complexity of the problem of answering queries using materinlized views, This problem has attracted a lot of attention re-cently because of its relevance in data integration. Previous work considered only conjunctive view definitions. We examine the con-sequences of allowing more expressive view definition languages. Tl~olanguagcsweconsiderforviewdefinitionsanduserqueriesare: conjunctive qucrics with inequality, positive queries, datalog, and first-order logic. We show that the complexity of the problem de-pcnds on whether views are assumed to store all the tuples that sat-isfy the view definition, or only a subset of it. Finally, we apply the results to the view consistency and view self-maintainability prob-lems which nrise in data warehousing. 1
492|Semistructured data|In semistructured data, the information that is normally as-sociated with a schema is contained within the data, which is sometimes called “self-describing”. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tu-torial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expres-sive language for querying and transformation, and opti-mizat,ion problems. 1 The motivation The topic of semistructured data (also called unstructured data) is relatively recent, and a tutorial on the topic may well be premature. It represents, if anything, the conver-gence of a number of lines of thinking about new ways to represent and query data that do not completely fit with conventional data models. The purpose of this tutorial is to to describe this motivation and to suggest areas in which further research may be fruitful. For a similar exposition, the reader is referred to Serge Abiteboul’s recent survey pa-per PI. The slides for this tutorial will be made available from a section of the Penn database home page
493| On the Decidability of Query Containment under Constraints |Query containment under constraints is the problem of checking whether for every database satisfying a given set of constraints, the result of one query is a subset of the result of another query. Recent research points out that this is a central problem in several database applications, and we address it within a setting where constraints are specified in the form of special inclusion dependencies over complex expressions, built by using intersection and difference of relations, special forms of quantification, regular expressions over binary relations, and cardinality constraints. These types of constraints capture a great variety of data models, including the relational, the entity-relational, and the object-oriented model. We study the problem of checking whether q is contained in q ' with respect to the constraints specified in a schema S, where q and q ' are nonrecursive Datalog programs whose atoms are complex expressions. We present the following results on query containment. For the case where q does not contain regular expressions, we provide a method for deciding query containment, and analyze its computational complexity. We do the same for the case where neither S nor q, q ' contain number restrictions. To the best of our knowledge, this yields the first decidability result on containment of conjunctive queries with regular expressions. Finally, we prove that the problem is undecidable for the case where we admit inequalities in q'.  
494|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
495|Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity|Most databases contain &#034;name constants&#034; like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user&#039;s query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
496|Context Interchange: New Features and Formalisms for the Intelligent Integration of Information|The Context Interchange strategy presents a novel perspective for mediated data access in which semantic conflicts among heterogeneous systems are not identified a priori, but are detected and reconciled by a context mediator through comparison of contexts axioms corresponding to the systems engaged in data exchange. In this article, we show that queries formulated on shared views, export schema, and shared “ontologies ” can be mediated in the same way using the Context Interchange framework. The proposed framework provides a logic-based object-oriented formalism for representing and reasoning about data semantics in disparate systems, and has been validated in a prototype implementation providing mediated data access to both traditional and web-based information sources. Categories and Subject Descriptors: H.2.4 [Database Management]: Systems—Query processing; H.2.5 [Database Management]: Heterogeneous Databases—Data translation
497|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
498|Catching the Boat with Strudel: Experiences with a Web-Site Management System|The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel&#039;s key idea is separating the management of the site&#039;s data, the creation and management of the site&#039;s structure, and the visual presentation of the site&#039;s pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site&#039;s structure by applying a &#034;site-definition query&#034; to the underlying data. The result of evaluating this query is a &#034;site graph&#034;, which represents both the site&#039;s content and structure. Third, the builder specifies the visual presentation of pages in Strudel&#039;s HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel&#039;s key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing sev...
499|Answering Recursive Queries Using Views|We consider the problem of answering datalog queries using materialized views. The ability to answer queries using views is crucial in the context of information integration. Previous work on answering queries using views restricted queries to being conjunctive. We extend this work to general recursive queries: Given a datalog program P and a set of views, is it possible to find a datalog program that is equivalent to P and only uses views as EDB predicates? In this paper, we show that the problem of whether a datalog program can be rewritten into an equivalent program that only uses views is undecidable. On the other hand, we prove that a datalog program P can be effectively rewritten into a program that only uses views, that is contained in P,  and that contains all programs that only use views and are contained in P. As a consequence, if there exists a program equivalent to P that only uses views, then our construction is guaranteed to yield a program equivalent to P.  1 Introductio...
500|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
501|Managing Semantic Heterogeneity in Databases : A Theoretical Perspective, Tutorial at PODS|A full version of this tutorial appears at
502|Description Logics in Data Management|Description logics and reasoners, which are descendants of the kl-one language, have been studied in depth in Artificial Intelligence. After a brief introduction, we survey in this paper their application to the problems of information management, using the framework of an abstract information server equipped with several operations -- each involving one or more languages. Specifically, we indicate how one can achieve enhanced access to data and knowledge by using descriptions in languages for schema design and integration, queries, answers, updates, rules, and constraints.
503|The Information Manifold|We describe the Information Manifold (IM), a system for browsing and querying of multiple networked information sources. As a first contribution, the system demonstrates the viability of knowledge representation technology for retrieval and organization of information from disparate (structured and unstructured) information sources. Such an organization allows the user to pose high-level queries that use data from multiple information sources. As a second contribution, we describe novel query processing algorithms used to combine information from multiple sources. In particular, our algorithms are guaranteed to find exactly the set of information sources relevant to a query, and to  completely exploit knowledge about local closed world information (Etzioni et al. 1994). Introduction  We are currently witnessing an explosion in the amount of information that is available online. For example, the rapid rise in popularity of the World Wide Web (WWW) has increased the amount of information...
504|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
505|Description logic framework for information integration|Information Integration is one of the core problems in distributed databases, cooperative information systems, and data warehousing, which are key areas in the software development industry. Two critical factors for the design and maintenance of applications requiring Information Integration are conceptual modeling of the domain, and reasoning support over the conceptual representation. We demonstrate that Knowledge Representation and Reasoning techniques can play an important role for both of these factors, by proposing a Description Logic based framework for Information Integration. We show that the development of successful Information Integration solutions requires not only to resort to very expressive Description Logics, but also to significantly extend them. We present a novel approach to conceptual modeling for Information Integration, which allows for suitably modeling the global concepts of the application, the individual information sources, and the constraints among different sources. Moreover, we devise inference procedures for the fundamental reasoning services, namely relation and concept subsumption, and query containment. Finally, we present a methodological framework for Information Integration, which can be applied in several contexts, and highlights the role of reasoning services within the design process. 1
506|Equivalences among relational expressions with the union and difference operators|ABSTRACT Queries in relational databases can be formulated in terms of relational expressions using the relational operations elect, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators elect, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions i proposed This method of query representation can be viewed as a generahzatlon f tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator isnot applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete ven for tableaux that correspond to expressions with only one project and several join operators KEY WORDS AND PHRASES relatmnal database, relational algebra, query optimization, equivalence of queries, conjunctive query, tableau, NP-complete, polynomial-time hierarchy, H 2P-complete CR CATEGORIES 4 33, 5 25
507|Representing and Using Interschema Knowledge in Cooperative Information Systems|Managing interschema knowledge is an essential task when dealing with cooperative information systems. We propose a logical approach to the problem of both expressing interschema knowledge, and reasoning about it. In particular, we set up a structured representation language for expressing semantic interdependencies between classes belonging to different database schemas, and present a method for reasoning over such interdependencies. The language and the associated reasoning technique makes it possible to build a logic-based module that can draw useful inferences whenever the need arises of both comparing and combining the knowledge represented in the various schemas. Notable examples of such inferences include checking the coherence of interschema knowledge, and providing integrated access to a cooperative information system.
508|Navigational Plans For Data Integration|We consider the problem of building data integration systems when the data sources are webs of data, rather than sets of relations. Previous approaches to modeling data sources are inappropriate in this context because they do not capture the relationships between linked data and the need to navigate through paths in the data source in order to obtain the data. We describe a language for modeling data sources in this new context. We show that our language has the required expressive power, and that minor extensions to it would make query answering intractable. We provide a sound and complete algorithm for reformulating a user query into a query over the data sources, and we show how to create query execution plans that both query and navigate the data sources.  Introduction  The purpose of data integration is to provide a uniform  interface to a multitude of data sources. Data integration applications arise frequently as corporations attempt to provide their customers and employees wit...
509|Tableau Techniques For Querying Information Sources Through Global Schemas|. The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al. 1 Introduction Information Integration systems [Ull...
510|What Can Databases Do for Peer-to-Peer?|The Internet community has recently been focused on peer-to-peer systems like Napster, Gnutella, and Freenet. The  grand vision --- a decentralized community of machines pooling their resources to benefit everyone --- is compelling for  many reasons: scalability, robustness, lack of need for administration, and even anonymity and resistance to censorship.
511|Data Integration under Integrity Constraints|Data integratio n systemspro vide accessto a seto fhetero - geneo us, auto no mo us data so urces thro ugh a so -called glo bal schema. There are basically two appro aches fo r designing a data integratio n system. In the glo bal-centric appro ach,o ne defines the elementso f the glo bal schema as viewso ver the so urces, whereas in the lo cal-centric appro ach, o e characterizes the so rces as viewso ver theglo al schema. It is well kno wn that pro cessing queries in the latter appro ach is similar to query answering with inc o plete infoC atio , and, therefo9 is a c o plex task. On theo ther hand, it is a co mmo no pinio n that query pro cessing is much easier in the fo rmer appro ach. In this paper we sho w the surprising result that, when theglo al schema is expressed in the relatio al mo del with integrity c o straints, eveno f simple types, the pr o lemo f inco6 plete info rmatio n implicitly arises, making querypro cessing di#cult in the glo al-centric approC h as well. We thenfo cuso n glo al schemas with key andfo eign key co straints, which represents a situat io which is veryco#=W in practice, and we illustrate techniques fo e#ectively answering queries po sed to the data integratio n system in this case. 1 
512|Towards Heterogeneous Multimedia Information Systems: The Garlic Approach|Abstract: We provide an overview of the Garlic project, a new project at the IBM Almaden Research Center. The goal of this project is to develop a system and associated tools for the management of large quantities of heterogeneous multimedia information. Garlic permits traditional and multimedia data to be
513|Rewriting of Regular Expressions and Regular Path Queries|Recent work on semi-structured data has revitalized the interest in path queries, i.e., queries that ask for all pairs of objects in the database that are connected by a path conforming to a certain specification, in particular to a regular expression. Also, in semi-structured data, as well as in data integration, data warehousing, and query optimization, the problem of view-based query rewriting is receiving much attention: Given a query and a collection of views, generate a new query which uses the views and provides the answer to the original one. In this paper we address the problem of view-based query rewriting in the context of semi-structured data. We present a method for computing the rewriting of a regular expression E in terms of other regular expressions. The method computes the exact rewriting (the one that defines the same regular language as E) if it exists, or the rewriting that defines the maximal language contained in the one defined by E, otherwise. We present a complexity analysis of both the problem and the method, showing that the latter is essentially optimal. Finally, we illustrate how to exploit the method for view-based rewriting of regular path queries in semi-structured data. The complexity results established for the rewriting of regular expressions apply also to the case of regular path queries. 
514|CARIN: A Representation Language Combining Horn Rules and Description Logics|.  We describe CARIN, a novel family of representation languages, which integrate the expressive power of Horn rules and of description logics. We address the key issue in designing such a language, namely, providing a sound and complete inference procedure. We identify existential entailment as a core problem in reasoning in  CARIN, and describe an existential entailment algorithm for CARIN  languages whose description logic component is ALCNR. This algorithm entails several important results for reasoning in CARIN, most notably: (1) a sound and complete inference procedure for non recursive  CARIN-ALCNR, and (2) an algorithm for determining rule subsumption over ALCNR. 1 Introduction  Horn rule languages have formed the basis for many Artificial Intelligence application languages because their expressive power is sufficient for many applications, and they have good computational properties. One of the significant limitations of Horn rules is that they are not expressive enough to mod...
515|Query optimization in the presence of limited access patterns|We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the di erent conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best- rst search strategy in order to produce a rst complete plan early in the search. We describe experiments to illustrate the performance of our algorithm. 1
516|Quality-driven Integration of Heterogeneous Information Systems|Integrated access to information that is spread over multiple, distributed, and heterogeneous sources is an important problem in many scientific and commercial domains. While much work has been done on query processing and choosing plans under cost criteria, very little is known about the important problem of incorporating the information quality aspect into query planning. In this paper we describe a framework for multidatabase query processing that fully includes the quality of information in many facets, such as completeness, timeliness, accuracy, etc. We seamlessly include information quality into a multidatabase query processor based on a view-rewriting mechanism. We model information quality at different levels to ultimately find a set of high-quality query answering plans.
517|Query Containment for Conjunctive Queries With Regular Expressions|The management of semistructured data has recently received significant attention because of the need of several applications to model and query large volumes of irregular data. This paper considers the problem of query containment for a query language over semistructured data, StruQL0 , that contains the essential feature common to all such languages, namely the ability to specify regular path expressions over the data. We show here that containment of StruQL0 queries is decidable. First, we give a semantic criterion for StruQL0 query containment: we show that it suffices to check containment on only finitely many canonical databases. Second, we give a syntactic criteria for query containment, based on a notion of query mappings, which extends containment mappings for conjunctive queries. Third, we consider a certain fragment of StruQL0 , obtained by imposing restrictions on the regular path expressions, and show that query containment for this fragment of  StruQL0 is NP complete.  1 ...
518|Recursive Plans for Information Gathering|Generating query-answering plans for information gathering agents requires to translate a user query, formulated in terms of a set of virtual relations, to a query that uses relations that are actually stored in information sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle information sources with binding-pattern limitations, and to exploit functional dependencies in the domain model. As a result, these plans were incomplete w.r.t. sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive  information gathering plans, which enables us to settle two open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources in the presence of functional dependencies. Second, we describe an analogous algorithm in the presence of binding-pattern restrictions in the sources...
519|EQUIVALENCES AMONG RELATIONAL EXPRESSIONS|Many database queries can be formulated in terms of expressions whose operands represent tables of information (relations) and whose operators are the relational operations select, project, and join. This paper studies the equivalence problem for these relational expressions, with expression optimization in mind. A matrix, called a tableau, is proposed as a natural representative for the value of an expression. It is shown how tableaux can be made to reflect functional dependencies among attributes. A polynomial time algorithm is presented for the equivalence of tableaux that correspond to an important subset of expressions, although the equivalence problem is shown to be NP-complete under slightly more general circumstances.
520|On the equivalence of recursive and nonrecursive Datalog programs|vardi Abstract: We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Dat-alog programs are equivalent to unions of conjunctive queries, we study also the problem of determining whether a given recursive Datalog program is contained in a union of con-junctive queries. For this problem, we prove doubly exponential upper and lower time bounds. For the equivalence problem, we prove triply exponential upper and lower time bounds. 1
521|Containment of conjunctive regular path queries with inverse |Reasoning on queries is a basic problem both in knowledge representation and databases. A fundamental form of reasoning on queries is checking containment, i.e., verifying whether one query yields necessarily a subset of the result of another query. Query containment is crucial in several contexts, such as query optimization, knowledge base verification, information integration, database integrity checking, and cooperative answering. In this paper we address the problem of query containment in the context of semistructured knowledge bases, where the basic querying mechanism, namely regular path queries,
522|Rewriting Aggregate Queries Using Views|We investigate the problem of rewriting queries with aggregate operators using views that mayormay not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum. Our approach is based on syntactic characterizations of the equivalence of aggregate queries. One contribution of this paper are characterizations of the equivalence of disjunctive aggregate queries, which generalize our previous results for the conjunctive case. For each operator &amp;alpha;, we introduce several types of queries using views as candidates for rewritings. We unfold such a candidate by replacing each occurrence of a view predicate with ...
523|An Extensible Framework for Data Cleaning|Data integration solutions dealing with large amounts of data have been strongly required in the last few years.  Besides the traditional data integration problems (e.g. schema integration, local to global schema mappings),  three additional data problems have to be dealt with: (1) the absence of universal keys across dierent databases  that is known as the object identity problem, (2) the existence of keyboard errors in the data, and (3) the presence  of inconsistencies in data coming from multiple sources. Dealing with these problems is globally called the data  cleaning process. In this work, we propose a framework which oers the fundamental services required by this  process: data transformation, duplicate elimination and multi-table matching. These services are implemented  using a set of purposely designed macro-operators. Moreover, we propose an SQL extension for specifying  each of the macro-operators. One important feature of the framework is the ability of explicitly includ...
524|Answering regular path queries using views|Query answering using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, mobile computing, and maintaining physical data independence. We address query answering using views in a context where queries and views are regular path queries, i.e., regular expressions that denote the pairs of objects in the database connected by a matching path. Regular path queries are the basic query mechanism when the database is conceived as a graph, such as in semistructured data and data on the web. We study algorithms for answering regular path queries using views under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We characterize data, expression, and combined complexity of the problem, showing that the proposed algorithms are essentially optimal. Our results are the first to exhibit decidability in cases where the language for expressing the query and the views allows for recursion. 
525|Query Planning and Optimization in Information Integration|Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
526|Answering queries using views over description logics knowledge bases|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.
527|A Formal Perspective on the View Selection Problem|The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views t into a prespeci ed storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equalityselection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded. 1
528|Query containment for data integration systems|The problem of query containment is fundamental to many aspects of database systems,including query optimization,determining independence of queries from updates,and rewriting queries using views. In the data-integration framework,however,the standard notion of query containment does not suffice. We define relative containment,which formalizes the notion of query containment relative to the sources available to the data-integration system. First,we provide optimal bounds for relative containment for several important classes of datalog queries,including the common case of conjunctive queries. Next,we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly,we show that relative containment for conjunctive queries is still decidable in this case,even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally,we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.
529|Specifying and querying database repairs using logic programs with exceptions|Abstract Databases may be inconsistent with respect to a given set of integrity constraints. Nevertheless, most of the data may be consistent. In this paper we show how to specify consistent data and how to query a relational database in such a way that only consistent data is retrieved. The specification and queries are based on disjunctive extended logic programs with positive and negative exceptions that generalize those previously introduced by Kowalski and Sadri.
530|View-based query processing and constraint satisfaction|View-based query processing requires to answer a query posed to a database only on the basis of the information on a set of views, which are again queries over the same database. This problem is relevant in many aspects of database management, and has been addressed by means of two basic approaches, namely, query rewriting and query answering. In the former approach, one tries to compute a rewriting of the query in terms of the views, whereas in the latter, one aims at directly answering the query based on the view extensions. We study view-based query processing for the case of regular-path queries, which are the basic querying mechanisms for the emergent field of semistructured data. Based on recent results, we first show that a rewriting is in general a co-NP function wrt to the size of view extensions. Hence, the problem arises of characterizing which instances of the problem admit a rewriting that is PTIME. A second contribution of the work is to establish a tight connection between view-based query answering and constraint-satisfaction problems, which allows us to show that the above characterization is going to be difficult. As a third contribution of our work, we present two methods for computing PTIME rewritings of specific forms. The first method, which is based on the established connection with constraint-satisfaction problems, gives us rewritings expressed in Datalog with a fixed number of variables. The second method, based on automata-theoretic techniques, gives us rewritings that are formulated as unions of conjunctive regular-path queries with a fixed number of variables.  
531|Optimization Properties for Classes of Conjunctive Regular Path Queries |Abstract. We are interested in the theoretical foundations of the optimization of conjunctive regular path queries (CRPQs). The basic problem here is deciding query containment both in the absence and presence of constraints. Containment without constraints for CRPQs is EXPSPACE-complete, as opposed to only NP-complete for relational conjunctive queries. Our past experience with implementing similar algorithms suggests that staying in PSPACE might still be useful. Therefore we investigate the complexity of containment for a hierarchy of fragments of the CRPQ language. The classifying principle of the fragments is the expressivity of the regular path expressions allowed in the query atoms. For most of these fragments, we give matching lower and upper bounds for containment in the absence of constraints. We also introduce for every fragment a naturally corresponding class of constraints in whose presence we show both decidability and undecidability results for containment in various fragments. Finally, we apply our results to give a complete algorithm for rewriting with views in the presence of constraints for a fragment that contains Kleene-star and disjunction. 1
532|Deciding Containment for Queries with Complex Objects and Aggregations|We address the problem of query containment and query equivalence for complex objects. We show that for a certain conjunctive query language for complex objects, query containment and weak query equivalence are decidable. Our results have two consequences. First, when the answers of the two queries are guaranteed not to contain empty sets, then weak equivalence coincides with equivalence, and our result answers partially an open problem about the equivalence of nest; unnest queries for complex objects [GPG90]. Second, we derive an NP-complete algorithm for checking the equivalence of certain conjunctive queries with grouping and aggregates. Our results rely on a translation of the containment and equivalence conditions for complex objects into novel conditions on conjunctive queries, which we call simulation and strong simulation. These conditions are more complex than containment of conjunctive queries, because they involve arbitrary numbers of quantifier alternations. We prove that c...
533|Capability Based Mediation in TSIMMIS|Introduction  The TSIMMIS system [1] integrates data from multiple heterogeneous sources and provides users with seamless integrated views of the data. It translates a user query on the integrated views into a set of source queries and postprocessing steps that compute the answer to the user query from the results of the source queries. TSIMMIS uses a  mediation architecture [11] to accomplish this (Figure 1). User Source 1 Source 2 Source N  Mediator Figure 1: The TSIMMIS Architecture Many other data integration systems like Garlic [2, 9] and Information Manifold [4] employ a similar architecture. One of the distinguishing features of TSIMMIS is its use of a semi-structured data model (called the Object Exchange Model or OEM [7]) for dealing with the heterogeneity of the data sources. In particular, it employs source wrappers [3] that provide a uniform OEM interface to the mediator. In SIGMO
534|Query Answering in Information Systems with Integrity Constraints|The specifications of most of the nowadays ubiquitous informations systems include integrity constraints, i.e. conditions rejecting so-called &#034;invalid&#034; or &#034;inconsistent &#034; data. Information system consistency and query answering have been formalized referring to classical logic implicitly assuming that query answering only makes sense with consistent information systems. In practice, however, inconsistent as well as consistent information systems need to be queried. In this paper, it is first argued that classical logic is inappropriate for a formalization of information systems because of its global notion of inconsistency. It is claimed that information systems inconsistency should be understood as a  local notion. Then, it is shown that minimal logic, a constructivistic weakening of classical logic which precludes refutation proofs, provides for local inconsistencies that conveniently reflect a practitioner&#039;s intuition. Further, minimal logic is shown to be a convenient foundation fo...
535|Query Folding with Inclusion Dependencies|Query folding is a technique for determining how a query may be answered using a given set of resources, which may include materialized views, cached results of previous queries, or queries answerable by other databases. The power of query folding can be considerably enhanced by taking into account integrity constraints that are known to hold on base relations. This paper describes an extension of query folding that utilizes inclusion dependencies to find foldings of queries that would otherwise be overlooked. We describe a complete strategy for finding foldings in the presence of inclusion dependencies and present a basic algorithm that implements that strategy. We also describe extensions to this algorithm when both inclusion and functional dependencies are considered.
536|Information Integration: the MOMIS Project Demonstration|ranted provided that the copies are not made or distributed for direct commercial advantage, the VLDB copyright notice and the title of the publication and its date appear, and notice is given that copying is by permission of the Very Large Data Base Endowment. To copy otherwise, or to republish, requires a fee and/or special permission from the Endowment. Proceedings of the 26th VLDB Conference,  Cairo, Egypt, 2000.  2  MOMIS is a joint project among the Universit`a di Modena e Reggio Emilia, the Universit`a di Milano, and the Universit`a di Brescia within the national research project INTERDATA, theme n.3 &#034;Integration of Information over the Web&#034;, coordinated by V. De Antonellis, Universit`a di Brescia.  1. a common data model, ODM I 3  , which is defined according to the ODL I 3 language, to describe source schemas for integration purposes. ODM I 3 and ODL I 3&lt;F12.24
537|Query Planning with Limited Source Capabilities|In information-integration systems, sources may have diverse and limited query capabilities. In this paper we show that because sources have restrictions on retrieving their information, sources not mentioned in a query can contribute to the query result by providing useful bindings. In some cases we can access sources repeatedly to retrieve bindings to answer a query, and query planning thus becomes considerably more challenging. We find all the obtainable answers to a query by translating the query and source descriptions to a simple recursive Datalog program, and evaluating the program on the source relations. This program often accesses sources that are not in the query. Some of these accesses are essential, as they provide bindings that let us query sources, which we could not do otherwise. However, some of these accesses can be proven not to add anything to the query&#039;s answer. We show in which cases these off-query accesses are useless, and prove that in these cases we can comput...
538|Querying Aggregate Data|We introduce a first-order language with real polynomial arithmetic and aggregation operators (count, iterated sum and multiply), which is well suited for the definition of aggregate queries involving complex statistical functions. It offers a good trade-off between expressive power and complexity, with a tractable data complexity. Interestingly, some fundamental properties of first-order with real arithmetic are preserved in the presence of aggregates. In particular, there is an effective quantifier elimination for formulae with aggregation. We consider the problem of querying data that has already been aggregated in aggregate views, and focus on queries with an aggregation over a conjunctive query. Our main conceptual contribution is the introduction of a new equivalence relation among conjunctive queries, the isomorphism modulo a product. We prove that the equivalence of aggregate queries such as for instance averages reduces to it. Deciding if two queries are isomorphic modulo a p...
539|Accessing data integration systems through conceptual schemas|Abstract. Data integration systems provide access to a set of heterogeneous, autonomous data sources through a so-called global, or mediated view. There is a general consensus that the best way to describe the global view is through a conceptual data model, and that there are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. It is well known that processing queries in the latter approach is similar to query answering with incomplete information, and, therefore, is a complex task. On the other hand, it is a common opinion that query processing is much easier in the former approach. In this paper we show the surprising result that, when the global schema is expressed in terms of a conceptual data model, even a very simple one, query processing becomes difficult in the global-as-view approach also. We demonstrate that the problem of incomplete information arises in this case too, and we illustrate some basic techniques for effectively answering queries posed to the global schema of the data integration system. 1
540|Answering Queries Using Materialized Views With Disjunctions|We consider the problem of answering datalog queries using  materialized views. More  specifi.
541|On Answering Queries in the Presence of Limited Access Patterns|. In information-integration systems, source relations often  have limitations on access patterns to their data; i.e., when one must  provide values for certain attributes of a relation in order to retrieve its  tuples. In this paper we consider the following fundamental problem: can  we compute the complete answer to a query by accessing the relations  with legal patterns? The complete answer to a query is the answer that  we could compute if we could retrieve all the tuples from the relations.  We give algorithms for solving the problem for various classes of queries,  including conjunctive queries, unions of conjunctive queries, and conjunctive  queries with arithmetic comparisons. We prove the problem is  undecidable for datalog queries. If the complete answer to a query cannot  be computed, we often need to compute its maximal answer. The  second problem we study is, given two conjunctive queries on relations  with limited access patterns, how to test whether the maximal answer to...
542|Answering Queries Using Limited External Query Processors|When answering queries using external information sources, their contents can be described by views. To answer a query, we must rewrite it using the set of views presented by the sources. When the external information sources also have the ability to answer some (perhaps limited) sets of queries that require performing operations on their data, the set of views presented by the source may be infinite (albeit encoded in some finite fashion). Previous work on answering queries using views has only considered the case where the set of views is finite. In order to exploit the ability of information sources to answer more complex queries, we consider the problem of answering conjunctive queries using infinite sets of conjunctive views. Our first result is that an infinite set of conjunctive views can be partitioned into a finite number of equivalence classes, such that picking one view from every nonempty class is sufficient to determine whether the query can be answered using the views. Se...
543|On the Content of Materialized Aggregate Views|We consider the problem of answering queries using only materialized views. We first show that if the views subsume the query from the point of view of the information content, then the query can be answered using only the views, but the resulting query might be extremely inefficient. We then focus on aggregate views and queries over a single relation, which are fundamental in many applications such as data warehousing. We show that in this case, it is possible to guarantee that as soon as the views subsume the query, it can be completely rewritten in terms of the views in a simple query language. Our main contribution is the conception of various rewriting algorithms which run in polynomial time, and the proof of their completeness which relies on combinatorial arguments. Finally, we discuss the choice of materializing or not ratio views such as average and percentage, important for the design of materialized views. We show that it has an impact on the information content, which can b...
544|Lossless Regular Views|If the only information we have on a certain database is through a set of views, the question arises of whether this is sucient to answer completely a given query. We say that the set of views is lossless with respect to the query, if, no matter what the database is, we can answer the query by solely relying on the content of the views. The question of losslessness has various applications, for example in query optimization, mobile computing, data warehousing, and data integration. We study this problem in a context where the database is semistructured, and both the query and the views are expressed as regular path queries. The form of recursion present in this class prevents us from applying known results to our case.
545|View-based query answering and query containment over semistructured data|Abstract. The basic querying mechanism over semistructured data, namely regular path queries, asks for all pairs of objects that are connected by a path conforming to a regular expression. We consider conjunctive two-way regular path queries (C2RPQc’s), which extend regular path queries with two features. First, they add the inverse operator, which allows for expressing navigations in the database that traverse the edges both backward and forward. Second, they allow for using conjunctions of atoms, where each atom specifies that a regular path query with inverse holds between two terms, where each term is either a variable or a constant. For such queries we address the problem of view-based query answering, which amounts to computing the result of a query only on the basis of a set of views. More specifically, we present the following results: (1) We exhibit a mutual reduction between query containment and the recognition problem for view-based query answering for C2RPQc’s, i.e., checking whether a given tuple is in the certain answer to a query. Based on such a result, we can show that the problem of view-based query answering for C2RPQc’s is EXPSPACE-complete. (2) By exploiting techniques based on alternating two-way automata we show that for the restricted class of tree two-way regular path queries (in which the links between variables form a tree), query containment and view-based query answering are, rather surprisingly, in PSPACE (and hence, PSPACE-complete). (3) We present a technique to obtain view-based query answering algorithms that compute the whole set of tuples in the certain answer, instead of requiring to check each tuple separately. The technique is parametric wrt the query language, and can be applied both to C2RPQc’s and to tree-queries. 1
546|Models for Information Integration: Turning Local-as-View Into Global-as-View|There are basically two approaches for designing a data integration system. In the global-as-view approach, one defines the concepts in the global schema as views over the sources, whereas in the local-as-view approach, one characterizes the sources as views over the global schema. The goal of this paper is to verify whether we can transform a data integration system built with the local-as-view approach into a system following the global-as-view approach. We study the problem in a setting where the global schema is expressed in the relational model with inclusion dependencies, and the queries used in the integration systems (both the queries on the global schema, and the views in the mapping) are expressed in the language of conjunctive queries. The result we present is that such a transformation exists: we can always transform a local-as-view system into a global-as-view system such that, for each query, the set of answers to the query wrt the former is the same as the set of answers wrt the latter.
547|Query Rewriting using Semistructured Views|We address the problem of query rewriting for MSL, a semistructured language developed at Stanford in the TSIMMIS project for information integration. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting  queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and unification -- techniques which were developed for structured, relational data. At the same time we develop an algorithm for equivalence checking of MSL queries. We show that the rewriting algorithm is sound and complete, i.e., it always finds every conjunctive MSL rewriting query of q, and we discuss its complexity. We currently incorporate the algorithm in the TSIMMIS system. 1 Introduction  Recently, many semistructured data models, query and view definition languages have been proposed [GM  +  97, MAG  +  97, BDHS96, AV97a, MM97, KS95, PGMU96,...
548|Intensional Query Answering by Partial Evaluation|. Intensional query answering aims at providing a response to a query addressed to a knowledge base by making use of the intensional knowledge as opposed to extensional. Such a response is an abstract description of the conventional answer that can be of interest in many situations, for example it may increase the cooperativeness of the system, or it may replace the conventional answer in case access to the extensional part of the knowledge base is costly as for Mobile Systems. In this paper we present a general framework to generate intensional answers in knowledge bases adhering to the logic programming paradigm. Such a framework is based on a program transformation technique, namely Partial Evaluation, and allows for generating complete and procedurally complete (wrt SLDNF-resolution) sets of intensional answers, treating both recursion and negation conveniently. Keywords: Knowledge bases, intensional query answering, logic programs, partial evaluation 1. Introduction Intensional an...
549|The X-tree: An index structure for high-dimensional data|In this paper, we propose a new method for index-ing large amounts of point and spatial data in high-dimensional space. An analysis shows that index structures such as the R*-tree are not adequate for indexing high-dimensional data sets. The major problem of R-tree-based index structures is the overlap of the bounding boxes in the directory, which increases with growing dimension. To avoid this problem, we introduce a new organization of the directory which uses a split algorithm minimiz-ing overlap and additionally utilizes the concept of supernodes. The basic idea of overlap-minimizing split and supernodes is to keep the directory as hi-erarchical as possible, and at the same time to avoid splits in the directory that would result in high over-lap. Our experiments show that for high-dimen-sional data, the X-tree outperforms the well-known R*-tree and the TV-tree by up to two orders of magnitude. 1.
550|Static Scheduling of Synchronous Data Flow Programs for Digital Signal Processing|Large grain data flow (LGDF) programming is natural and convenient for describing digital signal processing (DSP) systems, but its runtime overhead is costly in real time or cost-sensitive applications. In some situations, designers are not willing to squander computing resources for the sake of program-mer convenience. This is particularly true when the target machine is a programmable DSP chip. However, the runtime overhead inherent in most LGDF implementations is not required for most signal processing systems because such systems are mostly synchronous (in the DSP sense). Synchronous data flow (SDF) differs from traditional data flow in that the amount of data produced and consumed by a data flow node is specified a priori for each input and output. This is equivalent to specifying the relative sample rates in signal processing system. This means that the scheduling of SDF nodes need not be done at runtime, but can be done at compile time (statically), so the runtime overhead evaporates. The sample rates can all be different, which is not true of most current data-driven digital signal processing programming methodologies. Synchronous data flow is closely related to computation graphs, a special case of Petri nets. This self-contained paper develops the theory necessary to statically schedule SDF programs on single or multiple proces-sors. A class of static (compile time) scheduling algorithms is proven valid, and specific algorithms are given for scheduling SDF systems onto single or multiple processors. 
551|Petri Nets|Over the last decade, the Petri net has gamed increased usage and acceptance as a basic model of systems of asynchronous concurrent computation. This paper surveys the basic concepts and uses of Petm nets. The structure of Petri nets, their markings and execution, several examples of Petm net models of computer hardware and software, and
552|A Compositional Approach to Performance Modelling|Performance modelling is concerned with the capture and analysis of the dynamic behaviour of computer and communication systems. The size and complexity of many modern systems result in large, complex models. A compositional approach decomposes the system into subsystems that are smaller and more easily modelled. In this thesis a novel compositional approach to performance modelling is presented. This approach is based on a suitably enhanced process algebra, PEPA (Performance Evaluation Process Algebra). The compositional nature of the language provides benefits for model solution as well as model construction. An operational semantics is provided for PEPA and its use to generate an underlying Markov process for any PEPA model is explained and demonstrated. Model simplification and state space aggregation have been proposed as means to tackle the problems of large performance models. These techniques are presented in terms of notions of equivalence between modelling entities. A framewo...
553|Lattice-Based Access Control Models|The objective of this article is to give a tutorial on lattice-based  access control models for computer security. The paper begins with a review  of Denning&#039;s axioms for information flow policies, which provide a theoretical  foundation for these models. The structure of security labels in the military and  government sectors, and the resulting lattice is discussed. This is followed by a  review of the Bell-LaPadula model, which enforces information flow policies by  means of its simple-security and *-properties. It is noted that information flow  through covert channels is beyond the scope of such access controls. Variations  of the Bell-LaPadula model are considered. The paper next discusses the Biba  integrity model, examining its relationship to the Bell-LaPadula model. The  paper then reviews the Chinese Wall policy, which arises in a segment of the  commercial sector. It is shown how this policy can be enforced in a lattice  framework.
554|Role-Based Access Control|While Mandatory Access Controls (MAC) are appropriate for multilevel secure military applications, Discretionary Access Controls (DAC) are often perceived as meeting the security processing needs of industry and civilian government. This paper argues that reliance on DAC as the principal method of access control is unfounded and inappropriate for many commercial and civilian government organizations. The paper describes a type of non-discretionary access control - role-based access control (RBAC) - that is more central to the secure processing needs of non-military systems then DAC. 1 Introduction  The U.S. government has been involved in developing security technology for computer and communications systems for some time. Although advances have been great, it is generally perceived that the current state of security technology has, to some extent failed to address the needs of all. [1], [2] This is especially true of organizations outside the Department of Defense (DoD). [3] The curre...
555|The Typed Access Matrix Model|The access matrix model as formalized by Harrison, Ruzzo, and Ullman (HRU) has broad expressive power. Unfortunately, HRU has weak safety properties (i.e., the determination of whether or not a given subject can ever acquire access to a given object). Most security policies of practical interest fall into the undecidable cases of HRU. This is true even for monotonic policies (i.e., where access rights can be deleted only if the deletion is itself reversible). In this paper we define the typed access matrix (TAM) model by introducing strong typing into HRU (i.e., each subject or object is created to be of a particular type which thereafter does not change). We prove that monotonic TAM (MTAM) has strong safety properties similar to Sandhu&#039;s Schematic Protection Model. Safety in MTAM&#039;s decidable case is, however, NP-hard. We develop a model called ternary MTAM which has polynomial safety for its decidable case, and which nevertheless retains the full expressive power of MTAM. There is compelling evidence that the decidable safety cases of ternary MTAM are quite adequate for modeling practial monotonic security policies.
556|Access Rights Administration in Role-Based Security Systems|This paper examines the concept of role-based protection and, in particular, role  organization. From basic role relationships, a model for role organization is developed.  The role graph model, its operator semantics based on graph theory and algorithms for  role administration are proposed. The role graph model, in our view, presents a very  generalized form of role organization for access rights administration. It is shown how  the model simulates other organizational structures such as hierarchies [TDH92] and  privilege graphs [Bal90]. 
557|Conceptual Foundations for a Model of Task-based Authorizations|In this paper we describe conceptual foundations to address integrity issues in computerized information systems from the enterprise perspective. Our motivation for this effort stems from the recognition that existing models are formulated at too low a level of abstraction, to be useful for modeling organizational requirements, policy aspects, and internal controls, pertaining to maintenance of integrity in information systems. In particular, these models are primarily concerned with the integrity of internal data components within computer systems, and thus lack the constructs necessary to model enterprise level integrity principles. The starting point in our investigation is the notion of authorization functions and tasks associated with business activities carried out in the enterprise. These functions identify the authorization requirements while the authorization tasks embody the concepts required to carry out such authorizations. We believe a model of task-based autho...
558|A Lattice Interpretation Of The Chinese Wall Policy|The Chinese Wall policy was identi#ed and so named by Brewer and Nash #2#.  This policy arises in the segment of the commercial sector which provides consulting  services to other companies. Consultants naturally have to deal with con#dential company  information for their clients. The objective of the Chinese Wall policy is to prevent  information #ows which cause con#ict of interest for individual consultants. Brewer and  Nash develop a mathematical model of the Chinese Wall policy, on the basis of which  they claim that this policy #cannot be correctly represented by a Bell-LaPadula model.&#034;  In this paper we demonstrate that the Brewer-Nash model is too restrictivetobeemployed  in a practical system. This is due to their treatment of users and subjects as  synonymous concepts, with the consequence that they do not distinguish security policy  as applied to human users versus security policy as applied to computer subjects. By  maintaining a careful distinction between users, princip...
559|Delegation Of Authority|This paper is concerned with the specification of discretionary access control policy for commercial security and the delegation of access control authority in a way which gives flexibility while retaining management control. Large distributed processing systems have very large numbers of users and resource objects so that it is impractical to specify access control policy in terms of individual objects or individual users. We need to be able to specify it as relationships between groups of users and groups of objects. The systems typically consist of multiple interconnected networks and span a number of different organisations. Authority cannot be delegated or imposed from one central point, but has to be negotiated between independent managers who wish to cooperate but who may have a very limited trust in each other. The paper proposes the use of access rules to specify, in terms of their domain memberships, what operations a user can perform on a target object. The delegation of aut...
560|Exploiting Generative Models in Discriminative Classifiers|Generative probability models such as hidden Markov models provide a principled way of treating missing information and dealing with variable length sequences. On the other hand, discriminative methods such as support vector machines enable us to construct flexible decision boundaries and often result in classification performance superior to that of the model based approaches. An ideal classifier should combine these two complementary approaches. In this paper, we develop a natural way of achieving this combination by deriving kernel functions for use in discriminative methods such as support vector machines from generative probability models. We provide a theoretical justification for this combination as well as demonstrate a substantial improvement in the classification performance in the context of DNA and protein sequence analysis.
561|The Nature of Statistical Learning Theory| Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more
562|Multiple alignment using hidden Markov models|eddy~genetics.wustl.edu A simulated annealing method is described for training hidden Markov models and producing multiple sequence alignments from initially unaligned protein or DNA sequences. Simulated annealing in turn uses a dynamic programming algorithm for correctly sampling suboptimal multiple alignments according to their probability and a Boltzmann temperature factor. The quality of simulated annealing alignments is evaluated on structural alignments of ten different protein families, and compared to the performance of other HMM training methods and the ClnstalW program. Simulated annealing is better able to find near-global optima in the multiple alignment probability landscape than the other tested HMM training methods. Neither ClustalW nor simulated annealing produce consistently better alignments compared to each other. Examination of the specific cases in which ClustalW outperforms simulated annealing, and vice versa, provides insight into the strengths and weaknesses of current hidden Markov model approaches.
563|Predicting protein structure using hidden Markov models|We discuss how methods based on hidden Markov models performed in the fold recognition section of the CASP2 experiment. Hidden Markov models were built for a set of about a thousand structures from the PDB database, and each CASP2 target sequence was scored against this library of hidden Markov models. In addition, a hidden Markov model was built for each of the target sequences, and all of the sequences in PDB were scored against that target model. Having high scores from both methods was found to be highly indicative of the target and a structure being homologous. Predictions were made based on several criteria: the scores with the structure models, the scores with the target models, consistency between the secondary structure in the known structure and predictions for the target (using the program PhD), human examination of predicted alignments between target and structure (using RASMOL), and solvation preferences in the alignment of the target and structure. The method worked well in comparison to other methods used at CASP2 for targets of moderate difficulty, where the closest structure in PDB could be aligned to the target with at least 15 % residue identity. There was no evidence for the method&#039;s e ectiveness for harder cases, where the residue identity was much lower than 15%.
564|Modeling Strategic Relationships for Process Reengineering|Existing models for describing a process (such as a business process or a software development process) tend to focus on the \what &#034; or the \how &#034; of the process. For example, a health insurance claim process would typically be described in terms of a number of steps for assessing and approving a claim. In trying to improve orredesign a process, however, one also needs to have an understanding of the \why &#034;  { for example, why dophysicians submit treatment plans to insurance companies before giving treatment? and why do claims managers seek medical opinions when assessing treatment plans? An understanding of the motivations and interests of process participants is often crucial to the successful redesign of processes. This thesis proposes a modelling framework i (pronounced i-star) consisting of two modelling components. The Strategic Dependency (SD) model describes a process in terms of intentional dependency relationships among agents. Agents depend on each other for goals to be achieved, tasks to be performed, and resources to be furnished. Agents are intentional in that they have desires and wants, and strategic in that they are concerned about opportunities and vulnerabilities. The Strategic Rationale (SR) model describes the issues and concerns that agents
565|A Field Study of the Software Design Process for Large Systems|The problems of designing large software systems were studied through interviewing personnel from 17 large projects. A layered behavioral model is used to analyze how three lgf these problems-the thin spread of application domain knowledge, fluctuating and conflicting requirements, and communication bottlenecks and breakdowns-affected software productivity and quality through their impact on cognitive, social, and organizational processes.
566|Value-Based Software Engineering|Abstract—This paper provides a definition of the term “software engineering ” and a survey of the current state of the art and likely future trends in the field. The survey covers the technology available in the various phases of the software life cycle—requirements engineering, design, coding, test, and maintenance—and in the overall area of software management and integrated technology-management approaches. It is oriented primarily toward discussing the domain of applicability of techniques (where and when they work), rather than how they work in detail. To cover the latter, an extensive set of 104 references is provided. Index Terms—Computer software, data systems, information systems,
567|Representing and Using Non-Functional Requirements: A Process-Oriented Approach|The paper proposes a comprehensive framework for representing and using non-functional requirements during the development process. The framework consists of five basic components which provide for the representation of non-functional requirements in terms of interrelated goals. Such goals can be refined through refinement methods and can be evaluated in order to determine the degree to which a set of non-functional requirements is supported by a particular design. Evidence for the power of the framework is provided through the study of accuracy and performance requirements for information systems.  1 
568|Telos: Representing Knowledge About Information Systems|This paper describes a language that is intended to support software engineers in the development of information systems throughout the software lifecycle. This language is not a programming language. Following the example of a number of other software engineering projects, our work is based on the premise that information system development is knowledge-intensive and that the primary responsibility of any language intended to support this task is to be able to formally represent the relevant knowledge.
569|Knowledge Representation and Reasoning in the Design of Composite Systems|Abstract- Our interest is in the design process that spans the gap between the requirements acquisition process and the implementation process, in which the basic architecture of a system is defined, and functions are allocated to software, hard-ware, and human agents. We call this process composite system design. Our goal is an interactive model of composite system design incorporating deficiency-driven design, formal analysis, incremental design and rationalization, and design reuse. We discuss knowledge representations and reasoning techniques for the product (composite system) that we are designing, and for the design process, which support these goals. To evaluate our model, we report on its use to rationally reconstruct the design of two existing composite systems. Index Terms-Automated analysis, composite systems, knowl-edge-based design, rational reconstruction, software specification. 0
570|Characterizing and Assessing a Large-Scale Software Maintenance Organization|One important component of a software process is the organizational context in which the process is enacted. This component is often missing or incomplete in current process modeling approaches. One technique for modeling this perspective is the Actor-Dependency (AD) Model. This paper reports on a case study which used this approach to analyze and assess a large software maintenance organization. Our goal was to identify the approach&#039;s strengths and weaknesses while providing practical recommendations for improvement. The AD model was found to be very useful in capturing the important properties of the organizational context of the maintenance process, and aided in the understanding of the flaws found in this process. However, a number of opportunities for extending and improving the AD model were identified. Among others, there is a need to incorporate quantitative information to complement the qualitative model. 1. Introduction It has now been recognized that, in order to improve the...
571|Goal-Based Process Analysis: A Method for Systematic Process Redesign|A method is proposed for systematically analyzing and redesigning processes. The method, Goal-based Process Analysis (GPA), helps its user to systematically identify missing objectives, ensure implementation of all the objectives, identify non-functional parts of a process, and explore alternative processes for achieving a given set of objectives. As such, GPA addresses a critical component in process reengineering, that of identifying which part of a given process needs to be improved and what alternatives could be used instead.  Keywords  Process Redesign, Process Analysis, Goal Analysis, Work Flow Design, Organizational Design  1. INTRODUCTION  Critical in process reengineering is some way of identifying what needs to be redesigned as well as understanding what alternatives we have. This paper proposes a method, Goal-based Process Analysis (GPA), that provide a systematic way to: . identify missing goals . ensure implementation of all the goals . identify non-functional parts of a p...
572|Representation and Utilization of Non-Functional Requirements for Information System Design|The complexity and usefulness of large information systems are determined partly by their functionality, i.e., what they do, and partly by global constraints on their accuracy, security, cost, user-friendliness, performance, and the like. Even with the growing interest in developing higher-level models and design paradigms, current technology is inadequate both representationally for expressing such global constraints as formal non-functional requirements and methodologically for utilizing them in generating designs. We propose both a representational and methodological framework for non-functional requirements, focusing on accuracy requirements. With the premise that accuracy is an inherent semantic attribute of information, we take a first step towards establishing a representational basis for accuracy. To guide the design process and justify design decisions, we propose a goal-oriented methodology. In the methodology, accuracy requirements are treated as (potentially conflicting) go...
573|Using Quality Requirements To Systematically Develop Quality Software|. Although quality issues such as accuracy, security, and performance are often crucial to the success of a software system, there has been no systematic way to achieve quality requirements during system development. We offer a framework and an implemented tool which treat quality requirements as goals to be achieved systematically during the system development process. We illustrate the process that a developer would go through, in building quality into a system. We have tested the framework on a number of studies involving a variety of quality requirements, organisational settings, and system types. Keywords: non-functional requirements, accuracy, security, performance, information systems, process, software quality, defect detection, conflicts. 1 Problem  Software development is traditionally driven by functional requirements, i.e., the desired functionality of the system. For example, a credit card system should debit and credit accounts, check credit limits, charge interest, issue...
574|Linked Data -- The story so far  |The term Linked Data refers to a set of best practices for publishing and connecting structured data on the Web. These best practices have been adopted by an increasing number of data providers over the last three years, leading to the creation of a global data space containing billions of assertions- the Web of Data. In this article we present the concept and technical principles of Linked Data, and situate these within the broader context of related technological developments. We describe progress to date in publishing Linked Data on the Web, review applications that have been developed to exploit the Web of Data, and map out a research agenda for the Linked Data community as it moves forward.
575|DBpedia: A Nucleus for a Web of Open Data|Abstract DBpedia is a community effort to extract structured informa-tion from Wikipedia and to make this information available on the Web. DBpedia allows you to ask sophisticated queries against datasets derived from Wikipedia and to link other datasets on the Web to Wikipedia data. We describe the extraction of the DBpedia datasets, and how the resulting information is published on the Web for human- and machine-consumption. We describe some emerging applications from the DBpedia community and show how website authors can facilitate DBpedia content within their sites. Finally, we present the current status of interlinking DBpedia with other open datasets on the Web and outline how DBpedia could serve as a nucleus for an emerging Web of open data. 1
576|Duplicate record detection: A survey|Often, in the real world, entities have two or more representations in databases. Duplicate records do not share a common key and/or they contain errors that make duplicate matching a dif cult task. Errors are introduced as the result of transcription errors, incomplete information, lack of standard formats or any combination of these factors. In this article, we present a thorough analysis of the literature on duplicate record detection. We cover similarity metrics that are commonly used to detect similar eld entries, and we present an extensive set of duplicate detection algorithms that can detect approximately duplicate records in a database. We also cover multiple techniques for improving the ef ciency and scalability of approximate duplicate detection algorithms. We conclude with a coverage of existing tools and with a brief discussion of the big open problems in the area.  
577|Sindice.com: A document-oriented lookup index for open linked data |Developers of Semantic Web applications face a challenge with respect to the decentralised publication model: how and where to find statements about encountered resources. The “linked data” approach mandates that resource URIs should be de-referenced to return resource metadata. But for data discovery linkage itself is not enough, and crawling and indexing of data is necessary. Existing Semantic Web search engines are focused on database-like functionality, compromising on index size, query performance and live updates. We present Sindice, a lookup index over resources crawled on the Semantic Web. Our index allows applications to automatically locate documents containing information about a given resource. In addition, we allow resource retrieval through uniquely identifying inverse-functional properties, offer a full-text search and index SPARQL endpoints. Finally we introduce an extension to the sitemap protocol which allows us to efficiently index large Semantic Web datasets with minimal impact on the data providers.
578|Querying Distributed RDF Data Sources with SPARQL|Abstract. Integrated access to multiple distributed and autonomous RDF data sources is a key challenge for many semantic web applications. As a reaction to this challenge, SPARQL, the W3C Recommendation for an RDF query language, supports querying of multiple RDF graphs. However, the current standard does not provide transparent query federation, which makes query formulation hard and lengthy. Furthermore, current implementations of SPARQL load all RDF graphs mentioned in a query to the local machine. This usually incurs a large overhead in network traffic, and sometimes is simply impossible for technical or legal reasons. To overcome these problems we present DARQ, an engine for federated SPARQL queries. DARQ provides transparent query access to multiple SPARQL services, i.e., it gives the user the impression to query one single RDF graph despite the real data being distributed on the web. A service description language enables the query engine to decompose a query into sub-queries, each of which can be answered by an individual service. DARQ also uses query rewriting and cost-based query optimization to speed-up query execution. Experiments show that these optimizations significantly improve query performance even when only a very limited amount of statistical information is available. DARQ is available under GPL License at
579|Principles of dataspace systems|The most acute information management challenges today stem from organizations relying on a large number of diverse, interrelated data sources, but having no means of managing them in a convenient, integrated, or principled fashion. These challenges arise in enterprise and government data management, digital libraries, “smart ” homes and personal information management. We have proposed dataspaces as a data management abstraction for these diverse applications and DataSpace Support Platforms (DSSPs) as systems that should be built to provide the required services over dataspaces. Unlike data integration systems, DSSPs do not require full semantic integration of the sources in order to provide useful services. This paper lays out specific technical challenges to realizing DSSPs and ties them to existing work in our field. We focus on query answering in DSSPs, the DSSP’s ability to introspect on its content, and the use of human attention to enhance the semantic relationships in a dataspace.  
581|Triplify -- Light-Weight Linked Data Publication from Relational Databases|In this paper we present Triplify – a simplistic but effective approach to publish Linked Data from relational databases. Triplify is based on mapping HTTP-URI requests onto relational database queries. Triplify transforms the resulting relations into RDF statements and publishes the data on the Web in various RDF serializations, in particular as Linked Data. The rationale for developing Triplify is that the largest part of information on the Web is already stored in structured form, often as data contained in relational databases, but usually published by Web applications only as HTML mixing structure, layout and content. In order to reveal the pure structured information behind the current Web, we have implemented Triplify as a light-weight software component, which can be easily integrated into and deployed by the numerous, widely installed Web applications. Our approach includes a method for publishing update logs to enable incremental crawling of linked data sources. Triplify is complemented by a library of configurations for common relational schemata and a REST-enabled data source registry. Triplify configurations containing mappings are provided for many popular Web applications, including osCommerce, WordPress, Drupal, Gallery, and phpBB. We will show that despite its light-weight architecture Triplify is usable to publish very large datasets, such as 160GB of geo data from the OpenStreetMap project.
582|Named Graphs|The Semantic Web consists of many RDF graphs nameable by URIs. This paper extends the syntax and semantics of RDF to cover such named graphs. This enables RDF statements that describe graphs, which is beneficial in many Semantic Web application areas. Named graphs are given an abstract syntax, a formal semantics, an XML syntax, and a syntax based on N3. SPARQL is a query language applicable to named graphs. A specific application area discussed in detail is that of describing provenance information. This paper provides a formally defined framework suited to being a foundation for the Semantic Web trust layer.
583|Bootstrapping pay-as-you-go data integration systems|Data integration systems offer a uniform interface to a set of data sources. Despite recent progress, setting up and maintaining a data integration application still requires significant upfront effort of creating a mediated schema and semantic mappings from the data sources to the mediated schema. Many application contexts involving multiple data sources (e.g., the web, personal information management, enterprise intranets) do not require full integration in order to provide useful services, motivating a pay-as-you-go approach to integration. With that approach, a system starts with very few (or inaccurate) semantic mappings and these mappings are improved over time as deemed necessary. This paper describes the first completely self-configuring data integration system. The goal of our work is to investigate how advanced of a starting point we can provide a pay-as-you-go system. Our system is based on the new concept of a probabilistic mediated schema that is automatically created from the data sources. We automatically create probabilistic schema mappings between the sources and the mediated schema. We describe experiments in multiple domains, including 50-800 data sources, and show that our system is able to produce high-quality answers with no human intervention.
584|Provenance Information in the Web of Data|The openness of the Web and the ease to combine linked data from different sources creates new challenges. Systems that consume linked data must evaluate quality and trustworthiness of the data. A common approach for data quality assessment is the analysis of provenance information. For this reason, this paper discusses provenance of data on the Web and proposes a suitable provenance model. While traditional provenance research usually addresses the creation of data, our provenance model also represents data access, a dimension of provenance that is particularly relevant in the context of Web data. Based on our model we identify options to obtain provenance information and we raise open questions concerning the publication of provenance-related metadata for linked data on the Web.
585|Automatic Interlinking of Music Datasets on the Semantic Web |In this paper, we describe current efforts towards interlinking music-related datasets on the Web. We first explain some initial interlinking experiences, and the poor results obtained by taking a naïve approach. We then detail a particular interlinking algorithm, taking into account both the similarities of web resources and of their neighbours. We detail the application of this algorithm in two contexts: to link a Creative Commons music dataset to an editorial one, and to link a personal music collection to corresponding web identifiers. The latter provides a user with personally meaningful entry points for exploring the web of data, and we conclude by describing some concrete tools built to generate and use such links.
586|The Open Provenance Model|The Open Provenance Model (OPM) is a community-driven data model for Provenance that is designed to support inter-operability of provenance technology. Underpinning OPM, is a notion of directed acyclic graph, used to represent data products and processes involved in past computations, and causal dependencies between these. The Open Provenance Model was derived following two “Provenance Challenges”, international, multidisciplinary activities trying to investigate how to exchange information between multiple systems supporting provenance and how to query it. The OPM design was mostly driven by practical and pragmatic considerations, and is being tested in a third Provenance Challenge, which has just started. The purpose of this paper is to investigate the theoretical foundations of this data model. The formalisation consists of a set-theoretic definition of the data model, a definition of the inferences by transitive closure that are permitted, a formal description of how the model can be used to express dependencies in past computations, and finally, a description of the kind of time-based inferences that are supported. A novel element that OPM introduces is the concept of an account, by which multiple descriptions of a same execution are allowed to co-exist in a same graph. Our formalisation gives a precise meaning to such accounts and associated notions of alternate and refinement. Warning It was decided that this paper should be released as early as possible since it brings useful clarifications on the Open Provenance Model, and therefore can benefit the Provenance Challenge 3 community. The reader should recognise that this paper is however an early draft, and several sections are incomplete. Additionally, figures rely on colours but these may be difficult to read when printed in a black and white. It is advisable to print the paper in colour. 1 1
587|Which Semantic Web?|Through scenarios in the popular press and technical papers in the research literature, the promise of the Semantic Web has raised a number of different expectations. These expectations can be traced to three different perspectives on the Semantic Web. The Semantic Web is portrayed as: (1) a universal library, to be readily accessed and used by humans in a variety of information use contexts; (2) the backdrop for the work of computational agents completing sophisticated activities on behalf of their human counterparts; and (3) a method for federating particular knowledge bases and databases to perform anticipated tasks for humans and their agents. Each of these perspectives has both theoretical and pragmatic entailments, and a wealth of past experiences to guide and temper our expectations. In this paper, we examine all three perspectives from rhetorical, theoretical, and pragmatic viewpoints with an eye toward possible outcomes as Semantic Web efforts move forward.
588|M.: Linked movie data base|The Linked Movie Database (LinkedMDB) project provides a demonstration of the first open linked dataset connecting several major existing (and highly popular) movie web resources. The database exposed by LinkedMDB contains millions of RDF triples with hundreds of thousands of RDF links to existing web data sources that are part of the growing Linking Open Data cloud, as well as to popular movierelated web pages such as IMDb. LinkedMDB uses a novel way of creating and maintaining large quantities of high quality links by employing state-of-the-art approximate join techniques for finding links, and providing additional RDF metadata about the quality of the links and the techniques used for deriving them.
589|A Framework for Semantic Link Discovery over Relational Data |In this paper, we present a framework for online discovery of semantic links from relational data. Our framework is based on declarative specification of the linkage requirements by the user, that allows matching data items in many real-world scenarios. These requirements are translated to queries that can run over the relational data source, potentially using the semantic knowledge to enhance the accuracy of link discovery. Our framework lets data publishers to easily find and publish high-quality links to other data sources, and therefore could significantly enhance the value of the data in the next generation of web.
590|How will we interact with the web of data|The Semantic Web is a global information space of linked data, designed not for human use but for consumption by machines. Right? Well, yes and no. It&#039;s true to say that machine-readable data,  given explicit semantics and published online,  coupled with the ability to link data in distributed data sets are the key selling points of the Semantic Web. Together, these features allow aggregation and integration of heterogeneous data on an unprecedented scale,  and machines will do the grunt work for us. However, without a human being somewhere in this process, to reap the rewards of these new capabilities, the endeavour is meaningless. Far from removing human beings from the equation, a Web of machine-readable data creates significant challenges and significant opportunities for human-computer interaction. To date,  the Semantic Web community has mostly been busy developing the technical infrastructure to make the Web of Data feasible in principle and on publishing linked data sets in order to make it a reality. If we are to fully exploit the challenges and opportunities of a Web of Data from a human perspective,  we need to move beyond the initial phase and work to understand how this changes the user interaction paradigm of the Web.
591|What is the Size of the Semantic Web|Abstract: When attempting to build a scaleable Semantic Web application, one has to know about the size of the Semantic Web. In order to be able to understand the characteristics of the Semantic Web, we examined an interlinked dataset acting as a representative proxy for the Semantic Web at large. Our main finding was that regarding the size of the Semantic Web, there is more than the sheer number of triples; the number and type of links is an equally crucial measure.
592|Tabulator Redux: Browsing and Writing Linked Data |second frame shows information within that source expanded, the third frame shows another source within that source expanded, and finally, the last frame shows that the label of that source has been edited from “Music and artist data interlinked ” to “Music and artist data linked on the Semantic Web” A first category of Semantic Web browsers was designed to present a given dataset (an RDF graph) for perusal in various forms. These include mSpace, Exhibit, and to a certain extent
593|Integration of semantically annotated data by the knofuss architecture|Abstract. Most of the existing work on information integration in the Semantic Web concentrates on resolving schema-level problems. Specific issues of data-level integration (instance coreferencing, conflict resolu-tion, handling uncertainty) are usually tackled by applying the same techniques as for ontology schema matching or by reusing the solutions produced in the database domain. However, data structured according to OWL ontologies has its specific features: e.g., the classes are organized into a hierarchy, the properties are inherited, data constraints differ from those defined by database schema. This paper describes how these fea-tures are exploited in our architecture KnoFuss, designed to support data-level integration of semantic annotations. 1
594|DBpedia Mobile - A Location-Aware Semantic Web Client|Abstract. DBpedia Mobile is a location-aware client for the Semantic Web that can be used on an iPhone and other mobile devices. Based on the current GPS position of a mobile device, DBpedia Mobile renders a map indicating nearby locations from the DBpedia dataset. Starting from this map, the user can explore background information about his surroundings by navigating along data links into other Web data sources. DBpedia Mobile has been designed for the use case of a tourist exploring a city. As the application is not restricted to a fixed set of data sources but can retrieve and display data from arbitrary Web data sources, DBpedia Mobile can also be employed within other use cases, including ones un-foreseen by its developers. Besides accessing Web data, DBpedia Mobile also enables users to publish their current location, pictures and reviews to the Semantic Web so that they can be used by other Semantic Web applications. Instead of simply being tagged with geographical coordi-nates, published content is interlinked with a nearby DBpedia resource and thus contributes to the overall richness of the Geospatial Semantic Web.
595|Information-seeking on the Web with Trusted Social Networks – from Theory to Systems|This research investigates how synergies between the Web and social networks can enhance the process of obtaining relevant and trustworthy information. A review of literature on personalised search, social search, recommender systems, social networks and trust propagation reveals limitations of existing technology in areas such as relevance, collaboration, task-adaptivity and trust. In response to these limitations I present a Web-based approach to information-seeking using social networks. This approach takes a source-centric perspective on the information-seeking process, aiming to identify trustworthy sources of relevant information from within the user&#039;s social network. An empirical study of source-selection decisions in information- and recommendationseeking identified five factors that influence the choice of source, and its perceived trustworthiness. The priority given to each of these factors was found to vary according to the criticality and subjectivity of the task. A series of algorithms have been developed that operationalise three of these factors (expertise, experience, affinity) and generate from various data sources a number of trust metrics for use in social network-based information seeking. The most significant of these data sources is Revyu.com, a reviewing and rating Web site implemented as part of this research, that takes input from regular users and makes it available on the Semantic Web for easy re-use by the implemented algorithms. Output of the algorithms is used in Hoonoh.com, a Semantic Web-based system that has been developed to support users in identifying relevant and trustworthy information   sources within their social networks. Evaluation of this system&#039;s ability to predict source selections showed more promising results for the experience factor than for expertise or affinity. This may be attributed to the greater demands these two factors place in terms of input data. Limitations of the work and opportunities for future research are discussed.  
596|Power-law distributions in empirical data|Power-law distributions occur in many situations of scientific interest and have significant consequences for our understanding of natural and man-made phenomena. Unfortunately, the empirical detection and characterization of power laws is made difficult by the large fluctuations that occur in the tail of the distribution. In particular, standard methods such as least-squares fitting are known to produce systematically biased estimates of parameters for power-law distributions and should not be used in most circumstances. Here we describe statistical techniques for making accurate parameter estimates for power-law data, based on maximum likelihood methods and the Kolmogorov-Smirnov statistic. We also show how to tell whether the data follow a power-law distribution at all, defining quantitative measures that indicate when the power law is a reasonable fit to the data and when it is not. We demonstrate these methods by applying them to twentyfour real-world data sets from a range of different disciplines. Each of the data sets has been conjectured previously to follow a power-law distribution. In some cases we find these conjectures to be consistent with the data while in others the power law is ruled out.
597|A Brief History of Generative Models for Power Law and Lognormal Distributions |Recently, I became interested in a current debate over whether file size distributions are best modelled by a power law distribution or a a lognormal distribution. In trying
598|A Random Graph Model for Massive Graphs|  We propose a random graph model which is a special case of sparse random graphs with given degree sequences. This model involves only a small number of parameters, called logsize and log-log growth rate. These parameters capture some universal characteristics of massive graphs. Furthermore, from these parameters, various properties of the graph can be derived. For example, for certain ranges of the parameters, we will compute the expected distribu-tion of the sizes of the connected components which almost surely occur with high probability. We will illustrate the consistency of our model with the behavior of some massive graphs derived from data in telecommunications. We will also discuss the threshold function, the giant component, and the evolution of random graphs in this model.  
599|Collective entity resolution in relational data|Many databases contain uncertain and imprecise references to real-world entities. The absence of identifiers for the underlying entities often results in a database which contains multiple references to the same entity. This can lead not only to data redundancy, but also inaccuracies in query processing and knowledge extraction. These problems can be alleviated through the use of entity resolution. Entity resolution involves discovering the underlying entities and mapping each database reference to these entities. Traditionally, entities are resolved using pairwise similarity over the attributes of references. However, there is often additional relational information in the data. Specifically, references to different entities may cooccur. In these cases, collective entity resolution, in which entities for cooccurring references are determined jointly rather than independently, can improve entity resolution accuracy. We propose a novel relational clustering algorithm that uses both attribute and relational information for determining the underlying domain entities, and we give an efficient implementation. We investigate the impact that different relational similarity measures have on entity resolution quality. We evaluate our collective entity resolution algorithm on multiple real-world databases. We show that it improves entity resolution performance over both attribute-based baselines and over algorithms that consider relational information but do not resolve entities collectively. In addition, we perform detailed experiments on synthetically generated data to identify data characteristics that favor collective relational resolution over purely attribute-based algorithms.
600|Where mathematics meets the Internet|The Internet has experienced a fascinating evolution in the recent past, especially since the early days of the Web, a fact well-documented not only in the trade journals, but also in the popular press. Unprecedented in its growth, unparalleled in its heterogeneity, and unpredictable or even chaotic in the behavior of its tra c, \the Internet is its own revolution&#034;, as Anthony-Michael Rutkowski, former Executive Director of the Internet Society, likes to put it.
601|A Functional Approach to External Graph Algorithms|. We present a new approach for designing external graph algorithms  and use it to design simple external algorithms for computing connected components,  minimum spanning trees, bottleneck minimum spanning trees, and maximal  matchings in undirected graphs and multi-graphs. Our I/O bounds compete  with those of previous approaches. Unlike previous approaches, ours is purely  functional---without side effects---and is thus amenable to standard checkpointing  and programming language optimization techniques. This is an important  practical consideration for applications that may take hours to run.  1 Introduction  We present a divide-and-conquer approach for designing external graph algorithms, i.e., algorithms on graphs that are too large to fit in main memory. Our approach is simple to describe and implement: it builds a succession of graph transformations that reduce to sorting, selection, and a recursive bucketing technique. No sophisticated data structures are needed. We apply our t...
602|Problems with fitting to the powerlaw distribution |Abstract. This short communication uses a simple experiment to show that fitting to a power law distribution by using graphical methods based on linear fit on the log-log scale is biased and inaccurate. It shows that using maximum likelihood estimation (MLE) is far more robust. Finally, it presents a new
603|Functional and topological characterization of protein interaction networks|The elucidation of the cell’s large-scale organization is a primary challenge for post-genomic biology, and understanding the structure of protein interaction networks offers an important starting point for such studies. We compare four available databases that approximate the protein interaction network of the yeast, Saccharomyces cerevisiae, aiming to uncover the network’s generic large-scale properties and the impact of the proteins ’ function and cellular localization on the network topology. We show how each database supports a scale-free, topology with hierarchical modularity, indicating that these features represent a robust and generic property of the protein interactions network. We also find strong correlations between the network’s structure and the functional role and subcellular localization of its protein constituents, concluding that most functional and/or localization classes appear as relatively segregated subnetworks of the full protein interaction network. The uncovered systematic differences between the four protein interaction databases reflect their relative coverage for different functional and localization classes and provide a guide for their utility in various bioinformatics studies. Keywords: Bioinformatics / Protein interaction networks / Scale-free networks 1
604|On the bias of traceroute sampling: or, power-law degree distributions in regular graphs|Understanding the graph structure of the Internet is a crucial step for building accurate network models and designing efficient algorithms for Internet applications. Yet, obtaining this graph structure can be a surprisingly difficult task, as edges cannot be explicitly queried. For instance, empirical studies of the network of Internet Protocol (IP) addresses typically rely on indirect methods like traceroute to build what are approximately single-source, all-destinations, shortest-path trees. These trees only sample a fraction of the network’s edges, and a recent paper by Lakhina et al. found empirically that the resulting sample is intrinsically biased. Further, in simulations, they observed that the degree distribution under traceroute sampling exhibits a power law even when the underlying degree distribution is Poisson. In this paper, we study the bias of traceroute sampling mathematically and, for a very general class of underlying degree distributions, explicitly calculate the distribution that will be observed. As example applications of our machinery, we prove that traceroute sampling finds power-law degree distributions in both d-regular and Poisson-distributed random graphs. Thus, our work puts the observations of Lakhina et al. on a rigorous footing, and extends them to nearly arbitrary degree distributions.
605|Currency and commodity metabolites: Their identification and relation to the modularity of metabolic networks|The large-scale shape and function of metabolic networks are intriguing topics of systems biology. Such networks are on one hand commonly regarded as modular (i.e. built by a number of relatively independent subsystems), but on the other hand they are robust in a way not expected of a purely modular system. To address this question we carefully discuss the partition of metabolic networks into subnetworks. The practice of preprocessing such networks by removing the most abundant substrates, “currency metabolites,” is formalized into a network-based algorithm. We study partitions for metabolic networks of many organisms and find cores of currency metabolites and modular peripheries of what we call “commodity metabolites.” The networks are found to be more modular than random networks but far from perfectly divisible into modules. We argue that cross-modular edges are the key for the robustness of metabolism. 
606|Likelihood-Based Inference for Stochastic Models of Sexual Network Formation|Sexually-Transmitted Diseases (STDs) constitute a major public health concern. Mathematical models for the transmission dynamics of STDs indicate that heterogeneity in sexual activity level allow them to persist even when the typical behavior of the population would not support endemicity. This insight focuses attention on the distribution of sexual activity level in a population. In this paper, we develop several stochastic process models for the f&#039;ormation of sexual partnership networks. Using likelihood-based model selection procedures, we assess the fit of the different models to three large distributions of sexual partner counts: (1) Rakai, Uganda, (2) Sweden, and (3) the USA. Five of&#039; the six single-sex networks were fit best by the negative binomial model. The American women&#039;s network was best fit by a power-law model, the Yule. For most networks, several competing models fit approximately equally well. These results sug- gest three conclusions: (1) no single unitary process clearly underlies the formation of these sexual networks, (2) behavioral heterogeneity plays an essential role in network structure, (3) substantial model uncertainty exists for sexual network degree distributions. Behavioral research focused on the mechanisms of partnership f&#039;ormation will play an essential role in specifying the best model for empirical degree distributions. We discuss the limitations of inferences f&#039;rom such data, and the utility of degree-based epidemiological models more generally.
607|Editorial: The future of power law research |Abstract. I argue that power law research must move from focusing on observation, interpretation, and modeling of power law behavior to instead considering the challenging problems of validation of models and control of systems. 1. The Problem with Power Law Research To begin, I would like to recall a humorous insight from the paper of Fabrikant, Koutsoupias, and Papadimitriou [Fabrikant et al. 01], consisting of this quote and the following footnote. “Power laws... have been termed ‘the signature of human activity’... ” 1 The study of power laws, especially in networks, has clearly exploded over the last decade, with seemingly innumerable papers and even popular books, such as Barabási’s Linked [Barabási 02] and Watts ’ Six Degrees [Watts 03]. Power laws are, indeed, everywhere. Despite this remarkable success, I believe that research into power laws in computer networks (and networks more generally) suffers from glaring deficiencies that need to be addressed by the community. Coping with these deficiencies should lead to another great burst of exciting and compelling research. To explain the problem, I would like to make an analogy to the area of string theory. String theory is incredibly rich and beautiful mathematically, with a simple and compelling basic starting assumption: the universe’s building blocks do not really correspond to (zero-dimensional) points, but to small 1 “They are certainly the product of one particular kind of human activity: looking for power laws... ” [Fabrikant et al. 01]
608|DYNAMICS OF BAYESIAN UPDATING WITH DEPENDENT DATA AND MISSPECIFIED MODELS|Recent work on the convergence of posterior distributions under Bayesian updating has established conditions under which the posterior will concentrate on the truth, if the latter has a perfect representation within the support of the prior, and under various dynamical assumptions, such as the data being independent and identically distributed or Markovian. Here I establish sufficient conditions for the convergence of the posterior distribution in non-parametric problems even when all of the hypotheses are wrong, and the data-generating process has a complicated dependence structure. The main dynamical assumption is the generalized asymptotic equipartition (or “Shannon-McMillan-Breiman”) property of information theory. I derive a kind of large deviations principle for the posterior measure, and discuss the advantages of predicting using a combination of models known to be wrong. An appendix sketches connections between the present results and the “replicator dynamics” of evolutionary theory.  
609|The QQ-Estimator And Heavy Tails|. A common visual technique for assessing goodness of fit and estimating location and scale is the qq--plot. We apply this technique to data from a Pareto distribution and more generally to data generated by a distribution with a heavy tail. A procedure for assessing the presence of heavy tails and for estimating the parameter of regular variation is discussed which can supplement other standard techniques such as the Hill plot. 1. Introduction.  A graphical technique called the qq-plot is a commonly used method of visually assessing goodness of fit and of estimating location and scale parameters. The method is standard and ubiquitious in various forms. See for example Rice (1988) and Castillo (1988). The method is based on the following simple observation: If  U 1;n  U 2;n  : : : U n;n  are the order statistics from n iid observations which are uniformly distributed on [0; 1], then by symmetry E(U i+1;n \Gamma U i;n ) = 1  n + 1 and hence  EU i;n =  i n + 1  :  Thus since U i;n should...
610|On the frequency of severe terrorist events|The online version of this article can be found at:
611|Radial structure of the internet|The structure of the Internet at the autonomous system (AS) level has been studied by the
612|Estimating heavy–tail exponents through max self–similarity|2 Heavy tailed data • A random variable X is said to be heavy–tailed if P{|X |  = x}  ~ L(x)x -a, as x ? 8, for some a&gt; 0 and a slowly varying function L. ? Here we focus on the simpler but important context: X = 0, a.s. and P{X&gt; x}  ~ Cx -a, as x ? 8. ? X (infinite moments) For p&gt; 0, EX p &lt; 8 if and only if p &lt; a. In particular, and 0 &lt; a = 2 ? Var(X)  = 8 0 &lt; a = 1 ? E|X |  = 8. • The estimation of the heavy–tail exponent a is an important problem with rich history. • Why do we need heavy–tail models? Every finite sample X1,..., Xn has finite sample mean, variance and all sample moments! Why consider heavy tailed models in practice?! 3 Why use heavy–tailed models? “All models are wrong, but some are useful.” George Box Let F and G be any two distributions with positive densities on (0, 8). Let ?&gt; 0 and x1,..., xn ? (0, 8) be arbitrary, then both: and PF {Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 are positive! PG{Xi ? (xi - ?, xi + ?), i = 1,..., n}&gt; 0 • For a given sample, very many models apply. • The ones that continue to work as the sample grows are most suitable. We next present real data sets of Financial, Insurance and Internet data. They can be very heavy tailed. 4 Traded volumes on the Intel stock
613|Empirical distributions of logreturns: Between the stretched exponential and the power law? Quantitative Finance |A large consensus now seems to take for granted that the distributions of empirical returns of financial time series are regularly varying, with a tail exponent b close to 3. First, we show by synthetic tests performed on time series with time dependence in the volatility with both Pareto and Stretched-Exponential distributions that for sample of moderate size, the standard generalized extreme value (GEV) estimator is quite inefficient due to the possibly slow convergence toward the asymptotic theoretical distribution and the existence of biases in presence of dependence between data. Thus it cannot distinguish reliably between rapidly and regularly varying classes of distributions. The Generalized Pareto distribution (GPD) estimator works better, but still lacks power in the presence of strong dependence. Then, we use a parametric representation of the tail of the distributions of returns of 100 years of daily return of the Dow Jones Industrial Average and over 1 years of 5-minutes returns of the Nasdaq Composite index, encompassing both a regularly varying distribution in one limit of the parameters and rapidly varying distributions of the class of the Stretched-Exponential (SE) and Log-Weibull distributions in other limits. Using the method of nested hypothesis testing (Wilks ’ theorem),
614|A model for technical inefficiency effects in a stochastic frontier production function for panel data|Abstract: A stochastic frontier production function is defined for panel data on firms, in which the non-negative technical inetGciency effects are assumed to be a function of firm-specific variables and time. The inefficiency effects are assumed to be independently distributed as truncations of normal distributions with constant variance, but with means which are a linear function of observable variables. This panel data model is an extension of recently proposed models for inefTiciency effects in stochastic frontiers for cross-sectional data. An empirical application of the model is obtained using up to ten years of data on paddy farmers from an Indian village. The null hypotheses, that the inefficiency effects are not stochastic or do not depend on the farmer-specific variables and time of observation, are rejected for these data.
616|Oligomorphic permutation groups|A permutation group G (acting on a set ?, usually infinite) is said to be oligomorphic if G has only finitely many orbits on ? n (the set of n-tuples of elements of ?). Such groups have traditionally been linked with model theory and combinatorial enumeration; more recently their group-theoretic properties have been studied, and links with graded algebras, Ramsey theory, topological dynamics, and other areas have emerged. This paper is a short summary of the subject, concentrating on the enumerative and algebraic aspects but with an account of grouptheoretic properties. The first section gives an introduction to permutation groups and to some of the more specific topics we require, and the second describes the links to model theory and enumeration. We give a spread of examples, describe results on the growth rate of the counting functions, discuss a graded algebra associated with an oligomorphic group, and finally discuss group-theoretic properties such as simplicity, the small index property, and “almost-freeness”.
617|Structure and Complexity of Relational Queries|This paper is an attempt at laying the foundations for the classification of queries on  relational data bases according to their structure and their computational complexity. Using  the operations of composition and fixpoints, a Z--// hierarchy of height w 2, called the  fixpoint query hierarchy, is defined, and its properties investigated. The hierarchy includes  most of the queries considered in the literathre including those of Codd and Aho and Ullman
618|Toward Logic Tailored for Computational Complexity|Whereas first-order logic was developed to confront the infinite it is often used in computer science in such a way that infinite models are meaningless. We discuss the first-order theory of finite structures and alternatives to first-order logic, especially polynomial time logic. 
619|The Mordell-Lang conjecture for function fields|In [La65], Lang formulated a hypothesis including as special cases the Mordell conjecture concerning rational points on curves, and the Manin-Mumford conjecture on torsion points of Abelian varieties. Sometimes generalized to semi-Abelian varieties, and to positive characteristic, this has been called the Mordell-Lang conjecture;
620|Extending partial automorphisms and the profinite topology on free groups|Abstract. A class of structures C is said to have the extension property for partial automorphisms (EPPA) if, whenever C1 and C2 are structures in C, C1 finite, C1 ? C2, and p1,p2,...,pn are partial automorphisms of C1 extending to automorphisms of C2, then there exist a finite structure C3 in C and automorphisms a1,a2,...,an of C3 extending the pi. We will prove that some classes of structures have the EPPA and show the equivalence of these kinds of results with problems related with the profinite topology on free groups. In particular, we will give a generalisation of the theorem, due to Ribes and Zalesskii stating that a finite product of finitely generated subgroups is closed for this topology. 1.
621|Logarithmic-Exponential Power Series|. We use generalized power series to construct algebraically a nonstandard model of the theory of the real field with exponentiation. This model enables us to show the undefinability of the zeta function and certain non-elementary and improper integrals. We also use this model to answer a question of Hardy by showing that the compositional inverse to the function (log x)(log log x) is not asymptotic as x ! +1 to a composition of semialgebraic functions, log and exp. x1 Introduction and preliminaries  Let RfX 1 ; : : : ; Xm  g denote the ring of all real power series in X 1 ; : : : ; Xm that converge in a neighborhood of I  m  , where I = [\Gamma1; 1]. For f 2 RfX 1 ; : : : ; Xm  g we let  e f : R  m  ! R be given by:  e f(x) =  ae  f(x); for x 2 I  m  , 0; x 62 I  m  . We call the e f &#039;s restricted analytic functions. Let L an be the language of ordered rings  f!; 0; 1; +; \Gamma; \Deltag augmented by a new function symbol for each function e f . We let R an be the reals with its natur...
622|A zero-one law for logic with a fixed-point operator |The logic obtained by adding the least-fixed-point operator to first-order logic was proposed as a query language by Aho and Ullman (in &#034;Proc. 6th ACM Sympos. on Principles of Programming Languages, &#034; 1979, pp. 110-120) and has been studied, particularly in connection with finite models, by numerous authors. We extend to this logic, and to the logic containing the more powerful iterative-fixedpoint operator, the zero-one law proved for first-order logic in (Glebskii, Kogan, Liogonki, and Talanov (1969), Kibernetika 2, 31-42; Fagin (1976), J. Symbolic Logic 41, 50-58). For any sentence q ~ of the extended logic, the proportion of models of q ~ among all structures with universe {1, 2,..., n} approaches 0 or 1 as n tends to infinity. We also show that the problem of deciding, for any cp, whether this proportion approaches 1 is complete for exponential time, if we consider only q)&#039;s with a fixed finite vocabulary (or vocabularies of bounded arity) and complete for double-exponential time if ~0 is unrestricted. In addition, we establish some related results. © 1985 Academic Press, Inc.
623|A universal algorithm for sequential data compression|A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.
624|Sequential data assimilation with a nonlinear quasi-geostrophic model using Monte Carlo methods to forecast error statistics|. A new sequential data assimilation method is discussed. It is based on forecasting the error statistics using Monte Carlo methods, a better alternative than solving the traditional and computationally extremely demanding approximate error covariance equation used in the extended Kalman filter. The unbounded error growth found in the extended Kalman filter, which is caused by an overly simplified closure in the error covariance equation, is completely eliminated. Open boundaries can be handled as long as the ocean model is well posed. Well-known numerical instabilities associated with the error covariance equation are avoided because storage and evolution of the error covariance matrix itself are not needed. The results are also better than what is provided by the extended Kalman filter since there is no closure problem and the quality of the forecast error statistics therefore improves. The method should be feasible also for more sophisticated primitive equation models. The computati...
625|Using the Extended Kalman Filter with a Multilayer Quasi-Geostrophic Ocean Model|this paper the extended Kalman filter is used with a nonlinear multilayer quasi-geostrophic (QG) model. This provides us with both a realistic ocean model and a very sophisticated error statistics scheme. The extended Kalman filter is an extension of the common Kalman filter and may be used when the model dynamics or the measurement equation is nonlinear. It consists of an approximative equation for the propagation of error covariances, and also approximative filter equations if the measurement equation is nonlinear. When changing from a linear system to nonlinear dynamics the possible existence of a wide variety of phenomena which are nonexistent in the linear theory is introduced. Nonlinear systems may have solutions with multiple equilibria, where the solutions sometimes abruptly undergo transitions from one equilibrium to another as parameters change (bifurcations). Also chaotic behavior occurs in many deterministic systems, where solutions exhibit an apparently random behavior. The Lorenz [1963] model is probably the best known example of chaotic systems. It has solutions which undergo &#034;unpredictable&#034; transitions between two different equilibria (chaos). As discussed by Miller and Ghil
626|Simplification of the Kalman Filter for Meteorological Data Assimilation|We propose a new statistical data assimilation method that is based on a simplification of the Kalman filter equations. The forecast error covariance evolution is approximated by simply advecting the mass error covariance field, by deriving the remaining covariances geostrophically, and by accounting for external model error forcing only at the end of each forecast cycle. This greatly reduces the cost of the forecast error covariance computation, which is the central and most expensive aspect of the Kalman filter algorithm. In simulations with a linear, one-dimensional shallow-water model and artificially generated data, the performance of the simplified filter is compared with that of the Kalman filter and the optimal interpolation (OI) method. These experiments are designed to isolate the effect of simplifying the forecast error covariance evolution. The simplified filter produces analyses that are nearly optimal, and represents a significant improvement over OI. ae  1 Introduction ...
627|Open Boundary Conditions for the Extended Kalman Filter With a Quasi-Geostrophic Ocean Model|this paper the work in Part I is extended to include open boundaries with inflow and outflow. The use of inflow boundaries with the QG model severely complicates the numerical treatment, but it is also of vital importance if mesoscale circulation is to be studied, using an extended Kalman filter to assimilate data in the QG model. Open and closed boundaries have quite different properties and are normally treated differently in a way that leads to a well-posed problem. It should be remembered that an open boundary with inflow or outflow is an artificial boundary. No knowledge is therefore available about how an open boundary shall be updated unless external data or information can be used. The general boundary conditions for the QG model have been discussed in several publications where Charney et al.
628|Active Learning with Statistical Models|For manytypes of learners one can compute the statistically &#034;optimal&#034; way to select data. We review how  these techniques have been used with feedforward neural networks [MacKay, 1992# Cohn, 1994]. We then  showhow the same principles may be used to select data for two alternative, statistically-based learning  architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural  networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted  regression are both efficient and accurate.
629|Learning Bayesian networks: The combination of knowledge and statistical data|We describe scoring metrics for learning Bayesian networks from a combination of user knowledge and statistical data. We identify two important properties of metrics, which we call event equivalence and parameter modularity. These properties have been mostly ignored, but when combined, greatly simplify the encoding of a user’s prior knowledge. In particular, a user can express his knowledge—for the most part—as a single prior Bayesian network for the domain. 1
630|Improving generalization with active learning|Abstract. Active learning differs from &amp;quot;learning from examples &amp;quot; in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples. In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers &amp;quot;useful. &amp;quot; We test our implementation, called an SGnetwork, on three domains and observe significant improvement in generalization.
631|Information-Based Objective Functions for Active Data Selection|Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed which measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness. 1 Introduction  Theories for data modelling often assume that the data is provided by a source that we do not control. However, there are two scenarios in which we are able to actively select training data. In the first, data measurements are relatively expensive or slow, and we want to know where to look next so as to learn as much as possible. According to Jaynes (1986), Bayesian reasoning was first applied to this problem two centuries ago by Laplace, who in consequence made more important discoveries...
632|Supervised learning from incomplete data via an EM approach|Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data sets. We use mixture models for the density estimates and make two distinct appeals to the ExpectationMaximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm---EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark---the iris data set---are presented. 1 Introduction  Adaptive systems generally operate in environments that are fraught with imperfections; nonetheless they must cope with these imperfections and learn to extract as much relevant information as needed for their particular goals. One form of imperfection is incompleteness in sensing information. Inc...
633|Neural network exploration using optimal experiment design|We consider the question &#034;How should one act when the only goal is to learn as much as possible?&#034; Building  on the theoretical results of Fedorov [1972] and MacKay [1992], we apply techniques from Optimal  Experiment Design (OED) to guide the query/action selection of a neural network learner. We demonstrate  that these techniques allow the learner to minimize its generalization error by exploring its domain  efficiently and completely.We conclude that, while not a panacea, OED-based query/action has muchto  offer, especially in domains where its high computational costs can be tolerated.
634|Active Exploration in Dynamic Environments|Whenever an agent learns to control an unknown environment, two opposing principles have to be combined, namely: exploration (long-term optimization) and exploitation (short-term optimization). Many real-valued connectionist approaches to learning control realize exploration by randomness in action selection. This might be disadvantageous when costs are assigned to &#034;negative experiences&#034;. The basic idea presented in this paper is to make an agent explore unknown regions in a more directed manner. This is achieved by a so-called competence map, which is trained to predict the controller&#039;s accuracy, and is used for guiding exploration. Based on this, a bistable system enables smoothly switching attention between two behaviors -- exploration and exploitation -- depending on expected costs and knowledge gain. The appropriateness of this method is demonstrated by a simple robot navigation task.   
635|Reinforcement Driven Information Acquisition In Non-Deterministic Environments|For an agent living in a non-deterministic Markov environment (NME), what is, in theory, the fastest way of acquiring information about its statistical properties? The answer is: to design &#034;optimal&#034; sequences of &#034;experiments&#034; by performing action sequences that maximize expected information gain. This notion is implemented by combining concepts from information theory and reinforcement learning. Experiments show that the resulting method, reinforcement driven information acquisition, can explore certain NMEs much faster than conventional random exploration.  
636|Estimation of probabilities from sparse data for the language model component of a speech recognizer|Abstract-The description of a novel type of rn-gram language model is given. The model offers, via a nonlinear recursive procedure, a com-putation and space efficient solution to the problem of estimating prob-abilities from sparse data. This solution compares favorably to other proposed methods. While the method has been developed for and suc-cessfully implemented in the IBM Real Time Speech Recognizers, its generality makes it applicable in other areas where the problem of es-timating probabilities from sparse data arises. Sparseness of data is an inherent property of any real text, and it is a problem that one always encounters while collecting fre-quency statistics on words and word sequences (m-grams) from a text of finite size. This means that even for a very large data col-lection, the maximum likelihood estimation method does not allow Turing’s estimate PT for a probability of a word (m-gram) which occurred in the sample r times is r* PT = where r We call a procedure of replacing a count r with a modified count r ’ “discounting ” and a ratio rt/r a discount coefficient dr. When r ’  = r*, we have Turing’s discounting. Let us denote the m-gram wl,  *.., w, as wy and the number of times it occurred in the sample text as c(wT). Then the maxi-mum likelihood estimate is
637|An empirical Bayes approach to statistics|Let X be a random variable which for simplicity we shall assume to have discrete values x and which has a probability distribution depending in a known way on an un-known real parameter A, (1) p (xIX) =Pr [X = xIA =X], A-itself being a random variable with a priori distribution function (2) G (X) =Pr [A-&lt; X. The unconditional probability distribution of X is then given by
638|MapReduce: Simplified Data Processing on Large Clusters|MapReduce is a programming model and an associated implementation for processing and generating large data sets. Users specify a map function that processes a key/value pair to generate a set of intermediate key/value pairs, and a reduce function that merges all intermediate values associated with the same intermediate key. Many real world tasks are expressible in this model, as shown in the paper. Programs written in this functional style are automatically parallelized and executed on a large cluster of commodity machines. The run-time system takes care of the details of partitioning the input data, scheduling the program’s execution across a set of machines, handling machine failures, and managing the required inter-machine communication. This allows programmers without any experience with parallel and distributed systems to easily utilize the resources of a large distributed system. Our implementation of MapReduce runs on a large cluster of commodity machines and is highly scalable: a typical MapReduce computation processes many terabytes of data on thousands of machines. Programmers find the system easy to use: hundreds of MapReduce programs have been implemented and upwards of one thousand MapReduce jobs are executed on Google’s clusters every day.  
639|Efficient dispersal of information for security, load balancing, and fault tolerance|Abstract. An Information Dispersal Algorithm (IDA) is developed that breaks a file F of length L =  ( F ( into n pieces F,, 1 5 i 5 n, each of length ( F, 1 = L/m, so that every m pieces suffice for reconstructing F. Dispersal and reconstruction are computationally efficient. The sum of the lengths ( F, 1 is (n/m). L. Since n/m can be chosen to be close to I, the IDA is space eflicient. IDA has numerous applications to secure and reliable storage of information in computer networks and even on single disks, to fault-tolerant and efficient transmission of information in networks, and to communi-cations between processors in parallel computers. For the latter problem provably time-efftcient and highly fault-tolerant routing on the n-cube is achieved, using just constant size buffers. Categories and Subject Descriptors: E.4 [Coding and Information Theory]: nonsecret encoding schemes
640|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
641|Cluster-Based Scalable Network Services|This paper has benefited from the detailed and perceptive comments of our reviewers, especially our shepherd Hank Levy. We thank Randy Katz and Eric Anderson for their detailed readings of early drafts of this paper, and David Culler for his ideas on TACC&#039;s potential as a model for cluster programming. Ken Lutz and Eric Fraser configured and administered the test network on which the TranSend scaling experiments were performed. Cliff Frost of the UC Berkeley Data Communications and Networks Services group allowed us to collect traces on the Berkeley dialup IP network and has worked with us to deploy and promote TranSend within Berkeley. Undergraduate researchers Anthony Polito, Benjamin Ling, and Andrew Huang implemented various parts of TranSend&#039;s user profile database and user interface. Ian Goldberg and David Wagner helped us debug TranSend, especially through their implementation of the rewebber
642|Parallel Prefix Computation|ABSTRACT The prefix problem is to compute all the products x t o x2.... o xk for i ~ k. ~ n, where o is an associative operation A recurstve construction IS used to obtain a product circuit for solving the prefix problem which has depth exactly [log:n] and size bounded by 4n An application yields fast, small Boolean ctrcmts to simulate fimte-state transducers. By simulating a sequentml adder, a Boolean clrcmt which has depth 2[Iog2n] + 2 and size bounded by 14n Is obtained for n-bit binary addmon The size can be decreased significantly by permitting the depth to increase by an addmve constant
643|Cluster I/O with River: Making the Fast Case Common|We introduce River, a data-flow programming environment and I/O substrate for clusters of computers. River is designed to provide maximum performance in the common case --- even in the face of nonuniformities in hardware, software, and workload. River is based on two simple design features: a high-performance distributed queue, and a storage redundancy mechanism called graduated declustering. We have implemented a number of data-intensive applications on River, which validate our design with near-ideal performance in a variety of non-uniform performance scenarios. 
644|Diamond: A storage architecture for early discard in interactive search|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
645|Explicit Control in a Batch-Aware Distributed File System |We present the design, implementation, and evaluation of the Batch-Aware Distributed File System (BAD-FS), a system designed to orchestrate large, I/O-intensive batch workloads on remote computing clusters distributed across the wide area. BAD-FS consists of two novel components: a storage layer which exposes control of traditionally fixed policies such as caching, consistency, and replication; and a scheduler that exploits this control as needed for different users and workloads. By extracting these controls from the storage layer and placing them in an external scheduler, BAD-FS manages both storage and computation in a coordinated way while gracefully dealing with cache consistency, fault-tolerance, and space management issues in an application-specific manner. Using both microbenchmarks and real applications, we demonstrate the performance benefits of explicit control, delivering excellent end-to-end performance across the wide-area.  
646|Bigtable: A distributed storage system for structured data|Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers. Many projects at Google store data in Bigtable, including web indexing, Google Earth, and Google Finance. These applications place very different demands on Bigtable, both in terms of data size (from URLs to web pages to satellite imagery) and latency requirements (from backend bulk processing to real-time data serving). Despite these varied demands, Bigtable has successfully provided a flexible, high-performance solution for all of these Google products. In this paper we describe the simple data model provided by Bigtable, which gives clients dynamic control over data layout and format, and we describe the design and implementation of Bigtable.  
647|Space/Time Trade-offs in Hash Coding with Allowable Errors|this paper trade-offs among certain computational factors in hash coding are analyzed. The paradigm problem considered is that of testing a series of messages one-by-one for membership in a given set of messages. Two new hash- coding methods are examined and compared with a particular conventional hash-coding method. The computational factors considered are the size of the hash area (space), the time required to identify a message as a nonmember of the given set (reject time), and an allowable error frequency
648|The Google File System |We have designed and implemented the Google File Sys-tem, a scalable distributed file system for large distributed data-intensive applications. It provides fault tolerance while running on inexpensive commodity hardware, and it delivers high aggregate performance to a large number of clients. While sharing many of the same goals as previous dis-tributed file systems, our design has been driven by obser-vations of our application workloads and technological envi-ronment, both current and anticipated, that reflect a marked departure from some earlier file system assumptions. This has led us to reexamine traditional choices and explore rad-ically different design points. The file system has successfully met our storage needs. It is widely deployed within Google as the storage platform for the generation and processing of data used by our ser-vice as well as research and development efforts that require large data sets. The largest cluster to date provides hun-dreds of terabytes of storage across thousands of disks on over a thousand machines, and it is concurrently accessed by hundreds of clients. In this paper, we present file system interface extensions designed to support distributed applications, discuss many aspects of our design, and report measurements from both micro-benchmarks and real world use. Categories and Subject Descriptors D [4]: 3—Distributed file systems
649|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
650|Resource Containers: A New Facility for Resource Management in Server Systems|General-purpose operating systems provide inadequate support for resource management in large-scale servers. Applications lack sufficient control over scheduling and management of machine resources, which makes it difficult to enforce priority policies, and to provide robust and controlled service. There is a fundamental mismatch between the original design assumptions underlying the resource management mechanisms of current general-purpose operating systems, and the behavior of modern server applications. In particular, the operating system’s notions of protection domain and resource principal coincide in the process abstraction. This coincidence prevents a process that manages large numbers of network connections, for example, from properly allocating system resources among those connections. We propose and evaluate a new operating system abstraction called a resource container, which separates the notion of a protection domain from that of a resource principal. Resource containers enable fine-grained resource management in server systems and allow the development of robust servers, with simple and firm control over priority policies. 1
651|Operating System Support for Planetary-Scale Network Services|PlanetLab is a geographically distributed overlay network designed to support the deployment and evaluation of planetary-scale network services. Two high-level goals shape its design. First, to enable a large research community to share the infrastructure, PlanetLab provides distributed virtualization, whereby each service runs in an isolated slice of PlanetLab’s global resources. Second, to support competition among multiple network services, PlanetLab decouples the operating system running on each node from the networkwide services that define PlanetLab, a principle referred to as unbundled management. This paper describes how Planet-Lab realizes the goals of distributed virtualization and unbundled management, with a focus on the OS running on each node. 1
652|Paxos made live: an engineering perspective|We describe our experience building a fault-tolerant data-base using the Paxos consensus algorithm. Despite the existing literature in the field, building such a database proved to be non-trivial. We describe selected algorithmic and engineering problems encountered, and the solutions we found for them. Our measurements indicate that we have built a competitive system. 1
653|Integrating compression and execution in column-oriented database systems|Column-oriented database system architectures invite a reevaluation of how and when data in databases is compressed. Storing data in a column-oriented fashion greatly increases the similarity of adjacent records on disk and thus opportunities for compression. The ability to compress many adjacent tuples at once lowers the per-tuple cost of compression, both in terms of CPU and space overheads. In this paper, we discuss how we extended C-Store (a column-oriented DBMS) with a compression sub-system. We show how compression schemes not traditionally used in roworiented DBMSs can be applied to column-oriented systems. We then evaluate a set of compression schemes and show that the best scheme depends not only on the properties of the data but also on the nature of the query workload.
654|Weaving Relations for Cache Performance|Relational database systems have traditionally optimzed for I/O performance and organized records sequentially on disk pages using the N-ary Storage Model (NSM) (a.k.a., slotted pages). Recent research, however, indicates that cache utilization and performance is becoming increasingly important on modern platforms. In this paper, we first demonstrate that in-page data placement is the key to high cache performance and that NSM exhibits low cache utilization on modern platforms. Next, we propose a new data organization model called PAX (Partition Attributes Across), that significantly improves cache performance by grouping together all values of each attribute within each page. Because PAX only affects layout inside the pages, it incurs no storage penalty and does not affect I/O behavior. According to our experimental results, when compared to NSM (a) PAX exhibits superior cache and memory bandwidth utilization, saving at least 75% of NSM&#039;s stall time due to data cache accesses, (b) range selection queries and updates on memoryresident relations execute 17-25% faster, and (c) TPC-H queries involving I/O execute 11-48% faster.
655|Data Placement in Bubba|Thus paper examines the problem of data placement in Bubba, a highly-parallel system for data-intensive applications bemg developed at MCC “Highly-parallel” lmplres that load balancmng IS a cntlcal performance issue “Data-mtenave” means data IS so large that operatrons should be executed where the data resides As a result, data placement becomes a cntlcal performance issue In general, determmmg the optimal placement of data across processmg nodes for performance IS a difficult problem We describe our heuristic approach to solvmg the data placement problem w Bubba We then present expenmental results using a specific workload to provide msrght into the problem Several researchers have argued the benefits of deelustering (1 e, spreading each base relation over many nodes) We show that as declustermg IS increased. load balancing contmues to improve However, for transactions mvolvmg complex Joins, further declusterrng reduces throughput because of communications, startup and termmatron overhead We argue that data placement, especially declustermg, m a highly-parallel system must be considered early in the design, so that mechanrsms can be included for supportmg variable declustermg, for mmtmlzmg the most significant overheads associated with large-scale declustenng, and for gathering the required statistics.
656|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
657|Using Bayesian networks to analyze expression data|DNA hybridization arrays simultaneously measure the expression level for thousands of genes. These measurements provide a “snapshot ” of transcription levels within the cell. A major challenge in computational biology is to uncover, from such measurements, gene/protein interactions and key biological features of cellular systems. In this paper, we propose a new framework for discovering interactions between genes based on multiple expression measurements. This framework builds on the use of Bayesian networks for representing statistical dependencies. A Bayesian network is a graph-based model of joint multivariate probability distributions that captures properties of conditional independence between variables. Such models are attractive for their ability to describe complex stochastic processes and because they provide a clear methodology for learning from (noisy) observations. We start by showing how Bayesian networks can describe interactions between genes. We then describe a method for recovering gene interactions from microarray data using tools for learning Bayesian networks. Finally, we demonstrate this method on the S. cerevisiae cell-cycle measurements of Spellman et al. (1998). Key words: gene expression, microarrays, Bayesian methods. 1.
658|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
659|A Bayesian method for the induction of probabilistic networks from data|This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems.
661|Bayesian Network Classifiers|Recent work in supervised learning has shown that a surprisingly simple Bayesian classifier with strong assumptions of independence among features, called naive Bayes, is competitive with state-of-the-art classifiers such as C4.5. This fact raises the question of whether a classifier with less restrictive assumptions can perform even better. In this paper we evaluate approaches for inducing classifiers from data, based on the theory of learning Bayesian networks. These networks are factored representations of probability distributions that generalize the naive Bayesian classifier and explicitly represent statements about independence. Among these approaches we single out a method we call Tree Augmented Naive Bayes (TAN), which outperforms naive Bayes, yet at the same time maintains the computational simplicity (no search involved) and robustness that characterize naive Bayes. We experimentally tested these approaches, using problems from the University of California at Irvine repository, and compared them to C4.5, naive Bayes, and wrapper methods for feature selection.
662|Clustering Gene Expression Patterns|Recent advances in biotechnology allow researchers to measure expression levels for thousands of genes simultaneously, across different conditions and over time. Analysis of data produced by such experiments offers potential insight into gene function and regulatory mechanisms. A key step in the analysis of gene expression data is the detection of groups of genes that manifest similar expression patterns. The corresponding algorithmic problem is to cluster multi-condition gene expression patterns. In this paper we describe a novel clustering algorithm that was developed for analysis of gene expression data. We define an appropriate stochastic error model on the input, and prove that under the conditions of the model, the algorithm recovers the cluster structure with high probability. The running time of the algorithm on an n-gene dataset is O(n 2 (log(n)) c ). We also present a practical heuristic based on the same algorithmic ideas. The heuristic was implemented and its p...
664|Being Bayesian about network structure|Abstract. In many multivariate domains, we are interested in analyzing the dependency structure of the underlying distribution, e.g., whether two variables are in direct interaction. We can represent dependency structures using Bayesian network models. To analyze a given data set, Bayesian model selection attempts to find the most likely (MAP) model, and uses its structure to answer these questions. However, when the amount of available data is modest, there might be many models that have non-negligible posterior. Thus, we want compute the Bayesian posterior of a feature, i.e., the total posterior probability of all models that contain it. In this paper, we propose a new approach for this task. We first show how to efficiently compute a sum over the exponential number of networks that are consistent with a fixed order over network variables. This allows us to compute, for a given order, both the marginal probability of the data and the posterior of a feature. We then use this result as the basis for an algorithm that approximates the Bayesian posterior of a feature. Our approach uses a Markov Chain Monte Carlo (MCMC) method, but over orders rather than over network structures. The space of orders is smaller and more regular than the space of structures, and has much a smoother posterior “landscape”. We present empirical results on synthetic and real-life datasets that compare our approach to full model averaging (when possible), to MCMC over network structures, and to a non-Bayesian bootstrap approach.
665|Learning the structure of dynamic probabilistic networks|Dynamic probabilistic networks are a compact representation of complex stochastic processes. In this paper we examine how to learn the structure of a DPN from data. We extend structure scoring rules for standard probabilistic networks to the dynamic case, and show how to search for structure when some of the variables are hidden. Finally, we examine two applications where such a technology might be useful: predicting and classifying dynamic behaviors, and learning causal orderings in biological processes. We provide empirical results that demonstrate the applicability of our methods in both domains. 1
666|A Theory Of Inferred Causation|This paper concerns the empirical basis of causation, and addresses the following issues: 1. the clues that might prompt people to perceive causal relationships in uncontrolled observations. 2. the task of inferring causal models from these clues, and 3. whether the models inferred tell us anything useful about the causal mechanisms that underly the observations. We propose a minimal-model semantics of causation, and show that, contrary to common folklore, genuine causal influences can be distinguished from spurious covariations following standard norms of inductive reasoning. We also establish a sound characterization of the conditions under which such a distinction is possible. We provide an effective algorithm for inferred causation and show that, for a large class of data the algorithm can uncover the direction of causal influences as defined above. Finally, we address the issue of non-temporal causation.  
667|Theory Refinement on Bayesian Networks|Theory refinement is the task of updating a domain theory in the light of new cases, to be done automatically or with some expert assistance. The problem of theory refinement under uncertainty is reviewed here in the context of Bayesian statistics, a theory of belief revision. The problem is reduced to an incremental learning task as follows: the learning system is initially primed with a partial theory supplied by a domain expert, and thereafter maintains its own internal representation of alternative theories which is able to be interrogated by the domain expert and able to be incrementally refined from data. Algorithms for refinement of Bayesian networks are presented to illustrate what is meant by &#034;partial theory&#034;, &#034;alternative theory representation &#034;, etc. The algorithms are an incremental variant of batch learning algorithms from the literature so can work well in batch and incremental mode. 1 Introduction Theory refinement is the task of updating a domain theory in the light of...
668|Minimum complexity density estimation|The minimum complexity or minimum description-length criterion developed by Kolmogorov, Rissanen, Wallace, So&amp;in, and others leads to consistent probability density estimators. These density estimators are defined to achieve the best compromise between likelihood and simplicity. A related issue is the compromise between accuracy of approximations and complexity relative to the sample size. An index of resolvability is studied which is shown to bound the statistical accuracy of the density estimators, as well as the information-theoretic redundancy.  
669|Learning Bayesian network structure from massive datasets: the “sparse candidate” algorithm|Learning Bayesian networks is often cast as an optimization problem, where the computational task is to find a structure that maximizes a sta-tistically motivated score. By and large, existing learning tools address this optimization problem using standard heuristic search techniques. Since the search space is extremely large, such search procedures can spend most of the time examining candidates that are extremely unreasonable. This problem becomes critical when we deal with data sets that are large either in the number of in-stances, or the number of attributes. In this paper, we introduce an algorithm that achieves faster learning by restricting the search space. This iterative algorithm restricts the par-ents of each variable to belong to a small sub-set of candidates. We then search for a network that satisfies these constraints. The learned net-work is then used for selecting better candidates for the next iteration. We evaluate this algorithm both on synthetic and real-life data. Our results show that it is significantly faster than alternative search procedures without loss of quality in the learned structures. 1
670|Modeling Regulatory Networks With Weight Matrices|Systematic gene expression analyses provide comprehensive information about the  transcriptional response to different environmental and developmental conditions. With  enough gene expression data points, computational biologists may eventually generate  predictive computer models of transcription regulation. Such models will require  computational methodologies consistent with the behavior of known biological systems that  remain tractable. We represent regulatory relationships between genes as linear coefficients or  weights, with the &#034;net&#034; regulation influence on a gene&#039;s expression being the mathematical  summation of the independent regulatory inputs. Test regulatory networks generated with this  approach display stable and cyclically stable gene expression levels, consistent with known  biological systems. We include variables to model the effect of environmental conditions on  transcription regulation and observed various alterations in gene expression patterns in  response to e...
671|On the Logic of Causal Models|This paper explores the role of Directed Acyclic Graphs (DAGs) as a representation of conditional independence relationships. We show that DAGs offer polynomially sound and complete inference mechanisms for inferring conditional independence relationships from a given causal set of such relationships. As a consequence, d-separation, a graphical criterion for identifying independencies in a DAG, is shown to uncover more valid independencies then any other criterion. In addition, we employ the Armstrong property of conditional independence to show that the dependence relationships displayed by a DAG are inherently consistent, i.e. for every DAG D there exists some probability distribution P that embodies all the conditional independencies displayed in D and none other.  
672|The max-min hill-climbing bayesian network structure learning algorithm|Abstract. We present a new algorithm for Bayesian network structure learning, called Max-Min Hill-Climbing (MMHC). The algorithm combines ideas from local learning, constraint-based, and search-and-score techniques in a principled and effective way. It first reconstructs the skeleton of a Bayesian network and then performs a Bayesian-scoring greedy hill-climbing search to orient the edges. In our extensive empirical evaluation MMHC outperforms on average and in terms of various metrics several prototypical and state-of-the-art algorithms, namely the PC, Sparse Candidate, Three Phase Dependency Analysis, Optimal Reinsertion, Greedy Equivalence Search, and Greedy Search. These are the first empirical results simultaneously comparing most of the major Bayesian network algorithms against each other. MMHC offers certain theoretical advantages, specifically over the Sparse Candidate algorithm, corroborated by our experiments. MMHC and detailed results of our study are publicly available at
673|A Transformational Characterization of Equivalent Bayesian Network Structures|We present a simple characterization  of equivalentBayesian network structures  based on local transformations.  The significance of the characterization  is twofold. First, we are able  to easily proveseveral new invariant  properties of theoretical interest  for equivalent structures. Second,  we use the characterization to derive  an efficient algorithm that identifies  all of the compelled edges in a  structure. Compelled edge identification  is of particular importance for  learning Bayesian network structures  from data because these edges indicate  causal relationships when certain  assumptions hold.  1 
674|A Bayesian Approach to Causal Discovery|We examine the Bayesian approach to the discovery of directed acyclic causal models and compare it to the constraint-based approach. Both approaches rely on the Causal Markov assumption, but the two differ significantly in theory and practice. An important difference between the approaches is that the constraint-based approach uses categorical information about conditional-independence constraints in the domain, whereas the Bayesian approach weighs the degree to which such constraints hold. As a result, the Bayesian approach has three distinct advantages over its constraint-based counterpart. One, conclusions derived from the Bayesian approach are not susceptible to incorrect categorical decisions about independence facts that can occur with data sets of finite size. Two, using the Bayesian approach, finer distinctions among model structures---both quantitative and qualitative---can be made. Three, information from several models can be combined to make better inferences and to better ...
675|Causal discovery from a mixture of experimental and observational data|This paper describes a Bayesian method for combining an arbitrary mixture of observational and experimental data in order to learn causal Bayesian networks. Observational data are passively observed. Experimental data, such as that produced by randomized controlled trials, result from the experimenter manipulating one or more variables (typically randomly) and observing the states of other variables. The paper presents a Bayesian method for learning the causal structure and parameters of the underlying causal process that is generating the data, given that (1) the data contains a mixture of observational and experimental case records, and (2) the causal process is modeled as a causal Bayesian network. This learning method was applied using as input various mixtures of experimental and observational data that were generated from the ALARM causal Bayesian network. In these experiments, the absolute and relative quantities of experimental and observational data were varied systematically. For each of these training datasets, the learning method was applied to predict the causal structure and to estimate the causal parameters that exist among randomly selected pairs of nodes in ALARM that are not confounded. The paper reports how these structure predictions and parameter estimates compare with the true causal structures and parameters as given by the ALARM network. 1
677|Data Analysis with Bayesian Networks: A Bootstrap Approach|In recent years there has been significant  progress in algorithms and methods for inducing  Bayesian networks from data. However, in complex  data analysis problems, we need to go beyond  being satisfied with inducing networks with  high scores. We need to provide confidence measures  on features of these networks: Is the existence  of an edge between two nodes warranted?  Is the Markov blanket of a given node robust?  Can we say something about the ordering of the  variables? We should be able to address these  questions, even when the amount of data is not  enough to induce a high scoring network. In this  paper we propose Efron&#039;s Bootstrap as a computationally  efficient approach for answering these  questions. In addition, we propose to use these  confidence measures to induce better structures  from the data, and to detect the presence of latent  variables. 
678|Identifying Gene Regulatory Networks from Experimental Data|this paper, we propose a methodology for making sense of large, multiple time-series data sets arising in expression analysis, and evaluate it both theoretically and through a case study. First, we build a graph representing all putative activation/inhibition relationships by analyzing the expression profiles for all pairs of genes. Second, we prune this graph by solving a combinatorial optimization problem to identify a small set of interesting candidate regulatory elements. We do not assert that we identify &#034;the&#034; regulatory network as a result of this computation. However, we believe that our approach quickly enables biologists to identify and visualize interesting features from raw expression array data sets. We have implemented our methodology and applied it to the analysis of the Saccharomyces cerevisiae data set. In this paper, we report on our implementation and the results of our data analysis. The problem of inducing gene regulation networks has recently come to the computational biology community. Initial attempts at modeling gene expression abd programs to induce regulatory networks from data include [2, 10, 13]. To take fullest advantage of laboratory experiments that can be performed in which a given set of genes can be explicitly expressed or repressed, and the consequences of these genes on expression biologically determined, Akutsu, et.al. [1] considers the problem of designing a minimum-size series of experiments guaranteed to result in the identification of the correct regulatory network. The candidate regulatory network proposed by our system depends upon the specific optimization criteria employed in the second phase of our procedure, although our experiments suggest that the optimal network is surprisingly robust to changes in the objective function...
679|On the Sample Complexity of Learning Bayesian Networks|In recent years there has been an increasing interest in learning Bayesian networks from data. One of the most effective methods for learning such networks is based on the minimum description length (MDL) principle. Previous work has shown that this learning procedure is asymptotically successful: with probability one, it will converge to the target distribution, given a sufficient number of samples. However, the rate of this convergence has been hitherto unknown. In this work we examine the sample complexity of MDL based learning procedures for Bayesian networks. We show that the number of samples needed to learn an ffl-close approximation (in terms of entropy distance) with confidence ffi is O  i  (  1  ffl )  4 3  log  1  ffl log  1  ffi log log  1  ffi  j  . This means that the sample complexity is a low-order polynomial in the error threshold and sublinear in the confidence bound. We also discuss how the constants in this term depend on the complexity of the target distribution. F...
680|Learning Bayesian Networks: A unification for discrete and Gaussian domains|We examine Bayesian methods for learning Bayesian networks from a combination of prior knowledge and statistical data. In particular, we unify the approaches we presented at last year&#039;s conference for discrete and Gaussian domains. We derive a general Bayesian scoring metric, appropriate for both domains. We then use this metric in combination with well-known statistical facts about the Dirichlet and normal{Wishart distributions to derive our metrics for discrete and Gaussian domains.  
681|Causal Inference in the Presence of Latent Variables and Selection Bias|This paper uses Bayesian network models for that investigation. Bayesian networks, or directed acyclic graph (DAG) models have proved very useful in representing both causal and statistical hypotheses. The nodes of the graph represent vertices, directed edges represent direct influences, and the topology of the graph encodes statistical constraints. We will consider features of such models that can be determined from data under assumptions that are related to those routinely applied in experimental situations:
682|Learning Mixtures of Bayesian Networks|We describe a heuristic method for learning mixtures of Bayesian Networks (MBNs) from possibly incomplete data. The considered class of models is mixtures in which each mixture component is a Bayesian network encoding a conditional Gaussian distribution over a fixed set of variables. Some variables may be hidden or otherwise have missing observations. A key idea in our approach is to treat expected data as real data. This allows us to interleave structure and parameter search and to take advantage of closed form approximations for marginal likelihood. In addition, by treating expected data as real data, the search criterion factors by variable, making the search processes more efficient. We evaluate our approach on synthetic and real-world data sets.  Keywords : Mixture models, Bayesian networks, structure learning, parameter learning, hidden variables, EM algorithm. 1 Introduction  There is growing interest in a class of models for density estimation known as Bayesian networks. In the...
684|Modeling and Analysis of Heterogeneous Regulation in Biological Networks|In this study we propose a novel model for the representation of biological networks and provide algorithms for learning model parameters from experimental data. Our approach is to build an initial model based on extant biological knowledge, and refine it to increase the consistency between model predictions and experimental data. Our model encompasses networks which contain heterogeneous biological entities (mRNA, proteins, metabolites) and aims to capture diverse regulatory circuitry on several levels (metabolism, transcription, translation, post-translation and feedback loops among them).
685|Latent classification models|Abstract. One of the simplest, and yet most consistently well-performing set of classifiers is the Naïve Bayes models. These models rely on two assumptions: (i) All the attributes used to describe an instance are conditionally independent given the class of that instance, and (ii) all attributes follow a specific parametric family of distributions. In this paper we propose a new set of models for classification in continuous domains, termed latent classification models. The latent classification model can roughly be seen as combining the Naïve Bayes model with a mixture of factor analyzers, thereby relaxing the assumptions of the Naïve Bayes classifier. In the proposed model the continuous attributes are described by a mixture of multivariate Gaussians, where the conditional dependencies among the attributes are encoded using latent variables. We present algorithms for learning both the parameters and the structure of a latent classification model, and we demonstrate empirically that the accuracy of the proposed model is significantly higher than the accuracy of other probabilistic classifiers. Keywords: classification, probabilistic graphical models, Naïve Bayes, correlation
687|Multi-level Modeling and Inference of Transcription Regulation |The understanding of transcription regulation is a major goal of today&#039;s biology. The challenge is to utilize  diverse high-throughput data in order to infer mechanistic models of transcription control. We propose a new  model which integrates transcription factor-gene anity, protein abundance and gene expression pro  les. The  model provides detailed, yet computationally tractable description of the relations between transcription factors,  their binding sites at genes promoters and the combinatorial regulation of transcription. At the core, our model  manipulates dose-anity-response functions that associate transcription factor concentrations and transcription  factor-DNA anities to determine the rate of transcription factor-DNA reactions. We study computational  problems that arise in optimizing such models and develop polynomial algorithms for certain problems. We  show how to assess missing values (notably protein abundance) and describe a novel framework to infer models  from currently available data. On budding yeast carbohydrate metabolism data, our results demonstrate the  sensitivity and speci  city of the approach. They also suggest new active binding sites and a regulation model for  the transcription program of the galactose system.
688|A rapid hierarchical radiosity algorithm|This paper presents a rapid hierarchical radiosity algorithm for illuminating scenes containing lar e polygonal patches. The afgorithm constructs a hierarchic“J representation of the form factor matrix by adaptively subdividing patches into su bpatches according to a user-supplied error bound. The algorithm guarantees that all form factors are calculated to the same precision, removing many common image artifacts due to inaccurate form factors. More importantly, the al o-rithm decomposes the form factor matrix into at most O? n) blocks (where n is the number of elements). Previous radiosity algorithms represented the element-to-element transport interactions with n2 form factors. Visibility algorithms are given that work well with this approach. Standard techniques for shooting and gathering can be used with the hierarchical representation to solve for equilibrium radiosities, but we also discuss using a brightness-weighted error criteria, in conjunction with multigrldding, to even more rapidly progressively refine the image.
689|An efficient program for many-body simulation|Abstract. The simulation of N particles interacting in a gravitational force field is useful in astrophysics, but such simulations become costly for large N. Representing the universe as a tree structure with the particles at the leaves and internal nodes labeled with the centers of mass of their descendants allows several simultaneous attacks on the computation time required by the problem. These approaches range from algorithmic changes (replacing an O(N’) algorithm with an algorithm whose time-complexity is believed to be O(N log N)) to data structure modifications, code-tuning, and hardware modifications. The changes reduced the running time of a large problem (N 10,000) by a factor of four hundred. This paper describes both the particular program and the methodology underlying such speedups. 1. Introduction. Isaac Newton calculated the behavior of two particles interacting through the force of gravity, but he was unable to solve the equations for three particles. In this he was not alone [7, p. 634], and systems of three or more particles can be solved only numerically. Iterative methods are usually used, computing at each discrete time interval the force on each particle, and then computing the new velocities and
690|Link-Sharing and Resource Management Models for Packet Networks| This paper discusses the use of link-sharing mechanisms in packet networks and presents algorithms for hierarchical link-sharing. Hierarchical link-sharing allows multiple agencies, protocol families, or traflic types to share the bandwidth on a tink in a controlled fashion. Link-sharing and real-time services both require resource management mechanisms at the gateway. Rather than requiring a gateway to implement separate mechanisms for link-sharing and real-time services, the approach in this paper is to view link-sharing and real-time service requirements as simultaneous, and in some respect complementary, constraints at a gateway that can be implemented with a unified set of mechanisms. White it is not possible to completely predict the requirements that might evolve in the Internet over the next decade, we argue that controlled link-sharing is an essential component that can provide gateways with the flexibility to
691|Random Early Detection Gateways for Congestion Avoidance|This paper presents Random Early Detection (RED) gate-ways for congestion avoidance in packet-switched networks. The gateway detects incipient congestion by com-puting the average queue size. The gateway could notify connections of congestion either by dropping packets ar-riving at the gateway or by setting a bit in packet headers. When the average queue size exceeds a preset threshold,the gateway drops or marks each arriving packet with a certain probability, where the exact probability is a func-tion of the average queue size. RED gateways keep the average queue size low while allowing occasional bursts of packets in the queue. During congestion, the probability that the gateway notifies a particular connection to reduce its window is roughly proportional to that connection&#039;s share of the bandwidth throughthe gateway. RED gateways are designed to accompany a transport-layer congestion control protocol such as TCP.The RED gateway has no bias against bursty traffic and avoids the global synchronization of many connectionsdecreasing their window at the same time. Simulations of a TCP/IP network are used to illustrate the performance of RED gateways. 
693|Analysis, Modeling and Generation of Self-Similar VBR Video Traffic|We present a detailed statistical analysis of a 2-hour long empirical sample of VBR video. The sample was obtained by applying a simple intraframe video compression code to an action movie. The main findings of our analysis are (1) the tail behavior of the marginal bandwidth distribution can be accurately described using &#034;heavy-tailed&#034; distributions (e.g., Pareto); (2) the autocorrelation of the VBR video sequence decays hyperbolically (equivalent to long-range dependence) and can be modeled using self-similar processes. We combine our findings in a new (non-Markovian) source model for VBR video and present an algorithm for generating synthetic traffic. Trace-driven simulations show that statistical multiplexing results in significant bandwidth efficiency even when long-range dependence is present. Simulations of our source model show long-range dependence and heavy-tailed marginals to be important components which are not accounted for in currently used VBR video traffic models. 1 I...
694|Scalable Feedback Control for Multicast Video Distribution in the Internet|We describe a mechanism for scalable control of multicast continuous media streams. The mechanism uses a novel probing mechanism to solicit feedback information in a scalable manner and to estimate the number of receivers. In addition, it separates the congestion signal from the congestion control algorithm, so as to cope with heterogeneous networks. This mechanism has been implemented in the IVS videoconference system using options within RTP to elicit information about the quality of the video delivered to the receivers. The H.261 coder of IVS then uses this information to adjust its output rate, the goal being to maximize the perceptual quality of the image received at the destinations while minimizing the bandwidth used by the video transmission. We find that our prototype control mechanism is well suited to the Internet environment. Furthermore, it prevents video sources from creating congestion in the Internet. Experiments are underway to investigate how the scalable probing mech...
695|Connections with Multiple Congested Gateways in Packet-Switched Networks Part 1: One-way Traffic|In this paper we explore the bias in TCP/IP networks against connections with multiple congested gateways. We consider the interaction between the bias against connections with multiple congested gateways, the bias of the TCP window modification algorithm against connections with longer roundtrip times, and the bias of Drop Tail and Random Drop gateways against bursty traffic. Using simulations and a heuristic analysis, we show that in a network with the window modification algorithm in 4.3 tahoe BSD TCP and with Random Drop or Drop Tail gateways, a longer connection with multiple congested gateways can receive unacceptably low throughput. We show that in a network with no bias against connections with longer roundtrip times and with no bias against bursty traffic, a connection with multiple congested gateways can receive an acceptable level of throughput. We discuss the application of several current measures of fairness to networks with multiple congested gateways, and show that diff...
696|Network Support For Multimedia: A Discussion of the Tenet Approach|Multimedia communication can be supported in an integrated-services network in the general framework of realtime communication. The Tenet Group has devised an approach that provides some initial solutions to the realtime communication problem. This paper attempts to identify the principles behind these solutions. We also describe a suite of protocols, and their implementations in several environments, that embody these principles, and work in progress that will lead towards more complete solutions.   This research was supported by the National Science Foundation and the Defense Advanced Research Projects Agency (DARPA) under CooperativeAgreement NCR-8919038 with the Corporation for National Research Initiatives, by AT&amp;T Bell Laboratories, Digital Equipment Corporation, Hitachi, Ltd., Hitachi America, Ltd., Pacific Bell, the University of California under a MICRO grant, and the International Computer Science Institute. The views and conclusions contained in this document are those of th...
697|A Scheduling Service Model and a Scheduling Architecture for an Integrated Services Packet Network|Integrated Services Packet Networks (ISPN) are designed to integrate the network service requirements of a wide variety of computer-based applications. Some of these services are delivered primarily through the packet scheduling algorithms used in the network switches. This paper addresses two questions related to these scheduling algorithms. The first question is: what scheduling services should an ISPN offer? In answer, we propose a scheduling service model for ISPN&#039;s which is based on our projections about future application and institutional service requirements. Our service model includes both a delay-related component designed to meet the ergonomic requirements of individual applications, and also a hierarchical link-sharing component designed to meet the economic needs of resource sharing between different entities. The second question we address is: what implications does this service model have for the packet scheduling algorithms? We answer this question by construc...
698|Implementing Real Time Packet Forwarding Policies using Streams|This paper describes an implementation of the class based queueing (CBQ) mechanisms proposed by Sally Floyd and Van Jacobson [1] [2] to provide real time policies for packet forwarding. CBQ allows the traffic flows sharing a data link to be guaranteed a share of the bandwidth when the link is congested, yet allows flexible sharing of the unused bandwidth when the link is unloaded. In addition, CBQ provides mechanisms which give flows requiring low delay priority over other flows. In this way, links can be shared by multiple flows yet still meet the policy and Quality of Service (QoS) requirements of the flows.  We present a brief description of the implementation and some preliminary performance measurements. The problems of packet classification are addressed in a flexible and extensible, yet efficient manner, and whilst the Streams implementation cannot cope with very high speed interfaces, it can cope with the serial link speeds that are likely to be loaded. 
699|Learning probabilistic relational models|A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with &amp;quot;flat &amp;quot; data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning — the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases. 1
700|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
701|Loopy Belief Propagation for Approximate Inference: An Empirical Study|Recently, researchers have demonstrated that  &#034;loopy belief propagation&#034; --- the use of  Pearl&#039;s polytree algorithm in a Bayesian  network with loops --- can perform well  in the context of error-correcting codes.  The most dramatic instance of this is the  near Shannon-limit performance of &#034;Turbo  Codes&#034; --- codes whose decoding algorithm  is equivalent to loopy belief propagation in a  chain-structured Bayesian network.  In this paper we ask: is there something special  about the error-correcting code context,  or does loopy propagation work as an approximate  inference scheme in a more general  setting? We compare the marginals computed  using loopy propagation to the exact  ones in four Bayesian network architectures,  including two real-world networks: ALARM  and QMR. We find that the loopy beliefs often  converge and when they do, they give a  good approximation to the correct marginals.  However, on the QMR network, the loopy beliefs  oscillated and had no obvious relationship  ...
702|Learning to Extract Symbolic Knowledge from the World Wide Web|The World Wide Web is a vast source of information  accessible to computers, but understandable  only to humans. The goal of the research described  here is to automatically create a computer  understandable world wide knowledge base whose  content mirrors that of the World Wide Web. Such a 
703|Context-Specific Independence in Bayesian Networks|Bayesiannetworks provide a languagefor qualitatively  representing the conditional independence properties  of a distribution. This allows a natural and compact  representation of the distribution, eases knowledge acquisition,  and supports effective inference algorithms.
704|Correctness of Local Probability Propagation in Graphical Models with Loops|This article analyzes the behavior of local propagation rules in graphical models with a loop.
705|Object-Oriented Bayesian Networks|Bayesian networks provide a modeling language and associated inference algorithm for stochastic domains. They have been successfully applied in a variety of medium-scale applications. However, when faced with a large complex domain, the task of modeling using Bayesian networks begins to resemble the task of programming using logical circuits. In this paper, we describe an object-oriented Bayesian network (OOBN) language, which allows complex domains to be described in terms of inter-related objects. We use a Bayesian network fragment to describe the probabilistic relations between the attributes of an object. These attributes can themselves be objects, providing a natural framework for encoding part-of hierarchies. Classes are used to provide a reusable probabilistic model which can be applied to multiple similar objects. Classes also support inheritance of model fragments from a class to a subclass, allowing the common aspects of related classes to be defined only once. Our language h...
706|Probabilistic Frame-Based Systems|Two of the most important threads of work in knowledge representation today are frame-based representation systems (FRS&#039;s) and Bayesian networks (BNs). FRS&#039;s provide an excellent representation for the organizational structure of large complex domains, but their applicability is limited because of their inability to deal with uncertainty and noise. BNs provide an intuitive and coherent probabilistic representation of our uncertainty, but are very limited in their ability to handle complex structured domains. In this paper, we provide a language that cleanly integrates these approaches, preserving the advantages of both. Our approach allows us to provide natural and compact definitions of probability models for a class, in a way that is local to the class frame. These models can be instantiated for any set of interconnected instances, resulting in a coherent probability distribution over the instance properties. Our language also allows us to represent important types of uncertainty tha...
707|A Bayesian approach to learning Bayesian networks with local structure|Recently several researchers have investigated techniques for using data to learn Bayesian networks containing compact representations for the conditional probability distributions (CPDs) stored at each node. The majority of this work has concentrated on using decision-tree representations for the CPDs. In addition, researchers typically apply non-Bayesian (or asymptotically Bayesian) scoring functions such as MDL to evaluate the goodness-of-fit of networks to the data. In this paper we investigate a Bayesian approach to learning Bayesian networks that contain the more general decision-graph representations of the CPDs. First, we describe how to evaluate the posterior probability— that is, the Bayesian score—of such a network, given a database of observed cases. Second, we describe various search spaces that can be used, in conjunction with a scoring function and a search procedure, to identify one or more high-scoring networks. Finally, we present an experimental evaluation of the search spaces, using a greedy algorithm and a Bayesian scoring function. 1
708|Learning Belief Networks in the Presence of Missing Values and Hidden Variables|In recent years there has been a flurry of works on learning probabilistic belief networks. Current state of the art methods have been shown to be successful for two learning scenarios: learning both network structure and parameters from  complete data, and learning parameters for a fixed network from incomplete data---that is, in the presence of missing values or hidden variables. However, no method has yet been demonstrated to effectively learn network structure from incomplete data. In this paper, we propose a new method for learning network structure from incomplete data. This method is based on an extension of the  Expectation-Maximization (EM) algorithm for model selection problems that performs search for the best structure inside the EM procedure. We prove the convergence of this algorithm, and adapt it for learning belief networks. We then describe how to learn networks in two scenarios: when the data contains missing values, and in the presence of hidden variables. We provide...
709|Unsupervised Learning from Dyadic Data|Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one element from either set. This includes event co-occurrences, histogram data, and single stimulus preference data as special cases. Dyadic data arises naturally in many applications ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent framework for unsupervised learning from dyadic data by statistical mixture models. Our approach covers different models with flat and hierarchical latent class structures and unifies probabilistic modeling and structure discovery. Mixture models provide both, a parsimonious yet flexible parameterization of probability distributions with good generalization performance on sparse data, as well as structural information about data-inherent grouping structure. We propose an annealed version of the standard Expectation Maximization algorithm for model fitting which is empirically evaluated on a variety of data sets from different domains.
710|Answering Queries from Context-Sensitive Probabilistic Knowledge Bases|We define a language for representing context-sensitive probabilistic knowledge. A knowledge base consists of a set of universally quantified probability sentences that include context constraints, which allow inference to be focused on only the relevant portions of the probabilistic knowledge. We provide a declarative semantics for our language. We present a query answering procedure which takes a query Q and a set of evidence E and constructs a Bayesian network to compute P (QjE). The posterior probability is then computed using any of a number of Bayesian network inference algorithms. We use the declarative semantics to prove the query procedure sound and complete. We use concepts from logic programming to justify our approach. Keywords: reasoning under uncertainty, Bayesian networks, Probability model construction, logic programming Submitted to Theoretical Computer Science special issue on Uncertainty in Databases and Deductive Systems. This work was partially supported by NSF g...
711|Probabilistic Reasoning for Complex Systems|ii
712|Learning Statistical Models from Relational Data|This workshop is the second in a series of workshops held in conjunction with AAAI and IJCAI. The first workshop was held in July, 2000 at AAAI. Notes from that workshop are available at
713|Learning Probabilities for Noisy First-Order Rules|First-order logic is the traditional basis for knowledge representation languages. However, its applicability to many real-world tasks is limited by its inability to represent uncertainty. Bayesian belief networks, on the other hand, are inadequate for complex KR tasks due to the limited expressivity of the underlying (propositional) language. The need to incorporate uncertainty into an expressive language has led to a resurgence of work on first-order probabilistic logic. This paper addresses one of the main objections to the incorporation of probabilities into the language: &#034;Where do the numbers come from?&#034; We present an approach that takes a knowledge base in an expressive rule-based first-order language, and learns the probabilistic parameters associated with those rules from data cases. Our approach, which is based on algorithms for learning in traditional Bayesian networks, can handle data cases where many of the relevant aspects of the situation are unobserved. It is also capabl...
714|Prospective assessment of ai technologies for fraud detection: A case study|In September 1995, the Congressional O ce of Technology Assessment completed a study of the potential for AI technologies to detect money laundering by screening wire transfers. The study, conducted at the request of the Senate Permanent Subcommittee on Investigations, evaluates the technical and public policy implications of widespread use of AI technologies by the Federal government for fraud detection. Its conclusions are relevant tomanyother uses of AI technologies for fraud detection in both the public and private sectors.
715|PRL: A probabilistic relational language|In this paper, we describe the syntax and semantics for a probabilistic relational language (PRL). PRL is a recasting of recent work in Probabilistic Relational Models (PRMs) into a logic programming framework. We show how to represent varying degrees of complexity in the semantics including attribute uncertainty, structural uncertainty and identity uncertainty. Our approach is similar in spirit to the work in Bayesian Logic Programs (BLPs), and Logical Bayesian Networks (LBNs). However, surprisingly, there are still some important differences in the resulting formalism; for example, we introduce a general notion of aggregates based on the PRM approaches. One of our contributions is that we show how to support richer forms of structural uncertainty in a probabilistic logical language than have been previously described. Our goal in this work is to present a unifying framework that supports all of the types of relational uncertainty yet is based on logic programming formalisms. We also believe that it facilitates understanding the relationship between the frame-based approaches and alternate logic programming approaches, and allows greater
716|Indexing by latent semantic analysis|A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents (“semantic structure”) in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca. 100 or-thogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca. 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are re-turned. initial tests find this completely automatic method for retrieval to be promising.
717|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
718|Probabilistic Latent Semantic Indexing|Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain-specific synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing (LSI) by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and defines a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methodsaswell as over LSI. In particular, the combination of models with different dimensionalities has proven to be advantageous.  
719|Text Classification from Labeled and Unlabeled Documents using EM|  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve ...
720|Learning in graphical models|Statistical applications in fields such as bioinformatics, information retrieval, speech processing, image processing and communications often involve large-scale models in which thousands or millions of random variables are linked in complex ways. Graphical models provide a general methodology for approaching these problems, and indeed many of the models developed by researchers in these applied fields are instances of the general graphical model formalism. We review some of the basic ideas underlying graphical models, including the algorithmic ideas that allow graphical models to be deployed in large-scale data analysis problems. We also present examples of graphical models in bioinformatics, error-control coding and language processing. 
721|Using Maximum Entropy for Text Classification|This paper proposes the use of maximum entropy techniques for text classification. Maximum entropy is a probability distribution estimation technique widely used for a variety of natural language tasks, such as language modeling, part-of-speech tagging, and text segmentation. The underlying principle of maximum entropy is that without external knowledge, one should prefer distributions that are uniform. Constraints on the distribution, derived from labeled training data, inform the technique where to be minimally non-uniform. The maximum entropy formulation has a unique solution which can be found by the improved iterative scaling algorithm. In this paper, maximum entropy is used for text classification by estimating the conditional distribution of the class variable given the document. In experiments on several text datasets we compare accuracy to naive Bayes and show that maximum entropy is sometimes significantly better, but also sometimes worse. Much future work remains, but the re...
722|Latent semantic indexing: A probabilistic analysis| Latent semantic indexing (LSI) is an information retrieval technique based on the spectral analysis of the term-document matrix, whose empirical success had heretofore been without rigorous prediction and explanation. We prove that, under certain conditions, LSI does succeed in capturing the underlying semantics of the corpus and achieves improved retrieval performance. We also propose the technique of random projection as a way of speeding up LSI. We complement our theorems with encouraging experimental results. We also argue that our results may be viewed in a more general framework, as a theoretical basis for the use of spectral methods in a wider class of applications such as collaborative filtering.
723|Estimating a Dirichlet distribution|The Dirichlet distribution and its compound variant, the Dirichlet-multinomial, are two of the most basic models for proportional data, such as the mix of vocabulary words in a text document. Yet the maximum-likelihood estimate of these distributions is not available in closed-form. This paper describes simple and efficient iterative schemes for obtaining parameter estimates in these models. In each case, a fixed-point iteration and a Newton-Raphson (or generalized Newton-Raphson) iteration is provided. 1 The Dirichlet distribution The Dirichlet distribution is a model of how proportions vary. Let p denote a random vector whose elements sum to 1, so that pk represents the proportion of item k. Under the Dirichlet model with parameter vector a, the probability density at p is p(p)  ~ D(a1,...,aK)  = G(?k ak) k G(ak)
724|Probabilistic models for unified collaborative and content-based recommendation in sparsedata environments|Recommender systems leverage product and community information to target products to consumers. Researchers have developed collaborative recommenders, content-based recommenders, and a few hybrid systems. We propose a unified probabilistic framework for merging collaborative and content-based recommendations. We extend Hofmann’s (1999) aspect model to incorporate three-way co-occurrence data among users, items, and item content. The relative influence of collaboration data versus content data is not imposed as an exogenous parameter, but rather emerges naturally from the given data sources. However, global probabilistic models coupled with standard EM learning algorithms tend to drastically overfit in the sparsedata situations typical of recommendation applications. We show that secondary content information can often be used to overcome sparsity. Experiments on data from the ResearchIndex library of Computer Science publications show that appropriate mixture models incorporating secondary data produce significantly better quality recommenders than-nearest neighbors (-NN). Global probabilistic models also allow more general inferences than local methods like-NN. 1
725|An experimental comparison of several clustering and intialization methods|We examine methods for clustering in high dimensions. In the first part of the paper, we perform an experimental comparison between three batch clustering algorithms: the Expectation–Maximization (EM) algorithm, a “winner take all ” version of the EM algorithm reminiscent of the K-means algorithm, and model-based hierarchical agglomerative clustering. We learn naive-Bayes models with a hidden root node, using high-dimensional discrete-variable data sets (both real and synthetic). We find that the EM algorithm significantly outperforms the other methods, and proceed to investigate the effect of various initialization schemes on the final solution produced by the EM algorithm. The initializations that we consider are (1) parameters sampled from an uninformative prior, (2) random perturbations of the marginal distribution of the data, and (3) the output of hierarchical agglomerative clustering. Although the methods are substantially different, they lead to learned models that are strikingly similar in quality. 1
726|Improving Multi-class Text Classification with Naive Bayes|There are numerous text documents available in electronic form. More and more are becoming available every day. Such documents represent a massive amount of information that is easily accessible. Seeking value in this huge collection requires organization; much of the work of organizing documents can be automated through text classification. The accuracy and our understanding of such systems greatly influences their usefulness. In this paper, we seek 1) to advance the understanding of commonly used text classification techniques, and 2) through that understanding, improve the tools that are available for text classification. We begin by clarifying the assumptions made in the derivation of Naive Bayes, noting basic properties and proposing ways for its extension and improvement. Next, we investigate the quality of Naive Bayes parameter estimates and their impact on classification. Our analysis leads to a theorem which gives an explanation for the improvements that can be found in multiclass classification with Naive Bayes using Error-Correcting Output Codes. We use experimental evidence on two commonly-used data sets to exhibit an application of the theorem. Finally, we show fundamental flaws in a commonly-used feature selection algorithm and develop a statistics-based framework for text feature selection. Greater understanding of Naive Bayes and the properties of text allows us to make better use of it in text classification.
727|A Probabilistic Approach to Semantic Representation|Semantic networks produced from human data have statistical properties that cannot be easily captured by spatial representations. We explore a probabilistic approach to semantic representation that explicitly models the probability with which words occur in different contexts, and hence captures the probabilistic relationships between words. We show that this representation has statistical properties consistent with the large-scale structure of semantic networks constructed by humans, and trace the origins of these properties.
728|Toward a model of text comprehension and production|The semantic structure of texts can be described both at the local microlevel and at a more global macrolevel. A model for text comprehension based on this notion accounts for the formation of a coherent semantic text base in terms of a cyclical process constrained by limitations of working memory. Furthermore, the model includes macro-operators, whose purpose is to reduce the information in a text base to its gist, that is, the theoretical macrostructure. These opera-tions are under the control of a schema, which is a theoretical formulation of the comprehender&#039;s goals. The macroprocesses are predictable only when the control schema can be made explicit. On the production side, the model is con-cerned with the generation of recall and summarization protocols. This process is partly reproductive and partly constructive, involving the inverse operation of the macro-operators. The model is applied to a paragraph from a psycho-logical research report, and methods for the empirical testing of the model are developed. The main goal of this article is to describe the system of mental operations that underlie the processes occurring in text comprehension and in the production of recall and summariza-tion protocols. A processing model will be outlined that specifies three sets of operations. First, the meaning elements of a text become
729|Context and cognition: Knowledge frames and speech act comprehension |This paper is about the cognitive foundations of pragmatic theories. Besides the fact that the usual appropriateness conditions for speech acts, which are given in cognitive terms, such as S knows/believes/wants... (that) p, require empirical investigation, a sound theory of prag-matics must also explain how certain utterances in certain contexts are actually understood as certain speech acts. Speech act comprehension is based on rules and strategies for so-called context analysis, in which (epistemic) frames play an important role in the analysis of social context, social frames, and interaction type. Results of context analysis are then matched with those of pragmatic sentence analysis, viz. the illocutionary act indicating devices. Finally, some results from the cognitive analysis of discourse processing are applied in a brief account of the comprehension of speech act sequences and macro-speech acts. 1. The foundations of pragmatics The philosophical and linguistic study of pragmatics requires an analysis of its foundations. This basis of pragmatic theories is on the one hand conceptual, e.g. in the analysis of action and interaction, and on the other hand empirical, viz. in the investigation of the psychological and social properties of language processing in
730|Convex Analysis|In this book we aim to present, in a unified framework, a broad spectrum of mathematical theory that has grown in connection with the study of problems of optimization, equilibrium, control, and stability of linear and nonlinear systems. The title Variational Analysis reflects this breadth. For a long time, ‘variational ’ problems have been identified mostly with the ‘calculus of variations’. In that venerable subject, built around the minimization of integral functionals, constraints were relatively simple and much of the focus was on infinite-dimensional function spaces. A major theme was the exploration of variations around a point, within the bounds imposed by the constraints, in order to help characterize solutions and portray them in terms of ‘variational principles’. Notions of perturbation, approximation and even generalized differentiability were extensively investigated. Variational theory progressed also to the study of so-called stationary points, critical points, and other indications of singularity that a point might have relative to its neighbors, especially in association with existence theorems for differential equations.
731|Factor Graphs and the Sum-Product Algorithm|A factor graph is a bipartite graph that expresses how a &#034;global&#034; function of many variables factors into a product of &#034;local&#034; functions. Factor graphs subsume many other graphical models including Bayesian networks, Markov random fields, and Tanner graphs. Following one simple computational rule, the sum-product algorithm operates in factor graphs to compute---either exactly or approximately---various marginal functions by distributed message-passing in the graph. A wide variety of algorithms developed in artificial intelligence, signal processing, and digital communications can be derived as specific instances of the sum-product algorithm, including the forward/backward algorithm, the Viterbi algorithm, the iterative &#034;turbo&#034; decoding algorithm, Pearl&#039;s belief propagation algorithm for Bayesian networks, the Kalman filter, and certain fast Fourier transform algorithms. 
732|Monte Carlo Statistical Methods|This paper is also the originator of the Markov Chain Monte Carlo methods developed in the following chapters. The potential of these two simultaneous innovations has been discovered much latter by statisticians (Hastings 1970; Geman and Geman 1984) than by of physicists (see also Kirkpatrick et al. 1983).  5.5.5 ] PROBLEMS 211
733|Low-Density Parity-Check Codes|Preface The Noisy Channel Coding Theorem discovered by C. E. Shannon in 1948 offered communication engineers the possibility of reducing error rates on noisy channels to negligible levels without sacrificing data rates. The primary obstacle to the practical use of this theorem has been the equipment complexity and the computation time required to decode the noisy received data.
734|Graphical models, exponential families, and variational inference|The formalism of probabilistic graphical models provides a unifying framework for capturing complex dependencies among random variables, and building large-scale multivariate statistical models. Graphical models have become a focus of research in many statistical, computational and mathematical fields, including bioinformatics, communication theory, statistical physics, combinatorial optimization, signal and image processing, information retrieval and statistical machine learning. Many problems that arise in specific instances — including the key problems of computing marginals and modes of probability distributions — are best studied in the general setting. Working with exponential family representations, and exploiting the conjugate duality between the cumulant function and the entropy for exponential families, we develop general variational representations of the problems of computing likelihoods, marginal probabilities and most probable configurations. We describe how a wide varietyof algorithms — among them sum-product, cluster variational methods, expectation-propagation, mean field methods, max-product and linear programming relaxation, as well as conic programming relaxations — can all be understood in terms of exact or approximate forms of these variational representations. The variational approach provides a complementary alternative to Markov chain Monte Carlo as a general source of approximation methods for inference in large-scale statistical models.
736|Design of capacity-approaching irregular low-density parity-check codes|We design low-density parity-check (LDPC) codes that perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on [1]. Assuming that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph possess a certain symmetry. Using this symmetry property we then show that, under the assumption of no cycles, the message densities always converge as the number of iterations tends to infinity. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief-propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present some simulation results for the codes found which show that the performance of the codes is very close to the asymptotic theoretical bounds. 
737|Complexity of finding embeddings in a k-tree|  A k-tree is a graph that can be reduced to the k-complete graph by a sequence of removals of a degree k vertex with completely connected neighbors. We address the problem of determining whether a graph is a partial graph of a k-tree. This problem is motivated by the existence of polynomial time algorithms for many combinatorial problems on graphs when the graph is constrained to be a partial k-tree for fixed k. These algorithms have practical applications in areas such as reliability, concurrent broadcasting and evaluation of queries in a relational database system. We determine the complexity status of two problems related to finding the smallest number k such that a given graph is a partial k-tree. First, the corresponding decision problem is NP-complete. Second, for a fixed (predetermined) value of k, we present an algorithm with polynomially bounded (but exponential in k) worst case time complexity. Previously, this problem had only been solved for k = 1,2,3.
738|A family of algorithms for approximate Bayesian inference|One of the major obstacles to using Bayesian methods for pattern recognition has been its computational expense. This thesis presents an approximation technique that can perform Bayesian inference faster and more accurately than previously possible. This method, &#034;Expectation Propagation,&#034; unifies and generalizes two previous techniques: assumeddensity filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. The unification shows how both of these algorithms can be viewed as approximating the true posterior distribution with a simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation exploits the best of both algorithms: the generality of assumed-density filtering and the accuracy of loopy belief propagation. Loopy belief propagation, because it propagates exact belief states, is useful for limited types of belief networks, such as purely discrete networks. Expectation Propagati...
739|The generalized distributive law |Abstract—In this semitutorial paper we discuss a general message passing algorithm, which we call the generalized dis-tributive law (GDL). The GDL is a synthesis of the work of many authors in the information theory, digital communications, signal processing, statistics, and artificial intelligence communities. It includes as special cases the Baum–Welch algorithm, the fast Fourier transform (FFT) on any finite Abelian group, the Gal-lager–Tanner–Wiberg decoding algorithm, Viterbi’s algorithm, the BCJR algorithm, Pearl’s “belief propagation ” algorithm, the Shafer–Shenoy probability propagation algorithm, and the turbo decoding algorithm. Although this algorithm is guaranteed to give exact answers only in certain cases (the “junction tree ” condition), unfortunately not including the cases of GTW with cycles or turbo decoding, there is much experimental evidence, and a few theorems, suggesting that it often works approximately even when it is not supposed to. Index Terms—Belief propagation, distributive law, graphical models, junction trees, turbo codes. I.
740|The Power of Amnesia: Learning Probabilistic Automata with Variable Memory Length|. We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Suffix Automata (PSA). Though hardness results are known for learning distributions generated by general probabilistic automata, we prove that the algorithm we present can efficiently learn distributions generated by PSAs. In particular, we show that for any target PSA, the KL-divergence between the distribution generated by the target and the distribution generated by the hypothesis the learning algorithm outputs, can be made small with high confidence in polynomial time and sample complexity. The learning algorithm is motivated by applications in human-machine interaction. Here we present two applications of the algorithm. In the first one we apply the algorithm in order to construct a model of the English language, and use this model to correct corrupted text. In the second ...
741|Axioms for probability and belief-function propagation|In this paper, we describe an abstract framework and axioms under which exact local computation of marginals is possible. The primitive objects of the framework are variables and valuations. The primitive operators of the framework are combination and marginalization. These operate on valuations. We state three axioms for these operators and we derive the possibility of local computation from the axioms. Next, we describe a propagation scheme for computing marginals of a valuation when we have a factorization of the valuation on a hypertree. Finally we show how the problem of computing marginals of joint probability distributions and joint belief functions fits the general framework. 1.
742|Propagation Algorithms for Variational Bayesian Learning|Variational approximations are becoming a widespread tool for Bayesian learning of graphical models. We provide some theoretical results for the variational updates in a very general family of conjugate-exponential graphical models. We show how the belief propagation and the junction tree algorithms can be used in the inference step of variational Bayesian learning. Applying these results to the Bayesian analysis of linear-Gaussian state-space models we obtain a learning procedure that exploits the Kalman smoothing propagation, while integrating over all model parameters. We demonstrate how this can be used to infer the hidden state dimensionality of the state-space model in a variety of synthetic problems and one real high-dimensional data set.
743|Linear Time Inference in Hierarchical HMMs|The hierarchical hidden Markov model (HHMM) is a generalization of  the hidden Markov model (HMM) that models sequences with structure  at many length/time scales [FST98]. Unfortunately, the original inference  algorithm is rather complicated, and takes O(T    ) time, where T is  the length of the sequence, making it impractical for many domains. In  this paper, we show how HHMMs are a special kind of dynamic Bayesian  network (DBN), and thereby derive a much simpler inference algorithm,  which only takes O(T ) time. Furthermore, by drawing the connection  between HHMMs and DBNs, we enable the application of many standard  approximation techniques to further speed up inference.
744|Loopy Belief Propagation and Gibbs Measures|We address the question of convergence in the  loopy belief propagation (LBP) algorithm.
745|Graphical models and automatic speech recognition|Graphical models provide a promising paradigm to study both existing and novel techniques for automatic speech recognition. This paper first provides a brief overview of graphical models and their uses as statistical models. It is then shown that the statistical assumptions behind many pattern recognition techniques commonly used as part of a speech recognition system can be described by a graph – this includes Gaussian distributions, mixture models, decision trees, factor analysis, principle component analysis, linear discriminant analysis, and hidden Markov models. Moreover, this paper shows that many advanced models for speech recognition and language processing can also be simply described by a graph, including many at the acoustic-, pronunciation-, and language-modeling levels. A number of speech recognition techniques born directly out of the graphical-models paradigm are also surveyed. Additionally, this paper includes a novel graphical analysis regarding why derivative (or delta) features improve hidden Markov model-based speech recognition by improving structural discriminability. It also includes an example where a graph can be used to represent language model smoothing constraints. As will be seen, the space of models describable by a graph is quite large. A thorough exploration of this space should yield techniques that ultimately will supersede the hidden Markov model.
746|Mixed memory Markov models: decomposing complex stochastic processes as mixtures of simpler ones|. We study Markov models whose state spaces arise from the Cartesian product of two or more discrete random variables. We show how to parameterize the transition matrices of these models as a convex combination---or mixture---of simpler dynamical models. The parameters in these models admit a simple probabilistic interpretation and can be fitted iteratively by an Expectation-Maximization (EM) procedure. We derive a set of generalized Baum-Welch updates for factorial hidden Markov models that make use of this parameterization. We also describe a simple iterative procedure for approximately computing the statistics of the hidden states. Throughout, we give examples where mixed memory models provide a useful representation of complex stochastic processes. Keywords: Markov models, mixture models, discrete time series 1. Introduction  The modeling of time series is a fundamental problem in machine learning, with widespread applications. These include speech recognition (Rabiner, 1989), natu...
747|Empirical and Hierarchical Bayesian Estimation of Ancestral States| Several methods have been proposed to infer the states at the ancestral nodes on a phylogeny. These methods assume a specific tree and set of branch lengths when estimating the ancestral character state. Inferences of the ancestral states, then, are conditioned on the tree and branch lengths being true. We develop a hierarchical Bayes method for inferring the ancestral states on a tree. The method integrates over uncertainty in the tree, branch lengths, and substitution model parameters by using Markov chain Monte Carlo. We compare the hierarchical Bayes inferences of ancestral states with inferences of ancestral states made under the assumption that a specific tree is correct. We find that the methods are correlated, but that accommodating uncertainty in parameters of the phylogenetic model can make inferences of ancestral states even more uncertain than they would be in an empirical Bayes analysis.
748|Design of Provably Good Low-Density Parity Check Codes|We design sequences of low-density parity check codes that provably perform at rates extremely close to the Shannon capacity. The codes are built from highly irregular bipartite graphs with carefully chosen degree patterns on both sides. Our theoretical analysis of the codes is based on [1]. Additionally, based on the assumption that the underlying communication channel is symmetric, we prove that the probability densities at the message nodes of the graph satisfy a certain symmetry. This enables us to derive a succinct description of the density evolution for the case of a belief propagation decoder. Furthermore, we prove a stability condition which implies an upper bound on the fraction of errors that a belief propagation decoder can correct when applied to a code induced from a bipartite graph with a given degree distribution. Our codes are found by optimizing the degree structure of the underlying graphs. We develop several strategies to perform this optimization. We also present s...
749|Multilocus linkage analysis by blocked Gibbs sampling|The problem of multilocus linkage analysis is expressed as a graphical model, making explicit a previously implicit connection, and recent developments in the field are described in this context. A novel application of blocked Gibbs sampling for Bayesian networks is developed to generate inheritance matrices from an irreducible Markov chain. This is used as the basis for reconstruction of historical meiotic states and approximate calculation of the likelihood function for the location of an unmapped genetic trait. We believe this to be the only approach that currently makes fully informative multilocus linkage analysis possible on large extended pedigrees.
750|Semidefinite Relaxations for Approximate Inference on Graphs With Cycles|We present a new method for calculating approximate marginals for probability distributions  defined by graphs with cycles, based on a Gaussian entropy bound combined with a semidefinite  outer bound on the marginal polytope. This combination leads to a log-determinant maximization  problem that can be solved by efficient interior point methods [13]. As with the Bethe  approximation and its generalizations [18], the optimizing arguments of this problem can be  taken as approximations to the exact marginals. In contrast to Bethe/Kikuchi approaches,  our variational problem is strictly convex and so has a unique global optimum. An additional  desirable feature is that the value of the optimal solution is guaranteed to provide an upper  bound on the log partition function. Such upper bounds are of interest in their own right (e.g.,  for parameter estimation, large deviations exponents, combinatorial enumeration). Finally, we  show that taking the zero-temperature limit of our log-determinant relaxation recovers a class  of well-known semidefinite relaxations for integer programming [e.g., 6].
751|Mixture models in measurement error problems, with reference to epidemiological studies|This paper focuses on Bayesian measurement error problems and on the question of the specication of the prior distribution of the unknown covariates. It presents a exible semiparametric model for this distribution based on a mixture of normals with an unknown number of components. Implementation of this prior model as part of a full Bayesian analysis of measurement error problems is described in classical set-ups encountered in epidemiological studies: logistic regression between unknown covariates and outcome, with normal or lognormal error model and a validation group. The feasibility of this combined model is tested and its performance is demonstrated in a simulation study that includes an assessment of the inuence of misspecication of the prior distribution of the unknown covariates and a comparison with the semiparametric ML method of Roeder, Carroll and Lindsay (1996). Finally, the methodology is illustrated on a data set on coronary heart disease and blood level of cholester
752|General Lower Bounds based on Computer Generated Higher Order Expansions|In this article we show the rough outline of a computer algorithm to generate lower bounds on the exponential function of (in principle) arbitrary precision. We implemented this to generate all necessary analytic terms for the Boltzmann machine partition function thus leading to lower bounds of any order. It turns out that the extra variational parameters can be optimized analytically. We show that bounds upto nineth order are still reasonably calculable in practical situations. The generated terms can also be used as extra correction terms (beyond tap) in mean field expansions.
753|The Coordination of Arm Movements: An Experimentally Confirmed Mathematical Model|This paper presents studies of the coordination of volun-tary human arm movements. A mathematical model is for-mulated which is shown to predict both the qualitative fea-tures and the quantitative details observed experimentally in planar, multijoint arm movements. Coordination is modeled mathematically by defining an objective function, a measure of performance for any possi-ble movement. The unique trajectory which yields the best performance is determined using dynamic optimization the-ory. In the work presented here, the objective function is the square of the magnitude of jerk (rate of change of accelera-tion) of the hand integrated over the entire movement. This is equivalent to assuming that a major goal of motor coordi-nation is the production of the smoothest possible movement
754|Posture control and trajectory formation during arm movement|One hypothesis for the generation of spatially oriented arm movements by the central nervous system is that a desired joint position is determined by the ratio of the tensions of agonist and antagonist muscles. According to this hypothesis, the transition between equilibrium states should be solely a function of the contraction time of the motor units and the mechanical properties of the arm. We tested this hypothesis in intact and deafferented rhesus monkeys by holding the forearm and measuring the accelerative transient after release of the forearm and by directly measuring the time course of the increase in torque during the movement. Both methods indicated an average time of 400 msec for attaining peak torque in a movement with a duration of 700 msec. In addition, by displacing the arm from its normal trajectory during the movement., we observed that the arm returned neither to the initial nor to the final equilibrium positions, but to points intermediate between them. We conclude that the nrocesses underlying traiectorv formation must be more complex than a simple switch between one equilibrium position and another. ”-A major problem in the study of motor control has been elucidation of the mechanisms directing the arm to a new spatial location. A distinction is often made (Brooks et al.,
755|Invariant characteristics of a pointing movement in man|Simple arm movements involving forward projection of the hand toward a target were studied by measuring simultaneous wrist position in three-dimensional space and changes in elbow angle. An attempt was made to identify those features of the movement which exhibit invariant characteristics under the hypothesis that such invariances may reflect the operations by which central processes participate in the organization of the movement. The first such invariance to be identified was that the trajectory in space is independent of movement speed. Secondly, the movement can be viewed as consisting of two phases, an acceleratory phase and a deceleratory one, with the movement during the acceleratory phase being so organized as to maintain the ratio of elbow angular velocity to shoulder angular velocity invariant with respect to target location in the deceleratory phase. It is suggested that proprioceptive information is used to control the movement and that the latter invariance may result from a negative feedback of force involving tendon organ afferents. Learned movements have been characterized by aspects such as the reaction time to their initiation, speed and accuracy and the inter-relationships between these factors and target size and location (cf., Fitts, 1954; Glencross, 1977; Keele, 1968; Prablanc et al., 1979; Woodworth, 1899). By contrast, relatively little work has been done on the manner in which even simple skilled movements are executed in three-dimensional space (Beggs
756|Constructing Free Energy Approximations and Generalized Belief Propagation Algorithms|Important inference problems in statistical physics, computer vision, error-correcting coding theory, and artificial intelligence can all be reformulated as the computation of marginal probabilities on factor graphs. The belief propagation (BP) algorithm is an efficient way to solve these problems that is exact when the factor graph is a tree, but only approximate when the factor graph has cycles. We show that BP fixed points correspond to the stationary points of the Bethe approximation of the free energy for a factor graph. We explain how to obtain regionbased free energy approximations that improve the Bethe approximation, and corresponding generalized belief propagation (GBP) algorithms. We emphasize the conditions a free energy approximation must satisfy in order to be a “valid ” or “maxent-normal ” approximation. We describe the relationship between four different methods that can be used to generate valid approximations: the “Bethe method, ” the “junction graph method, ” the “cluster variation method, ” and the “region graph method.” Finally, we explain how to tell whether a region-based approximation, and its corresponding GBP algorithm, is likely to be accurate, and describe empirical results showing that GBP can significantly outperform BP.
757|Belief Propagation|When a pair of nuclear-powered Russian submarines was reported patrolling off the eastern seaboard of the U.S. last summer, Pentagon officials expressed wariness over the Kremlin’s motivations. At the same time, these officials emphasized their confidence in the U.S. Navy’s tracking capabilities: “We’ve known where they were,” a senior Defense Department official told the New York Times, “and we’re not concerned about our ability to track the subs.” While the official did not divulge the methods used by the Navy to track submarines, the Times added that such
758|Codes and Decoding on General Graphs|Iterative decoding techniques have become a viable alternative for constructing high performance coding systems. In particular, the recent success of turbo codes indicates that performance close to the Shannon limit may be achieved. In this thesis, it is showed that many iterative decoding algorithms are special cases of two generic algorithms, the min-sum and sum-product algorithms, which also include non-iterative algorithms such as Viterbi decoding. The min-sum and sum-product algorithms are developed and presented as generalized trellis algorithms, where the time axis of the trellis is replaced by an arbitrary graph, the &#034;Tanner graph&#034;. With cycle-free Tanner graphs, the resulting decoding algorithms (e.g., Viterbi decoding) are maximum-likelihood but suffer from an exponentially increasing complexity. Iterative decoding occurs when the Tanner graph has cycles (e.g., turbo codes); the resulting algorithms are in general suboptimal, but significant complexity reductions are possible compared to the cycle-free case. Several performance estimates for iterative decoding are developed, including a generalization of the union bound used with Viterbi decoding and a characterization of errors that are uncorrectable after infinitely many decoding iterations.
759|A New Class of Upper Bounds on the Log Partition Function|Bounds on the log partition function are important in a variety of contexts, including approximate inference, model fitting, decision theory, and large deviations analysis [11, 5, 4]. We introduce a new class of upper bounds on the log partition function, based on convex combinations of distributions in the exponential domain, that is applicable to an arbitrary undirected graphical model. In the special case of convex combinations of tree-structured distributions, we obtain a family of variational problems, similar to the Bethe free energy, but distinguished by the following desirable properties: (i) they are convex, and have a unique global minimum; and (ii) the global minimum gives an upper bound on the log partition function. The global minimum is defined by stationary conditions very similar to those defining xed points of belief propagation (BP) or tree-based reparameterization [see 13, 14]. As with BP fixed points, the elements of the minimizing argument can be used as approximations to the marginals of the original model. The analysis described here can be extended to structures of higher treewidth (e.g., hypertrees), thereby making connections with more advanced approximations (e.g., Kikuchi and variants [15, 10]).
761|Multiresolution markov models for signal and image processing|This paper reviews a significant component of the rich field of statistical multiresolution (MR) modeling and processing. These MR methods have found application and permeated the literature of a widely scattered set of disciplines, and one of our principal objectives is to present a single, coherent picture of this framework. A second goal is to describe how this topic fits into the even larger field of MR methods and concepts–in particular making ties to topics such as wavelets and multigrid methods. A third is to provide several alternate viewpoints for this body of work, as the methods and concepts we describe intersect with a number of other fields. The principle focus of our presentation is the class of MR Markov processes defined on pyramidally organized trees. The attractiveness of these models stems from both the very efficient algorithms they admit and their expressive power and broad applicability. We show how a variety of methods and models relate to this framework including models for self-similar and 1/f processes. We also illustrate how these methods have been used in practice. We discuss the construction of MR models on trees and show how questions that arise in this context make contact with wavelets, state space modeling of time series, system and parameter identification, and hidden
762|A Double-Loop Algorithm to Minimize the Bethe and Kikuchi Free Energies|Recent work (Yedidia, Freeman, Weiss [22]) has shown that stable points of belief propagation (BP) algorithms [12] for graphs with loops correspond to extrema of the Bethe free energy [3]. These BP algorithms have been used to obtain good solutions to problems for which alternative algorithms fail to work [4], [5], [10] [11]. In this paper we rst obtain the dual energy of the Bethe free energy which throws light on the BP algorithm. Next we introduce a discrete iterative algorithm which we prove is guaranteed to converge to a minimum of the Bethe free energy. We call this the double-loop algorithm because it contains an inner and an outer loop. It extends a class of mean eld theory algorithms developed by [7],[8] and, in particular, [13]. Moreover, the double-loop algorithm is formally very similar to BP which may help understand when BP converges. Finally, we extend all our results to the Kikuchi approximation which includes the Bethe free energy as a special case [3]. (Yedidia et al [22] showed that a \generalized belief propagation&#034; algorithm also has its xed points at extrema of the Kikuchi free energy). We are able both to obtain a dual formulation for Kikuchi but also obtain a double-loop discrete iterative algorithm that is guaranteed to converge to a minimum of the Kikuchi free energy. It is anticipated that these double-loop algorithms will be useful for solving optimization problems in computer vision and other applications.
763|Linear programming relaxations and belief propagation – an empirical study|The problem of finding the most probable (MAP) configuration in graphical models comes up in a wide range of applications. In a general graphical model this problem is NP hard, but various approximate algorithms have been developed. Linear programming (LP) relaxations are a standard method in computer science for approximating combinatorial problems and have been used for finding the most probable assignment in small graphical models. However, applying this powerful method to real-world problems is extremely challenging due to the large numbers of variables and constraints in the linear program. Tree-Reweighted Belief Propagation is a promising recent algorithm for solving LP relaxations, but little is known about its running time on large problems. In this paper we compare tree-reweighted belief propagation (TRBP) and powerful generalpurpose LP solvers (CPLEX) on relaxations of real-world graphical models from the fields of computer vision and computational biology. We find that TRBP almost always finds the solution significantly faster than all the solvers in CPLEX and more importantly, TRBP can be applied to large scale problems for which the solvers in CPLEX cannot be applied. Using TRBP we can find the MAP configurations in a matter of minutes for a large range of real world problems. 1.
764|MAP Estimation, Linear Programming and Belief Propagation with Convex Free Energies|Finding the most probable assignment (MAP) in a general graphical model is known to be NP hard but good approximations have been attained with max-product belief propagation (BP) and its variants. In particular, it is known that using BP on a single-cycle graph or tree reweighted BP on an arbitrary graph will give the MAP solution if the beliefs have no ties. In this paper we extend the setting under which BP can be used to provably extract the MAP. We define Convex BP as BP algorithms based on a convex free energy approximation and show that this class includes ordinary BP with single-cycle, tree reweighted BP and many other BP variants. We show that when there are no ties, fixed-points of convex max-product BP will provably give the MAP solution. We also show that convex sum-product BP at sufficiently small temperatures can be used to solve linear programs that arise from relaxing the MAP problem. Finally, we derive a novel condition that allows us to derive the MAP solution even if some of the convex BP beliefs have ties. In experiments, we show that our theorems allow us to find the MAP in many real-world instances of graphical models where exact inference using junction-tree is impossible. 
765|Variational Approximations between Mean Field Theory and the Junction Tree Algorithm|Recently, variational approximations such as  the mean field approximation have received  much interest. We extend the standard mean  field method by using an approximating distribution  that factorises into cluster potentials.  This includes undirected graphs, directed  acyclic graphs and junction trees. We  derive generalised mean field equations to optimise  the cluster potentials. We show that  the method bridges the gap between the standard  mean field approximation and the exact  junction tree algorithm. In addition, we address  the problem of how to choose the structure  and the free parameters of the approximating  distribution. From the generalised  mean field equations we derive rules to simplify  the approximation in advance without  affecting the potential accuracy of the model  class. We also show how the method fits into  some other variational approximations that  are currently popular.  1 INTRODUCTION  Graphical models, such as Bayesian networks, Markov fields, and Bolt...
766|Fractional Belief Propagation|We consider approximate inference in probabilistic graphical models with approximate free energy methods. By considering equivalent factor-graph representations of a probabilistic model, we write down a family of different approximate tree-like free energies. We show that this family interpolates between the naive mean-field free energy and the Bethe free energy. We derive fixed-point equations that lead to fractional belief propagation algorithms, which include standard mean-field equations and loopy belief propagation as special cases. Using a cavity-field argument, we compute the fractional algorithm that gives, in lowest order, a correction around the Bethe (loopy belief) approximation for the means. Simulation results illustrate the potential merits of the approach.
767|Minimizing and learning energy functions for side-chain prediction|Side-chain prediction is an important subproblem of the general protein folding problem. Despite much progress in side-chain prediction, performance is far from satisfactory. As an example, the ROSETTA protocol that uses simulated annealing to select the minimum energy conformations, correctly predicts the first two side-chain angles for approximately 72 % of the buried residues in a standard data set. Is further improvement more likely to come from better search methods, or from better energy functions? Given that exact minimization of the energy is NP hard, it is difficult to get a systematic answer to this question. In this paper, we present a novel search method and a novel method for learning energy functions from training data that are both based on Tree Reweighted Belief Propagation (TRBP). We find that TRBP can find the global optimum of the ROSETTA energy function in a few minutes of computation for approximately 85 % of the proteins in a standard benchmark set. TRBP can also effectively bound the partition function which enables using the Conditional Random Fields (CRF) framework for learning. Interestingly, finding the global minimum does not significantly improve side-chain prediction for
768|Log-Determinant Relaxation for Approximate Inference in Discrete Markov Random Fields| Graphical models are well suited to capture the complex and non-Gaussian statistical dependencies that arise in many real-world signals. A fundamental problem common to any signal processing application of a graphical model is that of computing approximate marginal probabilities over subsets of nodes. This paper proposes a novel method, applicable to discrete-valued Markov random fields (MRFs) on arbitrary graphs, for approximately solving this marginalization problem. The foundation of our method is a reformulation of the marginalization problem as the solution of a low-dimensional convex optimization problem over the marginal polytope. Exactly solving this problem for general graphs is intractable; for binary Markov random fields, we describe how to relax it by using a Gaussian bound on the discrete entropy and a semidefinite outer bound on the marginal polytope. This combination leads to a log-determinant maximization problem that can be solved efficiently by interior point methods, thereby providing approximations to the exact marginals. We show how a slightly weakened log-determinant relaxation can be solved even more efficiently by a dual reformulation. When applied to denoising problems in a coupled mixture-of-Gaussian model defined on a binary MRF with cycles, we find that the performance of this log-determinant relaxation is comparable or superior to the widely used sum-product algorithm over a range of experimental conditions. 
769|Structured region graphs: Morphing EP into GBP|GBP and EP are two successful algorithms for approximate probabilistic inference, which are based on different approximation strategies. An open problem in both algorithms has been how to choose an appropriate approximation structure. We introduce “structured region graphs, ” a formalism which marries these two strategies, reveals a deep connection between them, and suggests how to choose good approximation structures. In this formalism, each region has an internal structure which defines an exponential family, whose sufficient statistics must be matched by the parent region. Reduction operators on these structures allow conversion between EP and GBP free energies. Thus it is revealed that all EP approximations on discrete variables are special cases of GBP, and conversely that some wellknown GBP approximations, such as overlapping squares, are special cases of EP. Furthermore, region graphs derived from EP have a number of good structural properties, including maxent-normality and overall counting number of one. The result is a convenient framework for producing high-quality approximations with a user-adjustable level of complexity. 1
770|Treewidth-Based Conditions for Exactness Of The Sherali-Adams and Lasserre    Relaxations|The Sherali-Adams (SA) and Lasserre (LS) approaches are &#034;lift-and-project&#034; methods that  generate nested sequences of linear and/or semidefinite relaxations of an arbitrary 0-1 polytope      . Although both procedures are known to terminate with an exact description of  P after n steps, there are various open questions associated with characterizing, for particular  problem classes, whether exactness is obtained at some step s  n. This paper provides sufficient conditions for exactness of these relaxations based on the hypergraph-theoretic notion of  treewidth. More specifically, we relate the combinatorial structure of a given polynomial system  to an underlying hypergraph. We prove that the complexity of assessing the global validity of  moment sequences, and hence the tightness of the SA and LS relaxations, is determined by the  treewidth of this hypergraph. We provide some examples to illustrate this characterization.
771|Bayesian random fields: The Bethe-Laplace approximation|While learning the maximum likelihood value of parameters of an undirected graphical model is hard, modelling the posterior distribution over parameters given data is harder. Yet, undirected models are ubiquitous in computer vision and text modelling (e.g. conditional random fields). But where Bayesian approaches for directed models have been very successful, a proper Bayesian treatment of undirected models in still in its infant stages. We propose a new method for approximating the posterior of the parameters given data based on the Laplace approximation. This approximation requires the computation of the covariance matrix over features which we compute using the linear response approximation based in turn on loopy belief propagation. We develop the theory for conditional and “unconditional ” random fields with or without hidden variables. In the conditional setting we introduce a new variant of bagging suitable for structured domains. Here we run the loopy max-product algorithm on a “super-graph ” composed of graphs for individual models sampled from the posterior and connected by constraints. Experiments on real world data validate the proposed methods. 1
772|Linear Response for Approximate Inference|Belief propagation on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. In this paper we propose two new algorithms for approximating joint probabilities of arbitrary pairs of nodes and prove a number of desirable properties that these estimates fulfill. The first algorithm is a propagation...
773|Estimating Wealth Effects without Expenditure Data— or Tears|Abstract: We use the National Family Health Survey (NFHS) data collected in Indian states in 1992 and 1993 to estimate the relationship between household wealth and the probability a child (aged 6 to 14) is enrolled in school. A methodological difficulty to overcome is that the NFHS, modeled closely on the Demographic and Health Surveys (DHS), measures neither household income nor consumption expenditures. As a proxy for long-run household wealth we construct a linear index from a set of asset indicators using principal components analysis to derive the weights. This “asset index ” is robust, produces internally coherent results, and provides a close correspondence with State Domestic Product (SDP) and poverty rates data. We validate the asset index using data from Indonesia, Pakistan and Nepal which contain data on both consumption expenditures and asset ownership. The asset index has reasonable coherence with current consumption expenditures and most importantly, works as well, or better, than traditional expenditure based measures in predicting enrollment status. When the asset index is applied to the Indian data the results show large, and variable, wealth gaps in the enrollment of children across states of India. While on average across India a rich (top 20 percent of the asset index) child is 31 percentage points more likely to be enrolled than a poor child (bottom 40 percent), this wealth gap varies from only 4.6 in Kerala, to 38.2 in Uttar Pradesh and 42.6 percentage points in Bihar. The findings, interpretations, and conclusions expressed in this paper are entirely those of the authors. They do not necessarily represent the views of the World Bank, its Executive Directors, or the countries they represent. Estimating Wealth Effects without Expenditure Data-- or Tears: An Application to Educational Enrollments in States of India 1
774|The effect of household wealth on educational attainment: evidence from 35 countries,” Population and Development Review 25(1  (1999) |Abstract. We use household survey data from the Demographic and Health Surveys (DHS) from 44 surveys (in 35 countries) to document different patterns in the enrollment and attainment of children from rich and poor households. We overcome the lack of income or expenditure data in the DHS by constructing a proxy for long-run wealth of the household from the asset information in the survey using the statistical technique of principal components. There are three major findings. First, the enrollment profiles of the poor differ across countries but fall into distinctive regional patterns: in some regions the poor reach nearly universal enrollment in first grade, but then drop out in droves leading to low attainment (typical of South America),while in other regions the poor never enroll in school (typical of South Asia and Western/Central Africa). Second, there are enormous differences across countries in the “ wealth gap, ” the difference in enrollment and educational attainment of the rich and poor. While in some countries the difference in the median years of school completed of the rich and poor is only a year or two, in other countries the wealth gap in attainment is 9 or 10 years. Third, the attainment profiles can be used as diagnostic tools to examine issues in the educational system, such as the extent to which low enrollment is due to physical unavailability of schools. The findings, interpretations, and conclusions expressed in this paper are entirely those of the authors. They do not necessarily represent the views of the World Bank, its Executive Directors, or the countries they represent. The Effect of Household Wealth on Educational Attainment Around the World: Demographic and Health Survey Evidence 1
776|Household Income and Child Schooling in Vietnam.” World Bank Economic Review 13(2  (1999) |The stronger are the associations between household income and child schooling, the lower is intergenerational social mobility and the less equal is opportunity. This study estimates the associations between household income and children&#039;s school success in Vietnam. The estimates indicate that these associations are considerable. For example, the income elasticity of completed grades is five times the median estimate of earlier studies. Moreover, this association is strongest for grades completed per year of school, not for completed grades, on which most of the previous literature has focused. There are some gender differences, the most important being a smaller association between income and grades completed per year of school for boys than for girls. This difference implies that schooling of girls is treated as more of a luxury (less of a necessity) than is schooling of boys. This article also investigates some ways in which policies relate to household in-comes. School fees are progressive, but school fees are only about one-third of what households pay directly to schools and are a much smaller proportion of a household&#039;s total school-related expenditures. Total household expenditures paid directly to schools
777|Generalized Additive Models|Likelihood based regression models, such as the normal linear regression model and the linear logistic model, assume a linear (or some other parametric) form for the covariate effects. We introduce the Local Scotinq procedure which replaces the liner form C Xjpj by a sum of smooth functions C Sj(Xj)a The Sj(.) ‘s are unspecified functions that are estimated using scatterplot smoothers. The technique is applicable to any likelihood-based regression model: the class of Generalized Linear Models contains many of these. In this class, the Locul Scoring procedure replaces the linear predictor VI = C Xj@j by the additive predictor C ai ( hence, the name Generalized Additive Modeb. Local Scoring can also be applied to non-standard models like Cox’s proportional hazards model for survival data. In a number of real data examples, the Local Scoring procedure proves to be useful in uncovering non-linear covariate effects. It has the advantage of being completely automatic, i.e. no “detective work ” is needed on the part of the statistician. In a further generalization, the technique is modified to estimate the form of the link function for generalized linear models. The Local Scoring procedure is shown to be asymptotically equivalent to Local Likelihood estimation, another technique for estimating smooth covariate functions. They are seen to produce very similar results with real data, with Local Scoring being considerably faster. As a theoretical underpinning, we view Local Scoring and Local Likelihood as empirical maximizers of the ezpected log-likelihood, and this makes clear their connection to standard maximum likelihood estimation. A method for estimating the “degrees of freedom” of the procedures is also given.
778|On distortion functionals |The high-pressure phase transitions of silicon and gallium
779|THE ESTIMATION OF SMOOTH CURVES*|Smooth curves a.re often used to illustrate the relationship between two vari-ables. They are also an important building block in many recent statistical models. A procedure to estimate such a curve is called a smoother. This paper discusses currently available smoothers and introduces the class of maximum likelihood smoothers. A variety of other statistical techniques are shown to be applicable to the problem of smoothing, and some idea of the scope of models that can benefit from the use of smoothing is given.
781|A Maximum-Entropy-Inspired Parser|We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5,9,10,15,17] &#034;stan- dard&#034; sections of the Wall Street Journal tree- bank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innova- tion is the use of a &#034;maximum-entropy-inspired&#034; model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head&#039;s pre-terminal before guessing the lexical head.
782|Class-Based n-gram Models of Natural Language|We address the problem of predicting a word from previous words in a sample of text. In particular we discuss n-gram models based on calsses of words. We also discuss several statistical algoirthms for assigning words to classes based on the frequency of their co-occurrence with other words. We find that we are able to extract classes that have the flavor of either syntactically based groupings or semantically based groupings, depending on the nature of the underlying statistics.
783|Generation and Synchronous Tree-Adjoining Grammars|Tree-adjoining grammars (TAG) have been proposed as a formalism for generation based on the intuition that the extended domain of syntactic locality that TAGs provide should aid in localizing semantic dependencies as well, in turn serving as an aid to generation from semantic representations. We demonstrate that this intuition can be made concrete by using the formalism of synchronous tree-adjoining grammars. The use of synchronous TAGs for generation provides solutions to several problems with previous approaches to TAG generation. Furthermore, the semantic monotonicity requirement previously advocated for generation gram- mars as a computational aid is seen to be an inherent property of synchronous TAGs.
784|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
785|Lexical-Functional Grammar:  A Formal System for Grammatical Representation|In learning their native language, children develop a remarkable set of capabilities. They acquire knowledge and skills that enable them to produce and comprehend an indefinite number of novel utterances, and to make quite subtle judgments about certain of their properties. The major goal of psycholinguistic research is to devise an explanatory account of the mental operations that underlie these linguistic abilities. In pursuing this goal, we have adopted what we call the Competence Hypothesis as a methodological principle. We assume that an explanatory model of human language performance will incorporate a theoretically justi ed representation of the native speaker&#039;s linguistic knowledge (a grammar) as a component separate both from the computational mechanisms that operate on it (a processor) and from other nongrammatical processing parameters that might influence the processor&#039;s behavior.  To a certain extent the various components that we postulate can be studied independently, guided where appropriate by the well-established methods and evaluation standards of linguistics, computer science, and experimental psychology. However, the requirement that the various components ultimately must fit together in a consistent and coherent model imposes even stronger constraints on their structure and operation.
786|Three Generative, Lexicalised Models for Statistical Parsing|In this paper we first propose a new statistical  parsing model, which is a generative  model of lexicalised context-free gram-  mar. We then extend the model to in-  clude a probabilistic treatment of both subcategorisation  and wh~movement. Results  on Wall Street Journal text show that the  parser performs at 88.1/87.5% constituent  precision/recall, an average improvement  of 2.3% over (Collins 96).
787|A New Statistical Parser Based on Bigram Lexical Dependencies|This paper describes a new statistical  parser which is based on probabilities of  dependencies between head-words in the  parse tree. Standard bigram probability estimation  techniques are extended to calculate  probabilities of dependencies between  pairs of words. Tests using Wall Street  Journal data show that the method per-  forms at least as well as SPATTER (Magerman  95; Jelinek et al. 94), which has  the best published results for a statistical  parser on this task. The simplicity of the  approach means the model trains on 40,000  sentences in under 15 minutes. With a  beam search strategy parsing speed can be  improved to over 200 sentences a minute  with negligible loss in accuracy.
788|Statistical Parsing with a Context-free Grammar and Word Statistics|We describe a parsing system based upon a language  model for English that is, in turn, based upon assigning  probabilities to possible parses for a sentence. This  model is used in a parsing system by finding the parse  for the sentence with the highest probability. This system  outperforms previous schemes. As this is the third  in a series of parsers by different authors that are similar  enough to invite detailed comparisons but different  enough to give rise to different levels of performance,  we also report on some experiments designed to identify  what aspects of these systems best explain their  relative performance.  Introduction  We present a statistical parser that induces its grammar and probabilities from a hand-parsed corpus (a tree-bank). Parsers induced from corpora are of interest both as simply exercises in machine learning and also because they are often the best parsers obtainable by any method. That is, if one desires a parser that produces trees in the tree-bank ...
789|The Penn Treebank: Annotating Predicate Argument Structure|The Penn Treebank has recently implemented a new syntactic annotation scheme, designed to highlight aspects of predicate-argument structure. This paper discusses the implementation of crucial aspects of this new annotation scheme. It incorporates a more consistent treatment of a wide range of grammatical phenomena, provides a set of coindexed null elements in what can be thought of as &#034;underlying &#034; position for phenomena such as wh-movement, passive, and the subjects of infinitival constructions, provides some non-context free annotational mechanism to allow the structure of discontinuous constituents to be easily recovered, and allows for a clear, concise tagging system for some semantic roles. 1. INTRODUCTION During the first phase of the The Penn Treebank project [10], ending in December 1992, 4.5 million words of text were tagged for part-of-speech, with about two-thirds of this material also annotated with a skeletal syntactic bracketing. All of this material has been hand corre...
791|Three New Probabilistic Models for Dependency Parsing: An Exploration|After presenting a novel O(n³) parsing algorithm  for dependency grammar, we develop  three contrasting ways to stochasticize  it. We propose (a) a lexical affinity model  where words struggle to modify each other,  (b) a sense tagging model where words fluctuate  randomly in their selectional preferences,  and (c) a generative model where  the speaker fleshes out each word&#039;s syntactic  and conceptual structure without regard to  the implications for the hearer. We also give  preliminary empirical results from evaluating  the three models&#039; parsing performance  on annotated Wall Street Journal training  text (derived from the Penn Treebank). In  these results, the generative model performs  significantly better than the others, and  does about equally well at assigning part-of-speech tags.  
792|Treebank Grammars|By a “tree-bank grammar ” we mean a context-free grammar created by reading the production rules directly from hand-parsed sentences in a tree bank. Common wisdom has it that such grammars do not perform we &amp; though we know of no published data on the issue. The primary purpose of this paper is to show that the common wisdom is wrong. In particular, we present results on a tree-bank grammar based on the Penn WaII Street Journal tree bank. To the best of our knowledge, this grammar outperforms ah other non-word-based statistical parsers/grammars on this corpus. That is, it outperforms parsers that consider the input as a string of tags and ignore the actual words of the corpus.
793|Gemini: A Natural Language System For Spoken-Language Understanding|This paper describes the details of the system, and includes relevant measurements of size, efficiency, and performance of each of its components
794|A Linear Observed Time Statistical Parser Based on Maximum Entropy Models|This paper presents a statistical parser for  natural language that obtains a parsing  accuracy--roughly 87% precision and 86%  recall--which surpasses the best previously  published results on the Wall St. Journal  domain. The parser itself requires very little  human intervention, since the information  it uses to make parsing decisions is  specified in a concise and simple manner,  and is combined in a fully automatic way  under the maximum entropy framework.
795|A Rule-Based Approach to Prepositional Phrase Attachment Disambiguation|I.n this paper, we describe a new corpus-based ap  proach to prepositional phrase attachment disambiguation,  and 10resent results comparing perlbrmance  of this algorithm with ol,her corpus-based  approaches to this problem.
796|Towards History-based Grammars: Using Richer Models for Probabilistic Parsing|We describe a generarive probabilistic model of natural language, which we call HBG, that takes advantage of detailed linguistic information to resolve ambiguity. HBG incorporates lexical, syntactic, semantic, and structural information from the parse tree into the disambiguation process in a novel way. We use a corpus of bracketed sentences, called a Treebank, in combination with decision tree building to tease out the relevant aspects of a parse tree that will determine the correct parse of a sentence. This stands in contrast to the usual approach of further grammar tailoring via the usual linguistic introspection in the hope of generating the correct parse. In head-to-head tests against one of the best existing robust probabilistic parsing models, which we call P-CFG, the HBG model significantly outperforms P-CFG, increasing the parsing accuracy rate from 60% to 75%, a 37% reduction in error.
799|A Statistical Parser for Czech|This paper considers statistical parsing of Czech, which differs radically from English in at least two  respects: (1) it is a highly infiected language, and (2) it has relatively free word order. These dif- ferences are likely to .pose new problems for tech- niques that have been developed on English. We describe our experience in building on the parsing model of (Collins 97). Our final results - 80% dependency accuracy - represent good progress towards the 91% accuracy of the parser on English (Wall Street Journal) text.
800|Automatic Grammar Induction and Parsing Free Text: A Transformation-Based Approach|In this paper we describe a new technique for parsing free text: a transformational grammar  is automatically learned that is capable of accurately parsing text into binary-branching syntactic trees with nonterminals unlabelled. The algorithm works by beginning in a very naive state of knowledge about phrase structure. By repeatedly comparing the results of bracketing in the current state to proper bracketing provided in the training corpus, the system learns a set of simple structural transformations that can be applied to reduce error. After describing the algorithm, we present results and compare these results to other recent results in automatic grammar induction.
801|Equations for Part-of-Speech Tagging|We derive from first principles the basic equations for a few of the basic hidden-Markov-model word taggers as well as equations for other models which may be novel (the descriptions in previous papers being too spare to be sure). We give performance results for all of the models. The results from our best model (96.45% on an unused test sample from the Brown corpus with 181 distinct tags) is on the upper edge of reported results. We also hope these results clear up some confusion in the literature about the best equations to use. However, the major purpose of this paper is to show how the equations for a variety of models may be derived and thus encourage future authors to give the equations for their model and the derivations thereof.  Introduction  The last few years have seen a fair number of papers on part-of-speech tagging --- assigning the correct part of speech to each word in a text [1,2,4,5,7,8,9,10]. Most of these systems view the text as having been produced by a hidden Mar...
802|A novel use of statistical parsing to extract information from text|Since 1995, a few statistical parsing algorithms have demonstrated a breakthrough in parsing accuracy, as measured against the UPenn TREEBANK as a gold standard. In this paper we report adapting a lexicalized, probabilistic context-free parser to information extraction and evaluate this new technique on MUC-7 template elements and template relations.
803|Efficient Parsing for Bilexical Context-Free Grammars and Head Automaton Grammars|Several recent stochastic parsers use bilexical grammars, where each word type idiosyncratically prefers particular complements with particular head words. We present O(n^4) parsing algorithms for two bilexical formalisms, improving the prior upper bounds of O(n^5). For a common special case that was known to allow O(n³) parsing (Eisner, 1997), we present an O(n³) algorithm with an improved grammar constant.
804|Parsing Inside-Out|Probabilistic Context-Free Grammars (PCFGs) and variations on them have recently become some of the most common formalisms for parsing. It is common with PCFGs to compute the inside and outside probabilities. When these probabilities are multiplied together and normalized, they produce the probability that any given non-terminal covers any piece of the input sentence. The traditional use of these probabilities is to improve the probabilities of grammar rules. In this thesis we show that these values are useful for solving many other problems in Statistical Natural Language Processing. We give a framework for describing parsers. The framework generalizes the inside and outside values to semirings. It makes it easy to describe parsers that compute a wide variety of interesting quantities, including the inside and outside probabilities, as well as related quantities such as Viterbi probabilities and n-best lists. We also present three novel uses for the inside and outside probabilities. T...
805|Learning Parse and Translation Decisions from Examples with Rich Context|We present a knowledge and context-based system for parsing and translating natural language and evaluate it on sentences from the Wall Street Journal. Applying machine learning techniques, the system uses parse action examples acquired under supervision to generate a deterministic shift-reduce parser in the form of a decision structure. It relies heavily on context, as encoded in features which describe the morphological, syntactic, semantic and other aspects of a given parse state.
806|Efficient Algorithms for Parsing the DOP Model|Excellent results have been reported for DataOriented Parsing (DOP) of natural language texts (Bod, 1993c). Unfortunately, existing algorithms are both computationally intensive and difficult to implement. Previous algorithms are expensive due to two factors: the exponential number of rules that must be generated and the use of a Monte Carlo p arsing algorithm. In this paper we solve the first problem by a novel reduction of the DOP model toga small, equivalent probabilistic context-free grammar. We solve the second problem by a novel deterministic parsing strategy that maximizes the expected number of correct con- stituents, rather than the probability of a correct parse tree. Using ithe optimizations, experiments yield a 97% crossing brackets rate and 88% zero crossing brackets rate. This differs significantly from the results reported by Bod, and is compara- ble to results from a duplication of Pereira and Schabes&#039;s (1992) experiment on the same data. We show that Bod&#039;s results are at least partially due to an extremely fortuitous choice of test data, and partially due to using cleaner data than other researchers.
807|Conditional Structure versus Conditional Estimation in NLP Models|This paper separates conditional parameter estimation,  which consistently raises test set accuracy on  statistical NLP tasks, from conditional model structures,  such as the conditional Markov model used  for maximum-entropy tagging, which tend to lower  accuracy. Error analysis on part-of-speech tagging  shows that the actual tagging errors made by the  conditionally structured model derive not only from  label bias, but also from other ways in which the independence  assumptions of the conditional model  structure are unsuited to linguistic sequences. The  paper presents new word-sense disambiguation and  POS tagging experiments, and integrates apparently  conflicting reports from other recent work.
809|Pearl: A Probabilistic Chart Parser|This i)al)cr descrihcs a natural language i)ars - ing algorith,n for unrestricted text whicll uses a i)rol)ability-based scoring fimctiou to select the &#034;}mst&#034; I)arse of a sentence. The parser, earl, is a I. ime-a.synchronous bottom-ul) chart I)arscr with Earicy-type top-down prediction which pursues the highest-scoring theory i} the chart, where the score of a theory represents the cxteut I,o which t. he context of the sentmice predicts that iuterpretation. This parser differs h&#039;om previous attempts at stochastic parsers in thai. it uses a richer form of conditional probalfilitics based on context to l)rcdiet likelihood. Pearl also provides a fralnework for incorporating l.he results of previous work iu Imrt-olLsl)cech assignnmnt, mlknown word models, and ol.her Irol)al)ilistic models of linguistic features iuto one parslug tool, interleaving these techniques instead of using the traditional pipeline archiLecture. In preliminary tests, &#039;Pearl has been successl&#039;ul aL resolving parL-o[-speech and word (in speech processing) ambiguiLy, de[ermiuing categories [or unknown words, and selecLing cotreeL parses first. using a very loosely fiLing covering grammar. 1  
810|Development and Evaluation of a Broad-Coverage Probabilistic Grammar of English-Language Computer Manuals|We present an approach to grammar development where the task is decomposed into two separate subtasks. The first task is linguistic, with the goal of producing a set of rules that have a large coverage (in the sense that the correct parse is among the proposed parses) on a blind test set of sentences. The second task is statistical, with the goal of developing a model of the grammax which assigns maximum probability for the correct parse. We give parsing results on text from computer manuals.
811|Decision tree parsing using a hidden derivation model|Parser development is generally viewed as a primarily linguistic enterprise. A grammarian examines sentences, skillfully extracts the linguistic generalizations evident in the data, and writes grammar rules which cover the language. The grammarian
812|Efficiency, Robustness and Accuracy in Picky Chart Parsing|This paper describes Picky, a probabilistic agenda-based chart parsing algorithm which uses a technique called probabilistic prediction to predict which grammar rules are likely to lead to an acceptable parse of the input. Using a suboptimal search method, Picky significantly reduces the number of edges produced by CKY-like chart parsing algorithms, while maintaining the robustness of pure bottom-up parsers and the accuracy of existing probabilistic parsers. Experiments using Picky demonstrate how probabilistic modelling can impact upon the efficiency, robustness and accuracy of a parser. 
813|Head Automata and Bilingual Tiling: Translation with Minimal Representations|We present a language model consisting of  a collection of costed bidirectional finite  state automata associated with the head  words of phrases. The model is suitable  for incremental application of lexical associations  in a dynamic programming search  for optimal dependency tree derivations. We also
814|Corpus Statistics Meet the Noun Compound: Some Empirical Results|A variety of statistical methods for noun  compound analysis are implemented and  compared. The results support two main  conclusions. First, the use of conceptual  association not only enables a broad coverage,  but also improves the accuracy. Second,  an analysis model based on dependency  grammar is substantially more accurate  than one based on deepest constituents,  even though the latter is more preva-  lent in the literature.
815|Context-Sensitive Statistics for Improved Grammatical Language Models|We develop a language model using probabilistic context-free grammars (PCFGs) that is &#034;pseudo context-sensitive&#034; in that the probability that a non-terminal N expands using a rule r depends on N &#039;s parent. We derive the equations for estimating the necessary probabilities using a variant of the inside-outside algorithm. We give experimental results showing that, beginning with a high-performance PCFG, one can develop a pseudo PCSG that yields significant performance gains. Analysis shows that the benefits from the context-sensitive statistics are localized, suggesting that we can use them to extend the original PCFG. Experimental results confirm that this is both feasible and the resulting grammar retains the performance gains. This implies that our scheme may be useful as a novel method for PCFG induction. 1 Introduction  Like its non-stochastic brethren, probabilistic parsing has been based upon context-free grammars (CFGs), and for similar reasons: CFGs support a simple and efficien...
816|Statistical Parsing of Messages|The recent trend in natural language processing research has been to develop systems that deal with text concerning small, well defined domains. One practical application for such systems is to process messages pertaining to some very specific task or activity [5]. The advantage of dealing with such domains is twofold- firstly, due to the narrowness of the domain, it is possible to encode most of the knowledge related to the domain and to make this knowledge accessible to the natural language processing system, which in turn can use this knowledge to disambiguate the meanings of the messages. Secondly, in such a domain, there is not a great diversity of language constructs and therefore it becomes easier to construct a grammar which will capture all the constructs which exist in this sub-language. However, some practical aspects of such domains tend to make the problem somewhat difficult. Often, the messages tend not to be absolutely grammatically correct. As a result, the grammar designed for such a system needs to be far more forgiving than one designed for the task of parsing edited English. This can result in a proliferation of parses, which in turn makes the disambiguation task more difficult. This problem is further compounded by the telegraphic nature of the discourse, since telegraphic discourse is more prone to be syntactically ambiguous. Statistical Parsing The major objective of the research described in this paper is to use statistical data to evaluate the likelihood of a parse in order to help the parser prune out unlikely parses. Our conjecture- supported by our results and some prior, similar experiments- is that a more probable parse has a greater chance of being the correct one. The related work by the research team at UCREL
817|Global Thresholding and Multiple-Pass Parsing|We present a variation on classic beam  thresholding techniques that is up to an order  of magnitude faster than the traditional  method, at the same performance level. We  also present a new thresholding technique,  global thresholding, which, combined with  the new beam thresholding, gives an additional  factor of two improvement, and a  novel technique, multiple pass parsing, that  can be combined with the others to yield  yet another 50% improvement. We use a  new search algorithm to simultaneously op-  timize the thresholding parameters of the  various algorithms.
818|Probabilistic Feature Grammars|We present a new formalism, probabilistic feature grammar (PFG). PFGs combine most of the best properties of several other formalisms, including those of Collins, Magerman, and Charniak, and in experiments have comparable or better performance. PFGs generate features one at a time, probabilistically, conditioning the probabilities of each feature on other features in a local context. Because the conditioning is local, efficient polynomial time parsing algorithms exist for computing inside, outside, and Viterbi parses. PFGs can produce probabilities of strings, making them potentially useful for language modeling. Precision and recall results are comparable to the state of the art with words, and the best reported without words. 1 Introduction  Recently, many researchers have worked on statistical parsing techniques which try to capture additional context beyond that of simple probabilistic context-free grammars (PCFGs), including Magerman (1995), Charniak (1996), Collins (1996; 1997), ...
819|Stochastic HPSG|In this paper we provide a probabilistic  interpretation for typed feature structures  very similar to those used by Pollard  nd Sag. We begin with a version  of the interpretation which lacks  a treatment of re-entrant feature struc-  tures, then provide an extended interpre-  tation which allows them. We sketch al-  gorithms allowing the numerical parameters  of our probabilistic interpretations  of HPSG to be estimated from corpora.
820|What is the Minimal Set of Fragments that Achieves Maximal Parse Accuracy?|We aim at finding the minimal set of  fragments which achieves maximal parse  accuracy in Data Oriented Parsing. Experiments  with the Penn Wall Street  Journal treebank show that counts of  almost arbitrary fragments within parse  trees are important, leading to improved  parse accuracy over previous models  tested on this treebank (a precision of 90.8% and a recall of 90.6%). We  isolate some dependency relations which  previous models neglect but which  contribute to higher parse accuracy.
821|Automatic Learning for Semantic Collocation|The real difficulty in development of practical NLP systems comes from the fact that we do not have effective means for gathering &#034;knowledge &#034;. In this paper, we propose an algorithm which acquires automatically knowledge of semantic collocations among &#034;words&#034; from sample corpora. The algorithm
822|A Statistical Model for Parsing and Word-Sense Disambiguation|This paper describes a first attempt at a statistical model for simultaneous syntactic parsing and generalized word-sense disambiguation. On a new data set we have constructed for the task, while we were disappointed not to find parsing improvement over a traditional parsing model, our model achieves a recall of 84.0% and a precision of 67.3% of exact synset matches on our test corpus, where the gold standard has a reported inter-annotator agreement of 78.6%.
823|On The Unsupervised Induction Of Phrase-Structure Grammars|This paper examines why some previous approaches have failed to acquire desired grammars without supervision, and proposes that with a different conception of phrase-structure supervision might not be necessary. In particular, it describes in detail some reasons why SCFGs are poor mod-  2 CARL DE MARCKEN els to use for learning human language, especially when combined with the inside-outside algorithm. Following up on these arguments, it proposes that head-driven grammatical formalisms like link grammars (Sleator and Temperley, 1991) are better suited to the task, and introduces a framework for CFG induction that sidesteps many of the search problems that previous schemes have had. In the end, we hope the analysis presented here convinces others to look carefully at their representations and search strategies before blindly applying them to the language learning task. We start the discussion by examining the differences between the linguistic and statistical motivations for phrase structure; this frames our subsequent analysis. Then we introduce a simple extension to stochastic context-free grammars, and use this new class of language models in two experiments that pinpoint specific problems with both SCFGs and the search strategies commonly applied to them. Finally, we explore fixes to these problems.
824|The Effect of Alternative Tree Representations on Tree Bank Grammars|The performance of PCFGs estimated from  tree banks is shown to be sensitive to the particular  way in which linguistic constructions  are represented as trees in the tree bank. This  paper presents a theoretical analysis of the  effect of different tree representations for PP  attachment on PCFG models, and introduces  a new methodology for empirically examining  such effects using tree transformations. It  shows that one transformation, which copies  the label of a parent node onto the labels of  its children, can improve the performance of  a PCFG model in terms of labelled precision  and recall on held out data from 73% (precision)  and 69% (recall) to 80% and 79% respectively.  It also points out that if only maximum  likelihood parses are of interest then  many productions can be ignored, since they  are subsumed by combinations of other productions  in the grammar. In the Penn II tree  bank grammar, almost 9% of productions are  subsumed in this way.  1 
825|A Probabilistic Parser Applied to Software Testing Documents|We describe an approach to training a statistical parser from a bracketed corpus, and demonstrate its use in a software testing application that translates English specifications into an automated testing language. A grammar is not explicitly specified; the rules and contextual probabilities of occurrence are automatically generated from the corpus. The parser is extremely successful at producing and identifying the correct parse, and nearly deterministic in the number of parses that it produces. To compensate for undertraining, the parser also uses general, linguistic subtheories which aid in guessing some types of novel structures.  Introduction  In constrained domains, natural language processing can often provide leverage. In software testing at AT&amp;T, for example, 20,000 English test cases prescribe the behavior of a telephone switching system. A test case consists of about a dozen sentences describing the goal of the test, the actions to perform, and the conditions to verify. Figu...
826|A Probabilistic Parser and Its Application|We describe a general approach to the probabilistic parsing of context-free grammars. The method integrates context-sensitive statistical knowledge of various types (e.g., syntactic and semantic) and can be trained incrementally from a bracketed corpus. We introduce a variant of the GHR contextfree recognition algorithm, and explain how to adapt it for efficient probabilistic parsing. On a real-world corpus of sentences from software testing documents, with 23 possible parses for a sentence of average length, the system accurately finds the correct parse in 99% of cases, while producing only 1.02 parses per sentence. Significantly, the success rate would be only 66% without the semantic statistics.  Introduction  In constrained domains, natural language processing can often provide leverage. At AT&amp;T, for instance, NL technology can potentially help automate many aspects of software development. A typical example occurs in the software testing area. Here 250,000 English sentences specif...
828|Improved Alignment Models for Statistical Machine Translation|In this paper, we describe improved alignment  models for statistical machine translation. The  statistical translation approach uses two types  of information: a translation model and a lan-  guage model. The language model used is a  bigram or general m-gram model. The translation  model is decomposed into a lexical and an  alignment model. We describe two different approaches  for statistical translation and present  experimental results. The first approach is  based on dependencies between single words,  the second approach explicitly takes shallow  phrase structures into account, using two different  alignment levels: a phrase level alignment  between phrases and a word level alignment  between single words. We present results us-  ing the Verbmobil task (German-English, 6000word  vocabulary) which is a limited-domain  spoken-language task. The experimental tests  were performed on both the text transcription  and the speech recognizer output.
830|Models of Translational Equivalence among Words|This article presents methods for biasing statistical translation models to reflect these properties. Evaluation with respect to independent human judgments has confirmed that translation models biased in this fashion are significantly more accurate than a baseline knowledge-free model. This article also shows how a statistical translation model can take advantage of preexisting knowledge that might be available about particular language pairs. Even the simplest kinds of languagespecific knowledge, such as the distinction between content words and function words, are shown to reliably boost translation model performance on some tasks. Statistical models that reflect knowledge about the model domain combine the best of both the rationalist and empiricist paradigms
831|Decoding Complexity in Word-Replacement Translation Models|This paper looks at decoding complexity.
832|Using cognates to align sentences in bilingual corpora|1
833|Inducing Multilingual Text Analysis Tools via Robust Projection across Aligned Corpora|This paper describes a system and set of algorithms for automatically inducing stand-alone monolingual part-of-speech taggers, base noun-phrase bracketers, named-entity taggers and morphological analyzers for an arbitrary foreign language. Case studies include French, Chinese, Czech and Spanish. Existing text analysis tools for English are applied to bilingual text corpora and their output projected onto the second language via statistically derived word alignments. Simple direct annotation projection is quite noisy, however, even with optimal alignments. Thus this paper presents noise-robust tagger, bracketer and lemmatizer training procedures capable of accurate system bootstrapping from noisy and incomplete initial projections. Performance of the induced stand-alone part-of-speech tagger applied to French achieves 96% core part-of-speech (POS) tag accuracy, and the corresponding induced noun-phrase bracketer exceeds 91% F-measure. The induced morphological analyzer achieves over 99% lemmatization accuracy on the complete French verbal system. This achievement is particularly noteworthy in that it required absolutely no hand-annotated training data in the given language, and virtually no language-specific knowledge or resources beyond raw text. Performance also significantly exceeds that obtained by direct annotation projection. Keywords multilingual, text analysis, part-of-speech tagging, noun phrase bracketing, named entity, morphology, lemmatization, parallel corpora 1. TASK OVERVIEW A fundamental roadblock to developing statistical taggers, bracketers and other analyzers for many of the world&#039;s 200 major languages is the shortage or absence of annotated training data for the large majority of these languages. Ideally, one would like to lever- . [ ] [ ] IN N...
834|The Candide system for machine translation|We present an overview of Candide, a system for automatic translation of French text to English text. Candide uses methods of information theory and statistics to develop a probability model of the translation process. This model, which is made to accord as closely as possible with a large body of French and English sentence pairs, is then used to generate English translations of previously unseen French sentences. This paper provides a tutorial in these methods, discussions of the training and operation of the system, and a summary of test results. 1.
835|Manual annotation of translational equivalence: The Blinker project|Bilingual annotators were paid to link roughly sixteen thousand corresponding words between on-line versions of the Bible in modern French and modern English. These annotations are freely available to the researchcommunity from
836|Automated Dictionary Extraction for &#034;Knowledge-Free&#034; Example-Based Translation|An Example-Based Machine Translation system is supplied with a sentence-aligned bilingual corpus, but no other knowledge sources. Using the knowledge implicit in the corpus, it generates a bilingual word-for-word dictionary for alignment during translation. With such an automatically-generated dictionary, the system covers (with equivalent quality) more of its input on unseen texts than the same system does when provided with a manually-created general-purpose dictionary and other knowledge sources.
837|But dictionaries are data too|Although empiricist approaches to machine translation depend vitally on data in the form of large bilingual corpora, bilingual dictionaries are also a source of information. We show how to model at least a part of the information contained in a bilingual dictionary so that we can treat a bilingual dictionary and a bilingual corpus as two facets of a unified collection of data from which to extract values for the parameters of a probabilistic machine translation system. We give an algorithm for obtaining maximum iikefihood estimates of the parameters of a probabilistic model from this combined data and we show how these parameters are affected by inclusion of the dictionary for some sample words. There is a sharp dichotomy today between rationalist
838|A Class-based Approach to Word Alignment|This paper presents an algorithm capable of identifying the translation for each word in a bilingual corpus. Previously proposed methods rely heavily on word-based statistics. Under a word-based approach, frequent words with a consistent translation can be aligned at a high rate of precision. However, words that are less frequent or exhibit diverse translations generally do not have statistically significant evidence for confident alignment, thereby leading to incomplete or incorrect alignments. The algorithm proposed herein attempts to broaden coverage by exploiting lexicographic resources. To this end, we draw on the two classification systems of words in Longman Lexicon of Contemporary English (LLOCE) and Tongyici Cilin (Synonym Forest, CILIN). Automatically acquired class-based alignment rules are used to compensate for what is lacking in a bilingual dictionary such as the English-Chinese version of the Longman Dictionary of Contemporary English (LecDOCE). In addition, this alignment method is implemented using LecDOCE examples and their translations for training and testing, while further examples from a technical manual in both English and Chinese are used for an open test. Quantitative results of the closed and open tests are also summarized
839|A DP based Search Algorithm for Statistical Machine Translation|We introduce a novel search algorithm for statistical machine translation based on dynamic programming (DP). During the search process two statistical knowledge sources are combined: a translation model and a bigram language model. This search algorithm expands hypotheses along the positions of the target string while guaranteeing progressive coverage of the words in the source string. We present experimental results on the Verbmobil task.
840|An Efficient A* Search Algorithm for Statistical Machine Translation|In this paper, we describe an efficient A* search algorithm for statistical machine translation. In contrary to beamsearch or greedy approaches it is possible to guarantee the avoidance of search errors with A*. We develop various sophisticated admissible and almost admissible heuristic functions. Especially our newly developped method to perform a multi-pass A* search with an iteratively improved heuristic function allows us to translate even long sentences. We compare the A* search algorithm with a beam-search approach on the Hansards task. 1
841|Improving Statistical Natural Language Translation with Categories and Rules|This paper describes an all level approach on statistical natural language translation (SNLT). Without any predefined knowledge the system learns a statistical translation lexicon (STL), word classes (WCs) and translation rules (TRs) from a parallel corpus thereby producing a gen-eralized form of a word alignment (WA). The translation process itself is realized as a beam search. In our method example-based tech-niques enter an overall statistical approach lead-ing to about 50 percent correctly translated sentences applied to the very ditficult English-German VERBMOBIL spontaneous speech cor-pus. 1
842|Fast Decoding for Statistical Machine Translation|We investigated an e cient decoding algorithm for statistical machine translation. Compared to the other algorithms, this new algorithm is applicable to di erent translation models, and it is much faster. Experiments showed that the algorithm achieved an overall performance comparable to the state of the art decoding algorithms. 1.
843|Verbal reports as data|The central proposal of this article is that verbal reports are data. Accounting for verbal reports, as for other kinds of data, requires explication of the mech-anisms by which the reports are generated, and the ways in which they are sensitive to experimental factors (instructions, tasks, etc.). Within the theoret-ical framework of human information processing, we discuss different types of processes underlying verbalization and present a model of how subjects, in re-sponse to an instruction to think aloud, verbalize information that they are attending to in short-term memory (STM). Verbalizing information is shown to affect cognitive processes only if the instructions require verbalization of information that would not otherwise be attended to. From an analysis of what would be in STM at the time of report, the model predicts what can reliably be reported. The inaccurate reports found by other research are shown to result from requesting information that was never directly heeded, thus forcing subjects to infer rather than remember their mental processes. After a long period of time during which stimulus-response relations were at the focus of attention, research in psychology is now seeking to understand in detail the mecha-nisms and internal structure of cognitive pro-cesses that produce these relations. In the limiting case, we would like to have process models so explicit that they could actually produce the predicted behavior from the in-formation in the stimulus.
844|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
845|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
846|Missing data: Our view of the state of the art|Statistical procedures for missing data have vastly improved, yet misconception and unsound practice still abound. The authors frame the missing-data problem, review methods, offer advice, and raise issues that remain unresolved. They clear up common misunderstandings regarding the missing at random (MAR) concept. They summarize the evidence against older procedures and, with few exceptions, dis-courage their use. They present, in both technical and practical language, 2 general approaches that come highly recommended: maximum likelihood (ML) and Bayes-ian multiple imputation (MI). Newer developments are discussed, including some for dealing with missing data that are not MAR. Although not yet in the main-stream, these procedures may eventually extend the ML and MI methods that currently represent the state of the art. Why do missing data create such difficulty in sci-entific research? Because most data analysis proce-dures were not designed for them. Missingness is usu-ally a nuisance, not the main focus of inquiry, but
847|Statistical Analysis with Missing Data|Subsample ignorable likelihood for regression
848|Analyzing Incomplete Political Science Data: An Alternative Algorithm for Multiple Imputation|We propose a remedy for the discrepancy between the way political scientists analyze data with missing values and the recommendations of the statistics community. Methodologists and statisticians agree that &#034;multiple imputation&#034; is a superior approach to the problem of missing data scattered through one&#039;s explanatory and dependent variables than the methods currently used in applied data analysis. The reason for this discrepancy lies with the fact that the computational algorithms used to apply the best multiple imputation models have been slow, difficult to implement, impossible to run with existing commercial statistical packages, and demanding of considerable expertise.  In this paper, we adapt an existing algorithm, and use it to implement a generalpurpose, multiple imputation model for missing data. This algorithm is considerably faster and easier to use than the leading method recommended in the statistics literature. We also quantify the risks of current missing data practices, ...
849|Multiple imputation for multivariate missing-data problems: a data analyst&#039;s perspective|Analyses of multivariate data are frequently hampered by missing values. Until re-cently, the only missing-data methods available to most data analysts have been relatively ad hoc practices such as listwise deletion. Recent dramatic advances in theoretical and com-putational statistics, however, have produced a new generation of flexible procedures with a sound statistical basis. These procedures involve multiple imputation (Rubin, 1987), a simu-lation technique that replaces each missing datum with a set of m&gt;1 plausible values. The m versions of the complete data are analyzed by standard complete-data methods, and the results are combined using simple rules to yield estimates, standard errors, and p-values that formally incorporate missing-data uncertainty. New computational algorithms and software described in a recent book (Schafer, 1997) allow us to create proper multiple imputations in complex multivariate settings. This article reviews the key ideas of multiple imputation, discusses the software programs currently available, and demonstrates their use on data from
850|Application of random-effects pattern-mixture models for missing data in longitudinal studies|Random-effects regression models have become increasingly popular for analysis of longitudinal data. A key advantage of the random-effects approach is that it can be applied when subjects are not measured at the same number of timepoints. In this article we describe use of random-effects pattern-mixture models to further handle and describe the influence of missing data in longitudinal studies. For this approach, subjects are first divided into groups depending on their missing-data pattern and then variables based on these groups are used as model covariates. In this way, researchers are able to examine the effect of missing-data patterns on the outcome (or outcomes) of interest. Furthermore, overall estimates can be obtained by averaging over the missing-data patterns. A psychiatric clinical trials data set is used to illustrate the random-effects pattern-mixture approach to longitudinal data analysis with missing data.  
851|Multiple Imputation for Missing Data: A Cautionary Tale|: Two algorithms for producing multiple imputations for missing data are evaluated with simulated data. Software using a propensity score classifier with the approximate Bayesian boostrap produces badly biased estimates of regression coefficients when data on predictor variables are missing at random or missing completely at random. On the other hand, a regression-based method employing the data augmentation algorithm produces estimates with little or no bias.  4 Multiple imputation (MI) appears to be one of the most attractive methods for generalpurpose handling of missing data in multivariate analysis. The basic idea, first proposed by Rubin (1977) and elaborated in his (1987) book, is quite simple: 1. Impute missing values using an appropriate model that incorporates random variation. 2. Do this M times (usually 3-5 times), producing M &#034;complete&#034; data sets. 3. Perform the desired analysis on each data set using standard complete-data methods. 4. Average the values of the parameter ...
852|On Structural Equation Modeling with Data that are not Missing Completely at Random|A general latent variable model is given which includes the specification of a missing data mechanism. This framework allows for an elucidating discussion of existing general multivariate theory bearing on maximum likelihood estimation with missing data. Here, missing completely at random is not a prerequisite for unbiased estimation in large samples, as when using the traditional listwise or pairwise present data approaches. The theory is connected with old and new results in the area of selection and factorial invariance. It is pointed out that in many applications, maximum likelihood estimation with missing data may be carried out by existing structural equation modeling software, such as LISREL and LISCOMP. Several sets of artifical data are generated within the general model framework. The proposed estimator is compared to the two traditional ones and found superior. Key words: maximum likelihood, ignorability, selectivity, factor analysis, factorial invariance,
853|Multiple Imputation in Practice: Comparison of Software Packages for Regression Models With Missing Variables |This article reviews multiple imputation, describes assumptions that it requires, and reviews software packages that implement this procedure. We apply the methods and compare the results using two examples---a child psychopathology dataset with missing outcomes and an artificial dataset with missing covariates. We conclude with some discussion of the strengths and weaknesses of these implementations as well as advantages and limitations of imputation
854|Imputation of the 1989 Survey of Consumer Finances: Stochastic Relaxation and Multiple Imputation” mimeo, Board of Governors of the Federal Reserve System|acknowledges the support for this work by staff in the Division of Research and Statistics including
855|ANALYSIS WITH MISSING DATA IN PREVENTION RESEARCH|Missing data are pervasive in alcohol and drug abuse prevention evaluation efforts: Researchers administer surveys, and some items are left unanswered. Slow readers often leave large portions incomplete at the end of the survey. Researchers administer the surveys at several points in time, and people fail to show up at one or more waves of measurement. Researchers often design their measures to include a certain amount of “missingness”; some measures are so expensive (in money or time) that researchers can afford to administer them only to some respondents. Missing data problems have been around for years. Until recently, researchers have fumbled with partial solutions and put up only the weakest counterarguments to the admonitions of the critics of prevention and applied psychological research. Things have changed, however. Statistically sound solutions are now available for virtually every missing data problem,
856|Inference with Imputed Conditional Means|In this paper, we develop analytic techniques that can be used to produce appropriate inferences from a data set in which imputation for missing values has been carried out using predictive means. Our derivations are based on asymptotic expansions of point estimators and their associated variance estimators, and the resulting formulas can be thought of as first-order approximations to the estimators that would be used with multiple imputation. The procedures developed can be used either for univariate missing data or for multivariate missing data in which the variables are either missing or observed together, and they are designed for situations in which the complete-data estimator is a smooth function of linear statistics. We illustrate properties of our methods in several examples, including abstract problems as well as applications to large data sets from studies carried out by the federal government. Key Words: Linearization; Missing data; Multiple Imputation; Nonresponse; Taylor s...
857|Maximum Likelihood Analysis of Generalized Linear Models with Missing Covariates|Missing data is a common occurrence in most medical research data collection enterprises. There is an extensive literature concerning missing data, much of which has focused on missing outcomes. Covariates in regression models are often missing, particularly if information is being collected from multiple sources. The method of weights is an implementation of the EM algorithm 8 for general maximum-likelihood analysis of regression models, including generalized linear models 32 (GLMs) with incomplete covariates. In this paper, we will describe the method of weights in detail, illustrate its application with several examples, discuss its advantages and limitations, and review extensions and applications of the method.
858|Multiple imputation and posterior simulation for multivariate missing data in longitudinal studies (pp  (1995) |SUMMARY. This paper outlines a multiple imputation method for handling missing data in designed lon-gitudinal studies. A random coefficients model is developed to accommodate incomplete multivariate con-tinuous longitudinal data. Multivariate repeated measures are jointly modeled; specifically, an i.i.d. normal model is assumed for time-independent variables and a hierarchical random coefficients model is assumed for time-dependent variables in a regression model conditional on the time-independent variables and time, with heterogeneous error variances across variables and time points. Gibbs sampling is used to draw model parameters and for imputations of missing observations. An application to data from a study of startle reactions illustrates the model. A simulation study compares the multiple imputation procedure to the weighting approach of Robins, Rotnitzky, and Zhao (1995, Journal of the American Statistical Association 90, 106-121) that can be used to address similar data structures. KEY WORDS: Gibbs sampling; Missing data; Multiple imputation; Multivariate longitudinal data 1. Background In designed longitudinal studies, missing data often occur be-cause subjects miss visits during the study, because some vari-ables may not be measured at particular visits, or because
859|Knowledge-based Analysis of Microarray Gene Expression Data By Using Support Vector Machines|We introduce a method of functionally classifying genes by using gene expression data from DNA microarray hybridization experiments. The method is based on the theory of support vector machines (SVMs). SVMs are considered a supervised computer learning method because they exploit prior knowledge of gene function to identify unknown genes of similar function from expression data. SVMs avoid several problems associated with unsupervised clustering methods, such as hierarchical clustering and self-organizing maps. SVMs have many mathematical features that make them attractive for gene expression analysis, including their exibility in choosing a similarity function, sparseness of solution when dealing with large data sets, the ability t...
860|Yago: A Core of Semantic Knowledge|We present YAGO, a light-weight and extensible ontology with high coverage and quality. YAGO builds on entities and relations and currently contains roughly 900,000 entities and 5,000,000 facts. This includes the Is-A hierarchy as well as non-taxonomic relations between entities (such as hasWonPrize). The facts have been automatically extracted from the unification of Wikipedia and WordNet, using a carefully designed combination of rule-based and heuristic methods described in this paper. The resulting knowledge base is a major step beyond WordNet: in quality by adding knowledge about individuals like persons, organizations, products, etc. with their semantic relationships – and in quantity by increasing the number of facts by more than an order of magnitude. Our empirical evaluation of fact correctness shows an accuracy of about 95%. YAGO is based on a logically clean model, which is decidable, extensible, and compatible with RDFS. Finally, we show how YAGO can be further extended by state-of-the-art information
861|Why and Where: A Characterization of Data Provenance|With the proliferation of database views and curated databases,  the issue of data provenance # where a piece of data came from and the  process by which it arrived in the database # is becoming increasingly  important, especially in scienti#c databases where understanding provenance  is crucial to the accuracy and currency of data. In this paper we  describe an approach to computing provenance when the data of interest  has been created by a database query.We adopt a syntactic approach  and present results for a general data model that applies to relational  databases as well as to hierarchical data such as XML. A novel aspect of  our work is a distinction between #why&#034; provenance #refers to the source  data that had some in#uence on the existence of the data# and #where&#034;  provenance #refers to the location#s# in the source databases from which  the data was extracted#.
862|ULDBs: Databases with uncertainty and lineage|This paper introduces ULDBs, an extension of relational databases with simple yet expressive constructs for representing and manipulating both lineage and uncertainty. Uncertain data and data lineage are two important areas of data management that have been considered extensively in isolation, however many applications require the features in tandem. Fundamentally, lineage enables simple and consistent representation of uncertain data, it correlates uncertainty in query results with uncertainty in the input data, and query processing with lineage and uncertainty together presents computational benefits over treating them separately. We show that the ULDB representation is complete, and that it permits straightforward implementation of many relational operations. We define two notions of ULDB minimality—dataminimal and lineage-minimal—and study minimization of ULDB representations under both notions. With lineage, derived relations are no longer self-contained: their uncertainty depends on uncertainty in the base data. We provide an algorithm for the new operation of extracting a database subset in the presence of interconnected uncertainty. Finally, we show how ULDBs enable a new approach to query processing in probabilistic databases. ULDBs form the basis of the Trio system under development at Stanford.
863|Semantic Wikipedia|Wikipedia is the world&#039;s largest collaboratively edited source of encyclopaedic knowledge. But in spite of its utility, its contents are barely machine-interpretable. Structural knowledge, e. g. about how concepts are interrelated, can neither be formally stated nor automatically processed. Also the wealth of numerical data is only available as plain text and thus can not be processed by its actual meaning. We provide
864|Schema mediation in peer data management systems|permission of the IEEE. Such permission of the IEEE does not in any way imply IEEE endorsement of any of the University of Pennsylvania’s products or services. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to
865|The Chatty Web: Emergent Semantics Through Gossiping|This paper describes a novel approach for obtaining semantic interoperability among data sources in a bottom-up, semi-automatic manner without relying on pre-existing, global semantic models. We assume that large amounts of data exist that have been organized and annotated according to local schemas. Seeing semantics as a form of agreement, our approach enables the participating data sources to incrementally develop global agreement in an evolutionary and completely decentralized process that solely relies on pair-wise, local interactions: Participants provide translations between schemas they are interested in and can learn about other translations by routing queries (gossiping). To support the participants in assessing the semantic quality of the achieved agreements we develop a formal framework that takes into account both syntactic and semantic criteria. The assessment process is incremental and the quality ratings are adjusted along with the operation of the system. Ultimately, this process results in global agreement, i.e., the semantics that all participants understand. We discuss strategies to efficiently find translations and provide results from a case study to justify our claims. Our approach applies to any system which provides a communication infrastructure (existing websites or databases, decentralized systems, P2P systems) and offers the opportunity to study semantic interoperability as a global phenomenon in a network of information sharing parties.
866|What have Innsbruck and Leipzig in common? Extracting Semantics from Wiki Content|Abstract Wikis are established means for the collaborative authoring, versioning and publishing of textual articles. The Wikipedia project, for example, succeeded in creating the by far largest encyclopedia just on the basis of a wiki. Recently, several approaches have been proposed on how to extend wikis to allow the creation of structured and semantically enriched content. However, the means for creating semantically enriched structured content are already available and are, although unconsciously, even used by Wikipedia authors. In this article, we present a method for revealing this structured content by extracting information from template instances. We suggest ways to efficiently query the vast amount of extracted information (e.g. more than 8 million RDF statements for the English Wikipedia version alone), leading to astonishing query answering possibilities (such as for the title question). We analyze the quality of the extracted content, and propose strategies for quality improvements with just minor modifications of the wiki systems being currently used. 1
867|Practical Lineage Tracing in Data Warehouses|We consider the view data lineage problem in a warehousing environment: For a given data item in a materialized warehouse view, we want to identify the set of source data items that produced the view item. We formalize the problem and present a lineage tracing algorithm for relational views with aggregation. Based on our tracing algorithm, we propose a number of schemes for storing auxiliary views that enable consistent and efficient lineage tracing in a multisource data warehouse. We report on a performance study of the various schemes, identifying which schemes perform best in which settings. Based on our results, we have implemented a lineage tracing package in the WHIPS data warehousing system prototype at Stanford. With this package, users can select view tuples of interest, then efficiently &#034;drill down&#034; to examine the source data that produced them. 1 Introduction Data warehousing systems collect data from multiple distributed sources, integrate the information as materialized v...
868|Wikipedia and the Semantic Web - The Missing Links|Wikipedia is the biggest collaboratively created source of encyclopaedic  knowledge. Growing beyond the borders of any traditional  encyclopaedia, it is facing new problems of knowledge management: The  current excessive usage of article lists and categories witnesses the fact  that 19th century content organization technologies like inter-article references  and indices are no longer su#cient for today&#039;s needs.
869|Crossing the Structure Chasm|It has frequently been observed that most of the world&#039;s data lies outside  database systems. The reason is that database systems focus on structured data, leaving the unstructured realm to others. The world of unstructured data has several very appealing properties, such as ease of authoring, querying and data sharing. In contrast, authoring, querying and sharing structured data require significant effort, albeit with the benefit of rich query languages and exact answers. We argue
870|SCOP, Structural Classification of Proteins Database: Applications to Evaluation of the Effectiveness of Sequence Alignment Methods and Statistics of Protein Structural Data|The Structural Classification of Proteins (SCOP) database provides a detailed and comprehensive description of the relationships of all known protein structures. The classification is on hierarchical levels: the first two levels, family and superfamily, describe near and far evolutionary relationships; the third, fold, describes geometrical relationships. The distinction between evolutionary relationships and those that arise from the physics and chemistry of proteins is a feature that is unique to this database, so far. The database can be used as a source of data to calibrate sequence search algorithms and for the generation of population statistics on protein structures. The database and its associated les are freely accessible from a number of WWW sites mirrored from URL http://scop.mrc-lmb. cam.ac.uk/scop/.
871|The design and implementation of hierarchical software systems with reusable components|We present a domain-independent model of hierarchical software system design and construction that is based on interchangeable software components and largescale reuse. The model unifies the conceptualizations of two independent projects, Genesis and Avoca, that are successful examples of software component/building-block technologies and domain modeling. Building-block technologies exploit large-scale reuse, rely on open architecture software, and elevate the granularity of programming to the subsystem level. Domain modeling formalizes the similarities and differences among systems of a domain. We believe our model is a blue-print for achieving software component technologies in many domains.
872|On the Criteria To Be Used in Decomposing Systems into Modules|This paper discusses modularization as a mechanism for improving the flexibility and comprehensibility of a system while allowing the shortening of its development time. The effectiveness of a “modularization ” is dependent upon the criteria used in dividing the system into modules. A system design problem is presented and both a conventional and unconventional decomposition are described. It is shown that the unconventional decompositions have distinct advantages for the goals outlined. The criteria used in arriving at the decompositions are discussed. The unconventional decomposition, if implemented with the conventional assumption that a module consists of one or more subroutines, will be less efficient in most cases. An alternative approach to implementation which does not have this effect is sketched.
873|The x-Kernel: An Architecture for Implementing Network Protocols|This paper describes a new operating system kernel, called the x-kernel, that provides an  explicit architecture for constructing and composing network protocols. Our experience  implementing and evaluating several protocols in the x-kernel shows that this architecture  is both general enough to accommodate a wide range of protocols, yet efficient enough to  perform competitively with less structured operating systems.  1 Introduction  Network software is at the heart of any distributed system. It manages the communication hardware that connects the processors in the system and it defines abstractions through which processes running on those processors exchange messages. Network software is extremely complex: it must hide the details of the underlying hardware, recover from transmission failures, ensure that messages are delivered to the application processes in the appropriate order, and manage the encoding and decoding of data. To help manage this complexity, network software is divi...
874|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
875|The sprite network operating system|Sprite is a new operating system for networked uniprocessor and multiprocessor workstations with large physical memories. It implements a set of kernel calls much like those of 4.3 BSD UNIX, with extensions to allow processes on the same workstation to share memory and to allow processes to migrate between workstations. The implementation of the Sprite kernel contains several interesting features, including a remote procedure call facility for communication between kernels, the use of prefix tables to implement a single file name space and to provide flexibility in administering the network file system, and large variable-size file caches on both client and server machines, which provide high performance even for diskless workstations.
876|Lightweight remote procedure call|Lightweight Remote Procedure Call (LRPC) is a communication facility designed and optimized for communication between protection domains on the same machine. In contemporary small-kernel operating systems, existing RPC systems incur an unnecessarily high cost when used for the type of communication that predominates-between protection domains on the same machine. This cost leads system designers to coalesce weakly related subsystems into the same protection domain, trading safety for performance. By reducing the overhead of same-machine communication, LRPC encourages both safety and performance. LRPC combines the control transfer and communication model of capability systems with the programming semantics and large-grained protection model of RPC. LRPC achieves a factor-of-three performance improvement over more traditional approaches based on independent threads exchanging messages, reducing the cost of same-machine communication to nearly the lower bound imposed by conventional hardware. LRPC has been integrated into the Taos operating system of the DEC SRC Firefly multiprocessor workstation.
877|Query optimization in database systems|Efficient methods of processing unanticipated queries are a crucial prerequisite for the success of generalized database management systems. A wide variety of approaches to improve the performance of query evaluation algorithms have been proposed: logic-based and semantic transformations, fast implementations of basic operations, and combinatorial or heuristic algorithms for generating alternative access plans and choosing among them. These methods are presented in the framework of a general query evaluation procedure using the relational calculus representation of queries. In addition, nonstandard query optimization issues such as higher level query evaluation, query optimization in distributed databases, and use of database machines are addressed. The focus, however, is on query optimization in centralized database systems.
878|The Effect of Context Switches on Cache Performance|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
879|The Design and Implementation of INGRES|The currently operational (March 1976) version of the INGRES database management system is described. This multiuser system gives a relational view of data, supports two high level nonprocedural data sublanguages, and runs as a collection of user processes on top of the UNIX operating system for Digital Equipment Corporation PDP 11/40, 11/45, and 11/70 computers. Emphasis is on the design decisions and tradeoffs related to (1) structuring the system into processes, (2) embedding one command language in a general purpose programming language, (3) the algorithms implemented to process interactions, (4) the access methods implemented, (5) the concurrency and recovery control currently provided, and (6) the data structures used for system catalogs and the role of the database administrator. Also discussed are (1) support for integrity constraints (which is only partly operational), (2) the not yet supported features concerning views and protection, and (3) future plans concerning the system.
880|A Logical Design Methodology for Relational Databases Using the Extended Entity-Relationship Model|A database design methodology is defined for the design of large relational databases. First, the data requirements are conceptualized using an extended entity-relationship model, with the extensions being additional semantics such as ternary relationships, optional relationships, and the generalization abstraction. The extended entity-relationship model is then decomposed according to a set of basic entity-relationship constructs, and these are transformed into candidate relations. A set of basic transformations has been developed for the three types of relations: entity relations, extended entity relations, and relationship relations. Candidate relations are further analyzed and modified to attain the highest degree of normalization desired. The methodology produces database designs that are not only accurate representations of reality, but flexible enough to accommodate future processing requirements. It also reduces the number of data dependencies that must be analyzed, using the extended ER model conceptualization, and maintains data integrity through normalization. This approach can be implemented manually or in a simple software package as long as a “good ” solution is acceptable and absolute optimality is not required.
881|Using semi-joins to solve relational queries|ABSTRACT. The semi-join is a relational algebraic operation that selects a set of tuples in one relation that match one or more tuples of another relation on the joining domains. Semi-joins have been used as a basic ingredient in query processing strategies for a number of hardware and software database systems. However, not all queries can be solved entirely using semi-joins. In this paper the exact class of relational queries that can be solved using semi-joins is shown. It is also shown that queries outside of this class may not even be partially solvable using &amp;quot;short &amp;quot; semi-join programs. In addition, a linear-time membership test for this class is presented.
882|Decomposition - a strategy for query processing|Strategy for processing multivariable queries in the database management system INGRES is considered. The general procedure is to decompose the query into a sequence of one-variable queries by alternating between (a) reduction: breaking off components of the query which are joined to it by a single variable, and (b) tuple substitution: substituting for one of the variables a tuple at a time. Algorithms for reduction and for choosing the variable to be substituted are given. In most cases the latter decision depends on estimation of costs; heuristic procedures for making such estimates are outlined.
883|Layered Multiplexing Considered Harmful|Traditionally, computer communication networks have been optimized with respect to throughput, robustness and absolute delay, with little or no concern for the variation in delay (jitter) induced by the network. It is now desirable that high speed networks support a greater range of telecommunication services by providing a multi-service environment. If the transmission of jitter-sensitive tra c is not to be arbitrarily precluded, then new protocol architectures must take account of, and provide support for, the constraint of jitter. The ATM approach to broadband networking is presently being pursued within the CCITT (and elsewhere) as the unifying mechanism for the support of service integration, rate adaption, and jitter control within the lower layers of the network architecture. This position paper is speci cally concerned with the jitter arising from the design of the middle and upper layers that operate within the end systems and relays of multi-service networks (MSNs). 1 What is Layered Multiplexing?
884|Query Processing in a System for Distributed Databases (SDD-1  (1981) |Thii paper describes the techniques used to optimize relational queries in the SDD-1 distributed database system. Queries are submitted to SDD-1 in a high-level procedural language called Datalan-guage. Optimization begins by translating each Datalanguage query into a relational calculus form called an envelope, which is essentially an aggregate-free QUEL query. This paper is primarily concerned with the optimization of envelopes. Envelopes are processed in two phases. The first phase executes relational operations at various sites of the distributed database in order to delimit a subset of the database that contains all data relevant to the envelope. This subset is called a reduction of the database. The second phase transmits the reduction to one designated site, and the query is executed locally at that site. The critical optimization problem is to perform the reduction phase efficiently. Success depends on designing a good repertoire of operators to use during this phase, and an effective algorithm for deciding which of these operators to use in processing a given envelope against a given database. The principal reduction operator that we employ is called a
885|A Procedure for Designing Abstract Interfaces for Device Interface Modules |This paper describes the abstract interface principl~: and shows how it can be applied in the design ol device interface modules. The purpose of this principle is to reduce maintenance costs for embedded real-time software by facilitating the adaptation of the software to altered hardware interfaces. This principle has been applied in the Naval Research Laboratory&#039;s redesign of the flight software for the Navy&#039;s A-7 aircraft. This paper interface principle and presents solutions to interest~.ng problems encountered in the A-7 re-design. The specification document for the A-7 device interface modules is available on request; it provides a fully worked out example of the design approach discussed in this paper. Keywords software design techniques module specifications abstract interfaces software maintenance cost reduction information-hiding modules real-time software embedded software device interface modules virtual devices
886|Modeling the Storage Architectures of Commercial Database Systems|Modeling the storage structures of a DBMS is a prerequisite to understanding and optimizing database performance. Previously, such modeling was very difficult because the fundamental role of conceptual-to-internal mappings in DBMS implementations went unrecognized. In this paper we present a model of physical databases, called the transformation model, that makes conceptual-to-internal mappings explicit. By exposing such mappings, we show that it is possible to model the storage architectures (i.e., the storage structures and mappings) of many commercial DBMSs in a precise, systematic, and comprehendible way. Models of the INQUIRE, ADABAS, and SYSTEM 2000 storage architectures are presented as examples of the model’s utility. We believe the transformation model helps bridge the gap between physical database theory and practice. It also reveals the possibility of a technology to automate the development of physical database software.
887|Unsupervised Models for Named Entity Classification|This paper discusses the use of unlabeled examples for the problem of named entity classification. A large number of rules is needed for coverage of the domain, suggesting that a fairly large number of labeled examples should be required to train a classifier. However, we show that the use of unlabeled data can reduce the requirements for supervision to just 7 simple “seed ” rules. The approach gains leverage from natural redundancy in the data: for many named-entity instances both the spelling of the name and the context in which it appears are sufficient to determine its type. We present two algorithms. The first method uses a similar algorithm to that of (Yarowsky 95), with modifications motivated by (Blum and Mitchell 98). The second algorithm extends ideas from boosting algorithms, designed for supervised learning tasks, to the framework suggested by (Blum and Mitchell 98). 1
888|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
889|Combining labeled and unlabeled data with co-training|We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular, we consider a setting in which the description of each example can be partitioned into two distinct views, motivated by the task of learning to classify web pages. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks that point to that page. We assume that either view of the example would be su cient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment amuch smaller set of labeled examples. Speci cally, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm&#039;s predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to signi cant improvement of hypotheses in practice. As part of our analysis, we provide new re-
890|Automatic Acquisition of Hyponyms from Large Text Corpora|We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidante of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also he acquirable iu this way. A subset of the acquisitiou algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.
891|Improved Boosting Algorithms Using Confidence-rated Predictions| We describe several improvements to Freund and Schapire’s AdaBoost boosting algorithm, particularly in a setting in which hypotheses may assign confidences to each of their predictions. We give a simplified analysis of AdaBoost in this setting, and we show how this analysis can be used to find improved parameter settings as well as a refined criterion for training weak hypotheses. We give a specific method for assigning confidences to the predictions of decision trees, a method closely related to one used by Quinlan. This method also suggests a technique for growing decision trees which turns out to be identical to one proposed by Kearns and Mansour. We focus next on how to apply the new boosting algorithms to multiclass classification problems, particularly to the multi-label case in which each example may belong to more than one class. We give two boosting methods for this problem, plus a third method based on output coding. One of these leads to a new method for handling the single-label case which is simpler but as effective as techniques suggested by Freund and Schapire. Finally, we give some experimental results comparing a few of the algorithms discussed in this paper.
892|The Strength of Weak Learnability|Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.
893|Extracting patterns and relations from the world wide web|Abstract. The World Wide Web is a vast resource for information. At the same time it is extremely distributed. A particular type of data such as restaurant lists may be scattered across thousands of independent information sources in many di erent formats. In this paper, we consider the problem of extracting a relation for such a data type from all of these sources automatically. We present a technique which exploits the duality between sets of patterns and relations to grow the target relation starting from a small sample. To test our technique we use it to extract a relation of (author,title) pairs from the World Wide Web. 1
894|Finding Parts in Very Large Corpora|We present a method for extracting parts of objects from wholes (e.g. &#034;speedometer&#034; from &#034;car&#034;). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon.
895|Unsupervised Learning of Disambiguation Rules for Part of Speech Tagging|In this paper we describe an unsupervised learning algorithm for automatically training a rule-based part of speech tagger without using a manually tagged corpus. We compare this algorithm to the Baum-Welch algorithm, used for unsupervised training of stochastic taggers. Next, we show a method for combining unsupervised and supervised rule-based training algorithms to create a highly accurate tagger using only a small amount of manually tagged text.
896|Additive Models, Boosting, and Inference for Generalized Divergences|We present a framework for designing incremental learning algorithms derived from generalized entropy functionals. Our approach is based on the use of Bregman divergences together with the associated class of additive models constructed using the Legendre transform. A particular one-parameter family of Bregman divergences is shown to yield a family of loss functions that includes the log-likelihood criterion of logistic regression as a special case, and that closely approximates the exponential loss criterion used in the AdaBoost algorithms of Schapire et al., as the natural parameter of the family varies. We also show how the quadratic approximation of the gain in Bregman divergence results in a weighted least-squares criterion. This leads to a family of incremental learning algorithms that builds upon and extends the recent interpretation of boosting in terms of additive models proposed by Friedman, Hastie, and Tibshirani. 1 Introduction  Logistic regression is a widely used statisti...
897|Evaluation of Word Alignment Systems|Recent years have seen a few serious attempts to develop methods and measures for the evaluation of word alignment systems, notably the Blinker project (Melamed, 1998) and the ARCADE project (Vronis and Langlais, forthcoming). In this paper we discuss different approaches to the problem and report on results from a project where two word alignment systems have been evaluated. These results include methods and tools for the generation of reference data and a set of measures for system performance. We note that the selection and sampling of reference data can have a great impact on scoring results.
898|An intrusion-detection model|A model of a real-time intrusion-detection expert system capable of detecting break-ins, penetrations, and other forms of computer abuse is described. The model is based on the hypothesis that security violations can be detected by monitoring a system&#039;s audit records for abnormal patterns of system usage. The model includes profiles for representing the behavior of subjects with respect to objects in terms of metrics and statistical models, and rules for acquiring knowledge about this behavior from audit records and for detecting anomalous behavior. The model is independent of any particular system, application environment, system vulnerability, or type of intrusion, thereby providing a framework for a general-purpose intrusion-detection expert system.
899|Nonparametric model for background subtraction|Abstract. Background subtraction is a method typically used to seg-ment moving regions in image sequences taken from a static camera by comparing each new frame to a model of the scene background. We present a novel non-parametric background model and a background subtraction approach. The model can handle situations where the back-ground of the scene is cluttered and not completely static but contains small motions such as tree branches and bushes. The model estimates the probability of observing pixel intensity values based on a sample of intensity values for each pixel. The model adapts quickly to changes in the scene which enables very sensitive detection of moving targets. We also show how the model can use color information to suppress detec-tion of shadows. The implementation of the model runs in real-time for both gray level and color imagery. Evaluation shows that this approach achieves very sensitive detection with very low false alarm rates. Key words: visual motion, active and real time vision, motion detection, non-parametric estimation, visual surveillance, shadow detection 1
900|Pfinder: Real-time tracking of the human body|Pfinder is a real-time system for tracking people and interpreting their behavior. It runs at 10Hz on a standard SGI Indy computer, and has performed reliably on thousands of people in many different physical locations. The system uses a multiclass statistical model of color and shape to obtain a 2D representation of head and hands in a wide range of viewing conditions. Pfinder has been successfully used in a wide range of applications including wireless interfaces, video databases, and low-bandwidth coding. 
901|Image segmentation in video sequences: A probabilistic approach|&#034;Background subtraction&#034; is an old technique for finding moving objects in a video sequence---for example, cars driving on a freeway. The idea is that subtracting the current image from a time-averaged background image will leave only nonstationary objects. It is, however, a crude approximation to the task of classifying each pixel of the current image
902|Using adaptive tracking to classify and monitor activities in a site|We describe a vision system that monitors activity in a site over extended periods of time. The system uses a distributed set of sensors to cover the site, and an adaptive tracker detects multiple moving objects in the sensors. Our hypothesis is that motion tracking is sufficient to support a range of computations about site activities. We demonstrate using the tracked motion data: to calibrate the distributed sensors, to construct rough site models, to classify detected objects, to learn common patterns of activity for different object classes, and to detect unusual activities. 
903|Object Recognition as Machine Translation: Learning a Lexicon for a Fixed Image Vocabulary|We describe a model of object recognition as machine translation. In this model,
904|Blobworld: Image segmentation using Expectation-Maximization and its application to image querying|Retrieving images from large and varied collections using image content as a key is a challenging and important problem. We present a new image representation which provides a transformation from the raw pixel data to a small set of image regions which are coherent in color and texture. This &#034;Blobworld&#034; representation is created by clustering pixels in a joint color-texture-position feature space. The segmentation algorithm is fully automatic and has been run on a collection of 10,000 natural images. We describe a system that uses the Blobworld representation to retrieve images from this collection. An important aspect of the system is that the user is allowed to view the internal representation of the submitted image and the query results. Similar systems do not offer the user this view into the workings of the system; consequently, query results from these systems can be inexplicable, despite the availability of knobs for adjusting the similarity metrics. By finding image regions whi...
905|Pedestrian Detection Using Wavelet Templates|This paper presents a trainable object detection architecture that is applied to detecting people in static images of cluttered scenes. This problem poses several challenges. People are highly non-rigid objects with a high degree of variability in size, shape, color, and texture. Unlike previous approaches, this system learns from examples and does not rely on any a priori (handcrafted) models or on motion. The detection technique is based on the novel idea of the wavelet template that defines the shape of an object in terms of a subset of the wavelet coefficients of the image. It is invariant to changes in color and texture and can be used to robustly define a rich and complex class of objects such as people. We show how the invariant properties and computational efficiency of the wavelet template make it an effective tool for object detection.  1 Introduction  The problem of object detection has seen a high degree of interest over the years. The fundamental problem is how to characte...
906|Multiple-Instance Learning for Natural Scene Classification|Multiple-Instance learning is a way of modeling ambiguity in supervised learning examples. Each example is a bag of instances, but only the bag is labeled - not the individual instances. A bag is labeled negative if all the instances are negative, and positive if at least one of the instances in positive. We apply the Multiple-Instance learning framework to the problem of learning how to classify natural images. Images are inherently ambiguous since they can represent many different things. A user labels an image as positive if the image somehow contains the concept. Each image is a bag, and the instances are various sub-regions in the image. From a small collection of positive and negative examples, we can learn the concept and then use it to retrieve images that contain the concept from a large database. We show that the Diverse Density algorithm performs well in this task, that simple hypothesis classes are sufficient to classify natural images, and that user interaction helps to im...
907|Finding Naked People|. This paper demonstrates a content-based retrieval strategy that can tell whether there are naked people present in an image. No manual intervention is required. The approach combines color and texture properties to obtain an effective mask for skin regions. The skin mask is shown to be effective for a wide range of shades and colors of skin. These skin regions are then fed to a specialized grouper, which attempts to group a human figure using geometric constraints on human structure. This approach introduces a new view of object recognition, where an object model is an organized collection of grouping hints obtained from a combination of constraints on geometric properties such as the structure of individual parts, and the relationships between parts, and constraints on color and texture. The system is demonstrated to have 60% precision and 52% recall on a test set of 138 uncontrolled images of naked people, mostly obtained from the internet, and 1401 assorted control images, drawn f...
908|Combining Textual and Visual Cues for Content-based Image Retrieval on the World Wide Web|A system is proposed that combines textual and visual statistics in a single index vector for content-based search of a WWW image database. Textual statistics are captured in vector form using latent semantic indexing (LSI) based on text in the containing HTML document. Visual statistics are captured in vector form using color and orientation histograms. By using an integrated approach, it becomes possible to take advantage of possible statistical couplings between the content of the document (latent semantic content) and the contents of images (visual statistics). The combined approach allows improved performance in conducting content-based search. Search performance experiments are reported for a database containing 100,000 images collected from the WWW.  1 Introduction  The growing importance of the world wide web has led to the birth of a number of image search engines [6, 7, 11, 12]. The web&#039;s staggering scale puts severe limitations on the types of indexing algorithms that can be...
909|End-User Searching Challenges Indexing Practices in the Digital Newspaper Photo Archive|Previous research in conceptual indexing methods of images has furnished us  with refined theoretical frameworks characterising various aspects of images that could  and should be indexed using textual descriptors. The development of digital image  processing technologies has bred a brigade of content-based indexing and retrieval  methods available for applications. What the users need and in what kinds of  environments different indexing and retrieval methods are relevant, has remained an area  of less intensive research work.
910|Learning from Ambiguity|There are many learning problems for which the examples given by the teacher are ambiguously labeled. In this thesis, we will examine one framework of learning from ambiguous examples known as Multiple-Instance learning. Each example is a bag, consisting of any number of instances. A bag is labeled negative if all instances in it are negative. A bag is labeled positive if at least one instance in it is positive. Because the instances themselves are not labeled, each positive bag is an ambiguous example. We would like to learn a concept which will correctly classify unseen bags. We have developed a measure called Diverse Density and algorithms for learning from multiple-instance examples. We have applied these techniques to problems in drug design, stock prediction, and image database retrieval. These serve as examples of how to translate the ambiguity in the application domain into bags, as well as successful...
911|Name-It: Association of Face and Name in Video|This paper proposes a novel approach to extract meaningful content information from video by collaborative integration of image understanding and natural language processing. As an actual example, we developed a system that associates faces and names in videos, called Name-It, which is given news videos as a knowledge source, then automatically extracts face and name association as content information. The system can infer the name of a given unknown face image, or guess faces which are likely to have the name given to the system. This paper explains the method with several successful matching results which reveal effectiveness in integrating heterogeneous techniques as well as the importance of real content information extraction from video, especially face-name association. This material is based upon work supported by the National Science Foundation under Cooperative Agreement No. IRI-9411299. Any opinions, findings, and conclusions or recommendations expressed in this material are ...
912|Visual Semantics: Extracting Visual Information from Text Accompanying Pictures|This research explores the interaction of textual and photographic information in document understanding. The problem of performing generalpurpose vision without a priori knowledge is difficult at best. The use of collateral information in scene understanding has been explored in computer vision systems that use scene context in the task of object identification. The work described here extends this notion by defining visual semantics, a theory of systematically extracting picture-specific information from text accompanying a photograph. Specifically, this paper discusses the multi-stage processing of textual captions with the following objectives: (i) predicting which objects (implicitly or explicitly mentioned in the caption) are present in the picture and (ii) generating constraints useful in locating /identifying these objects. The implementation and use of a lexicon specifically designed for the integration of linguistic and visual information is discussed. Finally, the research d...
914|Learning and Representing Topic -- A Hierarchical Mixture for  . . .|This paper presents a novel statistical mixture model for natural language learning in information retrieval. The described learning architecture is based on word occurrence statistics and extracts hierarchical relations between groups of documents as well as an abstractive organization of keywords. To train the model we derive a generalized, annealed version of the Expectation-Maximization (EM) algorithm for maximum likelihood estimation. The benefits of the model for interactive information retrieval and automated cluster summarization are experimentally investigated.  
915|Multi-Modal Browsing of Images in Web Documents|In this paper, we describe a system for performing browsing and retrieval on a collection of web images and associated text on an HTML page. Browsing is combined with retrieval to help a user locate interesting portions of the corpus, without the need to formulate a query well matched to the corpus. Multi-modal information, in the form of text surrounding an image and some simple image features, is used in this process. Using the system, a user progressively narrows a collection to a small number of elements of interest, similar to the Scatter/Gather system  1  developed for text browsing. We have extended the Scatter/Gather method to use multi-modal features. With the use of multiple features, some collection elements may have unknown or undened values for some features; we present a method for incorporating these elements into the result set. This method also provides a way to handle the case when a search is narrowed to a part of the space near a boundary between two clusters. A nu...
916|Use of Collateral Text in Image Interpretation|Our research explores the interaction of textual and photographic information in document understanding. Specifically, we have been working on a computational model whereby textual captions are used as collateral information in the interpretation of the corresponding photographs. The final understanding of the picture and caption reflects a consolidation of the information obtained from each of the two sources and can thus be used in intelligent information retrieval tasks. The problem of performing general-purpose vision without a priori knowledge is very difficult at best. The concept of using collateral information in scene understanding has been explored in systems that use general scene context in the task of object identification. The work described here extends this notion by incorporating picture specific information. A multi-stage system PICTION which uses captions to identify humans in an accompanying photograph is described.  1 Introduction  Our research has focused on devel...
917|A Model of Investor Sentiment|Recent empirical research in finance has uncovered two families of pervasive regularities: underreaction of stock prices to news such as earnings announcements, and overreaction of stock prices to a series of good or bad news. In this paper, we present a parsimonious model of investor sentiment, or of how investors form beliefs, which is consistent with the empirical findings. The model is based on psychological evidence and produces both underreaction and overreaction for a wide range of parameter values. ? 1998 Elsevier Science S.A. All rights reserved. JEL classification: G12; G14
918|The cross-section of expected stock returns|Your use of the JSTOR archive indicates your acceptance of JSTOR &#039; s Terms and Conditions of Use, available at
922|Market Efficiency, Long-Term Returns, and Behavioral Finance|Market efficiency survives the challenge from the literature on long-term return anomalies. Consistent with the market efficiency hypothesis that the anomalies are chance results, apparent overreaction to information is about as common as underreaction, and post-event continuation of pre-event abnormal returns is about as frequent as post-event reversal. Most important, consistent with the market efficiency prediction that apparent anomalies can be due to methodology, most long-term return anomalies tend to disappear with reasonable changes in technique.  
924|Market underreaction to open market share repurchases|We examine long-run firm performance following open market share repurchase announcements, 1980-1990. We find that the average abnormal four-year buy-and-hold return measured after the initial announcement is 12.1%. For ‘value ’ stocks, companies more likely to be repurchasing shares because of undervaluation, the average abnormal return is 45.3%. For repurchases announced by ‘glamour ’ stocks, where undervaluation is less likely to be an important motive, no positive drift in abnormal returns is observed. Thus, at least with respect to value stocks, the market errs in its initial response and appears to ignore much of the information conveyed through repurchase announcements.
926|Momentum strategies|We examine whether the predictability of future returns from past returns is due to the market&#039;s underreaction to information, in particular to past earnings news. Past return and past earnings surprise each predict large drifts in future returns after controlling for the other. Market risk, size, and book-to-market effects do not explain the drifts. There is little evidence of subsequent reversals in the returns of stocks with high price and earnings momentum. Security analysts &#039; earnings forecasts also respond sluggishly to past news, especially in the case of stocks with the worst past performance. The results suggest a market that responds only gradually to new information. AN EXTENSIVE BODY OF RECENT finance literature documents that the crosssection of stock returns is predictable based on past returns. For example, DeBondt and Thaler (1985, 1987)report that long-term past losers outperform long-term past winners over the subsequent three to five years. Jegadeesh (1990) and Lehmann (1990) find short-term return reversals. Jegadeesh and
928|Evidence that stock prices do not fully reflect the implications of current earnings for future earnings|Evidence presented here is consistent with a failure of stock prices to reflect fully the implications of current earnings for future earnings. Specifically, the three-day price reactions to announcements of earnings for quarters t + 1 through I + 4 are predictable, based on earnings of quarter r. Even more surprisingly, the signs and magnitudes of the three-day reactions are related to the autocorrelation structure of earnings, as if stock prices fail to reflect the extent to which each firm’s earnings series differs from a seasonal random walk. 1.
929|Value versus growth: The international evidence|Value stocks have higher returns than growth stocks in markets around the world. For the period 1975 through 1995, the difference between the average returns on global portfolios of high and low book-to-market stocks is 7.68 percent per year, and value stocks outperform growth stocks in twelve of thirteen major markets. An international capital asset pricing model cannot explain the value premium, but a two-factor model that includes a risk factor for relative distress captures the value premium in international returns. 
930|The weighting of evidence and the determinants of confidence|The pattern of overconfidence and underconfidence observed in studies of in-tuitive judgment is explained by the hypothesis that people focus on the strength or extremeness of the available evidence (e.g., the warmth of a letter or the size of an effect) with insufficient regard for its weight or credence (e.g., the credibility of the writer or the size of the sample). This mode of judgment yields overconfi-dence when strength is high and weight is low, and underconfidence when strength is low and weight is high. We first demonstrate this phenomenon in a chance setup where strength is defined by sample proportion and weight is defined by sample size, and then extend the analysis to more complex evidential prob-lems, including general knowledge questions and predicting the behavior of self and of others. We propose that people’s confidence is determined by the balance of arguments for and against the competing hypotheses, with insufficient regard for the weight of the evidence. We show that this account can explain the effect of item difficulty on overconfidence, and we relate the observed discrepancy between confidence judgments and frequency estimates to the illusion of validity.
931|Fads, martingales, and market efficiency|for helpful coments. They share no responsibiTfty for any remaining errors.
932|Reversible jump Markov chain Monte Carlo computation and Bayesian model determination|Markov chain Monte Carlo methods for Bayesian computation have until recently been restricted to problems where the joint distribution of all variables has a density with respect to some xed standard underlying measure. They have therefore not been available for application to Bayesian model determination, where the dimensionality of the parameter vector is typically not xed. This article proposes a new framework for the construction of reversible Markov chain samplers that jump between parameter subspaces of di ering dimensionality, which is exible and entirely constructive. It should therefore have wide applicability in model determination problems. The methodology is illustrated with applications to multiple change-point analysis in one and two dimensions, and toaBayesian comparison of binomial experiments.
933|Combining minimax shrinkage estimators|ar
934|Bayesian Inference By Simulation in a Stochastic Model From Hematology|A particular Markov chain Monte Carlo algorithm is constructed to allow Bayesian inference in a hidden Markov model used in hematology. The algorithm has an outer Gibbsian structure, and incorporates both Metropolis and Hastings updates to move through the space of possible hidden states. While somewhat sophisticated, this algorithm still has problems getting around the infinite-dimensional space of hidden states because of strong correlations between some of the variables. A two-step variant of the Metropolis algorithm is introduced for posterior simulation.  Keywords: hidden Markov model, Metropolis algorithm, Gibbs sampler, Hastings algorithm, hematopoiesis 1. A Model  Suppose that each of N people in a room is holding a coin--the probability of heads for each coin is p. Independently of one another, each person flips his/her coin at random exponentially-distributed time intervals specified by a rate parameter . Over time, X, the number of facing heads, fluctuates between 0 and N . ...
935|On active contour models and balloons |The use.of energy-minimizing curves, known as “snakes, ” to extract features of interest in images has been introduced by Kass, Witkhr &amp; Terzopoulos (Znt. J. Comput. Vision 1, 1987,321-331). We present a model of deformation which solves some of the problems encountered with the original method. The external forces that push the curve to the edges are modified to give more stable results. The original snake, when it is not close enough to contours, is not attracted by them and straightens to a line. Our model makes the curve behave like a balloon which is inflated by an additional force. The initial curve need no longer be close to the solution to converge. The curve passes over weak edges and is stopped only if the edge is strong. We give examples of extracting a ventricle in medical images. We have also made a first step toward 3D object reconstruction, by tracking the extracted contour on a series of successive cross sections. 0 1991 Academic press, 1~. I.
936|A computational approach to edge detection|Abstract-This paper describes a computational approach to edge detection. The success of the approach depends on the definition of a comprehensive set of goals for the computation of edge points. These goals must be precise enough to delimit the desired behavior of the detector while making minimal assumptions about the form of the solution. We define detection and localization criteria for a class of edges, and present mathematical forms for these criteria as functionals on the operator impulse response. A third criterion is then added to ensure that the detector has only one response to- a single edge. We use the criteria in numerical optimization to derive detectors for several common image features, including step edges. On specializing the analysis to step edges, we find that there is a natural uncertainty principle between detection and localization performance, which are the two main goals. With this principle we derive a single operator shape which is optimal at any scale. The optimal detector has a simple approximate implementation in which edges are marked at maxima in gradient magnitude of a Gaussian-smoothed image. We extend this simple detector using operators of several widths to cope with different signal-to-noise ratios in the image. We present a general method, called feature synthesis, for the fine-to-coarse integration of information from operators at different scales. Finally we show that step edge detector performance improves considerably as the operator point spread function is extended along the edge. This detection scheme uses several elongated operators at each point, and the directional operator outputs are integrated with the gradient maximum detector. Index Terms-Edge detection, feature extraction, image processing, machine vision, multiscale image analysis. I.
937|Snakes: Active contour models|A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the cap-ture region surrounding a feature. Snakes provide a unified account of a number of visual problems, in-cluding detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.
939|Using Canny’s criteria to derive a recursively implemented optimal edge detector|A highly efficient recursive algorithm for edge detection is presented. Using Canny&#039;s design [1], we show that a solution to his precise formulation of detection and localization for an infinite extent filter leads to an optimal operator in one dimension, which can be efficiently implemented by two recursive filters moving in opposite directions. In addition to the noise truncature immunity which results, the recursive nature of the filtering operations leads, with sequential machines, to a substantial saving in computational effort (five multiplications and five additions for one pixel, independent of the size of the neighborhood). The extension to the two-dimensional case is considered and the resulting filtering structures are im-plemented as two-dimensional recursive filters. Hence, the filter size can be varied by simply changing the value of one parameter without affecting the time execution of the algorithm. Performance measures of this new edge detector are given and compared to Canny&#039;s filters. Various experimental results are shown.
940|Model Driven Edge Detection|Standard edge detectors fail to nd most relevant edges, nding either too many or too few, because they lack a geometric model to guide their search. We present a technique that integrates both photometric and geometric models with an initial estimate of the boundary. The strength of this approach lies in the ability of the geometric model to overcome various photometric anomalies, thereby nding boundaries that could not otherwise be found. Furthermore, edges can be scored based on their goodness of t to the model, thus allowing one to use semantic model information to accept or reject the edges. 1
941|Modeling TCP Throughput: A Simple Model and its Empirical Validation|In this paper we develop a simple analytic characterization of the steady state throughput, as a function of loss rate and round trip time for a bulk transfer TCP flow, i.e., a flow with an unlimited amount of data to send. Unlike the models in [6, 7, 10], our model captures not only the behavior of TCP’s fast retransmit mechanism (which is also considered in [6, 7, 10]) but also the effect of TCP’s timeout mechanism on throughput. Our measurements suggest that this latter behavior is important from a modeling perspective, as almost all of our TCP traces contained more timeout events than fast retransmit events. Our measurements demonstrate that our model is able to more accurately predict TCP throughput and is accurate over a wider range of loss rates. This material is based upon work supported by the National Science Foundation under grants NCR-95-08274, NCR-95-23807 and CDA-95-02639. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.
942|End-to-End Internet Packet Dynamics|  We discuss findings from a large-scale study of Internet packet dynamics conducted by tracing 20 000 TCP bulk transfers between 35 Internet sites. Because we traced each 100-kbyte transfer at both the sender and the receiver, the measurements allow us to distinguish between the end-toend behaviors due to the different directions of the Internet paths, which often exhibit asymmetries. We: 1) characterize the prevalence of unusual network events such as out-of-order delivery and packet replication; 2) discuss a robust receiver-based algorithm for estimating “bottleneck bandwidth ” that addresses deficiencies discovered in techniques based on “packet pair;” 3) investigate patterns of packet loss, finding that loss events are not well modeled as independent and, furthermore, that the distribution of the duration of loss events exhibits infinite variance; and 4) analyze variations in packet transit delays as indicators of congestion periods, finding that congestion periods also span a wide range of time scales. 
943|The Macroscopic Behavior of the TCP Congestion Avoidance Algorithm|In this paper, we analyze a performance model for the TCP Congestion Avoidance algorithm. The model predicts the bandwidth of a sustained TCP connection subjected to light to moderate packet losses, such as loss caused by network congestion. It assumes that TCP avoids retransmission timeouts and always has sufficient receiver window and sender data. The model predicts the Congestion Avoidance performance of nearly all TCP implementations under restricted conditions and of TCP with SelectiveAcknowledgements over a much wider range of Internet conditions. We verify
944|Wide-area Internet traffic patterns and characteristics|Abstract – The Internet is rapidly growing in number of users, traffic levels, and topological complexity. At the same time it is increasingly driven by economic competition. These developments render the characterization of network usage and workloads more difficult, and yet more critical. Few recent studies have been published reporting Internet backbone traffic usage and characteristics. At MCI, we have implemented a high-performance, low-cost monitoring system that can capture traffic and perform analyses. We have deployed this monitoring tool on OC-3 trunks within internetMCI’s backbone and also within the NSF-sponsored vBNS. This paper presents observations on the patterns and characteristics of wide-area Internet traffic, as recorded by MCI’s OC-3 traffic monitors. We report on measurements from two OC-3 trunks in MCI’s commercial Internet backbone over two time ranges (24-hour and 7-day) in the presence of up to 240,000 flows. We reveal the characteristics of the traffic in terms of packet sizes, flow duration, volume, and percentage composition by protocol and application, as well as patterns seen over the two time scales. 1
945|Tcp-like congestion control for layered multicast data transfer|Abstract—We present a novel congestion control algorithm suitable for use with cumulative, layered data streams in the MBone. Our algorithm behaves similarly to TCP congestion control algorithms, and shares bandwidth fairly with other instances of the protocol and with TCP flows. It is entirely receiver driven and requires no per-receiver status at the sender, in order to scale to large numbers of receivers. It relies on standard functionalities of multicast routers, and is suitable for continuous stream and reliable bulk data transfer. In the paper we illustrate the algorithm, characterize its response to losses both analytically and by simulations, and analyse its behaviour using simulations and experiments in real networks. We also show how error recovery can be dealt with independently from congestion control by using FEC techniques, so as to provide reliable bulk data transfer.
946|Improving round-trip time estimates in reliable transport protocols|As a reliable, end-to-end transport protocol, the Transmission Control Protocol (TCP) uses positive acknowledgements and retransmission to guarantee delivery. TCP implementations are expected to measure and adapt to changing round-trip delay so that their retransmission behavior balances user throughput and network efficiency. However, TCP suffers from a problem we call retransmission ambiguzty: when an acknowledgement arrives for a datagram that has been retransmitted, there is no indication of which transmission is being acknowledged. As a result, an implementation maybe unable to determine if the round-trip time it measures is for an original transmission or a retransmission of a datagram. Many existing TCP implementa-tions do not handle this problem correctly. Furthermore, the problem of retransmission ambigu-ity is also a characteristic of other major transport protocols, including 0S1 TP4 and DECnet NSP This paper reviews the various approaches to retransmission and presents a novel and effective approach to the retransmission ambiguity problem. Categories and Subject Descriptors: C 20 [Computer Communications Networks]: General—open System Interconnection reference model (0S1); C.2. 1 [Computer Communications Networks]: Network Architecture and Design—packet networks, store and forward net-works; D.4.4 [Operating Systems]: Communications Management — message sending, network communication
947|Why We Don&#039;t Know How to Simulate the Internet|Simulating how the global Internet data network behaves is an immensely challenging undertaking because of the network&#039;s great heterogeneity and rapid change. The heterogeneity ranges from the individual links that carry the network&#039;s traffic, to the protocols that interoperate over the links, to the &#034;mix&#034; of different applications used at a site and the levels of congestion (load) seen on different links. We discuss two key strategies for developing meaningful simulations in the face of these difficulties: searching for invariants and judiciously exploring the simulation parameter space. We finish with a look at a collaborative effort to build a common simulation environment for conducting Internet studies.
948|Automated Packet Trace Analysis of TCP Implementations|We describe tcpanaly, a tool for automatically analyzing a TCP implementation&#039;s behavior by inspecting packet traces of the TCP&#039;s activity. Doing so requires surmounting a number of hurdles, including detecting packet filter measurement errors, coping with ambiguities due to the distance between the measurement point and the TCP, and accommodating a surprisingly large range of behavior among different TCP implementations. We discuss why our efforts to develop a fully general tool failed, and detail a number of significant differences among 8 major TCP implementations, some of which, if ubiquitous, would devastate Internet performance. The most problematic TCPs were all independently written, suggesting that correct TCP implementation is fraught with difficulty. Consequently, it behooves the Internet community to develop testing programs and reference implementations. 1 Introduction  There can be a world of difference between the behavior we expect of a transport protocol, and what we g...
949|The stationary behavior of ideal TCP congestion avoidance|This note derives the stationary behavior of idealized TCP congestion avoidance. More specifically, it derives the stationary distribution of the congestion window size if loss of packets are independentevents with equal probability. The mathematical derivation uses a fluid flow, continuous time, approximation to the discrete time process #W n #, where W n is the congestion window after the n-th packet. We derive explicit results for the stationary distribution and all its moments. Congestion avoidance is the algorithm used by TCP to set its window size (and indirectly its data rate) under moderate to light segment (packet) losses. The congestion avoidance mechanism we model is idealized in the sense that loss of multiple packets does not lead to time-out phenomena. Such idealized behavior can be implemented using Selective Acknowledgements (SACKs). As such, our model predicts behavior of TCP with SACKs. It also is an approximate model in other situations. Among the results are that if eve...
950|Control Mechanisms for Packet Audio in the Internet|Current packet-switched networks such as the Internet do not provide guaranteed performance measures such as maximum loss rate or delay jitter. One way to support packet audio in these networks is to use control mechanisms which adapt the audio coding and decoding processes based on the state of the network so as to maximize the quality of the audio delivered to the destinations. This quality essentially depends on the characteristics of the loss and the delay jitter processes in the network. In this paper, we describe and analyze a set of efficient control mechanisms which attempt to minimize the impact of these processes. The mechanisms include a a jitter control mechanism and a combined error and rate control mechanism. These mechanisms have been implemented and evaluated over the Internet and the MBone. Experiments indicate that they make it possible to establish and maintain good quality audioconferences even across fairly congested connections.
951|Experiments with a Layered Transmission Scheme over the Internet|Combining hierarchical coding of data with receiver-driven control appears to be an attractive scheme for the multicast transmission of audio/video flows in a heterogeneous multicast environment such as the Internet. However, little experimental data is available regarding the actual performance of such schemes over the Internet. Previous work such as that on receiver driven layered multicast uses join experiments to choose the best quality signal a receiver can subscribe to. In this paper, we present a receiver-based multicast rate control mechanism based on a recently proposed TCP-friendly unicast mechanism. We have implemented this mechanism and evaluate its performance in conjunction with a simple layered audio coding scheme. We find that it has interesting convergence and performance properties, but also bring out its limitations.
952|Bursty and Hierarchical Structure in Streams|A fundamental problem in text data mining is to extract meaningful structure from document streams that arrive continuously over time. E-mail and news articles are two natural examples of such streams, each characterized by topics that appear, grow in intensity for a period of time, and then fade away. The published literature in a particular research field can be seen to exhibit similar phenomena over a much longer time scale. Underlying much of the text mining work in this area is the following intuitive premise --- that the appearance of a topic in a document stream is signaled by a &#034;burst of activity,&#034; with certain features rising sharply in frequency as the topic emerges.
953|A tutorial on hidden Markov models and selected applications in speech recognition|Although initially introduced and studied in the late 1960s and early 1970s, statistical methods of Markov source or hidden Markov modeling have become increasingly popular in the last several years. There are two strong reasons why this has occurred. First the models are very rich in mathematical structure and hence can form the theoretical basis for use in a wide range of applications. Sec-ond the models, when applied properly, work very well in practice for several important applications. In this paper we attempt to care-fully and methodically review the theoretical aspects of this type of statistical modeling and show how they have been applied to selected problems in machine recognition of speech.  
954|Mining Sequential Patterns|We are given a large database of customer transactions, where each transaction consists of customer-id, transaction time, and the items bought in the transaction. We introduce the problem of mining sequential patterns over such databases. We present three algorithms to solve this problem, and empirically evaluate their performance using synthetic data. Two of the proposed algorithms, AprioriSome and AprioriAll, have comparable performance, albeit AprioriSome performs a little better when the minimum number of customers that must support a sequential pattern is low. Scale-up experiments show that both AprioriSome and AprioriAll scale linearly with the number of customer transactions. They also have excellent scale-up properties with respect to the number of transactions per customer and the number of items in a transaction.  
955|ATTENTION,  INTENTIONS,  AND THE STRUCTURE OF DISCOURSE|In this paper we explore a new theory of discourse structure that stresses the role of purpose and processing in discourse. In this theory, discourse structure is composed of three separate but interre-lated components: the structure of the sequence of utterances (called the linguistic structure), a struc-ture of purposes (called the intentional structure), and the state of focus of attention (called the attentional state). The linguistic structure consists of segments of the discourse into which the utter-ances naturally aggregate. The intentional structure captures the discourse-relevant purposes, expressed in each of the linguistic segments as well as relationships among them. The attentional state is an abstraction of the focus of attention of the participants as the discourse unfolds. The attentional state, being dynamic, records the objects, properties, and relations that are salient at each point of the discourse. The distinction among these components is essential to provide an adequate explanation of such discourse phenomena as cue phrases, referring expressions, and interruptions. The theory of attention, intention, and aggregation of utterances is illustrated in the paper with a number of example discourses. Various properties of discourse are described, and explanations for the behavior of cue phrases, referring expressions, and interruptions are explored. This theory provides a framework for describing the processing of utterances in a discourse. Discourse processing requires recognizing how the utterances of the discourse aggregate into segments, recognizing the intentions expressed in the discourse and the relationships among intentions, and track-ing the discourse through the operation of the mechanisms associated with attentional state. This processing description specifies in these recognition tasks the role of information from the discourse and from the participants &#039; knowledge of the domain. 1
956|A Bayesian approach to filtering junk E-mail|In addressing the growing problem of junk E-mail on the Internet, we examine methods for the automated construction of filters to eliminate such unwanted messages from a user’s mail stream. By casting this problem in a decision theoretic framework, we are able to make use of probabilistic learning methods in conjunction with a notion of differential misclassification cost to produce filters Which are especially appropriate for the nuances of this task. While this may appear, at first, to be a straight-forward text classification problem, we show that by considering domain-specific features of this problem in addition to the raw text of E-mail messages, we can produce much more accurate filters. Finally, we show the efficacy of such filters in a real world usage scenario, arguing that this technology is mature enough for deployment.
957|Email overload: exploring personal information management of email|Email is one of the most successful computer applications yet devised. Our empirical data show however, that although email was origirally designed as a communications application, it is now used for additional functions, that it was not designed for, such as task management and personal archiving. We call this email overload. We demonstrate that email overload creates problems for personal information management: users often have cluttered inboxes containing hundreds of messages, including outstanding tasks, partially read documents and conversational threads. Furthermore, user attemtps to rationalise their inboxes by filing are often unsuccessful, with the consequence that important messages get overlooked, or &#034;lost&#034; in archives. We explain how email overloading arises and propose technical solutions to the problem.
958|Principles of Mixed-Initiative User Interfaces|Recent debate has centered on the relative promise of focusing user-interface research on developing new  metaphors and tools that enhance users&#039; abilities to directly manipulate objects versus directing effort toward developing interface agents that provide automation. In this paper, we review principles that show promise for allowing engineers to enhance human---computer interaction through an elegant coupling of automated services with direct manipulation. Key ideas will be highlighted in terms of the LookOut system for scheduling and meeting management.  Keywords  Intelligent agents, direct manipulation, user modeling, probability, decision theory, UI design  INTRODUCTION  There has been debate among researchers about where great opportunities lay for innovating in the realm of human--- computer interaction [10]. One group of researchers has expressed enthusiasm for the development and application of new kinds of automated services, often referred to as interface &#034;agents.&#034; The effo...
959|The Hierarchical Hidden Markov Model: Analysis and Applications|. We introduce, analyze and demonstrate a recursive hierarchical generalization of the widely used hidden Markov models, which we name Hierarchical Hidden Markov Models (HHMM). Our model is motivated by the complex multi-scale structure which appears in many natural sequences, particularly in language, handwriting and speech. We seek a systematic unsupervised approach to the modeling of such structures. By extendingthe standard forward-backward(BaumWelch) algorithm, we derive an efficient procedure for estimating the model parameters from unlabeled data. We then use the trained model for automatic hierarchical parsing of observation sequences. We describe two applications of our model and its parameter estimation procedure. In the first application we show how to construct hierarchical models of natural English text. In these models different levels of the hierarchy correspond to structures on different length scales in the text. In the second application we demonstrate how HHMMs can b...
960|On-line New Event Detection and Tracking| We define and describe the related problems of new event detection and event tracking within a stream of broadcast news stories. We focus on a strict on-line setting-i.e., the system must make decisions about one story before looking at any subsequent stories. Our approach to detection uses a single pass clustering algorithm and a novel thresholding model that incorporates the properties of events as a major component. Our ap-proach to tracking is similar to typical information filtering methods. We discuss the value of “surprising” features that have unusual occurrence characteristics, and briefly explore on-line adaptive filtering to handle evolving events in the news. New event detection and event tracking are part of the Topic Detection and Tracking (TDT) initiative. 
961|Learning Rules that Classify E-Mail|wcohen~research.att.com Two methods for learning text classifiers are compared on classification problems that might arise in filtering and filing personM e-mail messages: a &#034;traxiitionM IR &#034; method based on TF-IDF weighting, and a new method for learning sets of &#034;keyword-spotting rules &#034; based on the RIPPER rule learning algorithm. It is demonstrated that both methods obtain significant generalizations from a small number of examples; that both methods are comparable in generalization performance on problems of this type; and that both methods axe reasonably efficient, even with fairly large training sets. However, the greater comprehensibility of the rules may be advantageous in a system that allows users to extend or otherwise modify a learned classifier.
962|SwiftFile: An Intelligent Assistant for Organizing E-Mail|While most e-mail clients allow users to file messages  into folders, the process they must go through to file  each message is often tedious and slow. For each message,  the user must first decide which folder is most  appropriate. Then, the user must inform the e-mail  reader of that choice by selecting the appropriate icon  or menu item from among what is typically a set of several  dozen choices. The combined effort of choosing a  folder and conveying that choice to the application often  discourages users from filing their mail, resulting in  unmanageable inboxes that contain hundreds or even  thousands of unfiled messages. SwiftFile encourages  users to file their mail by simplifying the task. Using  an adaptive classifier, it predicts the three folders that  are most likely to be appropriate for a given message  and provides shortcut buttons that permit the user to  effortlessly file it into a predicted folder. For typical  users, SwiftFile&#039;s predictions are accurate over 80% to...
963|ThemeRiver: Visualizing Theme Changes over Time|ThemeRiver ™ is a prototype system that visualizes thematic variations over time within a large collection of documents. The “river ” flows from left to right through time, changing width to depict changes in thematic strength of temporally associated documents. Colored “currents ” flowing within the river narrow or widen to indicate decreases or increases in the strength of an individual topic or a group of topics in the associated documents. The river is shown within the context of a timeline and a corresponding textual presentation of external events.
964|Concept Features in Re:Agent, an Intelligent Email Agent|An important issue in the application of machine learning techniques to information management tasks is the nature of features extracted from textual information. We have created an intelligent email agent that can learn actions such as filtering, prioritizing, downloading to palmtops, and forwarding email to voicemail using automatic feature extraction. Our agent&#039;s newfeature extraction approach is based on first learning concepts present within the mail, then using these concepts as features for learning actions to perform on the messages. What features should be chosen? This paper describes the concept features approach and considers two sources for learning conceptual features: groups defined by the user and groups defined by the agent&#039;s task. Additionally, features may be defined by vectorized examples or keywords. Experimental results are provided for an email sorting task. Keywords Electronic mail, intelligent agents, machine learning, information management, feature selection...
965|Interface Agents that Learn: An Investigation of Learning Issues in a Mail Agent Interface|In recent years, interface agents have been developed to assist users with various tasks. Some systems employ machine learning techniques to allow the agent to adapt to the user&#039;s changing requirements. With the increase in the volume of data on the Internet, agents have emerged which are able to monitor and learn from their users to identify topics of interest. One such agent, described here, has been developed to filter mail messages. We examine the issues involved in constructing an autonomous interface agent which employs a learning component, and explore the use of two different learning techniques in this context. Submitted to Applied Artificial Intelligence Journal. October 26, 1 INTRODUCTION 1 1 Introduction Agents were once seen as anthropomorphic entities which would assist users with daily tasks. They could be used, for example, to locate information of interest to their user (Kay 1984). Ten years later, many definitions of agents have been proposed. The basic concept of ...
966|ifile: An Application of Machine Learning to E-Mail Filtering|The rise of the World Wide Web and the ever-increasing amounts of machine-readable text has caused text classification to become a important aspect of machine learning. One specific application that has the potential to affect almost every user of the Internet is e-mail filtering. The WorldTalk Corporation estimates that over 60 million business people use e-mail [6]. Many more use e-mail purely on a personal basis and the pool of e-mail users is growing daily. And yet, automated techniques for learning to filter e-mail have yet to significantly affect the e-mail market. Here, I attack problems that plague practical e-mail ltering and suggest solutions that will bring us closer to the acceptance of using automated classification techniques to filter personal e-mail. I also present a filtering system, ifile, that is both effective and efficient, and which has been adapted to a popular e-mail client. Results are presented from a number of experiments and show that a system such as ifile could become a...
967|Extracting Significant Time Varying Features from Text|We propose a simple statistical model for the frequency of occurrence of features in a stream of text. Adoption of this model allows us to use classical significance tests to filter the stream for interesting events. We tested the model by building a system and running it on a news corpus. By a subjective evaluation, the system worked remarkably well: almost all of the groups of identified tokens corresponded to news stories and were appropriately placed in time. A preliminary objective evaluation was also used to measure the quality of the system and it showed some of the weaknesses and the power of our approach.  1 Introduction  We are interested in information organization and exploration for supporting human decision making. Much information comes in the form of streams, where a stream is a collection of tokens arriving in a fixed order, with each token having a time stamp. Examples of data that are in the form of streams are e-mail, Usenet postings, news corpora, financial and cor...
968|Improving Text Categorization Methods for Event Tracking|Automated tracking of events from chronologically ordered document streams is a new challenge for statistical text classification. Existing learning techniques must be adapted or improved in order to effectively handle difficult situations where the number of positive training instances per event is extremely small, the majority of training documents are unlabelled, and most of the events have a short duration in time. We adapted several supervised text categorization methods, specifically several new variants of the k-Nearest Neighbor (kNN) algorithm and a Rocchio approach, to track events. All of these methods showed significant improvement (up to 71% reduction in weighted error rates) over the performance of the original kNN algorithm on TDT benchmark collections, making kNN among the top-performing systems in the recent TDT3 official evaluation. Furthermore, by combining these methods, we significantly reduced the variance in performance of our event tracking system over different ...
969|Topic Islands - A Wavelet-Based Text Visualization System|We present a novel approach to visualize and explore unstructured text. The underlying technology, called TOPIC-O-GRAPHY  TM  , applies wavelet transforms to a custom digital signal constructed from words within a document. The resultant multiresolution wavelet energy is used to analyze the characteristics of the narrative flow in the frequency domain, such as theme changes, which is then related to the overall thematic content of the text document using statistical methods. The thematic characteristics of a document can be analyzed at varying degrees of detail, ranging from section-sized text partitions to partitions consisting of a few words. Using this technology, we are developing a visualization system prototype known as TOPIC ISLANDS  TM  to browse a document, generate fuzzy document outlines, summarize text by levels of detail and according to user interests, define meaningful subdocuments, query text content, and provide summaries of topic evolution.  Keywords: text visualizati...
970|Visualizing Sequential Patterns for Text Mining|A sequential pattern in data mining is a finite series of elements such as A  B  C  D where A, B, C, and D are elements of the same domain. The mining of sequential patterns is designed to find patterns of discrete events that frequently happen in the same arrangement along a timeline. Like association and clustering, the mining of sequential patterns is among the most popular knowledge discovery techniques that apply statistical measures to extract useful information from large datasets. As our computers become more powerful, we are able to mine bigger datasets and obtain hundreds of thousands of sequential patterns in full detail. With this vast amount of data, we argue that neither data mining nor visualization by itself can manage the information and reflect the knowledge effectively. Subsequently, we apply visualization to augment data mining in a study of sequential patterns in large text corpora. The result shows that we can learn more and more quickly in an integrated visual...
971|Topic Detection and Tracking Pilot Study|Topic Detection and Tracking (TDT) is a DARPA-sponsored initiative to investigate the state of the art in finding and following new events in a stream of broadcast news stories. The TDT problem consists of three major tasks: (1) segmenting a stream of data, especially recognized speech, into distinct stories; (2) identifying those news stories that are the first to discuss a new event occurring in the news; and (3) given a small number of sample news stories about an event, finding all following stories in the stream. The TDT Pilot Study ran from September 1996 through October 1997. The primary participants were DARPA, Carnegie Mellon University, Dragon Systems, and the University of Massachusetts at Amherst. This report summarizes the findings of the pilot study. The TDT work continues in a new project involving larger training and test corpora, more active participants, and a more broadly defined notion of &#034;topic&#034; than was used in the pilot study. The following individuals participat...
972|Ishmail: Immediate Identification of Important Information|This paper describes Ishmail, a program designed for people who get a lot of electronic mail. Most email programs do not address the main problem experienced by people who get a lot of email: information overload. Given a deluge of email, how does one maintain control over incoming message traffic and reduce the time required to find important messages? Some email programs support classification of messages into separate mailboxes, but this is only a partial solution. Ishmail is unique in that it not only sorts messages into mailboxes, but it orders mailboxes by a combination of user-specified priorities and alarms. While most mail programs only alert users about unread messages, Ishmail supports independent alarms on each mailbox with customizable thresholds and filters. Users control their alarms, mailboxes, and messages through customizable summaries that act as both views and interactive controls. Three additional unique features of Ishmail are 1) the ability to read messages safel...
973|Mail-by-Example: A visual query interface for managing large volumes of electronic messages|Email is a rich source of quality and up-to-date information, and users need advanced facilities to store, organize and retrieve information in large volumes of electronic messages. In this paper, we present MBE (Mail-by-Example), a visual interface integrated to the Lotus Notes email environment that enables users to define ad hoc queries for retrieving messages, folders, or information about them. MBE is based on a “by-example ” query style (QBE), to suit the requirements of typical users of email environments, i.e. unfamiliar with database concepts and query formulation. The paper describes the striking characteristics of MBE, the various information access modules, and the current implementation using Lotus Notes environment. It also reports an experiment to test the usability of the interface, which confirmed its easiness and user-friendliness with regard to its target users profile. Users who participated in the experiment expressed a generalized satisfaction towards MBE features. 1
974|Pig Latin: A Not-So-Foreign Language for Data Processing |There is a growing need for ad-hoc analysis of extremely large data sets, especially at internet companies where innovation critically depends on being able to analyze terabytes of data collected every day. Parallel database products, e.g., Teradata, offer a solution, but are usually prohibitively expensive at this scale. Besides, many of the people who analyze this data are entrenched procedural programmers, who find the declarative, SQL style to be unnatural. The success of the more procedural map-reduce programming model, and its associated scalable implementations on commodity hardware, is evidence of the above. However, the map-reduce paradigm is too low-level and rigid, and leads to a great deal of custom user code that is hard to maintain, and reuse. We describe a new language called Pig Latin that we have designed to fit in a sweet spot between the declarative style of SQL, and the low-level, procedural style of map-reduce. The accompanying system, Pig, is fully implemented, and compiles Pig Latin into physical plans that are executed over Hadoop, an open-source, map-reduce implementation. We give a few examples of how engineers at Yahoo! are using Pig to dramatically reduce the time required for the development and execution of their data analysis tasks, compared to using Hadoop directly. We also report on a novel debugging environment that comes integrated with Pig, that can lead to even higher productivity gains. Pig is an open-source, Apache-incubator project, and available for general use. 1.
975|Interpreting the Data: Parallel Analysis with Sawzall |Very large data sets often have a flat but regular structure and span multiple disks and machines. Examples include telephone call records, network logs, and web document repositories. These large data sets are not amenable to study using traditional database techniques, if only because they can be too large to fit in a single relational database. On the other hand, many of the analyses done on them can be expressed using simple, easily distributed computations: filtering, aggregation, extraction of statistics, and so on. We present a system for automating such analyses. A filtering phase, in which a query is expressed using a new procedural programming language, emits data to an aggregation phase. Both phases are distributed over hundreds or even thousands of computers. The results are then collated and saved to a file. The design—including the separation into two phases, the form of the programming language, and the properties of the aggregators—exploits the parallelism inherent in having data and computation distributed across many machines. 1
976|Programming Parallel Algorithms|In the past 20 years there has been treftlendous progress in developing and analyzing parallel algorithftls. Researchers have developed efficient parallel algorithms to solve most problems for which efficient sequential solutions are known. Although some ofthese algorithms are efficient only in a theoretical framework, many are quite efficient in practice or have key ideas that have been used in efficient implementations. This research on parallel algorithms has not only improved our general understanding ofparallelism but in several cases has led to improvements in sequential algorithms. Unf:ortunately there has been less success in developing good languages f:or prograftlftling parallel algorithftls, particularly languages that are well suited for teaching and prototyping algorithms. There has been a large gap between languages
977|Map-Reduce-Merge: simplified relational data processing on large clusters|than the artifacts. 2. MapReduce 3. Map-Reduce-Merge: extending MapReduce 4. Using Map-Reduce-Merge to implement
978|A Bayesian Framework for the Analysis of Microarray Expression Data: Regularized t-Test and Statistical Inferences of Gene Changes|Motivation: DNA microarrays are now capable of providing genome-wide patterns of gene expression across many different conditions. The first level of analysis of these patterns requires determining whether observed differences in expression are significant or not. Current methods are unsatisfactory due to the lack of a systematic framework that can accommodate noise, variability, and low replication often typical of microarray data.  Results: We develop a Bayesian probabilistic framework for microarray data analysis. At the simplest level, we model log-expression values by independent normal distributions, parameterized by corresponding means and variances with hierarchical prior distributions. We derive point estimates for both parameters and hyperparameters, and regularized expressions for the variance of each gene by combining the empirical variance with a local background variance associated with neighboring genes. An additional hyperparameter, inversely related to the number of empirical observations, determines the strength of the background variance. Simulations show that these point estimates, combined with a t-test, provide a systematic inference approach that compares favorably with simple t-test or fold methods, and partly compensate for the lack of replication.  Availability: The approach is implemented in a software called Cyber-T accessible through a Web interface at www.genomics.uci.edu/software.html. The code is available as Open Source and is written in the freely available statistical language R.  and Department of Biological Chemistry, College of Medicine, University of California, Irvine. To whom all correspondence should be addressed.  Contact: pfbaldi@ics.uci.edu, tdlong@uci.edu.  1 
979|Molecular classification of cancer: class discovery and class prediction by gene expression monitoring|Although cancer classification has improved over the past 30 years, there has been no general approach for identifying new cancer classes (class discovery) or for assigning tumors to known classes (class prediction). Here, a generic approach to cancer classification based on gene expression monitoring by DNA microarrays is described and applied to human acute leukemias as a test case. A class discovery procedure automatically discovered the distinction between acute myeloid leukemia (AML) and acute lymphoblastic leukemia (ALL) without previous knowledge of these classes. An automatically derived class predictor was able to determine the class of new leukemia cases. The results demonstrate the feasibility of cancer classification based solely on gene expression monitoring and suggest a general strategy for discovering and predicting cancer classes for other types of cancer, independent of previous biological knowledge. The challenge of cancer treatment has been to target specific therapies to pathogenetically distinct tumor types, to maximize efficacy
980|Bayesian Interpolation|Although Bayesian analysis has been in use since Laplace, the Bayesian method of model--comparison has only recently been developed in depth.  In this paper, the Bayesian approach to regularisation and model--comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other problems.  Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam&#039;s razor&#039; is automatically embodied by this framework.  The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling.  1 Data modelling and Occam&#039;s razor In science, a central task is to develop and compare models to a...
981|Support Vector Machine Classification and Validation of Cancer Tissue Samples Using Microarray Expression Data|Motivation: DNA microarray experiments generating thousands of gene expression measurements, are being used to gather information from tissue and cell samples regarding gene expression differences that will be useful in diagnosing disease. We have developed a new method to analyse this kind of data using support vector machines (SVMs). This analysis consists of both classification of the tissue samples, and an exploration of the data for mis-labeled or questionable tissue results.  Results: We demonstrate the method in detail on samples consisting of ovarian cancer tissues, normal ovarian tissues, and other normal tissues. The dataset consists of expression experiment results for 97 802 cDNAs for each tissue. As a result of computational analysis, a tissue sample is discovered and confirmed to be wrongly labeled. Upon correction of this mistake and the removal of an outlier, perfect classification of tissues is achieved, but not with high confidence. We identify and analyse a subset of genes from the ovarian dataset whose expression is highly differentiated between the types of tissues. To show robustness of the SVM method, two previously published datasets from other types of tissues or cells are analysed. The results are comparable to those previously obtained. We show that other machine learning methods also perform comparably to the SVM on many of those datasets.  Availability: The SVM software is available at http:// www. cs.columbia.edu/#bgrundy/svm.  Contact: booch@cse.ucsc.edu  
982|Comparison of Approximate Methods for Handling Hyperparameters |I examine two approximate methods for computational implementation of Bayesian  hierarchical models, that is, models which include unknown hyperparameters such as  regularization constants and noise levels. In the &#039;evidence framework&#039; the model parameters  are integrated over, and the resulting evidence is maximized over the hyperparameters. The optimized
983|Building a Large Annotated Corpus of English: The Penn Treebank|There is a growing consensus that significant, rapid progress can be made in both text understanding and spoken language understanding by investigating those phenomena that occur most centrally in naturally occurring unconstrained materials and by attempting to automatically extract information about language from very large corpora. Such corpora are beginning to serve as important research tools for investigators in natural language processing, speech recognition, and integrated spoken language systems, as well as in theoretical linguistics. Annotated corpora promise to be valuable for enterprises as diverse as the automatic construction of statistical models for the grammar of the written and the colloquial spoken language, the development of explicit formal theories of the differing grammars of writing and speech, the investigation of prosodic phenomena in speech, and the evaluation and comparison of the adequacy of parsing models. In this paper, we review our experience with constructing one such large annotated corpus--the Penn Treebank, a corpus 1 consisting of over 4.5 million words of American English. During the first three-year phase of the Penn Treebank Project (1989-1992), this corpus has been annotated for part-of-speech (POS) information. In addition, over half of it has been annotated for skeletal syntactic structure. These materials are available to members of the Linguistic Data Consortium; for details, see Section 5.1.
984|A statistical approach to machine translation|In this paper, we present a statistical approach to machine translation. We describe the application of our approach to translation from French to English and give preliminary results.
985|A maximum likelihood approach to continuous speech recognition|Abstract-Speech recognition is formulated as a problem of maximum likelihood decoding. This formulation requires statistical models of the speech production process. In this paper, we describe a number of sta-tistical models for use in speech recognition. We give special attention to determining the parameters for such models from sparse data. We also describe two decoding methods, one appropriate for constrained artificial languages and one appropriate for more realistic decoding tasks. To illustrate the usefulness of the methods described, we review a number of decoding results that have been obtained with them. Index Terms-Markov models, maximum likelihood, parameter esti-mation, speech recognition, statistical models. I.
986|Statistical Language Modeling Using The Cmu-Cambridge Toolkit|The CMU Statistical Language Modeling toolkit was released in 1994 in order to facilitate the construction and testing of bigram and trigram language models. It is currently in use in over 40 academic, government and industrial laboratories in over 12 countries. This paper presents a new version of the toolkit. We outline the conventional language modeling technology, as implemented in the toolkit, and describe the extra efficiency and functionality that the new toolkit provides as compared to previous software for this task. Finally,we give an example of the use of the toolkit in constructing and testing a simple language model.
987|A Gaussian prior for smoothing maximum entropy models|In certain contexts, maximum entropy (ME) modeling can be viewed as maximum likelihood train-ing for exponential models, and like other maximum likelihood methods is prone to overfitting of training data. Several smoothing methods for maximum entropy models have been proposed to address this problem, but previous results do not make it clear how these smoothing methods com-pare with smoothing methods for other types of related models. In this work, we survey previous work in maximum entropy smoothing and compare the performance of several of these algorithms with conventional techniques for smoothing n-gram language models. Because of the mature body of research in n-gram model smoothing and the close connection between maximum entropy and conventional n-gram models, this domain is well-suited to gauge the performance of maximum entropy smoothing methods. Over a large number of data sets, we find that an ME smoothing method proposed to us by Lafferty [1] performs as well as or better than all other algorithms under consideration. This general and efficient method involves using a Gaussian prior on the parame-ters of the model and selecting maximum a posteriori instead of maximum likelihood parameter values. We contrast this method with previous n-gram smoothing methods to explain its superior performance.
988|Entropy-based pruning of backoff language models |A criterion for pruning parameters from N-gram backoff language models is developed, based on the relative entropy between the original and the pruned model. It is shown that the relative entropy resulting from pruning a single N-gram can be computed exactly and efficiently for backoff models. The relative entropy measure can be expressed as a relative change in training set perplexity. This leads to a simple pruning criterion whereby all N-grams that change perplexity by less than a threshold are removed from the model. Experiments show that a production-quality Hub4 LM can be reduced to 26 % its original size without increasing recognition error. We also compare the approach to a heuristic pruning criterion by Seymore and Rosenfeld [9], and show that their approach can be interpreted as an approximation to the relative entropy criterion. Experimentally, both approaches select similar sets of N-grams (about 85% overlap), with the exact relative entropy criterion giving marginally better performance. 1.
990|The CMU Statistical Language Modeling Toolkit and its use in the 1994 ARPA CSR Evaluation|The Carnegie Mellon Statistical Language Modeling (CMU SLM) Toolkit is a set of Unix software tools designed to facilitate language modeling work in the research community. The package, including source code, is freely available for research purposes. As of December 1994, the toolkit is in active use by 23 research groups in 8 countries. It was recently used to process the 2.5 GBNAB corpus for the ARPA CSR community. In this paper, I first discuss the design principles and features of the toolkit. Then, I describe the composition of the NAB corpus, and report on the ngram statistics, standard vocabulary and language models created using the SLM tools. 1. OVERVIEW OF THE CMU SLM TOOLKIT 1.1. Introduction  The Carnegie Mellon University Statistical Language Modeling (CMU SLM) Toolkit is a set of Unix software tools designed to facilitate language modeling work in the research community. Some of the tools are used to process general textual data into: ffl word frequency lists and vocabula...
991|Scalable Backoff Language Models|When a trigram backoff language model is created from a large body of text, trigrams and bigrams that occur few times in the training text are often excluded from the model in order to decrease the model size. Generally, the elimination of n-grams with very low counts is believed to not significantly affect model performance. This project investigates the degradation of a trigram backoff model&#039;s perplexity and word error rates as bigram and trigram cutoffs are increased. The advantage of reduction in model size is compared to the increase in word error rate and perplexity scores. More importantly, this project also investigates alternative ways of excluding bigrams and trigrams from a backoff language model, using criteria other than the number of times an n-gram occurs in the training text. Specifically, a difference method has been investigated where the difference in the logs of the original and backed off trigram and bigram probabilities is used as a basis for n-gram exclusion from...
992|Evaluation Metrics For Language Models|The most widely-used evaluation metric for language models for speech recognition is the perplexity of test data. While perplexities can be calculated efficiently and without access to a speech recognizer, they often do not correlate well with speech recognition word-error rates. In this research, we attempt to find a measure that like perplexity is easily calculated but which better predicts speech recognition performance. We investigate two approaches; first, we attempt to extend perplexity by using similar measures that utilize information about language models that perplexity ignores. Second, we attempt to imitate the word-error calculation without using a speech recognizer by artificially generating speech recognition lattices. To test our new metrics, we have built over thirty varied language models. We find that perplexity correlates with word-error rate remarkably well when only considering n-gram models trained on in-domain data. When considering other types of models, our novel metrics are superior to perplexity for predicting speech recognition performance. However, we conclude that none of these measures predict word-error rate sufficiently accurately to be effective tools for language model evaluation in speech recognition.
994|Hub4 language modeling using domain interpolation and data clustering|In SRI’s language modeling experiments for the Hub4 domain, three basic approaches were pursued: interpolating multiple models estimated from Hub4 and non-Hub4 training data, adapting the language model (LM) to the focus conditions, and adapting the LM to different topic types. In the first approach, we built separate LMs for the closely transcribed Hub4 material (acoustic training transcripts) and the loosely transcribed Hub4 material (LM training data), as well as the North-American Business News (NABN) and Switchboard training data, projected onto the Hub4 vocabulary. By interpolating the probabilities obtained from these models, we obtained a 20 % reduction in perplexity and a 1.8 % reduction in word error rate, compared to a baseline Hub4-only language model. Two adaptation approaches are also described: adapting language models to the speech styles correlated with different focus conditions, and building cluster-specific LM mixtures. These two approaches give some reduction in perplexity, but no significant reduction in word error. Finally, we identify the problems and future directions of our work. 1.
995|The JANUS Speech Recognizer|JANUS [17] was designed for the translation of spontaneous human-to-human speech. Before the 1994 CSR evaluation, JANUS was run with vocabularies of up to 2500 words. JANUS was also tested on the Conference Registration and the Resource Management tasks. The best error rate on the &#039;89 Resource Management evaluation set was 5.9%. At the June 1994 Verbmobil speech component evaluation [1], JANUS scored best among eight participants on the German appointment scheduling task, a task of spontaneous human to human dialogs. In this paper we give a detailed description of the recognition engine of JANUS, focusing on the acoustic modeling and our first run with the WSJ task. 1. ACOUSTIC MODELING IN JANUS 1.1 PREPROCESSING  For the 1994 CSR evaluation we computed 16 mel scale spectral coefficients from an FFT with a window size of 256 sample points and a window shift (frame rate) of 10 ms. 16 mel spectral coefficients, 16 delta coefficients, and 16 delta-delta coefficients were used to build a 4...
996|Wide-Area Traffic: The Failure of Poisson Modeling|Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remotelogin and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into “connection bursts,” the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of widearea traffic.  
997|On the Self-similar Nature of Ethernet Traffic (Extended Version)  (1994) | We demonstrate that Ethernet LAN traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal-like behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based networks, and that aggregating streams of such traffic typically intensifies the self-similarity (“burstiness”) instead of smoothing it. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We also present traffic models based on self-similar stochastic processes that provide simple, accurate, and realistic descriptions of traffic scenarios expected during B-ISDN deployment. 
998|Supporting Real-Time Applications in an Integrated Services Packet Network: Architecture and Mechanism|This paper considers the support of real-time applications in an
999|The synchronization of periodic routing messages|Abstract — The paper considers a network with many apparently-independent periodic processes and discusses one method by which these processes can inadvertent Iy become synchronized. In particular, we study the synchronization of periodic routing messages, and offer guidelines on how to avoid inadvertent synchronization. Using simulations and analysis, we study the process of synchronization and show that the transition from unsynchronized to synchronized traffic is not one of gradual degradation but is instead a very abrupt ‘phase transition’: in general, the addition of a single router will convert a completely unsynchronized traffic stream into a completely synchronized one. We show that synchronization can be avoided by the addition of randomization to the tra~c sources and quantify how much randomization is necessary. In addition, we argue that the inadvertent synchronization of periodic processes is likely to become an increasing problem in computer networks.
1000|On Traffic Phase Effects in Packet-Switched Gateways|this paper we define the notion of traffic phase in a packet-switched network and describe how phase differences between competing traffic streams can be the dominant factor in relative throughput. Drop Tail gateways in a TCP/IP network with strongly periodic traffic can result in systematic discrimination against some connections. We demonstrate this
1001|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
1003|Telnet Protocol Specification|The Internet Protocol (IP) [1] is used for host-to-host datagram service in a system of interconnected networks called the Catenet [2]. The network connecting devices are called Gateways. These gateways communicate between themselves for control purposes
1004|Local Area Network Traffic Characteristics, with Implications for Broadband Network Congestion Management |This paper examines the phenomenon of congestion in order to better understand the congestion management techniques that will be needed in high-speed, cell-based networks. The first step of this study is to use high time-resolution local area network (LAN) traffic data to explore the nature of LAN traffic variability. Then, we use the data for a trace-driven simulation of a connectionless service that provides LAN interconnection. The simulation allows us to characterize what congestion might look like in a high-speed, cell-based network. The most
1005|An Empirical Workload Model for Driving Wide-Area TCP/IP Network Simulations|We present an artificial workload model of wide-area internetwork traffic. The model can be used to drive simulation experiments of communication protocols and flow and congestion control experiments. The model is based on analysis of wide-area TCP/IP traffic collected from  one industrial and two academic networks. The artificial workload model uses both detailed knowledge and measured characteristics of the user application programs responsible for the traffic. Observations drawn from our measurements contradict some commonly held beliefs regarding wide-area TCP/IP network traffic.
1006|Statistical analysis of CCSN/SS7 traffic data from working CCS subnetworks|In this paper we report on an ongoing statistical analysis of actual CCSN traffic data. The data consist of approximately 170 million signaling messages collected from a variety of different working CCS subnetworks. The key findings from our analysis concern: (1) the characteristics of both the telephone call arrival process and the signaling message arrival process, (2) the tail behavior of the call holding time distribution, and (3) the observed performance of the CCSN with respect to a variety of performance and reliability measurements. 1.
1007|Tcplib: A Library of TCP Internetwork Traffic Characteristics|This paper describes tcplib, a workload or source library for network simulation. This paper motivates the need for tools like tcplib and discusses how to incorporate it into a network simulator. Tcplib is available by anonymous ftp 1. 1.
1008|Growth Trends in Wide-Area TCP Connections|We analyze the growth of a medium-sized research laboratory &#039;s wide-area TCP connections over a period of more than two years. Our data consisted of six month-long traces of all TCP connections made between the site and the rest of the world. We find that smtp, ftp, and X11 traffic all exhibited exponential growth in the number of connections and bytes transferred, at rates significantly greater than that at which the site&#039;s overall computing resources grew; that individual users increasingly affected the site&#039;s traffic profile by making wide-area connections from background scripts; that the proportion of local computers participating in wide-area traffic outpaces the site&#039;s overall growth; that use of the network by individual computers appears to be constant for some protocols  (telnet) and growing exponentially for others (ftp, smtp);  and that wide-area traffic geography is diverse and dynamic. 1 Introduction  To properly design future networks, we need a thorough understanding of...
1009|Long-term traffic aspects of the NSFNET|We present the architecture for data collection for the T3 NSFNET backbone service, and difficulties with using the collected statistics for long-term network forecasting of certain traffic aspects. We describe relevant aspects of the T3 backbone architecture, describe the instrumentation for the statistics collection process, and how it differs from that on the T1 backbone. We then present longterm NSFNET data to elucidate long term trends in both the reachability of Internet components via the NSFNET as well as the growing cross-section of traffic. We focus on the difficulties of forecasting and planning for these two traffic aspects in an infrastructure whose protocol architecture and instrumentation for data collection was not designed to support such objectives. I. Introduction  NSFNET, the National Science Foundation Network, is a general purpose packet-switching network supporting access to scientific computing resources, data, and interpersonal electronic communications.  2  Cl...
1010|Goodness-of-fit Techniques|Estimating drizzle drop size and precipitation rate using two-colour lidar measurements 	 ABCDEFB
1011|Object Detection with Discriminatively Trained Part Based Models |We describe an object detection system based on mixtures of multiscale deformable part models. Our system is able to represent highly variable object classes and achieves state-of-the-art results in the PASCAL object detection challenges. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL datasets. Our system relies on new methods for discriminative training with partially labeled data. We combine a margin-sensitive approach for data-mining hard negative examples with a formalism we call latent SVM. A latent SVM is a reformulation of MI-SVM in terms of latent variables. A latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. This leads to an iterative training algorithm that alternates between fixing latent values for positive examples and optimizing the latent SVM objective function.
1012|Active Appearance Models|AbstractÐWe describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors. Index TermsÐAppearance models, deformable templates, model matching. 1
1013|Robust real-time face detection|We have constructed a frontal face detection system which achieves detection and false positive rates which are equivalent to the best published results [7, 5, 6, 4, 1]. This face detection system is most clearly distinguished from previous approaches in its ability to detect faces extremely rapidly. Operating on 384 by 288 pixel images, faces are detected at 15 frames per second on a conventional 700 MHz Intel Pentium III. In other face detection systems, auxiliary information, such as image differences in video sequences, or pixel color in color images, have been used to achieve high frame rates. Our system achieves high frame rates working only with the information present in a single grey scale image. These alternative sources of information can also be integrated with our system to achieve even higher frame rates.
1014|PCA-SIFT: A more distinctive representation for local image descriptors|Stable local feature detection and representation is a fundamental component of many image registration and object recognition algorithms. Mikolajczyk and Schmid [14] recently evaluated a variety of approaches and identified the SIFT [11] algorithm as being the most resistant to common image deformations. This paper examines (and improves upon) the local image descriptor used by SIFT. Like SIFT, our descriptors encode the salient aspects of the image gradient in the feature point&#039;s neighborhood; however, instead of using SIFT&#039;s smoothed weighted histograms, we apply Principal Components Analysis (PCA) to the normalized gradient patch. Our experiments demonstrate that the PCAbased local descriptors are more distinctive, more robust to image deformations, and more compact than the standard SIFT representation. We also present results showing that using these descriptors in an image retrieval application results in increased accuracy and faster matching.
1015|Support vector machines for multiple-instance learning|This paper presents two new formulations of multiple-instance learning as a maximum margin problem. The proposed extensions of the Support Vector Machine (SVM) learning approach lead to mixed integer quadratic programs that can be solved heuristically. Our generalization of SVMs makes a state-of-the-art classification technique, including non-linear classification via kernels, available to an area that up to now has been largely dominated by special purpose methods. We present experimental results on a pharmaceutical data set and on applications in automated image indexing and document categorization. 1
1016|Putting objects in perspective|Image understanding requires not only individually estimating elements of the visual world but also capturing the interplay among them. In this paper, we provide a framework for placing local object detection in the context of the overall 3D scene by modeling the interdependence of objects, surface orientations, and camera viewpoint. Most object detection methods consider all scales and locations in the image as equally likely. We show that with probabilistic estimates of 3D geometry, both in terms of surfaces and world coordinates, we can put objects into perspective and model the scale and location variance in the image. Our approach reflects the cyclical nature of the problem by allowing probabilistic object hypotheses to refine geometry and vice-versa. Our framework allows painless substitution of almost any object detector and is easily extended to include other aspects of image understanding. Our results confirm the benefits of our integrated approach. 1.
1017|Contextual Priming for Object Detection|There is general consensus that context can be a rich source of information about an object&#039;s identity, location and scale. In fact, the structure of many real-world scenes is governed by strong configurational rules akin to those that apply to a single object. Here we introduce a simple framework for modeling the relationship between context and object properties based on the correlation between the statistics of low-level features across the entire scene and the objects that it contains. The resulting scheme serves as an effective procedure for object priming, context driven focus of attention and automatic scale-selection on real-world scenes.
1018|Human Face Detection in Visual Scenes|We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates. This work was partially supported by a grant from Siemens Corporate Research, Inc., by the Department of the Army, Army Research Office under grant number DAAH04-94-G-0006, and by the Office of Naval Research under grant number N00014-95-1-0591. This work was started while Shumeet Balu...
1019|Multiple instance boosting for object detection|A good image object detection algorithm is accurate, fast, and does not require exact locations of objects in a training set. We can create such an object detector by taking the architecture of the Viola-Jones detector cascade and training it with a new variant of boosting that we call MIL-Boost. MILBoost uses cost functions from the Multiple Instance Learning literature combined with the AnyBoost framework. We adapt the feature selection criterion of MILBoost to optimize the performance of the Viola-Jones cascade. Experiments show that the detection rate is up to 1.6 times better using MILBoost. This increased detection rate shows the advantage of simultaneously learning the locations and scales of the objects in the training set along with the parameters of the classifier. 1
1020|Towards Automatic Discovery of Object Categories|We propose a method to learn heterogeneous models of object classes for visual recognition. The training images contain a preponderance of clutter and learning is unsupervised. Our models represent objects as probabilistic constellations of rigid parts (features). The variability within a class is represented by a joint probability density function on the shape of the constellation and the appearance of the parts. Our method automatically identifies distinctive features in the training set. The set of model parameters is then learned using expectation maximization (see the companion paper [11] for details). When trained on different, unlabeled and unsegmented views of a class of objects, each component of the mixture model can adapt to represent a subset of the views. Similarly, different component
1021|Graphical Templates For Model Registration|A new method of model registration is proposed using graphical templates. A graph of landmarks is chosen in the template image. All possible candidates for these landmarks are found in the data image using local operators. A dynamic programming algorithm on decomposable subgraphs of the template graph finds the optimal match to a subset of the candidate points in polynomial time. This combination of local operators to describe points of interest/landmarks and a graph to describe their geometric orientation in the plane, yields fast and precise matches of the model to the data, with no initialization required. Key words: Graphical templates, decomposable graphs, model registration, dynamic programming, image matching.  Research supported in part by The University of Chicago Block Fund, and ARO DAAL0392 -G-0322.  y Research supported in part by National Institutes of Health grant no. R01-GM46800  x1 Introduction In recent years there has been a growing interest in deformable models for ...
1022|Efficient Deformable Template Detection and Localization without User Initialization|A novel deformable template is presented which detects the boundary of an open hand in a grayscale image without initialization by the user. A dynamic programming algorithm enhanced by pruning techniques finds the hand contour in the image in as little as 19 seconds on a Pentium 150. The template is translation- and rotation-invariant and accomodates shape deformation, significant occlusion and background clutter, and the presence of multiple hands.  2  Symbols  Boldface letters, e.g. x, denote vectors.  P (xjy) denotes conditional probability of x given y.  p  a denotes the square root of a.  P  denotes summation.  Q  denotes repeated product.  R  denotes integration.  ? denotes &#034;perpendicular to.&#034;  !, ? denote less than and greater than, respectively.  rI(x) denotes the gradient of I with respect to x.  / denotes &#034;proportional to.&#034;   denotes &#034;approximately equal to.&#034;  argmax  x f(x) denotes the value of x that maximizes f(x).  f ? g denotes the convolution of f with g.   denotes the ...
1023|Efficient learning of relational object class models|We present an efficient method for learning part-based object class models. The models include part appearance, as well as location and scale relations between parts. The object class is generatively modeled using a simple Bayesian network with a central hidden node containing location and scale information, and nodes describing object parts. The model’s parameters, however, are optimized to reduce a loss function of the training error, as in discriminative methods. We show how boosting techniques can be extended to optimize the relational model proposed, with complexity linear in the number of parts and the number of features per image. This efficiency allows our method to learn relational models with many parts and features. The method has an advantage over purely generative and purely discriminative approaches, since the former are limited to a small number of parts and features, while the latter neglect geometrical relations between parts. Experimental results are described, using some bench-mark data sets and three sets of newly collected data, showing the relative merits of our method in recognition and localization tasks. 1
1024|Part-Based Statistical Models for Object Classification and Detection|We propose using simple mixture models to define a set of mid-level binary local features based on binary oriented edge input. The features capture natural local structures in the data and yield very high classification rates when used with a variety of classifiers trained on small training sets, exhibiting robustness to degradation with clutter. Of particular interest are the use of the features as variables in simple statistical models for the objects thus enabling likelihood based classification. Pre-training decision boundaries between classes, a necessary component of non-parametric techniques, is thus avoided. Class models are trained separately with no need to access data of other classes. Experimental results are presented for handwritten character recognition, classification of deformed L ATEX symbols involving hundreds of classes, and side view car detection. 1.
1025|Model-Based Analysis of Oligonucleotide Arrays: Model Validation, Design Issues and Standard Error Application|Background: A model-based analysis of oligonucleotide expression arrays we developed previously uses a probe-sensitivity index to capture the response characteristic of a specific probe pair and calculates model-based expression indexes (MBEI). MBEI has standard error attached to it as a measure of accuracy. Here we investigate the stability of the probe-sensitivity index across different tissue types, the reproducibility of results in replicate experiments, and the use of MBEI in perfect match (PM)-only arrays.  Results: Probe-sensitivity indexes are stable across tissue types. The target gene&#039;s presence in many arrays of an array set allows the probe-sensitivity index to be estimated accurately. We extended the model to obtain expression values for PM-only arrays, and found that the 20-probe PM-only model is comparable to the 10-probe PM/MM difference model, in terms of the expression correlations with the original 20-probe PM/MM difference model. MBEI method is able to extend the reliable detection limit of expression to a lower mRNA concentration. The standard errors of MBEI can be used to construct confidence intervals of fold changes, and the lower confidence bound of fold change is a better ranking statistic for filtering genes. We can assign reliability indexes for genes in a specific cluster of interest in hierarchical clustering by resampling clustering trees. A software dChip implementing many of these analysis methods is made available.  Conclusions: The model-based approach reduces the variability of low expression estimates, and provides a natural method of calculating expression values for PM-only arrays. The standard errors attached to expression values can be used to assess the reliability of downstream analysis.  Published: X Month 2001  Genome Biology...
1026|P.: A comparison of bayesian methods for haplotype reconstruction from population genotype data |In this report, we compare and contrast three previously published Bayesian methods for inferring haplotypes from genotype data in a population sample. We review the methods, emphasizing the differences between them in terms of both the models (“priors”) they use and the computational strategies they employ. We introduce a new algorithm that combines the modeling strategy of one method with the computational strategies of another. In comparisons using real and simulated data, this new algorithm outperforms all three existing methods. The new algorithm is included in the software package PHASE, version 2.0, available online
1027|Data Preparation for Mining World Wide Web Browsing Patterns|The World Wide Web (WWW) continues to grow at an astounding  rate in both the sheer volume of tra#c and the size and complexity  of Web sites. The complexity of tasks such as Web site design,  Web server design, and of simply navigating through a Web site have  increased along with this growth. An important input to these design  tasks is the analysis of how a Web site is being used. Usage analysis includes  straightforward statistics, such as page access frequency, as well as  more sophisticated forms of analysis, such as finding the common traversal  paths through a Web site. Web Usage Mining is the application of  data mining techniques to usage logs of large Web data repositories in  order to produce results that can be used in the design tasks mentioned  above. However, there are several preprocessing tasks that must be performed  prior to applying data mining algorithms to the data collected  from server logs. This paper presents several data preparation techniques  in order to identify unique users and user sessions. Also, a method to divide  user sessions into semantically meaningful transactions is defined  and successfully tested against two other methods. Transactions identified  by the proposed methods are used to discover association rules from  real world data using the WEBMINER system [15].
1028|Fast Algorithms for Mining Association Rules|We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database. 
1029|Mining Sequential Patterns: Generalizations and Performance Improvements|Abstract. The problem of mining sequential patterns was recently introduced in [3]. We are given a database of sequences, where each sequence is a list of transactions ordered by transaction-time, and each transaction is a set of items. The problem is to discover all sequential patterns with a user-speci ed minimum support, where the support of a pattern is the number of data-sequences that contain the pattern. An example of a sequential pattern is \5 % of customers bought `Foundation&#039; and `Ringworld &#039; in one transaction, followed by `Second Foundation &#039; in a later transaction&#034;. We generalize the problem as follows. First, we add time constraints that specify a minimum and/or maximum time period between adjacent elements in a pattern. Second, we relax the restriction that the items in an element of a sequential pattern must come from the same transaction, instead allowing the items to be present in a set of transactions whose transaction-times are within a user-speci ed time window. Third, given a user-de ned taxonomy (is-a hierarchy) on items, we allow sequential patterns to include items across all levels of the taxonomy. We present GSP, a new algorithm that discovers these generalized sequential patterns. Empirical evaluation using synthetic and real-life data indicates that GSP is much faster than the AprioriAll algorithm presented in [3]. GSP scales linearly with the number of data-sequences, and has very good scale-up properties with respect to the average datasequence size. 1
1030|Efficient and Effective Clustering Methods for Spatial Data Mining|Spatial data mining is the discovery of interesting relationships and characteristics that may exist implicitly in spatial databases. In this paper, we explore whether clustering methods have a role to play in spatial data mining. To this end, we develop a new clustering method called CLARANS which is based on randomized search. We also de- velop two spatial data mining algorithms that use CLARANS. Our analysis and experiments show that with the assistance of CLARANS, these two algorithms are very effective and can lead to discoveries that are difficult to find with current spatial data mining algorithms.
1032|Web Mining: Information and Pattern Discovery on the World Wide Web|Application of data mining techniques to the World Wide Web, referred to as Web mining, has been the focus of several recent research projects and papers. However, there is no established vocabulary, leading to confusion when comparing research efforts. The term Web mining has been used in two distinct ways. The first, called Web content mining in this paper, is the process of information discovery from sources across the World Wide Web. The second, called Web usage mining, is the process of mining for user browsing and access patterns. In this paper we define Web mining and present an overview of the various research issues, techniques, and development efforts. We briefly describe WEBMINER, a system for Web usage mining, and conclude this paper by listing research issues. 
1033|WebWatcher: A Tour Guide for the World Wide Web|We explore the notion of a tour guide software agent for assisting users browsing the World Wide Web. A Web tour guide agent provides assistance similar to that provided by ahuman tour guide in a museum -- it guides the user along an appropriate path through the collection, based on its knowledge of the user&#039;s interests, of the location and relevance of various items in the collection, and of the way in which others have interacted with the collection in the past. This paper describes a simple but operational tour guide, called Web-Watcher, which has given over 5000 tours to people browsing CMU&#039;s School of Computer Science Web pages. WebWatcher accompanies users from page to page, suggests appropriate hyperlinks, and learns from experience to improve its advice-giving skills. We describe the learning algorithms used by WebWatcher, experimental results showing their effectiveness, and lessons learned from this case study in Web tour guide agents.  
1034|Knowledge Discovery from Users Web-Page Navigation|We propose to detect users navigationpaths to the advantage of web-site owners. First, we explain the design and implementationof a profiler which captures client’s selected links and pages order, accurate page viewing time and cache references, using a Java based remote agent. The information captured by the profiler is then utilized by a knowledge discovery technique to cluster users with similar interests. We introduce a novel path clustering method based on the similarity of the history of user navigation. This approach is capable of capturing the interests of the user which could persist through several subsequent hypertext link selections. Finally, we evaluate our path clustering technique via a simulation study on a sample WWW-site. We show that depending on the level of inserted noise, we can recover the correct clusters by %10-%27 of average error margin. 1.
1035|Discovering Web Access Patterns and Trends by Applying OLAP and Data Mining Technology on Web Logs|As a con#uence of data mining and WWW technologies, it is now possible to perform data mining on web logrecords collectedfrom the Internet web page access history. The behaviour of the web page readers is imprinted in the web server log #les. Analyzing and exploring regularities in this behaviour can improve system performance, enhance the quality and delivery of Internet information services to the end user, and identify population of potential customers for electronic commerce. Thus, by observing people using collections of data, data mining can bring considerable contribution to digital library designers.
1036|Learning Information Retrieval Agents: Experiments with Automated Web Browsing|The current exponential growth of the Internet precipitates a need for new tools to help people cope with the volume of information. To complement recent work on creating searchable indexes of the World-Wide Web and systems for filtering incoming e-mail and Usenet news articles, we describe a system which helps users keep abreast of new and interesting information. Every day it presents a selection of interesting web pages. The user evaluates each page, and given this feedback the system adapts and attempts to produce better pages the following day. We present some early results from an AI programming class to whom this was set as a project, and then describe our current implementation. Over the course of 24 days the output of our system was compared to both randomly-selected and human-selected pages. It consistently performed better than the random pages, and was better than the human-selected pages half of the time.  Introduction  In recent years there has been a well-publicized expl...
1037|Data Mining for Path Traversal Patterns in a Web Environment|In this paper, we explore a new data mining capability which involves mining path traversal patterns in a distributed information providing environment like world-wide-web. First, we convert the original sequence of log data into a set of maximal forward references and filter out the effect of some backward references which are mainly made for ease of traveling. Second, we derive algorithms to determine the frequent traversal patterns, i.e., large reference sequences, from the maximal forward references obtained. Two algorithms are devised for determining large reference sequences: one is based on some hashing and pruning techniques, and the other is further improved with the option of determining large reference sequences in batch so as to reduce the number of database scans required. Performance of these two methods is comparatively analyzed.
1038|Web Mining: Pattern discovery from World Wide Web transactions|Web-based organizations often generate and collect large volumes of data in their daily operations. Analyzing such data can help these organizations to determine the life time value of clients, design cross marketing strategies across products and services, evaluate the e ectiveness of promotional campaigns, and nd the most e ective logical structure for their Web space. This type of analysis involves the discovery of meaningful relationships from a large collection of primarily unstructured data, often stored in Web server access logs. We propose a framework for Web mining, the applications of data mining and knowledge discovery techniques to data collected in World Wide Web transactions. We present data and transaction models for various Web mining tasks such as the discovery of association rules and sequential patterns from the Web data. We also present aWeb mining system, WEBMINER, which has been implemented based upon the proposed framework, and discuss our experimental results on real-world Web data using the WEBMINER.
1039|Learning from hotlists and coldlists: Towards a WWW information filtering and seeking agent|We describe a software agent that learns to find information on the World Wide Web (WWW), deciding what new pages might interest a user. The agent maintains a separate hotlist (for links that were interesting) and coldlist (for links that were not interesting) for each topic. By analyzing the information immediately accessible from each link, the agent learns the types of information the user is interested in. This can be used to inform the user when a new interesting page becomes available or to order the user&#039;s exploration of unseen existing links so that the more promising ones are investigated first. We compare four different learning algorithms on this task. We describe an experiment in which a simple Bayesian classifier acquires a user profile that agrees with a user&#039;s judgment over 90% of the time.
1040|The eyes have it: A task by data type taxonomy for information visualizations| A useful starting point for designing advanced graphical user interjaces is the Visual lnformation-Seeking Mantra: overview first, zoom and filter, then details on demand. But this is only a starting point in trying to understand the rich and varied set of information visualizations that have been proposed in recent years. This paper offers a task by data type taxonomy with seven data types (one-, two-, three-dimensional datu, temporal and multi-dimensional data, and tree and network data) and seven tasks (overview, Zoom, filter, details-on-demand, relate, history, and extracts). 
1041|Visual Information Seeking: Tight Coupling of Dynamic Query Filters with Starfield Displays|This paper offers new principles for visual information seeking (VIS). A key concept is to support browsing, which is distinguished from familiar query composition and information retrieval because of its emphasis on rapid filtering to reduce result sets, progressive refinement of search parameters, continuous reformulation of goals, and visual scanning to identify results. VIS principles developed include: dynamic query filters (query parameters are rapidly adjusted with sliders, buttons, maps, etc.), starfield displays (two-dimensional scatterplots to structure result sets and zooming to reduce clutter), and tight coupling (interrelating query components to preserve display invariants and support progressive refinement combined with an emphasis on using search output to foster search input). A FilmFinder prototype using a movie database demonstrates these principles in a VIS environment.
1042|Tree visualization with Tree-maps: A 2-d space-filling approach|this paper deals with a two-dimensional (2-d) space-filling approach in which each node is a rectangle whose area is proportional to some attribute such as node size. Research on relationships between 2-d images and their representation in tree structures has focussed on node and link representations of 2-d images. This work includes quad-trees (Samet, 1989) and their variants which are important in image processing. The goal of quad trees is to provide a tree representation for storage compression and efficient operations on bit-mapped images. XY-trees (Nagy &amp; Seth, 1984) are a traditional tree representation of two-dimensional layouts found in newspaper, magazine, or book pages. Related concepts include k-d trees (Bentley and Freidman, 1979), which are often explained with the help of a
1043|Treemaps: a space-filling approach to the visualization of hierarchical information structures|This paper describes a novel methodfor the visualization of hierarchically structured information. The Tree-Map visualization technique makes 100 % use of the available display space, mapping the full hierarchy onto a rectangular region in a space-filling manner. This efficient use of space allows very large hierarchies to be displayed in their entirety and facilitates the presentation of semantic information. 1
1044|The Table Lens: Merging Graphical and Symbolic Representations in an Interactive Focus+Context Visualization for Tabular Information|We present a new visualization, called the Table Lens, for visualizing and making sense of large tables. The visualization uses a focus context (fisheye) technique that works effectively on tabular information because it allows display of crucial label information and multiple distal focal areas. In addition, a graphical mapping scheme for depicting table contents has been developed for the most widespread kind of tables, the cases-by-variables table. The Table Lens fuses symbolic and graphical representations into a single coherent view that can be fluidly adjusted by the user. This fusion and interactivity enables an extremely rich and natural style of direct manipulation exploratory data analysis.
1045|Dynamic Queries for Information Exploration: An Implementation and Evaluation|We designed, implemented and evaluated a new concept for direct manipulation of databases, called dynamic queries, that allows users to formulate queries with graphical widgets, such as sliders. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. Eighteen undergraduate chemistry students performed statistically significantly faster usingadynamicqueries interface compared to two interfaces both providing form fillin as input method, one with graphical visualization output and one with all-textual output. The interfaces were used to expore the periodic table of elements and search on their properties. 1. INTRODUCTION Mostdatabasesystems require the user to create andformulate a complex query, whichpresumes that the user is familiar with the logical structure of the database [4]. The queries on a database are usually expressed in high level query languages (such as SQL,QUEL). This works well for many applications, but it ...
1046|Visualizing the non-visual: spatial analysis and interaction with information from test documents|This paper describes an approach to IV that involves spatializing text content for enhanced visual browsing and analysis. The application arena is large text document corpora such as digital libraries, regulations andprocedures, archived reports, etc. The basic idea is that text content from these sources may be transformed to a spatial representation that preserves informational characteristics from the documents. The spatial representation may then be visually browsed and analyzed in ways that avoid language processing and that reduce the analysts’ mental workload. The result is an interaction with text that more nearly resembles perception and action with the natural world than with the abstractions of written language. 1
1047|Dynamic Queries for Visual Information Seeking|Dynamic queries are a novel approach to information seeking that may enable users to cope with information overload. They allow users to see an overview of the database, rapidly (100 msec updates) explore and conveniently filter out unwanted information. Users fly through information spaces by incrementally adjusting a query (with sliders, buttons, and other filters) while continuously viewing the changing results. Dynamic queries on the chemical table of elements, computer directories, and a real estate database were built and tested in three separate exploratory experiments. These results show statistically significant performance improvements and user enthusiasm more commonly seen with video games. Widespread application seems possible but research issues remain in database and display algorithms, and user interface design. Challenges include methods for rapidly displaying and changing many points, colors, and areas; multidimensional pointing; incorporation of sound and visual displ...
1048|LifeLines: Visualizing Personal Histories|LifeLines provide a general visualization environment for personal histories that can be applied to medical and court records, professional histories and other types of biographical data. A one screen overview shows multiple facets of the records. Aspects, for example medical conditions or legal cases, are displayed as individual time lines, while icons indicate discrete events, such as physician consultations or legal reviews. Line color and thickness illustrate relationships or significance, rescaling tools and filters allow users to focus on part of the information. LifeLines reduce the chances of missing information, facilitate spotting anomalies and trends, streamline access to details, while remaining tailorable and easily transferable between applications. The paper describes the use of LifeLines for youth records of the Maryland Department of Juvenile Justice and also for medical records. User&#039;s feedback was collected using a Visual Basic prototype for the youth record. Techniq...
1049|Graphical Fisheye Views|A fisheye camera lens is a very wide angle lens that magnifies nearby objects while shrinking distant objects. It is a valuable tool for seeing both &#034;local detail&#034; and &#034;global context&#034; simultaneously. This paper describes a system for viewing and browsing graphs using a software analog of a fisheye lens. We first show how to implement such a view using solely geometric transformations. We then describe a more general transformation that allows global information about the graph to affect the view. Our general transformation is a fundamental extension to previous research in fisheye views.  
1050|The dynamic HomeFinder: evaluating dynamic queries in a real-estate information exploration system|We designed, implemented, and evaluated a new concept for visualizing and searching databases utilizing direct manipulation called dynarruc queries. Dynamic queries allow users to formulate queries by adjusting graphical widgets, such as sliders, and see the results immediately. By providing a graphical visualization of the database and search results, users can find trends and exceptions easily. User testing was done with eighteen undergraduate students who performed significantly faster using a dynamic queries interface compared to both a natural language system and paper printouts. The interfaces were used to explore a real-estate database and find homes meeting specific search criteria. 1
1051|InfoCrystal: a visual tool for information retrieval|This paper introduces a novel representation, called the I n f o C r y s t a l T M,  that can be used as a v isual i za t ion tool as well as a visual query lan-g u a g e to help users search for information. The lnfoCrysta1 visualizes all the possible relation-ships among N concepts. Users can assign relevance weights to the concepts and use thresholding to select relationships of interest. The lnfocrystal allows users to specify Boolean as well as v e c t o r-s p a c e queries graphically. Arbitrarily complex queries can be created by using the 1nfoCrystals as building blocks and organizing them in a hierarchical structure. The 1nfoCrystal enables users to explore and filter information in a flexible, dynamic and interactive way.
1052|IVEE: An Information Visualization &amp; Exploration Environment|The Information Visualization and Exploration Environment (IVEE) is a system for automatic creation of dynamic queries applications. IVEE imports database relations and automatically creates environments holding visualizations and query devices. IVEE offers multiple visualizations such as maps and starfields, and multiple query devices, such as sliders, alphasliders, and toggles. Arbitrary graphical objects can be attached to database objects in visualizations. Multiple visualizations may be active simultaneously. Users can interactively lay out and change between types of query devices. Users may retrieve details-on-demand by clicking on visualization objects. An HTML file may be provided along with the database, specifying how details-ondemand information should be presented, allowing for presentation of multimedia information in database objects. Finally, multiple IVEE clients running on separate workstations on a network can communicate by letting one users actions affect the visua...
1053|The Information Mural: A Technique for Displaying and Navigating Large Information Spaces|Abstract—Information visualizations must allow users to browse information spaces and focus quickly on items of interest. Being able to see some representation of the entire information space provides an initial gestalt overview and gives context to support browsing and search tasks. However, the limited number of pixels on the screen constrain the information bandwidth and make it difficult to completely display large information spaces. The Information Mural is a two-dimensional, reduced representation of an entire information space that fits entirely within a display window or screen. The Mural creates a miniature version of the information space using visual attributes, such as gray-scale shading, intensity, color, and pixel size, along with antialiased compression techniques. Information Murals can be used as stand-alone visualizations or in global navigational views. We have built several prototypes to demonstrate the use of Information Murals in visualization applications; subject matter for these views includes computer software, scientific data, text documents, and geographic information. Index Terms—Information visualization, software visualization, data visualization, focus+context, navigation, browsers. 1 INFORMATION MURALS
1054|Visdb: Database exploration using multidimensional visualization|In this paper we describe the VisDB system, which allows an exploration of large databases using visualization techniques. The goal of the system is to support the query specification process by using each pixel of the display to represent one data item of the database. By arranging and coloring the pixels according to the relevance of the data items with respect to the query, the user gets a visual impression of the resulting data set. Using sliders for each condition of the query, the user may change the query dynamically and receives immediate feedback from the visual representation of the resulting data set. Different visualization techniques are available for different stages of exploration. The first technique uses multiple windows for the different query parts, providing visual feedback for each part of the query and helping the user to understand the overall result. The second technique is an extension of the first one, providing additional information by assigning two dimensions to the axes. The third technique uses a grouping of dimensions and is designed to support a focused search on smaller data sets.
1055|The Alphaslider: A Compact and Rapid Selector|Research has suggested that rapid, serial, visual presentation of text (RSVP) may be an effective way to scan and search through lists of text strings in search of words, names, etc. The Alphaslider widget employs RSVP as a method for rapidly scanning and searching lists or menus in a graphical user interface environment. The Alphaslider only uses an area less than 7 cm x 2.5 cm. The tiny size of the Alphaslider allows it to be placed on a credit card, on a control panel for a VCR, or as a widget in a direct manipulation based database interface. An experiment was conducted with four Alphaslider designs which showed that novice Alphaslider users could locate one item in a list of 10,000 film titles in 24 seconds on average, an expert user in about 13 seconds.  KEYWORDS: Alphaslider, widget, selection technology, menus, dynamic queries  INTRODUCTION  Selecting items from lists is a common task in today&#039;s society. New and exciting applications for selection technology are credit card siz...
1056|The Continuous Zoom: A Constrained Fisheye Technique for Viewing and Navigating Large Information Spaces|Navigating and viewing large information spaces, such as hierarchically-organized networks from complex realtime systems, suffer the problems of viewing a large space on a small screen. Distorted-view approaches, such as fisheye techniques, have great potential to reduce these problems by representing detail within its larger context but introduce new issues of focus, transition between views and user disorientation from excessive distortion. We present a fisheyebased method which supports multiple focus points, enhances continuity through smooth transitions between views, and maintains location constraints to reduce the user’s sense of spatial disorientation. These are important requirements for the representation and navigation of networked systems in supervisory control applications. The method consists of two steps: a global allocation of space to rectangular sections of the display, based on scale factors, followed by degree-of-interest adjustments. Previous versions of the algorithm relied solely on relative scale factors to assign size; we present a new version which allocates space more efficiently using a dynamically calculated degree of interest. In addition to the automatic system sizing, manual user control over the amount of space assigned each area is supported. The amount of detail shown in various parts of the network is controlled by pruning the hierarchy and presenting those sections in summary form. KEYWORDS: graphical user interface, supervisory control systems, information space, hierarchical network, information visualization, fisheye view, navigation.
1057|Enhanced Dynamic Queries via Movable Filters|Traditional database query systems allow users to construct complicated database queries from specialized database language primitives. While powerful and expressive, such systems are not easy to use, especially for browsing or exploring the data. Information visualization systems address this problem by providing graphical presentations of the data and direct manipulation tools for exploring the data. Recent work in this area has reported the value of dynamic queries coupled with two-dimensional data representations for progressive refinement of user queries. However, the queries generated by these systems are limited to conjunctions of global ranges of parameter values. In this paper, we extend dynamic queries by encoding each operand of the query as a Magic Lens filter. Compound queries can be constructed by overlapping the lenses. Each lens includes a slider and a set of buttons to control the value of the filter function and to define the compostion operation generated by overlapp...
1058|Navigating Large Networks with Hierarchies|This paper is aimed at the exploratory visualization of networks where there is a strength or weight associated with each link, and makes use of any hierarchy present on the nodes to aid the investigation of large networks. It describes a method of placing nodes on the plane that gives meaning to their relative positions. The paper discusses how linking and interaction principles aid the user in the exploration. Two examples are given; one of electronic mail communication over eight months within a department, another concerned with changes to a large section of a computer program. I. THE PROBLEM It has almost become a clichŽ to start a paper with the observation that the amount of data in the world is growing rapidly, and that current efforts to extract useful information from data lag far behind the ability to create data. However the clichŽ is true, and no less so in the field of network analysis and visualization than in any other. In many areas, scientists are realizing that the tools they have been using are limited in utility when applied to large, information-rich networks. Not only are networks of interest large in terms of size (as measured by number of nodes or links between nodes), but also in terms of the data collected for each node or link. The ability to examine statistics on the nodes and relate them to the network is of crucial importance. Examples of areas in which the analysis of large networks is important include: i. Trade flows. The concern in this area is monitoring imports and exports of various products at several levels; international, interstate and local. Besides examining many types of trade goods, there is also strong interest in spotting temporal patterns. ii. Communication networks. This is an important and wide category, covering not only telecommunication networks, but also electronic mail (email), financial transaction, ATM/bank data transferal and other data distribution networks.
1059|Using Aggregation and Dynamic Queries for Exploring Large Data Sets|When working with large data sets, users perform three primary types of activities: data manipulation, data analysis, and data visualization. The data manipulation process involves the selection and transformation of data prior to viewing. This paper addresses user goals for this process and the interactive interface mechanisms that support them. We consider three classes of data manipulation goals: controlling the scope (selecting the desired portion of the data), selecting the focus of attention (concentrating on the attributes of data that are relevant to current analysis), and choosing the level of detail (creating and decomposing aggregates of data). We use this classification to evaluate the functionality of existing data exploration interface techniques. Based on these results, we have expanded an interface mechanism called the Aggregate Manipulator (AM) and combined it with Dynamic Query (DQ) to provide complete coverage of the data manipulation goals. We use real estate sales data to demonstrate how the AM and DQ synergistically function in our interface.
1060|Interacting with Huge Hierarchies: Beyond Cone Trees|This paper describes an implementation of a tool for visualizing and interacting with huge information hierarchies. Existing systems for visualizing huge hierarchies using cone trees &#034;break down&#034; once the hierarchy to be displayed exceeds roughly 1000 nodes, due to increasing visual clutter. This paper describes a system called fsviz which visualizes arbitrarily large hierarchies while retaining user control. This is accomplished by augmenting cone trees with several graphical and interaction techniques: usage-based filtering, animated zooming, handcoupled rotation, fish-eye zooming, coalescing of nodes, texturing, effective use of colour for depth cueing, and the applications of dynamic queries. The fsviz system also improves upon earlier cone tree visualization systems through a more elaborate node layout algorithm. This algorithm enhances the usefulness of cone tree visualization for large hierarchies by all but eliminating clutter.  Keywords: Information Visualization, Information ...
1061|Using treemaps to visualize the Analytic Hierarchy Process|this article. References
1062|Exploratory Access to Geographic Data Based on the Map-Overlay Metaphor|Many geographic information systems (GISs) attempt to imitate the manual process of laying transparent map layers over one another on a light table and analyzing the resulting configurations. While this map-overlay metaphor, familiar to many geo-scientists, has been used as a design principle for the underlying architecture of GISs, it has not yet been visually manifested at the user interface. To overcome this shortage, a new direct manipulation user interface for overlay-based GISs has been designed and prototyped. It is characterized by the separation of map layers into data cubes and map templates such that different thematic data can be combined and the same kind of data can be displayed in different formats. This paper introduces the conceptual objects that the user manipulates at the screen surface and discusses ways to visualize effectively the objects and operations upon them.
1063|Visualizing Network Data|Networks are critical to modern society, and a thorough understanding of how they behave is crucial to their efficient operation. Fortunately, data on networks is plentiful; by visualizing this data, it is possible to greatly improve our understanding. Our focus is on visualizing the data associated with a network and not on simply visualizing the structure of the network itself. We begin with three static network displays; two of these use geographical relationships, while the third is a matrix arrangement that gives equal emphasis to all network links. Static displays can be swamped with large amounts of data; hence we introduce directmanipulation techniques that permit the graphs to continue to reveal relationships in the context of much more data. In effect, the static displays are parameterized so that interesting views may easily be discovered interactively. The software to carry out this network visualization is called SeeNet.  1. INTRODUCTION  We are currently in the midst of a...
1064|Stochastic volatility: likelihood inference and comparison with ARCH models|In this paper, Markov chain Monte Carlo sampling methods are exploited to provide a unified, practical likelihood-based framework for the analysis of stochastic volatility models. A highly effective method is developed that samples all the unobserved volatilities at once using an approximating offset mixture model, followed by an importance reweighting procedure. This approach is compared with several alternative methods using real data. The paper also develops simulation-based methods for filtering, likelihood evaluation and model failure diagnostics. The issue of model choice using non-nested likelihood ratios and Bayes factors is also investigated. These methods are used to compare the fit of stochastic volatility and GARCH models. All the procedures are illustrated in detail. 1.
1066|Markov chains for exploring posterior distributions|Your use of the JSTOR archive indicates your acceptance of JSTOR&#039;s Terms and Conditions of Use, available at
1069|Contour Tracking By Stochastic Propagation of Conditional Density|.  In Proc. European Conf. Computer Vision, 1996, pp. 343--356, Cambridge, UK  The problem of tracking curves in dense visual clutter is  a challenging one. Trackers based on Kalman filters are of limited use;  because they are based on Gaussian densities which are unimodal, they  cannot represent simultaneous alternative hypotheses. Extensions to the  Kalman filter to handle multiple data associations work satisfactorily in  the simple case of point targets, but do not extend naturally to continuous  curves. A new, stochastic algorithm is proposed here, the Condensation   algorithm --- Conditional Density Propagation over time. It  uses `factored sampling&#039;, a method previously applied to interpretation  of static images, in which the distribution of possible interpretations is  represented by a randomly generated set of representatives. The Condensation   algorithm combines factored sampling with learned dynamical  models to propagate an entire probability distribution for object  pos...
1070|Bayesian Analysis of Stochastic Volatility Models|this article is to develop new methods for inference and prediction in a simple class of stochastic volatility models in which logarithm of conditional volatility follows an autoregressive (AR) times series model. Unlike the autoregressive conditional heteroscedasticity (ARCH) and gener- alized ARCH (GARCH) models [see Bollerslev, Chou, and Kroner (1992) for a survey of ARCH modeling], both the mean and log-volatility equations have separate error terms. The ease of evaluating the ARCH likelihood function and the ability of the ARCH specification to accommodate the timevarying volatility found in many economic time series has fostered an explosion in the use of ARCH models. On the other hand, the likelihood function for stochastic volatility models is difficult to evaluate, and hence these models have had limited empirical application
1073|Simulated Moments Estimator of Markov Models of Asset Prices|This paper provides a simulated moments estimator (SME) of the parameters of dynamic models in which the state vector follows a time-homogeneous Markov process. Conditions are provided for both weak and strong consistency as well as asymptotic normality. Various tradeoffs among the regularity conditions underlying the large sample properties of the SME are discussed in the context of an asset-pricing model.
1074|Stock Prices and Volume|We undertake a comprehensive investigation of price and volume co-movement using daily New York Stock Exchange data from 1928 to 1987. We adjust the data to take into account well-known calendar effects and long-run trends. To describt tbe process, we use a seminonparametric estimate of the joint density of current price change and volume conditional on past price changes and volume. Four empirical regularities are found: 1) positive correlation between conditional volatility and volume, 2) large price movements are followed by high volume, 3) conditioning on lagged volume substantially attenuates the &#034;leverage &#034; effect, and 4) after conditioning on lagged volume, there is a positive risk/return relation.  
1075|An exact likelihood analysis of the multinomial probit model|We develop new methods for conducting a finite sample, likelihood-based analysis of the multinomial probit model. Using a variant of the Gibbs sampler, an algorithm is developed to draw from the exact posterior of the multinomial probit model with correlated errors. This approach avoids direct evaluation of the likelihood and, thus, avoids the problems associated with calculating choice probabilities which affect both the standard likelihood and method of simulated moments approaches. Both simulated and actual consumer panel data are used to fit six-dimensional choice models. We also develop methods for analyzing random coefficient and multiperiod probit models.
1076|Markov Chain Monte Carlo Simulation Methods in Econometrics|We present several Markov chain Monte Carlo simulation methods that have been widely used in recent years in econometrics and statistics. Among these is the Gibbs sampler, which has been of particular interest to econometricians. Although the paper summarizes some of the relevant theoretical literature, its emphasis is on the presentation and explanation of applications to important models that are studied in econometrics. We include a discussion of some implementation issues, the use of the methods in connection with the EM algorithm, and how the methods can be helpful in model specification questions. Many of the applications of these methods are of particular interest to Bayesians, but we also point out ways in which frequentist statisticians may find the techniques useful.
1077|inference via Gibbs sampling of autoregressive time series subject to Markov mean and variance shifts|We examine autoregressive time series models that are subject to regime switching. These shifts are determined by the outcome of an unobserved two-state indicator variable that follows a Markov process with unknown transition probabilities. A Bayesian framework is developed in which the unobserved states, one for each time point, are treated as missing data and then analyzed via the simulation tool of Gibbs sampling. This method is expedient because the conditional posterior distribution f the parameters, given the states, and the conditional posterior distribution of the states, given the parameters, all have a form amenable to Monte Carlo sampling. The approach is straightforward and generates marginal posterior distributions for all parameters of interest. Posterior distributions of the states, future observations, and the residuals, averaged over the parameter space are also obtained. Several examples with real and artificial data sets and weak prior information illustrate the usefulness of the methodology.
1078|Estimation of Stochastic Volatility Models with Diagnostics|Efficient Method of Moments (EMM) is used to fit the standard stochastic volatility model and various extensions to several daily financial time series. EMM matches to the score of a model determined by data analysis called the score generator. Discrepancies reveal characteristics of data that stochastic volatility models cannot approximate. The two score generators employed here are &#034;Semiparametric ARCH&#034; and &#034;Nonlinear Nonparametric&#034;. With the first, the standard model is rejected, although some extensions are accepted. With the second, all versions are rejected. The extensions required for an adequate fit are so elaborate that nonparametric specifications are probably more convenient.   Corresponding author: George Tauchen, Duke University, Department of Economics, Social Science Building, Box 90097, Durham NC 27708-0097 USA, phone 1-919-660-1812, FAX 1-919-684-8974, e-mail get@tauchen.econ.duke.edu. 0  1 Introduction  The stochastic volatility model has been proposed as a descripti...
1079|Bayes inference in the Tobit censored regression model|We consider the Bayes estimation of the Tobit censored regression model with normally distributed errors. A simple condition for the existence of posterior moments is provided. Suitable versions of Monte Carlo procedures based on symmetric multivariate-t distributions, and Laplacian approximations in a certain parametrization, are developed and illustrated. Ideas involving data augmentation and Gibbs sampling [cf. Tanner and Wong (1987) and Gelfand and Smith (1990)] are also developed. The methods are compared in two examples with diffuse priors, and various combinations of sample sizes and degrees of censoring. 
1080|Bayesian comparison of econometric models|This paper integrates and extends some recent computational advances in Bayesian inference with the objective of more fully realizing the Bayesian promise of coherent inference and model comparison in economics. It combines Markov chain Monte Carlo and independence Monte Carlo with importance sampling to provide an efficient and generic method for updating posterior distributions. It exploits the multiplicative decomposition of marginalized likelihood into predictive factors, to compute posterior odds ratios efficiently and with minimal further investment in software. It argues for the use of predictive odds ratios in model comparison in economics. Finally, it suggests procedures for public reporting that will enable remote clients to conveniently modify priors, form posterior expectations of their own functions of interest, and update the posterior distribution with new observations. A series of examples explores the practicality and efficiency of these methods. 
1081|Bayes Regression with Autoregressive Errors: A Gibbs Sampling Approach|This paper develops a practical framework for the Bayesian analysis of Gaussian and Student-c regression models with autocorrelated errors. As is customary in classical estimation procedures, the posteriors are conditioned on the initial observations. Recourse is taken to the method of Gibbs sampling, an iterative Markovian sampling method, and it is shown that the proposed approach can readily deal with high-order autoregressive processes without requiring an importance sampling function or other tuning constants. Several examples, including one with AR(4) errors, are used to illustrate the ideas. 1.
1082|Dryad: Distributed Data-Parallel Programs from Sequential Building Blocks|Dryad is a general-purpose distributed execution engine for coarse-grain data-parallel applications. A Dryad applica-tion combines computational “vertices ” with communica-tion “channels ” to form a dataflow graph. Dryad runs the application by executing the vertices of this graph on a set of available computers, communicating as appropriate through files, TCP pipes, and shared-memory FIFOs. The vertices provided by the application developer are quite simple and are usually written as sequential programs with no thread creation or locking. Concurrency arises from Dryad scheduling vertices to run simultaneously on multi-ple computers, or on multiple CPU cores within a computer. The application can discover the size and placement of data at run time, and modify the graph as the computation pro-gresses to make efficient use of the available resources. Dryad is designed to scale from powerful multi-core sin-gle computers, through small clusters of computers, to data centers with thousands of computers. The Dryad execution engine handles all the difficult problems of creating a large distributed, concurrent application: scheduling the use of computers and their CPUs, recovering from communication or computer failures, and transporting data between ver-tices.
1083|The click modular router| Click is a new software architecture for building flexible and configurable routers. A Click router is assembled from packet processing modules called elements. Individual elements implement simple router functions like packet classification, queueing, scheduling, and interfacing with network devices. A router configuration is a directed graph with elements at the vertices; packets flow along the edges of the graph. Configurations are written in a declarative language that supports user-defined abstractions. This language is both readable by humans and easily manipulated by tools. We present language tools that optimize router configurations and ensure they satisfy simple invariants. Due to Click’s architecture and language, Click router configurations are modular and easy to extend. A standards-compliant Click IP router has sixteen elements on its forwarding path. We present extensions to this router that support dropping policies, fairness among flows, quality-of-service, and
1084|PVM: A Framework for Parallel Distributed Computing|The PVM system is a programming environment for the development and execution of large concurrent or parallel applications that consist of many interacting, but relatively independent, components. It is intended to operate on a collection of heterogeneous computing elements interconnected by one or more networks. The participating processors may be scalar machines, multiprocessors, or special-purpose computers, enabling application components to execute on the architecture most appropriate to the algorithm. PVM provides a straightforward and general interface that permits the description of various types of algorithms (and their interactions), while the underlying infrastructure permits the execution of applications on a virtual computing environment that supports multiple parallel computation models. PVM contains facilities for concurrent, sequential, or conditional execution of application components, is portable to a variety of architectures, and supports certain forms of error dete...
1085|Cilk: An Efficient Multithreaded Runtime System|Cilk (pronounced &#034;silk&#034;) is a C-based runtime system for multithreaded parallel programming. In this paper, we document the efficiency of the Cilk work-stealing scheduler, both empirically and analytically. We show that on real and synthetic applications, the &#034;work&#034; and &#034;critical-path length&#034; of a Cilk computation can be used to model performance accurately. Consequently, a Cilk programmer can focus on reducing the computation&#039;s work and critical-path length, insulated from load balancing and other runtime scheduling issues. We also prove that for the class of &#034;fully strict&#034; (well-structured) programs, the Cilk scheduler achieves space, time, and communication bounds all within a constant factor of optimal. The Cilk
1087|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
1088|Programmable stream processors|The Imagine Stream Processor is a single-chip programmable media processor with 48 parallel ALUs. At 400 MHz, this translates to a peak arithmetic rate of 16 GFLOPS on single-precision data and 32 GOPS on 16bit fixed-point data. The scalability of Imagine’s programming model and architecture enable it to achieve such high arithmetic rates. Imagine executes applications that have been mapped to the stream programming model. The stream model decomposes applications into a set of computation kernels that operate on data streams. This mapping exposes the inherent locality and parallelism in the application, and Imagine exploits the locality and parallelism to provide a scalable architecture that supports 48 ALUs on a single chip. This paper presents the Imagine architecture and programming model in the first half, and explores the scalability of the Imagine architecture in the second half. 1.
1089|Accelerator: using data parallelism to program GPUs for general-purpose uses|GPUs are difficult to program for general-purpose uses. Programmers can either learn graphics APIs and convert their applications to use graphics pipeline operations or they can use stream programming abstractions of GPUs. We describe Accelerator, a system that uses data parallelism to program GPUs for general-purpose uses instead. Programmers use a conventional imperative programming language and a library that provides only high-level data-parallel operations. No aspects of GPUs are exposed to programmers. The library implementation compiles the data-parallel operations on the fly to optimized GPU pixel shader code and API calls. We describe the compilation techniques used to do this. We evaluate the effectiveness of using data parallelism to program GPUs by providing results for a set of compute-intensive benchmarks. We compare the performance of Accelerator versions of the benchmarks against hand-written pixel shaders. The speeds of the Accelerator versions are typically within 50 % of the speeds of hand-written pixel shader code. Some benchmarks significantly outperform C versions on a CPU: they are up to 18 times faster than C code running on a CPU.
1090|Using Cohort Scheduling to Enhance Server Performance|A server application is commonly organized as a collection of concurrent threads, each of which executes the code necessary to process a request. This software architecture, which causes frequent control transfers between unrelated pieces of code, decreases instruction and data locality, and consequently reduces the effec- tiveness of hardware mechanisms such as caches, TLBs, and branch predictors. Numerous measurements demonstrate this effect in server applications, which often utilize only a fraction of a modern processor&#039;s computational throughput.
1091|The CODE 2.0 Graphical Parallel Programming Language |CODE 2.0 is a graphical parallel programming system that targets the three goals of ease of use, portability, and production of efficient parallel code. Ease of use is provided by an integrated graphical/textual interface, a powerful dynamic model of parallel computation, and an integrated concept of program component reuse. Portability is approached by the declarative expression of synchronization and communication operators at a high level of abstraction in a manner which cleanly separates overall computation structure from the primitive sequential computations that make up a program. Execution efficiency is approached through a systematic class hierarchy that supports hierarchical translation refinement including special case recognition. This paper reports results obtained through experimental use of a prototype implementation of the CODE 2.0 system. CODE 2.0 represents a major conceptual advance over its predecessor systems (CODE 1.0 and CODE 1.2) in terms of the expressive power ...
1092|Stream Computations Organized for Reconfigurable Execution (SCORE): Introduction and Tutorial  (2000) |A primary impediment to wide-spread exploitation of reconfigurable computing is the lack of a unifying computational model which allows application portability and longevity without sacrificing a substantial fraction of the raw capabilities. We introduce SCORE (Stream Computation Organized for Reconfigurable Execution), a streambased compute model which virtualizes reconfigurable computing resources (compute, storage, and communication) by dividing a computation up into fixed-size &#034;pages&#034; and time-multiplexing the virtual pages on available physical hardware. Consequently, SCORE applications can scale up or down automatically to exploit a wide range of hardware sizes. We hypothesize that the SCORE model will ease development and deployment of reconfigurable applications and expand the range of applications which can benefit from reconfigurable execution. Further, we believe that a well engineered SCORE implementation can be efficient, wasting little of the capabilities of the raw hardw...
1093|Paralex: An Environment for Parallel Programming in Distributed Systems|Modern distributed systems consisting of powerful workstations and high-speed interconnection networks are an economical alternative to special-purpose super computers. The technical issues that need to be addressed in exploiting the parallelism inherent in a distributed system include heterogeneity, high-latency communication, fault tolerance and dynamic load balancing. Current software systems for parallel programming provide little or no automatic support towards these issues and require users to be experts in fault-tolerant distributed computing. The Paralex system is aimed at exploring the extent to which the parallel application programmer can be liberated from the complexities of distributed systems. Paralex is a complete programming environment and makes extensive use of graphics to define, edit, execute and debug parallel scientific applications. All of the necessary code for distributing the computation across a network and replicating it to achieve fault tolerance and dynamic load balancing is automatically generated by the system. In this paper we give an overview of Paralex and present our experiences with a prototype implementation.
1094|Parallel and Distributed Haskells|Parallel and distributed languages specify computations on multiple processors and have a computation language to describe the algorithm, i.e. what to compute, and a coordination language to describe how to organise the computations across the processors. Haskell has been used as the computation language for a wide variety of parallel and distributed languages, and this paper is a comprehensive survey of implemented languages. We outline parallel and distributed language concepts and classify Haskell extensions using them. Similar example programs are used to illustrate and contrast the coordination languages, and the comparison is facilitated by the common computation language. A lazy language is not an obvious choice for parallel or distributed computation, and we address the question of why Haskell is a common functional computation language.
1095|Highly Available, Fault-Tolerant, Parallel Dataflows|We present a technique that masks failures in a cluster to provide high availability and fault-tolerance for long-running, parallelized dataflows. We can use these dataflows to implement a variety of continuous query (CQ) applications that require high-throughput, 24x7 operation. Examples include network monitoring, phone call processing, click-stream processing, and online financial analysis. Our main contribution is a scheme that carefully integrates traditional query processing techniques for partitioned parallelism with the process-pairs approach for high availability. This delicate integration allows us to tolerate failures of portions of a parallel dataflow  without sacrificing result quality. Upon failure, our technique provides quick fail-over, and automatically recovers the lost pieces on the fly. This piecemeal recovery provides minimal disruption to the ongoing dataflow computation and improved reliability as compared to the straight-forward application of the process-pairs technique on a per dataflow basis. Thus, our technique provides the high availability necessary for critical CQ applications. Our techniques are encapsulated in a reusable dataflow operator called Flux, an extension of the Exchange that is used to compose parallel dataflows. Encapsulating the fault-tolerance logic into Flux minimizes modifications to existing operator code and relieves the burden on the operator writer of repeatedly implementing and verifying this critical logic. We present experiments illustrating these features with an implementation of Flux in the TelegraphCQ code base [8].
1096|A comparison of stream-oriented high availability algorithms|Recently, significant efforts have focused on developing novel data-processing systems to support a new class of applications that commonly require sophisticated and timely processing of high-volume data streams. Early work in stream processing has primarily focused on streamoriented languages and resource-constrained, one-pass query-processing. High availability, an increasingly important goal for virtually all data processing systems, is yet to be addressed. In this paper, we first describe how the standard highavailability approaches used in data management systems can be applied to distributed stream processing. We then propose a novel stream-oriented approach that exploits the unique data-flow nature of streaming systems. Using analysis and a detailed simulation study, we characterize the performance of each approach and demonstrate that the stream-oriented algorithm significantly reduces runtime overhead at the expense of a small increase in recovery time. 1
1097|Dynamic Itemset Counting and Implication Rules for Market Basket Data|We consider the problem of analyzing market-basket data and present several important contributions. First, we present a new algorithm for finding large itemsets which uses fewer passes over the data than classic algorithms, and yet uses fewer candidate itemsets than methods based on sampling. We investigate the idea of item reordering, which can improve the low-level efficiency of the algorithm. Second, we present a new way of generating &#034;implication rules,&#034; which are normalized based on both the antecedent and the consequent and are truly implications (not simply a measure of co-occurrence), and we show how they produce more intuitive results than other methods. Finally, we show how different characteristics of real data, as opposed to synthetic data, can dramatically affect the performance of the system and the form of the results.  1 Introduction  Within the area of data mining, the problem of deriving associations from data has recently received a great deal of attention. The prob...
1098|Mining Association Rules between Sets of Items in Large Databases|We are given a large database of customer transactions. Each transaction consists of items purchased by a customer in a visit. We present an efficient algorithm that generates all significant association rules between items in the database. The algorithm incorporates buffer management and novel estimation and pruning techniques. We also present results of applying this algorithm to sales data obtained from a large retailing company, which shows the effectiveness of the algorithm. 
1099|Sampling Large Databases for Association Rules|Discovery of association rules is an important database mining problem. Current algorithms for nding association rules require several passes over the analyzed database, and obviously the role of I/O overhead is very signi cant for very large databases. We present new algorithms that reduce the database activity considerably. Theidea is to pick a random sample, to ndusingthis sample all association rules that probably hold in the whole database, and then to verify the results with the restofthe database. The algorithms thus produce exact association rules, not approximations based on a sample. The approach is, however, probabilistic, and inthose rare cases where our sampling method does not produce all association rules, the missing rules can be found inasecond pass. Our experiments show that the proposed algorithms can nd association rules very e ciently in only onedatabase pass. 1
1100|Database Mining: A Performance Perspective|We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers.  Index Terms. database mining, knowledge discovery, classification, associations, sequences, decision trees   Current address: Computer Science De...
1101|Fast Similarity Search in the Presence of Noise, Scaling, and Translation in Time-Series Databases|We introduce a new model of similarity of time sequences that captures the intuitive notion that two sequences should be considered similar if they have enough non-overlapping time-ordered pairs of subsequences thar are similar. The model allows the amplitude of one of the two sequences to be scaled by any suitable amount and its offset adjusted appropriately. Two subsequences are considered similar if one can be enclosed within an envelope of a specified width drawn around the other. The model also allows non-matching gaps in the matching subsequences. The matching subsequences need not be aligned along the time axis. Given this model of similarity,we present fast search techniques for discovering all similar sequences in a set of sequences. These techniques can also be used to find all (sub)sequences similar to a given sequence. We applied this matching system to the U.S. mutual funds data and discovered interesting matches.
1102|Approximating discrete probability distributions with dependence trees|A method is presented to approximate optimally an n-dimensional discrete probability distribution by a product of second-order distributions, or the distribution of the first-order tree dependence. The problem is to find an optimum set of n-1 first order dependence relationship among the n variables. It is shown that the procedure derived in this paper yields an approximation of a minimum difference in information. It is further shown that when this procedure is applied to empirical observations from an unknown distribution of tree dependence, the procedure is the maximum-likelihood estimate of the distribution.
1103|Fusion, Propagation, and Structuring in Belief Networks|Belief networks are directed acyclic graphs in which the nodes represent propositions (or variables), the arcs signify direct dependencies between the linked propositions, and the strengths of these dependencies are quantified by conditional probabilities. A network of this sort can be used to represent the generic knowledge of a domain expert, and it turns into a computational architecture if the links are used not merely for storing factual knowledge but also for directing and activating the data flow in the computations which manipulate this knowledge. The first part of the paper deals with the task of fusing and propagating the impacts of new information through the networks in such a way that, when equilibrium is reached, each proposition will be assigned a measure of belief consistent with the axioms of probability theory. It is shown that if the network is singly connected (e.g. tree-structured), then probabilities can be updated by local propagation in an isomorphic network of parallel and autonomous processors and that the impact of new information can be imparted to all propositions in time proportional to the longest path in the network. The second part of the paper deals with the problem of finding a tree-structured representation for a collection of probabilistically coupled propositions using auxiliary (dummy) variables, colloquially called &#034;hidden causes. &#034; It is shown that if such a tree-structured representation exists, then it is possible to uniquely uncover the topology of the tree by observing pairwise dependencies among the available propositions (i.e., the leaves of the tree). The entire tree structure, including the strengths of all internal relationships, can be reconstructed in time proportional to n log n, where n is the number of leaves.  
1104|Connectionist Learning Procedures|A major goal of research on networks of neuron-like processing units is to discover efficient learning procedures that allow these networks to construct complex internal representations of their environment. The learning procedures must be capable of modifying the connection strengths in such a way that internal units which are not part of the input or output come to represent important features of the task domain. Several interesting gradient-descent procedures have recently been discovered. Each connection computes the derivative, with respect to the connection strength, of a global measure of the error in the performance of the network. The strength is then adjusted in the direction that decreases the error. These relatively simple, gradient-descent learning procedures work well for small tasks and the new challenge is to find ways of improving their convergence rate and their generalization abilities so that they can be applied to larger, more realistic tasks. 
1106|An algorithm for fast recovery of sparse causal graphs|An algorithm for fast recovery of sparse causal graphs
1107|Decision Theory in Expert Systems and Artificial Intelligence|Despite their different perspectives, artificial intelligence (AI) and the disciplines of decision science have common roots and strive for similar goals. This paper surveys the potential for addressing problems in representation, inference, knowledge engineering, and explanation within the decision-theoretic framework. Recent analyses of the restrictions of several traditional AI reasoning techniques, coupled with the development of more tractable and expressive decisiontheoretic representation and inference strategies, have stimulated renewed interest in decision theory and decision analysis. We describe early experience with simple probabilistic schemes for automated reasoning, review the dominant expert-system paradigm, and survey some recent research at the crossroads of AI and decision science. In particular, we present the belief network and influence diagram representations. Finally, we discuss issues that have not been studied in detail within the expert-systems sett...
1108|Constructor: A system for the induction of probabilistic models|The probabilistic network technology is a knowledgebased technique which focuses on reasoning under uncertainty. Because of its well defined semantics and solid theoretical foundations, the technology is finding increasing application in fields such as medical diagnosis, machine vision, military situation assessment, petroleum exploration, and information retrieval. However, like other knowledge-based techniques, acquiring the qualitative and quantitative information needed to build these networks can be highly labor-intensive. CONSTRUCTQR integrates techniques and concepts from probabilistic networks, artificial intelligence, and statistics in order to induce Markov networks (i.e., undirected probabilistic networks). The resulting networks are useful both qualitatively for concept organization and quantitatively for the assessment of new data. The primary goal of CONSTRUCTOR is to find qualitative structure from data. CONSTRUCTOR finds structure by first, modeling each feature in a data set as a node in a Markov network and secondly, by finding the neighbors of each node in the network. In Markov networks, the neighbors of a node have the property of being the smallest set of nodes which “shield ” the node from being affected by other nodes in the graph. This property is used in a heuristic search to identify each node’s neighbors. The traditional x2 test for independence is used to test if a set of nodes “shield ” another node. Cross-validation is used to estimate the quality of alternative structures.
1109|Advances in probabilistic reasoning|This paper discuses multiple Bayesian networks representation paradigms for encoding asymmetric independence assertions. We offer three contributions: (1) an inference mechanism that makes explicit use of asymmetric independence to speed up computations, (2) a simplified definition of similarity networks and extensions of their theory, and (3) a generalized representation scheme that encodes more types of asymmetric independence assertions than do similarity networks. 1
1110|Update on the Pathfinder project|This material is posted here with permission of the IEEE. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org. By choosing to view this document, you agree to all provisions of the copyright laws protecting it. Update on the Pathfinder Project *
1111|Latent variables, causal models and overidentifying constraints|When is a statistical dependency between two variables best explained by the supposition that one of these variables causes the other, as opposed to the supposition that there is a (possibly unmeasured) common cause acting on both variables? In this paper, we describe an approach towards model specification developed more fully in our book Discovering Cuud Structure, and illustrate its application to the aforementioned question. Briefly, the approach is to determine constraints satisfied by the variance-covariance matrix of a sample, and then to conduct a quasi-automated search for the causal specifications that will best explain those constraints, 1.
1112|Causal Structure Among Measured Variables Preserved with Unmeasured Variables|Causal structure among measured variables preserved with unmeasured variables
1113|Eraser: a dynamic data race detector for multithreaded programs|Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1
1114|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
1115|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
1116|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
1117|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
1118|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
1120|Shasta: A LowOverhead Software-Only Approach to Fine-Grain Shared Memory|Digital Equipment Corporation This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software,
1121|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
1122|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
1123|Online Data-Race Detection via Coherency Guarantees|We present the design and evaluation of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word.  The novel aspect of this system is that we are able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer.  We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found read-write data races in a program that allows unsynchronized read access to a global tour bound, and a write-write race in a program from a standard benchmark suite. Overall, our mechanism reduced program performance by approximately a factor of two.   
1124|Compile-time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs|Data race detection strategies based on software runtime monitoring are notorious for dramatically inflating execution times of shared-memory parallel programs. Without significant reductions in the execution overhead incurred when using these techniques, there is little hope that they will be widely used. A promising approach to this problem is to apply compile-time analysis to identify variable references that need not be monitored at run time because they will never be involved in a data race. In this paper we describe eraser, a data race instrumentation tool that uses aggressive program analysis to prune the number of references to be monitored. To quantify the effectiveness of our analysis techniques, we compare the overhead of race detection with three levels of compile-time analysis ranging from little analysis to aggressive interprocedural analysis for a suite of test programs. For the programs tested, using interprocedural analysis and dependence analysis dramatically reduced ...
1125|Transactional Memory: Architectural Support for Lock-Free Data Structures |A shared data structure is lock-free if its operations do not require mutual exclusion. If one process is interrupted in the middle of an operation, other processes will not be prevented from operating on that object. In highly concurrent systems, lock-free data structures avoid common problems associated with conventional locking techniques, including priority inversion, convoying, and difficulty of avoiding deadlock. This paper introduces transactional memory, a new multiprocessor architecture intended to make lock-free synchronization as efficient (and easy to use) as conventional techniques based on mutual exclusion. Transactional memory allows programmers to define customized read-modify-write operations that apply to multiple, independently-chosen words of memory. It is implemented by straightforward extensions to any multiprocessor cache-coherence protocol. Simulation results show that transactional memory matches or outperforms the best known locking techniques for simple benchmarks, even in the absence of priority inversion, convoying, and deadlock.  
1127|Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors|Scalable shared-memory multiprocessors distribute memory among the processors and use scalable interconnection networks to provide high bandwidth and low latency communication. In addition, memory accesses are cached, buffered, and pipelined to bridge the gap between the slow shared memory and the fast processors. Unless carefully controlled, such architectural optimizations can cause memory accesses to be executed in an order different from what the programmer expects. The set of allowable memory access orderings forms the memory consistency model or event ordering model for an architecture.
1128|Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors|Busy-wait techniques are heavily used for mutual exclusion and barrier synchronization in shared-memory parallel programs. Unfortunately, typical implementations of busy-waiting tend to produce large amounts of memory and interconnect contention, introducing performance bottlenecks that become markedly more pronounced as applications scale. We argue that this problem is not fundamental, and that one can in fact construct busy-wait synchronization algorithms that induce no memory or interconnect contention. The key to these algorithms is for every processor to spin on separate locally-accessible ag variables, and for some other processor to terminate the spin with a single remote write operation at an appropriate time. Flag variables may be locally-accessible as a result of coherent caching, or by virtue of allocation in the local portion of physically distributed shared memory. We present a new scalable algorithm for spin locks that generates O(1) remote references per lock acquisition, independent of the number of processors attempting to acquire the lock. Our algorithm provides reasonable latency in the absence of contention, requires only a constant amount of space per lock, and requires no hardware support other than
1129|Distributed packet switching for local computer networks|Abstract: Ethernet is a branching broadcast communication system for carrying digital data packets among locally distributed computing stations. The packet transport mechanism provided by Ethernet has been used to build systems which can be viewed as either local computer networks or loosely coupled multiprocessors. An Ethernet&#039;s shared communication facility, its Ether, is a passive broadcast medium with no central control. Coordination of access to the Ether for packet broadcasts is distributed among the contending transmitting stations using controlled statistical arbitration. Switching of packets to their destinations on the Ether is distributed among the receiving stations using packet address recognition. Design principles and implementation are described based on experience.with an operating Ethernet of1 00 nodes along a kilometer of coaxial cable. A model for estimating performance under heavy loads and a packet protocol for error-controlled communication are included for completeness.
1130|A Methodology for Implementing Highly Concurrent Data Objects| A concurrent object is a data structure shared by concurrent processes. Conventional techniques for implementing concurrent objects typically rely on critical sections: ensuring that only one process at a time can operate on the object. Nevertheless, critical sections are poorly suited for asynchronous systems: if one process is halted or delayed in a critical section, other, nonfaulty processes will be unable to progress. By contrast, a concurrent object implementation is lock free if it always guarantees that some process will complete an operation in a finite number of steps, and it is wait free if it guarantees that each process will complete an operation in a finite number of steps. This paper proposes a new methodology for constructing lock-free and wait-free implementations of concurrent objects. The object’s representation and operations are written as stylized sequential programs, with no explicit synchronization. Each sequential operation is automatically transformed into a lock-free or wait-free operation using novel synchronization and memory management algorithms. These algorithms are presented for a multiple instruction/multiple data (MIMD) architecture in which n processes communicate by applying atomic read, wrzte, load_linked, and store_conditional operations to a shared memory.
1131|Memory access buffering in multiprocessors|In highly-pipelined machines, instructions and data are prefetched and buffered in both the processor and the cache. This is done to reduce the average memory access la-tency and to take advantage of memory interleaving. Lock-up free caches are designed to avoid processor blocking on a cache miss. Write buffers are often included in a pipelined machine to avoid processor waiting on writes. In a shared memory multiprocessor, there are more advantages in buffering memory requests, since each memory access has to traverse the memory- processor interconnection and has to compete with memory requests issued by different processors. Buffering, however, can cause logical problems in multipro-cessors. These problems are aggravated if each processor has a private memory in which shared writable data may be present, such as in a cache-based system or in a system with a distributed global memory. In this paper, we analyze the benefits and problems associated with the buffering of memory requests in shared memory multiprocessors. We show that the logical problem of buffering is directly related to the problem of synchronization. A simple model is presented to evaluate the performance improvement result-ing from buffering. 1.
1132|LimitLESS Directories: A Scalable Cache Coherence Scheme|Caches enhance the performance of multiprocessors by reducing network tra#c and average memory access latency. However, cache-based systems must address the problem of cache coherence. We propose the LimitLESS directory protocol to solve this problem. The LimitLESS scheme uses a combination of hardware and software techniques to realize the performance of a full-map directory with the memory overhead of a limited directory. This protocol is supported by Alewife, a large-scale multiprocessor. We describe the architectural interfaces needed to implement the LimitLESS directory, and evaluate its performance through simulations of the Alewife machine.
1133|Performance Evaluation of Memory Consistency Models for Shared-Memory Multiprocessors |The memory consistency model supported by a multiprocessor architecture determines the amount of buffering and pipelining that may be used to hide or reduce the latency of memory accesses. Several different consistency models have been proposed. These range from sequential consistency on one end, allowing very limited buffering, to release consistency on the other end, allowing extensive buffering and pipelining. The processor consistency and weak consistency models fall in between. The advantage of the less strict models is increased performance potential. The disadvantage is increased hardware complexity and a more complex programming model. To make an informed decision on the above tradeoff requires performance data for the various models. This paper addresses the issue of performance benefits from the above four consistency models. Our results are based on simulation studies done for three applications. The results show that in an environment where processor reads are blocking and writes are buffered, a significant performance increase is achieved from allowing reads to bypass previous writes. Pipelining of writes, which determines the rate at which writes are retired from the write buffer, is of secondary importance. As a result, we show that the sequential consistency model performs poorly relative to all other models, while the processor consistency model provides most of the benefits of the weak and release consistency models. 
1134|The Expandable Split Window Paradigm for Exploiting Fine-Grain Parallelism|We propose a new processing paradigm, called the Expandable Split Window (ESW) paradigm, for exploiting finegrain parallelism. This paradigm considers a window of instructions (possibly having dependencies) as a single unit, and exploits fine-grain parallelism by overlapping the execution of multiple windows. The basic idea is to connect multiple sequential processors, in a decoupled and decentralized manner, to achieve overall multiple issue. This processing paradigm shares a number of properties of the restricted dataflow machines, but was derived from the sequential von Neumann architecture. We also present an implementation of the Expandable Split Window execution model, and preliminary performance results. 1.
1135|A Lock-Free Multiprocessor OS Kernel|Typical shared-memory multiprocessor OS kernels use interlocking, implemented as spinlocks or waiting semaphores. We have implemented a complete multiprocessor OS kernel (including threads, virtual memory, and I/O including a window system and a file system) using only lock-free synchronization methods based on Compare-and-Swap. Lock-free synchronization avoids many serious problems caused by locks: considerable overhead, concurrency bottlenecks, deadlocks, and priority inversion in real-time scheduling. Measured numbers show the low overhead of our implementation, competitive with user-level thread management systems.  Contents  1 Introduction 1 2 Synchronization in OS Kernels 1  2.1 Disabling Interrupts : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 1 2.2 Locking Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : : 2 2.3 Lock-Free Synchronization Methods : : : : : : : : : : : : : : : : : : : : : : : : : 2  3 Lock-Free Quajects 3  3.1 LIFO ...
1136|Detecting Violations of Sequential Consistency|The performance of a multiprocessor is directly affected by the choice of the memory consistency model supported. Several different consistency models have been proposed in the literature. These range from sequential consistency on one end, allowing limited buffering of memory accesses, to release consistency on the other end, allowing extensive buffering and pipelining. While the relaxed models such as release consistency provide potential for higher performance, they present a more complex programming model than sequential consistency. Previous research has addressed this tradeoff by showing that a release consistent architecture provides sequentially consistent executions for programs that are free of data races. However, the burden of guaranteeing that the program is free of data races remains with the programmer or compiler.
1137|The Nash Bargaining Solution in Economic Modeling|This article establishes the relationship between the static axiomatic theory of bargaining and the sequential strategic approach to bargaining. We consider two strategic models of alternating offers. The models differ in the source of the incentive of the bargaining parties to reach agreement: the bargainers &#039; time preference and the risk of breakdown of negotiation. Each of the models has a unique perfect equilibrium. When the motivation to reach agreement is made negligible, in each model the unique perfect equilibrium outcome approaches the Nash bargaining solution, with utilities that reflect the incentive to settle and with the proper disagreement jfoint chosen. The results provide a guide for the application of the Nash bar-gaining solution in economic modelling. 1.
1139|Modeling Term Structures of Defaultable Bonds|This article presents convenient reduced-form models of the valuation of contingent claims subject to default risk, focusing on applications to the term structure of interest rates for corporate or sovereign bonds. Examples include the valuation of a credit-spread option
1142|Transform Analysis and Asset Pricing for Affine Jump-Diffusions|In the setting of ‘‘affine’ ’ jump-diffusion state processes, this paper provides an analytical treatment of a class of transforms, including various Laplace and Fourier transforms as special cases, that allow an analytical treatment of a range of valuation and econometric problems. Example applications include fixed-income pricing models, with a role for intensity-based models of default, as well as a wide range of option-pricing applications. An illustrative example examines the implications of stochastic volatility and jumps for option valuation. This example highlights the impact on option ‘smirks ’ of the joint distribution of jumps in volatility and jumps in the underlying asset price, through both jump amplitude as well as jump timing.
1143|A yield-factor model of interest rates|This paper presents a consistent and arbitrage-free multifactor model of the term structure of interest rates in which yields at selected fixed maturities follow a parametric multivariate Markov diffusion process with “stochastic volatility. ” The yield of any zero-coupon bond is taken to be a maturitydependent affine combination of the selected “basis ” set of yields. We provide necessary and sufficient conditions on the stochastic model for this affine representation. We include numerical techniques for solving the model, as wcll as numerical techniques for calculating the prices of term-structure derivative prices. The case of jump diffusions i \ also considered. I.
1144|Specification Analysis of Affine Term Structure Models|This paper explores the structural differences and relative goodness-of-fits of affine term structure models (ATSMs55). Within the family of ATSMs there is a tradeoff between flexibility in modeling the conditional correlations and volatilities of the risk factors. This trade-off is formalized by our classification of N-factor affine family into N + 1 non-nested subfamilies of models. Specializing to three-factor ATSMs, our analysis suggests, based on theoretical considerations and empirical evidence, that some subfamilies of ATSMs are better suited than others to explaining historical interest rate behavior. 
1146|Term structures of credit spreads with incomplete accounting information|  We study the implications of imperfect information for term structures of credit spreads on corporate bonds. We suppose that bond investors cannot observe the issuer’s assets directly, and receive instead only periodic and imperfect accounting reports. For a setting in which the assets of the firm are a geometric Brownian motion until informed equityholders optimally liquidate, we derive the conditional distribution of the assets, given accounting data and survivorship. Contrary to the perfect-information case, there exists a default-arrival intensity process. That intensity is calculated in terms of the conditional distribution of assets. Credit yield spreads are characterized in terms of accounting information. Generalizations are provided.   
1147|The relation between treasury yields and corporate bond yield spreads|Because the option to call a corporate bond should rise in value when bond yields fall, the relation between noncallable Treasury yields and spreads of corporate bond yields over Treasury yields should depend on the callability of the corporate bond. I confirm this hypothesis for investment-grade corporate bonds. Although yield spreads on both callable and noncallable corporate bonds fall when Treasury yields rise, this relation is much stronger for callable bonds. This result has important implications for interpreting the behavior of yields on commonly used corporate bond indexes, which are composed primarily of callable bonds. COMMONLY USED INDEXES OF CORPORATE bond yields, such as those produced by Moody’s or Lehman Brothers, are constructed using both callable and noncallable bonds. Because the objective of those producing the indexes is to track the universe of corporate bonds, this methodology is sensible. Until the mid-1980s, few corporations issued noncallable bonds, hence an index designed to measure the yield on a typical corporate bond would have to be
1149|The Central Tendency: A Second Factor in Bond Yields,&#034; The Review of Economics and Statistics|you have obtained prior permission, you may not download an entire issue of a journal or multiple copies of articles, and you may use content in the JSTOR archive only for your personal, non-commercial use. Please contact the publisher regarding any further use of this work. Publisher contact information may be obtained at
1150|The Valuation of Default Risk in Corporate Bonds and Interest Rate Swaps|: This paper implements a model for the valuation of the default risk implicit in the prices of corporate bonds. The analytical approach considers the two essential ingredients in the valuation of corporate bonds: interest rate uncertainty and default risk. The former is modeled as a diffusion process. The latter is modeled as a spread following a diffusion process, with the magnitude of this spread impacting on the probability of a Poisson process governing the arrival of the default event. We apply two variants of this model to the valuation of fixed-for-floating swaps. In the first, the swap is default-free, and the spread represents the appropriate discounted expected value of the instantaneous TED spread; in the second, we allow the swap to incorporate default risk. We propose to test our models using the entire term structure of corporate bonds prices for different ratings and industry categories, as well as the term structure of fixed-for-floating swaps.  The Valuation of Defau...
1151|An Algorithm that Learns What&#039;s in a Name|In this paper, we present IdentiFinder^TM, a hidden Markov model that learns to recognize and classify names, dates, times, and numerical quantities. We have evaluated the model in English (based on data from the Sixth and Seventh Message Understanding Conferences [MUC-6, MUC-7] and broadcast news) and in Spanish (based on data distributed through the First Multilingual Entity Task [MET-1]), and on speech input (based on broadcast news). We report results here on standard materials only to quantify performance on data available to the community, namely, MUC-6 and MET-1. Results have been consistently better than reported by any other learning algorithm. IdentiFinder&#039;s performance is competitive with approaches based on handcrafted rules on mixed case text and superior on text where case information is not available. We also present a controlled experiment showing the effect of training set size on performance, demonstrating that as little as 100,000 words of training data is adequate to get performance around 90% on newswire. Although we present our understanding of why this algorithm performs so well on this class of problems, we believe that significant improvement in performance may still be possible.
1152|Maximum Entropy Models for Natural Language Ambiguity Resolution|The best aspect of a research environment, in my opinion, is the abundance of bright people with whom you argue, discuss, and nurture your ideas. I thank all of the people at Penn and elsewhere who have given me the feedback that has helped me to separate the good ideas from the bad ideas. I hope that Ihave kept the good ideas in this thesis, and left the bad ideas out! Iwould like toacknowledge the following people for their contribution to my education: I thank my advisor Mitch Marcus, who gave me the intellectual freedom to pursue what I believed to be the best way to approach natural language processing, and also gave me direction when necessary. I also thank Mitch for many fascinating conversations, both personal and professional, over the last four years at Penn. I thank all of my thesis committee members: John La erty from Carnegie Mellon University, Aravind Joshi, Lyle Ungar, and Mark Liberman, for their extremely valuable suggestions and comments about my thesis research. I thank Mike Collins, Jason Eisner, and Dan Melamed, with whom I&#039;ve had many stimulating and impromptu discussions in the LINC lab. Iowe them much gratitude for their valuable feedback onnumerous rough drafts of papers and thesis chapters.
1153|Adaptive Statistical Language Modeling: A Maximum Entropy Approach|Language modeling is the attempt to characterize, capture and exploit regularities in natural language. In statistical language modeling, large amounts of text are used to automatically determine the model&#039;s parameters. Language modeling is useful in automatic speech recognition, machine translation, and any other application that processes natural language with incomplete knowledge.  In this thesis, I view language as an information source which emits a stream of symbols from a finite alphabet (the vocabulary). The goal of language modeling is then to identify and exploit sources of information in the language stream, so as to minimize its perceived entropy.  Most existing statistical language models exploit the immediate past only. To extract information from further back in the document&#039;s history, I use trigger pairs as the basic information bearing elements. This allows the model to adapt its expectations to the topic of discourse.  Next, statistical evidence from many sources must...
1154|Exploiting Diverse Knowledge Sources via Maximum Entropy in Named Entity Recognition|This paper describes a novel statistical namedentity (i.e. &#034;proper name&#034;) recognition system built around a maximum enti W framework. By working within the framework of maximum entropy. theory and utilizing a flexible object-based architecture, the system is able to make use of an extraordinarily diverse range of knowledge sources in making its tagging decisions. These knowledge sources include capitalization features, lexical features, features in- dicating the current section of text (i.e. headline or main body), and dictionaries of single or multi-wtrd terms. The purely statistical system contains no hand-generated patterns and achieves a result comparable with the best statistical systems. However, when combined with other handcoded systems, the system achieves scores that exceed the highest comparable scores thus-far published.
1155|Information Extraction Using Hidden Markov Models|This thesis shows how to design and tune a hidden Markov model to extract factual information from a corpus of machine-readable English prose. In particular, the thesis presents a HMM that classifies and parses natural language assertions about genes being located at particular positions on chromosomes. The facts extracted by this HMM can be inserted into biological databases. The HMM is trained on a small set of sentence fragments chosen from the collected scientific abstracts in the OMIM (On-Line Mendelian Inheritance in Man) database and judged to contain the target binary relationship between gene names and gene locations. Given a novel sentence, all contiguous fragments are ranked by log-odds score, i.e. the log of the ratio of the probability of the fragment according to the target HMM to that according to a &#034;null&#034; HMM trained on all OMIM sentences. The most probable path through the HMM gives bindings for the annotations with precision as high as 80%. In contrast with traditional natural language processing methods, this stochastic approach makes no use either of part-of-speech taggers or dictionaries, instead employing non-emitting states to assemble modules roughly corresponding to noun, verb, and prepostional phrases. Algorithms for reestimating parameters for HMMs with non-emitting states are presented in detail. The ability to tolerate new words and recognize a wide variety of syntactic forms arises from the judicious use of &#034;gap&#034; states.
1156|Question Answering from Frequently-Asked Question Files: Experiences with the FAQ Finder System|This paper describes FAQ Finder, a natural language question-answering system that uses files of frequently-asked questions as its knowledge base. Unlike AI question-answering systems that focus on the generation of new answers, FAQ Finder retrieves existing ones found in frequently-asked question files. Unlike information retrieval approaches that rely on a purely lexical metric of similarity between query and document, FAQ Finder uses a semantic knowledge base (WordNet) to improve its ability to match question and answer. We describe the design considerations that have entered into the system and various experiments that influence the system&#039;s current implementation. We include results from an evaluation of the system&#039;s performance against a corpus of user questions, and show that a combination of semantic and statistical techniques works better than any single approach. Introduction  In the vast information space of the Internet, individuals and groups have created small pockets of ...
1157|Efficient Sampling and Feature Selection in Whole Sentence Maximum Entropy Language Models  |Conditional Maximum Entropy models have been successfully
1158|Markov Processes on Curves for Automatic Speech Recognition|We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables---one continuous (x), one discrete (s)---evolve jointly in time. We suppose that the vector x traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the  arc length traversed along this curve. Since arc length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[sjx], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks---recognizing New Jersey town names and connected alpha-digits---we find that MPCs yield lower word error rates than comparably trained hidden Markov models. 1 Intr...
1160|A Heteroskedasticity-Consistent Covariance Matrix Estimator And A Direct Test For Heteroskedasticity|This paper presents a parameter covariance matrix estimator which is consistent even  when the disturbances of a linear regression model are heteroskedastic. This estimator  does not depend on a formal model of the structure of the heteroskedasticity. By  comparing the elements of the new estimator to those of the usual covariance estimator,  one obtains a direct test for heteroskedasticity, since in the absence of heteroskedasticity,  the two estimators will be approximately equal, but will generally diverge otherwise. The  test has an appealing least squares interpretation
1162|How much should we trust differences-in-differences estimates?|Most papers that employ Differences-in-Differences estimation (DD) use many years of data and focus on serially correlated outcomes but ignore that the resulting standard errors are incon-sistent. To illustrate the severity of this issue, we randomly generate placebo laws in state-level data on female wages from the Current Population Survey. For each law, we use OLS to compute the DD estimate of its “effect” as well as the standard error of this estimate. These conventional DD standard errors severely understate the standard deviation of the estimators: we find an “effect ” significant at the 5 percent level for up to 45 percent of the placebo interventions. We use Monte Carlo simulations to investigate how well existing methods help solve this problem. Econometric corrections that place a specific parametric form on the time-series process do not perform well. Bootstrap (taking into account the auto-correlation of the data) works well when the number of states is large enough. Two corrections based on asymptotic approximation of the variance-covariance matrix work well for moderate numbers of states and one correction that collapses the time series information into a “pre” and “post” period and explicitly takes into account the effective sample size works well even for small numbers of states.
1163|Market Timing and Capital Structure|It is well known that firms are more likely to issue equity when their market values are high, relative to book and past market values, and to repurchase equity when their market values are low. We document that the resulting effects on capital structure are very persistent. As a consequence, current capital structure is strongly related to historical market values. The results suggest the theory that capital structure is the cumulative outcome of past attempts to time the equity market. 
1164|Disappearing Dividends: Changing Firm Characteristics or Lower Propensity to Pay?|The percent of firms paying cash dividends falls from 66.5 in 1978 to 20.8 in 1999. The decline is due in part to the changing characteristics of publicly traded firms. Fed by new lists, the population of publicly traded firms tilts increasingly toward small firms with low profitability and strong growth opportunities -- characteristics typical of firms that have never paid dividends. More interesting, we also show that controlling for characteristics, firms become less likely to pay dividends. This lower propensity to pay is at least as important as changing characteristics in the declining incidence of dividend payers.  *  Graduate School of Business, University of Chicago (Fama) and Sloan School of Management, MIT (French). We acknowledge the comments of John Graham, Douglas Hannah, Anil Kashyap, Tobias Moskowitz, G. William Schwert, Andrei Shleifer, Paul Zarowin, two anonymous (and especially helpful) referees, and seminar participants at Harvard University, the University of Chica...
1165|Robust Inference with Multi-way Clustering|In this paper we propose a new variance estimator for OLS as well as for nonlinear estimators such as logit, probit and GMM. This variance estimator enables cluster-robust inference when there is two-way or multi-way clustering that is nonnested. The variance estimator extends the standard cluster-robust variance estimator or sandwich estimator for one-way clustering (e.g. Liang and Zeger (1986), Arellano (1987)) and relies on similar relatively weak distributional assumptions. Our method is easily implemented in statistical packages, such as Stata and SAS, that already offer cluster-robust standard errors when there is one-way clustering. The method is demonstrated by a Monte Carlo analysis for a two-way random effects model; a Monte Carlo analysis of a placebo law that extends the state-year effects example of Bertrand et al. (2004) to two dimensions; and by application to two studies in the empirical public/labor literature where two-way clustering is present.
1166|Testing Tradeoff and Pecking Order Predictions about Dividends and Debt|We test the dividend and leverage predictions of the tradeoff and pecking order models. As both models predict, more profitable firms have higher long-term dividend payouts, and firms with more investments have lower payouts. Confirming the pecking order model but contradicting the tradeoff model, more profitable firms are less levered. Firms with more investment opportunities are also less levered, which is in line with the tradeoff model and a complex version of the pecking order model. Firms with more investments have lower long-term dividend payouts, but dividends do not vary to accommodate short-term variation in investment. Confirming the pecking order model, short-term variation in investment and earnings is mostly absorbed by variation in debt.  *  Graduate School of Business, University of Chicago (Fama) and Sloan School of Management, MIT (French).  The finance literature offers two competing models of financing decisions. In the tradeoff model, firms identify their optimal l...
1167|Does the source of capital affect capital structure |Prior work on leverage implicitly assumes capital availability depends solely on firm characteristics. However, market frictions that make capital structure relevant may also be associated with a firm’s source of capital. Examining this intuition, we find firms that have access to the public bond markets, as measured by having a debt rating, have significantly more leverage. Although firms with a rating are fundamen-tally different, these differences do not explain our findings. Even after controlling for firm characteristics that determine observed capital structure, and instrumenting for the possible endogeneity of having a rating, firms with access have 35 % more debt. Under the tradeoff theory of capital structure, firms determine their preferred leverage ratio by calculating the tax advantages, costs of financial distress, mispricing, and incentive effects of debt versus equity. The empirical literature has searched for evidence that firms choose their capital structure, as this theory predicts, by estimating firm leverage as a function of firm characteristics. Firms for whom the tax shields of debt are greater, the costs of financial distress lower, and the mispricing of debt relative to equity more favorable are expected to be more highly levered. When these firms discover that the net benefit of debt is positive, they will move toward their preferred capital structure by issuing additional debt and/or reducing their equity. The implicit assumption has been that a firm’s leverage is completely a function of a firm’s demand for debt. In
1168|Do conglomerate firms allocate resources inefficiently across industries? Theory and evidence|We develop a profit-maximizing neoclassical model of optimal firm size and growth across different industries based on differences in industry fundamentals and firm productivity. In the model, a conglomerate discount is consistent with profit maximization. The model predicts how conglomerate firms will allocate resources across divisions over the business cycle and how their responses to industry shocks will differ from those of single-segment firms. Using plant level data, we find that growth and investment of conglomerate and single-segment firms is related to fundamental industry factors and individual segment level productivity. The majority of conglomerate firms exhibit growth across industry segments that is consistent with optimal behavior.  
1169|The Effects of Government Ownership on Bank Lending|This paper studies the effects of government ownership on bank lending behavior. Using information on individual loan contracts, I compare the interest rate charged to two sets of companies with identical characteristics borrowing respectively from stateowned and privately owned banks. State-owned banks charge lower interest rates than do privately owned banks to similar or identical firms, even if the company is able to borrow more from privately owned banks. State-owned banks mostly favor firms located in depressed areas and large firms. The lending behavior of state-owned banks is affected by the electoral results of the party affiliated with the bank: the stronger the political party in the area where the firm is borrowing, the lower the interest rates charged. This result is robust to including bank and firm fixed effects. I am indebted to Andrei Shleifer for guidance and encouragement. I also thank Alberto Alesina, Paul
1170|Simple formulas for standard errors that cluster by both firm and time|When estimating finance panel regressions, it is common practice to adjust stan-dard errors for correlation either across firms or across time. These procedures are valid only if the residuals are correlated either across time or across firms, but not across both. This note shows that it is very easy to calculate standard errors that are robust to simultaneous correlation along two dimensions, such as firms and time. The covariance estimator is equal to the estimator that clusters by firm, plus the the estimator that clusters by time, minus the usual heteroskedasticity-robust OLS covariance matrix. Any statistical package with a clustering command can be used to easily calculate these standard errors.
1171|Stock Valuation and Learning about Profitability|We develop a simple approach to valuing stocks in the presence of learning about average profitability. The market-to-book ratio (M/B) increases with uncertainty about average profitability, especially for firms that pay no dividends. M/B is predicted to decline over a firm&#039;s lifetime due to learning, with steeper decline when the firm is young. These predictions are confirmed empirically. Data also support the predictions that younger stocks and stocks that pay no dividends have more volatile returns. Firm profitability has become more volatile recently, helping explain the puzzling increase in average idiosyncratic return volatility observed over the past few decades.
1172|What Drives Firm-Level Stock Returns?|I use a vector autoregressive model (VAR) to decompose an individual firm’s stock return into two components: changes in cash-flow expectations (i.e., cash-flow news) and changes in discount rates (i.e., expected-return news). The VAR yields three main results. First, firm-level stock returns are mainly driven by cash-flow news. For a typical stock, the variance of cash-flow news is more than twice that of expected-return news. Second, shocks to expected returns and cash flows are positively correlated for a typical small stock. Third, expected-return-news series are highly correlated across firms, while cash-flow news can largely be diversified away in aggregate portfolios. 
1173|An Empirical Analysis of Personal Bankruptcy and Delinquency|This paper uses a unique new panel data set of credit card accounts to analyze credit card delinquency and more generally personal bankruptcy and the stability of credit risk models. We estimate duration models for default and assess the relative importance of different variables in predicting default. We investigate how the propensity to default has changed over time, disentangling the two leading explanations for the recent increase in default rates -- a deterioration in the risk-composition of borrowers versus a reduction in the social stigma of default. Even after controlling for risk-composition and other economic fundamentals, the propensity to default significantly increased between 1995 and 1997. By contrast, increases in credit limits and other changes in risk-composition explain only a small part of the change in default rates. Standard default models appear to have missed an important time-varying default factor, consistent with the stigma effect.  JEL classification: E21; E...
1174|Conditioning manager alphas on economic information: Another look at the persistence of performance|This article presents evidence on persistence in the relative investment performance of large, institutional equity managers. Similar to existing evidence for mutual funds, we find persistent performance concentrated in the managers with poor prior-period performance measures. A conditional approach, using time-varying measures of risk and abnormal performance, is better able to detect this persistence and to predict the future performance of the funds than are traditional methods.
1175|Forecasting crashes: Trading volume, past returns and conditional skewness in stock prices| This paper is an investigation into the determinants of asymmetries in stock returns. We develop a series of cross-sectional regression specifications which attempt to forecast skewness in the daily returns of individual stocks. Negative skewness is most pronounced in stocks that have experienced: 1) an increase in trading volume relative to trend over the prior six months; and 2) positive returns over the prior thirty-six months. The first finding is consistent with the model of Hong and Stein (1999), which predicts that negative asymmetries are more likely to occur when there are large differences of opinion among investors. The latter finding fits with a number of theories, most notably Blanchard and Watson’s (1982) rendition of stockprice bubbles. Analogous results also obtain when we attempt to forecast the skewness of the aggregate stock market, though our statistical power in this case is limited.
1177|Informed trading in stock and option markets|We investigate the contribution of option markets to price discovery, using a modification of Hasbrouck’s (1995) “information share ” approach. Based on five years of stock and options data for 60 firms, we estimate the option market’s contribution to price discovery to be about 17 percent on average. Option market price discovery is related to trading volume and spreads in both markets, and stock volatility. Price discovery across option strike prices is related to leverage, trading volume, and spreads. Our results are consistent with theoretical arguments that informed investors trade in both stock and option markets, suggesting an important informational role for options.
1178|Asymptotic properties of a robust variance matrix estimator for panel data when T is large. Journal of Econometrics. forthcoming|Abstract. In this paper, I consider the asymptotic properties of a robust covariance ma-trix estimator which is commonly advocated for use in panel data. The estimator is a generalization of the conventional heteroskedasticity consistent covariance matrix estimator for panel data which allows arbitrary correlation within each individual. Under the usual panel asymptotics where the cross-section dimension, n, grows large with the time series dimension, T, fixed, this estimator has good properties while allowing an essentially uncon-strained time series pattern of correlation. However, many panel data sets are characterized by a non-negligible time dimension. I extend the usual analysis by examining cases where n and T go to infinity jointly, considering both non-mixing and mixing cases, and show that conventional t and F tests based on the robust covariance matrix estimator are consistent. In addition, when T ? 8 with n fixed and other regularity conditions are satisfied, I show that the usual t and F statistics can be used for inference despite the fact that the robust covariance matrix estimator is not consistent but converges in distribution to a limiting random variable. I also explore the properties of a direct test of the assumptions underlying simpler covariance matrix estimators analogous to that of White (1980). The properties of the robust covariance matrix estimator and tests based upon are it examined in a short Monte Carlo study.
1179|2000): Do Behavioral Biases Affect Prices |This paper documents strong evidence of behavioral biases among Chicago Board of Trade proprietary traders and investigates the effect these biases have on prices. Our traders appear highly loss-averse. Traders who experience morning losses are about 15 percent more likely to assume above-average afternoon risk than traders with morning gains. This behavior has important short-term consequences for afternoon prices, as losing traders actively purchase contracts at higher prices and sell contracts at lower prices than those that prevailed previously. However, during the Þve minutes that follow these trades, prices revert strongly to their earlier levels. Consistent with these Þndings, short-term afternoon price volatility is positively related to the prevalence of morning losses among locals, but overall afternoon price volatility is not.
1180|t-statistic based correlation and heterogeneity Robust Inference|We develop a general approach to robust inference about a scalar parameter when the data is potentially heterogeneous and correlated in a largely unknown way. The key ingredient is the following result of Bakirov and Székely (2005) concerning the small sample properties of the standard t-test: For a significance level of 5 % or lower, the t-test remains conservative for underlying observations that are independent and Gaussian with heterogenous variances. One might thus conduct robust large sample inference as follows: partition the data into q = 2 groups, estimate the model for each group and conduct a standard t-test with the resulting q parameter estimators. This results in valid and in some sense efficient inference when the groups are chosen in a way that ensures the parameter estimators to be asymptotically independent, unbiased and Gaussian of possibly different variances. We provide examples of how to apply this approach to time series, panel, clustered and spatially correlated data.  
1181|Forthcoming. “Dividend Taxes and Share Prices: Evidence from Real Estate Investment Trusts|Prior empirical evidence regarding the impact of dividend taxes on firm valuation is mixed.This study avoids some of the complications encountered in previous empirical work by exploiting institutional characteristics of REITs, such as their limited discretion over dividend policy and the relative transparency of REITassets.We regress the market value of equity on the market value of assets and tax basis, which creates tax deductions that lower future dividend taxes without affecting future pretax cash flow. We find that firm value is positively related to tax basis, suggesting that future dividend taxes are capitalized into share prices. FINANCIAL ECONOMISTS have debated the impact of dividend taxes on firm valuation and the cost of equity capital for decades. In 1970, Brennan proposed that investors should impose a price penalty on the shares of high-dividend firms because capital gains are tax-preferred relative to dividends. In 1978, Miller and Scholes countered that prices may not reflect shareholder taxes because the marginal
1182|Market Reactions to Tangible and Intangible|Information
1183|The Limits of Noise Trading: An Experimental Analysis&amp;quot; Cornel University working paper|In this research we investigate the behavior of noise traders and their impact on the market. We do this in an experimental market setting that allows us to determine not only how noise traders fare in a competitive asset market with other traders, but also how the equilibrium changes if a securities transactions tax (“Tobin tax”) is imposed. We find that noise traders lose money on average: they do not engage in extensive liquidity provision, and their attempt to make money by trend chasing is unsuccessful as they lose most in securities whose prices experience large moves. Noise traders adversely affect the informational efficiency of the market: they drive prices away from fundamental values, and the further away the market gets from the true value, the stronger this effect becomes. With a securities transaction tax, noise traders submit fewer orders and lose less money in those securities that exhibit large price movements. The tax is associated with a decrease in market trading volume, but informational efficiency remains essentially unchanged and liquidity (as measured by the price impact of trades) actually improves. We find no significant effect, however, on market volatility, suggesting that at least this rationale for a securities transaction tax is not supported by our data. 2
1184|Panel Data inference in Finance: Least-Squares vs Fama-MacBeth” University of Maryland working paper|Empirical research in finance frequently involves analysis of panel data sets. In corporate finance, we typically encounter panels with large cross sections (“large N”), while in asset pricing, panels with long time series (“large T”) are more common. For each case, we examine four estimators: the Least-Squares (LS) and Fama-MacBeth (FM) estimators and their generalized versions. In particular, we offer a rigorous econometric analysis of the FM estimation procedure in the context of panel data. This covers the traditional FM method that is suitable for the large T case, as well as a novel modification of the method appropriate for the large N case. The generalized versions are more efficient, but the corresponding standard errors may be poorly estimated resulting in unreliable t-statistics. An extensive simulation study demonstrates that the estimators under consideration perform remarkably well in moderately small samples. In particular, we provide evidence that both estimation procedures (LS and FM), when properly applied, have comparable performance in the sense that they produce equally reliable t-statistics. Since the two approaches are justified under very similar assumptions, researchers are encouraged to use both approaches in their empirical work to ensure the validity of their results.
1185|USER ACCEPTANCE OF INFORMATION TECHNOLOGY: TOWARD A UNIFIED VIEW|Information technology (IT) acceptance research has yielded many competing models, each with different sets of acceptance determinants. In this paper, we (1) review user acceptance literature and discuss eight prominent models, (2) empirically compare the eight models and their extensions, (3) formulate a unified model that integrates elements across the eight models, and (4) empirically validate the unified model. The eight models reviewed are the theory of reasoned action, the technology acceptance model, the motivational model, the theory of planned behavior, a model combining the technology acceptance model and the theory of planned behavior, the model of PC utilization, the innovation diffusion theory, and the social cognitive theory. Using data from four organizations over a six-month period with three points of measurement, the eight models explained between 17 percent and 53 percent of the variance in user intentions to use information technology. Next, a unified model, called the Unified Theory of Acceptance and Use of Technology (UTAUT), was formulated, with four core determinants of intention and usage, and up to four moderators of key relationships. UTAUT was then tested using the original data and found to outperform the eight individual models (adjusted R 2 of 69 percent). UTAUT was then confirmed with data from two new organizations with similar
1186|The theory of planned behavior|Research dealing with various aspects of * the theory of planned behavior (Ajzen, 1985, 1987) is reviewed, and some unresolved issues are discussed. In broad terms, the theory is found to be well supported by empirical evidence. Intentions to perform behaviors of different kinds can be predicted with high accuracy from attitudes toward the behavior, subjective norms, and perceived behavioral control; and these intentions, together with perceptions of behavioral control, account for considerable variance in actual behavior. Attitudes, subjective norms, and perceived behavioral control are shown to be related to appropriate sets of salient behavioral, normative, and control beliefs about the behavior, but the exact nature of these relations is still uncertain. Expectancy — value formulations are found to be only partly successful in dealing with these relations. Optimal rescaling of expectancy and value measures is offered as a means of dealing with measurement limitations. Finally, inclusion of past behavior in the prediction equation is shown to provide a means of testing the theory*s sufficiency, another issue that remains unresolved. The limited available evidence concerning this question shows that the theory is predicting behavior quite well in comparison to the ceiling imposed by behavioral reliability. © 1991 Academic Press. Inc.
1187|Toward an integrative theory of training motivation: A meta-analytic path analysis of 20 years of research|This article meta-analytically summarizes the literature on training motivation, its antecedents, and its relationships with training outcomes such as declarative knowledge, skill acquisition, and transfer. Significant predictors of training motivation and outcomes included individual characteristics (e.g., locus of control, conscientiousness, anxiety, age, cognitive ability, self-efficacy, valence, job involvement) and situational characteristics (e.g., climate). Moreover, training motivation explained incremental variance in training outcomes beyond the effects of cognitive ability. Meta-analytic path analyses further showed that the effects of personality, climate, and age on training outcomes were only partially mediated by self-efficacy, valence, and job involvement. These findings are discussed in terms of their practical significance and their implications for an integrative theory of training motivation. Traditionally, training researchers have focused on the methods and settings that maximize the reaction, learning, and behavior change of trainees (Tannenbaum &amp; Yukl, 1992). This research has sought to understand the impact of training media, instructional settings, sequencing of content, and other factors on training effectiveness. However, several reviews of training research have emphasized that because the influence of these variables on individuals&#039; learning and behavior varies, research must examine how personal characteristics relate to training effectiveness (Campbell, 1988; Tannenbaum &amp; Yukl, 1992). For example, Pintrich, Cross,
1188|A longitudinal field investigation of gender differences in individual technology adoption decision-making processes|This research investigated gender differences in the over-looked context of individual adoption and sustained usage of technology in the workplace using the theory of planned behavior (TPB). User reactions and technology usage behavior were stud-ied over a 5-month period among 355 workers being introduced to a new software technology application. When compared to women&#039;s decisions, the decisions of men were more strongly influ-enced by their attitude toward using the new technology. In con-trast, women were more strongly influenced by subjective norm and perceived behavioral control. Sustained technology usage behavior was driven by early usage behavior, thus fortifying the lasting influence of gender-based early evaluations of the new technology. These findings were robust across income, organiza-tion position, education, and computer self-efficacy levels. q 2000
1189|Computer technology training in the workplace: a longitudinal investigation of the effect|How does a person&#039;s mood during technology training influence motivation, intentions, and, ultimately, usage of the new technol-ogy? Do these mood effects dissipate or are they sustainable over time? A repeated-measures field study (n 5 316) investigated the effect of mood on employee motivation and intentions toward using a specific computer technology at two points in time: imme-diately after training and 6 weeks after training. Actual usage behavior was assessed for 12 weeks after training. Each individ-ual was assigned to one of three mood treatments: positive, nega-tive, or control. Results indicated that there were only short-term boosts in intrinsic motivation and intention to use the technology among individuals in the positive mood intervention. However, a long-term lowering of intrinsic motivation and intention was observed among those in the negative mood condition. q 1999 Academic Press You spent a wonderful week in Hawaii. You are upbeat. You return to work for an important executive technology training program. Alternatively, you had an argument with your spouse. Your typical 20-min commute took over an hour due to bad traffic. You reach work and head straight for an important executive training program on a new computer software application. Will your mood, altered by either of these two highly plausible scenarios, affect your
1190|The 4+1 view model of architecture|The 4+1 View Model organizes a description of a software architecture using five concurrent views, each of which
addresses a specific set of concerns. Architects capture their design decisions in four views and use the fifth view to illustrate and validate them.
1191|A Study|on the rubrene emission sensitized by a phosphorescent Ir compound in the host of CBP
1192|From Domain Model to Architectures|A software system can be evaluated against criteria in two broad categories: • functional and performance attributes: how well does the system, during execution, satisfy its behavioral, functional, and performance requirements? Does it provide the required results? Does it provide them in a timely enough manner? Are the results correct, or within specified accuracy and stability tolerances? • non-functional attributes: how easy is the system to integrate, test, and modify? How expensive was it to develop? These two categories are orthogonal; systems that unfailingly meet all of their requirements may or may not have been prohibitively expensive to develop, and may or may not be impossible to modify. Highly modifiable systems may or may not produce correct results. Given a set of requirements for a system, the developer must choose an architecture that will allow the implementation of the system to proceed in a straightforward manner, producing a product that meets its functional and non-functional requirements. How is that done? 1.1 Producing architectures to meet functional requirements There is, unfortunately, no reliable automatic or semi-automatic technology that will produce
1193|Determining the Number of Factors in Approximate Factor Models|In this paper we develop some statistical theory for factor models of large dimensions. The focus is the determination of the number of factors, which is an unresolved issue in the rapidly growing literature on multifactor models. We propose a panel Cp criterion and show that the number of factors can be consistently estimated using the criterion. The theory is developed under the framework of large cross-sections (N) and large time dimensions (T). No restriction is imposed on the relation between N and T. Simulations show that the proposed criterion yields almost precise estimates of the number of factors for configurations of the panel data encountered in practice. The idea that variations in a large number of economic variables can be modelled bya small number of reference variables is appealing and is used in manyeconomic analysis. In the finance literature, the arbitrage pricing theory(APT) of Ross (1976) assumes that a small number of factors can be used to explain a large number of asset returns.  
1195|The Generalized Dynamic Factor Model: Identification and Estimation|This paper proposes a factor model with infinite dynamics and non-orthogonal idiosyncratic components. The model, which we call the generalized dynamic factor model, isnovel to the literature, and generalizes the static approximate factor model of Chamberlain and Rothschild (1983), as well as the exact factor model àlaSargent and Sims (1977). We provide identification conditions, propose an estimator of the common components, prove convergence as both time and cross-sectional size go to infinity at appropriate rates and present simulation results. We use our model to construct a coincident index for the European Union. Such index is defined as the common component of real GDP within a model including several macroeconomic variables for each European country.
1197|A semiparametric factor model of interest rates and tests of the affine term structure|Many continuous time term structure of interest rate models assume a factor structure where the drift and volatility functions are a ne functions of the state variable process. These models involve very speci c parametric choices of factors and functional speci cations of the drift and volatility. Moreover, under the a ne term structure restrictions not all factors necessarily a ect interest rates at all maturities simultaneously. This class of so called a ne models covers a wide variety of existing empirical as well as theoretical models in the literature. In this paper we take avery agnostic approach to the speci cation of these di usion functions and test implications of the a ne term structure restrictions. We do not test a speci c model among the class of a ne models per se. Instead, the a ne term structure restrictions we test are based on the derivatives of the responses of interest rates to the factors. We also test how many and which factors a ect a particular rate. These tests are conducted within a framework which models interest rates as functions of \fundamental &#034; factors, and the responses of interest rates to these factors are estimated with non-parametric methods. We consider two sets of factors, one based on key macroeconomic variables, and one based on interest rate spreads. In general, despite their common use we nd that the empirical evidence does not support the restrictions imposed by a ne models. Besides testing the a ne structure restrictions we also uncover a set of fundamental factors which appear remarkably robust in explaining interest rate dynamics at the long and short maturities we consider. We would like to thank the invaluable comments of three referees who helped us improve our paper. An early
1199|Markov Random Field Models in Computer Vision|. A variety of computer vision problems can be optimally posed as Bayesian labeling in which the solution of a problem is defined as the maximum a posteriori (MAP) probability estimate of the true labeling. The posterior probability is usually derived from a prior model and a likelihood model. The latter relates to how data is observed and is problem domain dependent. The former depends on how various prior constraints are expressed. Markov Random Field Models (MRF) theory is a tool to encode contextual constraints into the prior probability. This paper presents a unified approach for MRF modeling in low and high level computer vision. The unification is made possible due to a recent advance in MRF modeling for high level object recognition. Such unification provides a systematic approach for vision modeling based on sound mathematical principles. 1 Introduction  Since its beginning in early 1960&#039;s, computer vision research has been evolving from heuristic design of algorithms to syste...
1200|Constructing Simple Stable Descriptions for Image Partitioning|A new formulation of the image partitioning problem is presented: construct a complete and stable description of an image, in terms of a specified descriptive language, that is simplest in the sense of being shortest. We show that a descriptive language limited to a low-order polynomial description of the intensity variation within each region and a chain-code-like description of the region boundaries yields intuitively satisfying partitions for a wide class of images. The advantage of this formulation is that it can be extended to deal with subsequent steps of the image-understanding problem (or to deal with other image attributes, such as texture) in a natural way by augmenting the descriptive language. Experiments performed on a variety of both real and synthetic images demonstrate the superior performance of this approach over partitioning techniques based on clustering vectors of local image attributes and standard edge-detection techniques. 1 Introduction  The partitioning proble...
1201|A Markov Random Field Model for Object Matching under Contextual Constraints|This paper presents a Markov random field (MRF) model for object recognition in high level vision. The labeling state of a scene in terms of a model object is considered as an MRF or couples MRFs. Within the Bayesian framework, the optimal solution is defined as the maximum a posteriori (MAP) estimate of the MRF. The posterior distribution is derived based on sound mathematical principles from theories of MRF and probability, which is in contrast to heuristic formulations. An experimental result is presented. 1 Introduction  In object recognition, an object is usually represented by a set of primitives or features. These features are attributed by their properties and are constrained to one another by contextual inter-relations. Two issues must be addressed for successful recognition: how to use contextual constraints effectively and how to deal with uncertainties. Markov random field (MRF) theory provides a way of encoding contextual constraints. Since 1980&#039;s, there has been considera...
1202|Toward 3D Vision from Range Images: An Optimization Framework and Parallel Networks |We propose a unified approach to solve low, intermediate and high level computer vision problems for 3D object recognition from range images. All three levels of computation are cast in an optimization framework and can be implemented on neural network style architecture. In the low level computation, the tasks are to estimate curvature images from the input range data. Subsequent processing at the intermediate level is concerned with segmenting these curvature images into coherent curvature sign maps. In the high level, image features are matched against model features based on an object description called  attributed relational graph (ARG). We show that the above computational tasks at each of the three different levels can all be formulated as optimizing a two-term energy function. The first term encodes unary constraints while the second term binary ones. These energy functions are minimized using parallel and distributed relaxation-based algorithms which are well suited for neural...
1204|Shape and motion from image streams under orthography: a factorization method|Inferring scene geometry and camera motion from a stream of images is possible in principle, but is an ill-conditioned problem when the objects are distant with respect to their size. We have developed a factorization method that can overcome this difficulty by recovering shape and motion under orthography without computing depth as an intermediate step. An image stream can be represented by the 2FxP measurement matrix of the image coordinates of P points tracked through F frames. We show that under orthographic projection this matrix is of rank 3. Based on this observation, the factorization method uses the singular-value decomposition technique to factor the measurement matrix into two matrices which represent object shape and camera rotation respectively. Two of the three translation components are computed in a preprocessing stage. The method can also handle and obtain a full solution from a partially filled-in measurement matrix that may result from occlusions or tracking failures. The method gives accurate results, and does not introduce smoothing in either shape or motion. We demonstrate this with a series of experiments on laboratory and outdoor image streams, with and without occlusions. 
1205|Piecewise smooth surface reconstruction|We present a general method for automatic reconstruction of accurate, concise, piecewise smooth surface models from scattered range data. The method can be used in a variety of applications such as reverse engineering — the automatic generation of CAD models from physical objects. Novel aspects of the method are its ability to model surfaces of arbitrary topological type and to recover sharp features such as creases and corners. The method has proven to be effective, as demonstrated by a number of examples using both simulated and real data. A key ingredient in the method, and a principal contribution of this paper, is the introduction of a new class of piecewise smooth surface representations based on subdivision. These surfaces have a number of properties that make them ideal for use in surface reconstruction: they are simple to implement, they can model sharp features concisely, and they can be fit to scattered range data using an unconstrained optimization procedure.
1206|Synthetic Topiary|The paper extends Lindenmayer systems in a manner suitable for simulating the interaction between a developing plant and its environment. The formalism is illustrated by modeling the response of trees to pruning, which yields synthetic images of sculptured plants found in topiary gardens.
1207|Robust Shape Recovery from Occluding Contours Using a Linear Smoother|Recovering the shape of an object from two views fails at occluding contours of smooth objects because the extremal contours are view dependent. For three or more views, shape recovery is possible, and several algorithms have recently been developed for this purpose. We present a new approach to the multiframe stereo problem which does not depend on differential measurements in the image, which may be noise sensitive. Instead, we use a linear smoother to optimally combine all of the measurements available at the contours (and other edges) in all of the images. This allows us to extract a robust and dense estimate of surface shape, and to integrate shape information from both surface markings and occluding contours. Keywords: Computer vision, image sequence analysis, motion analysis and multiframe stereo, shape and object representation, occluding contours (profiles). c flDigital Equipment Corporation 1993. All rights reserved.  1  Computer and Information Science Department, University...
1208|Bucket Elimination: A Unifying Framework for Probabilistic Inference| Probabilistic inference algorithms for belief updating, finding the most probable explanation, the maximum a posteriori hypothesis, and the maximum expected utility are reformulated within the bucket elimination framework. This emphasizes the principles common to many of the algorithms appearing in the probabilistic inference literature and clarifies the relationship of such algorithms to nonserial dynamic programming algorithms. A general method for combining conditioning and bucket elimination is also presented. For all the algorithms, bounds on complexity are given as a function of the problem&#039;s structure.  
1209|Approximating probabilistic inference in Bayesian belief networks is NP-hard|Abstract- A belief network comprises a graphical representation of dependencies between variables of a domain and a set of conditional probabilities associated with each dependency. Unless P=NP, an efficient, exact algorithm does not exist to compute probabilistic inference in belief networks. Stochastic simulation methods, which often improve run times, provide an alternative to exact inference algorithms. We present such a stochastic simulation algorithm 2)-BNRAS that is a randomized approximation scheme. To analyze the run time, we parameterize belief networks by the dependence value PE, which is a measure of the cumulative strengths of the belief network dependencies given background evidence E. This parameterization defines the class of f-dependence networks. The run time of 2)-BNRAS is polynomial when f is a polynomial function. Thus, the results of this paper prove the existence of a class of belief networks for which inference approximation is polynomial and, hence, provably faster than any exact algorithm. I.
1210|The Wake-Sleep Algorithm for Unsupervised Neural Networks|We describe an unsupervised learning algorithm for a multilayer network of stochastic neurons. Bottom-up &#034;recognition&#034; connections convert the input into representations in successive hidden layers and top-down &#034;generative&#034; connections reconstruct the representation in one layer from the representation in the layer above. In the &#034;wake&#034; phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the &#034;sleep&#034; phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above. Supervised learning algorithms for multilayer neural networks face two problems: They require a teacher to specify the desired output of the network and they require some method of communicating error information to all of the connections. The wake-sleep alg...
1211|Keeping Neural Networks Simple by Minimizing the Description Length of the Weights |Supervised neural networks generalize well if there is much less information in the weights than there is in the output vectors of the training cases. So during learning, it is important to keep the weights simple by penalizing the amount of information they contain. The amount of information in a weight can be controlled by adding Gaussian noise and the noise level can be adapted during learning to optimize the trade-off between the expected squared error of the network and the amount of information in the weights. We describe a method of computing the derivatives of the expected squared error and of the amount of information in the noisy weights in a network that contains a layer of non-linear hidden units. Provided the output units are linear, the exact derivatives can be computed efficiently without time-consuming Monte Carlo simulations. The idea of minimizing the amount of information that is required to communicate the weights of a neural network leads to a number of interesting schemes for encoding the weights.  
1212|Probabilistic Diagnosis Using a Reformulation of the INTERNIST-1/QMR Knowledge Base - II. Evaluation of Diagnostic Performance|We have developed a probabilistic reformulation of the Quick Medical Reference (QMR) system. In Part I of this two-part series, we described a two-level, multiply connected belief-network representation of the QMR knowledge base and a simulation algorithm to perform probabilistic inference on the reformulated knowledge base. In Part II of this series, we report on an evaluation of the probabilistic QMR, in which we compare the performance of QMR to that of our probabilistic system on cases abstracted from continuing medical education materials from Scientific American Medicine. In addition, we analyze empirically several components of the probabilistic model and simulation algorithm.
1213|Bayesian Methods for Mixtures of Experts|We present a Bayesian framework for inferring the parameters of a mixture of experts model based on ensemble learning by variational free energy minimisation. The Bayesian approach avoids the over-fitting and noise level under-estimation problems of traditional maximum likelihood inference. We demonstrate these methods on artificial problems and sunspot time series prediction. INTRODUCTION The task of estimating the parameters of adaptive models such as artificial neural networks using Maximum Likelihood (ML) is well documented eg. Geman, Bienenstock
1214|Variational Methods for Inference and Estimation in Graphical Models|Graphical models enhance the representational power of probability models through qualitative characterization of their properties. This also leads to greater efficiency in terms of the computational algorithms that empower such representations. The increasing complexity of these models, however, quickly renders exact probabilistic calculations infeasible. We propose a principled framework for approximating graphical models based on variational methods. We develop variational techniques from the perspective that unifies and expands their applicability to graphical models. These methods allow the (recursive) computation of upper and lower bounds on the quantities of interest. Such bounds yield considerably more information than mere approximations and provide an inherent error metric for tailoring the approximations individually to the cases considered. These desirable properties, concomitant to the variational methods, are unlikely to arise as a result of other deterministic or stochas...
1215|Improving the Mean Field Approximation via the Use of Mixture Distributions|Introduction  Graphical models provide a formalism in which to express and manipulate conditional independence statements. Inference algorithms for graphical models exploit these independence statements, using them to compute conditional probabilities while avoiding brute force marginalization over the joint probability table. Many inference algorithms, in particular the clustering algorithms, make explicit their usage of conditional independence by constructing a data structure that captures the essential Markov properties underlying the graph. That is, the algorithm groups interacting variables into clusters, such that the hypergraph of clusters has Markov properties that allow simple local algorithms to be employed for inference. In the best case, in which the original graph is sparse and without long cycles, the clusters are small and inference is efficient. In the worst case, such as the case of a dense graph, the clusters are large and inference is inefficient (complexity
1216|Switching State-Space Models|We introduce a statistical model for times series data with nonlinear dynamics  which iteratively segments the data into regimes with approximately linear dynamics  and learns the parameters of each of those regimes. This model combines and generalizes  two of the most widely used stochastic time series models---the hidden Markov  model and the linear dynamical system---and is related to models that are widely used  in the control and econometrics literatures. It can also be derived by extending the  mixture of experts neural network model (Jacobs et al., 1991) to its fully dynamical  version, in which both expert and gating networks are recurrent. Inferring the posterior  probabilities of the hidden states of this model is computationally intractable,  and therefore the exact Expectation Maximization (EM) alogithm cannot be applied.  However, we present a variational approximation which maximizes a lower bound on  the log likelihood and makes use of both the forward--backward recursio...
1217|Localized Partial Evaluation of Belief Networks|Most algorithms for propagating evidence through belief networks have been exact and  exhaustive: they produce an exact (pointvalued) marginal probability for every node in the network. Often, however, an application will not need information about every node in the network nor will it need exact probabilities. We present the localized partial evaluation (LPE) propagation algorithm, which computes interval bounds on the marginal probability of a specified query node by examining a subset of the nodes in the entire network. Conceptually, LPE ignores parts of the network that are &#034;too far away&#034; from the queried node to have much impact on its value. LPE has the &#034;anytime&#034; property of being able to produce better solutions (tighter intervals) given more time to consider more of the network. 1 Introduction  Belief networks provide a way of encoding knowledge about the probabilistic dependencies and independencies of a set of variables in some domain. Variables are encoded as nodes in the ne...
1218|Computing Upper and Lower Bounds on Likelihoods in Intractable Networks|We present techniques for computing upper and lower bounds on the likelihoods of partial instantiations  of variables in sigmoid and noisy-OR networks. The bounds determine confidence intervals for the desired  likelihoods and become useful when the size of the network (or clique size) precludes exact computations.
1219|A hierarchical community of experts|We describe a hierarchical generative model that selects from a large collection of available linear units an appropriate subset to model each observation. The selection mechanism is a corresponding network of binary units each of which gates the output of a linear unit. Inference in the binary network is intractable, but the statistics required to learn maximum-likelihood model parameters can be approximated with Gibbs sampling, even if the sampling is so brief that the Markov chain is far from equilibrium. 1 Multilayer networks of linear-Gaussian units We consider directed acyclic networks of simple stochastic units, where the units are arranged in layers. The input to a unit is the weighted sum of the activities of units in the layer above, plus a bias. In the generative model, the joint probability of all of the units in the network taking on a particular set of values, or configuration, can be factored into a product of probabilities of individual units, conditioned on the units in the layer above. The simplest unit we will consider is a linear-Gaussian unit. The probability that a linear-Gaussian unit takes on a particular value is given by a Gaussian distribution centered at the top-down prediction of the unit’s parents. The top-down prediction for unit i, denoted ?yi, is the weighted sum of its parents ’ outputs, plus a bias: ?yi = ? j?P a(i)
1220|Approximating Posterior Distributions in Belief Networks using Mixtures|Exact inference in densely connected Bayesian networks is computationally intractable, and so there is considerable interest in developing effective approximation schemes. One approach which has been adopted is to bound the log likelihood using a mean-field approximating distribution. While this leads to a tractable algorithm, the mean field distribution is assumed to be factorial and hence unimodal. In this paper we demonstrate the feasibility of using a richer class of approximating distributions based on mixtures of mean field distributions. We derive an efficient algorithm for updating the mixture parameters and apply it to the problem of learning in sigmoid belief networks. Our results demonstrate a systematic improvement over simple mean field theory as the number of mixture components is increased. 1 Introduction Bayesian belief networks can be regarded as a fully probabilistic interpretation of feedforward neural networks. Maximum likelihood learning for Bayesian n...
1221|Recursive Algorithms for Approximating Probabilities in Graphical Models|We develop a recursive node-elimination formalism for efficiently approximating large probabilistic networks. No constraints are set on the network topologies. Yet the formalism can be straightforwardly integrated with exact methods whenever they are/become applicable. The approximations we use are controlled: they maintain consistently upper and lower bounds on the desired quantities at all times. We show that Boltzmann machines, sigmoid belief networks, or any combination (i.e., chain graphs) can be handled within the same framework. The accuracy of the methods is verified experimentally.   1 Introduction  Graphical models (see, e.g., Lauritzen 1996) provide a medium for rigorously embedding domain knowledge into network models. The structure in these graphical models embodies the qualitative assumptions about the independence relationships in the domain while the probability model attached to the graph permits a consistent computation of belief (or uncertainty) about the values of t...
1222|A Statistical Approach to Decision Tree Modeling|A statistical approach to decision tree modeling is described. In this approach, each decision in the tree is modeled parametrically as is the process by which an output is generated from an input and a sequence of decisions. The resulting model yields a likelihood measure of goodness of fit, allowing ML and MAP estimation techniques to be utilized. An efficient algorithm is presented to estimate the parameters in the tree. The model selection problem is presented and several alternative proposals are considered. A hidden Markov version of the tree is described for data sequences that have temporal dependencies.
1223|Learning in Boltzmann Trees|We introduce a large family of Boltzmann machines that can be trained using standard gradient descent. The networks can have one or more layers of hidden units, with tree-like connectivity. We show how to implement the supervised learning algorithm for these Boltzmann machines exactly, without resort to simulated or mean-field annealing. The stochastic averages that yield the gradients in weight space are computed by the technique of decimation. We present results on the problems of N-bit  parity and the detection of hidden symmetries. 1 Introduction  Boltzmann machines (Ackley, Hinton, &amp; Sejnowski, 1985) have several compelling virtues. Unlike simple perceptrons, they can solve problems that are not linearly separable. The learning rule, simple and locally based, lends itself to massive parallelism. The theory of Boltzmann learning, moreover, has a solid foundation in statistical mechanics. Unfortunately, Boltzmann machines--- as originally conceived---also have some serious drawbacks...
1224|Reduction of Computational Complexity in Bayesian Networks through Removal of Weak Dependences|The paper presents a method for reducing the computational complexity of Bayesian networks through identification and removal of weak dependences (removal of links from the (moralized) independence graph). The removal of a small number of links may reduce the computational complexity dramatically, since several fill-ins and moral links may be rendered superfluous by the removal. The method is described in terms of impact on the independence graph, the junction tree, and the potential functions associated with these. An empirical evaluation of the method using large real-world networks demonstrates the applicability of the method. Further, the method, which has been implemented in Hugin, complements the approximation method suggested by Jensen &amp; Andersen (1990). 
1225|Variational methods and the QMR-DT database|We describe variational approximation methods for e cient probabilistic reasoning, applying these methods to the problem of diagnostic inference in the QMR-DT database. The QMR-DT database is a large-scale belief network based on statistical and expert knowledge in internal medicine. The size and complexity of this network render exact probabilistic diagnosis infeasible for all but a small set of cases. This has hindered the development of the QMR-DT network as a practical diagnostic tool and has hindered researchers from exploring and critiquing the diagnostic behavior of QMR. In this paper we describe how variational approximation methods can be applied to the QMR network, resulting in fast diagnostic inference. We evaluate the accuracy of our methods on a set of standard diagnostic cases and compare to stochastic sampling methods. 1
1226|A Mean Field Learning Algorithm For Unsupervised Neural Networks|. We introduce a learning algorithm for unsupervised neural networks based on ideas from statistical mechanics. The algorithm is derived from a mean field approximation for large, layered sigmoid belief networks. We show how to (approximately) infer the statistics of these networks without resort to sampling. This is done by solving the mean field equations, which relate the statistics of each unit to those of its Markov blanket. Using these statistics as target values, the weights in the network are adapted by a local delta rule. We evaluate the strengths and weaknesses of these networks for problems in statistical pattern recognition. 1. Introduction  Multilayer neural networks trained by backpropagation provide a versatile framework for statistical pattern recognition. They are popular for many reasons, including the simplicity of the learning rule and the potential for discovering hidden, distributed representations of the problem space. Nevertheless, there are many issues that are...
1227|Annealed Theories of Learning|We study annealed theories of learning boolean functions using a concept class of finite cardinality. The naive annealed theory can be used to derive a universal learning curve bound for zero temperature learning, similar to the inverse square root bound from the Vapnik-Chervonenkis theory. Tighter, nonuniversal learning curve bounds are also derived. A more refined annealed theory leads to still tighter bounds, which in some cases are very similar to results previously obtained using one-step replica symmetry breaking. 1. Introduction  The annealed approximation  1  has proven to be an invaluable tool for studying the statistical mechanics of learning from examples. Previously it was found that the annealed approximation gave qualitatively correct results for several models of perceptrons learning realizable rules.  2  Because of its simplicity relative to the full quenched theory, the annealed approximation has since been used in studies of more complicated multilayer architectures. ...
1228|Deformable models in medical image analysis: A survey|This article surveys deformable models, a promising and vigorously researched computer-assisted medical image analysis technique. Among model-based techniques, deformable models offer a unique and powerful approach to image analysis that combines geometry, physics, and approximation theory. They have proven to be effective in segmenting, matching, and tracking anatomic structures by exploiting (bottom-up) constraints derived from the image data together with (top-down) a priori knowledge about the location, size, and shape of these structures. Deformable models are capable of accommodating the significant variability of biological structures over time and across different individuals. Furthermore, they support highly intuitive interaction mechanisms that, when necessary, allow medical scientists and practitioners to bring their expertise to bear on the model-based image interpretation task. This article reviews the rapidly expanding body of work on the development and application of deformable models to problems of fundamental importance in medical image analysis, includingsegmentation, shape representation, matching, and motion tracking.
1229|Geodesic Active Contours|  A novel scheme for the detection of object boundaries is presented. The technique is based on active contours evolving in time according to intrinsic geometric measures of the image. The evolving contours naturally split and merge, allowing the simultaneous detection of several objects and both interior and exterior boundaries. The proposed approach is based on the relation between active contours and the computation of geodesics or minimal distance curves. The minimal distance curve lays in a Riemannian space whose metric is defined by the image content. This geodesic approach for object segmentation allows to connect classical “snakes ” based on energy minimization and geometric active contours based on the theory of curve evolution. Previous models of geometric active contours are improved, allowing stable boundary detection when their gradients suffer from large variations, including gaps. Formal results concerning existence, uniqueness, stability, and correctness of the evolution are presented as well. The scheme was implemented using an efficient algorithm for curve evolution. Experimental results of applying the scheme to real images including objects with holes and medical data imagery demonstrate its power. The results may be extended to 3D object segmentation as well.
1230|Computer Vision|Driver inattention is one of the main causes of traffic accidents. Monitoring a driver to detect inattention is a complex problem that involves physiological and behavioral elements. Different approaches have been made, and among them Computer Vision has the potential of monitoring the person behind the wheel without interfering with his driving. In this paper I have developed a system that can monitor the alertness of drivers in order to prevent people from falling asleep at the wheel. The other main aim of this algorithm is to have efficient performance on low quality webcam and without the use of infrared light which is harmful for the human eye. Motor vehicle accidents cause injury and death, and this system will help to decrease the amount of crashes due to fatigued drivers. The proposed algorithm will work in three main stages. In first stage the face of the driver is detected and tracked. In the second stage the facial features are extracted for further processing. In last stage the most crucial parameter is monitored which is eye’s status. In the last stage it is determined that whether the eyes are closed or open. On the basis of this result the warning is issued to the driver to take a break.
1231|Shape modeling with front propagation: A level set approach|Abstract- Shape modeling is an important constituent of computer vision as well as computer graphics research. Shape models aid the tasks of object representation and recognition. This paper presents a new approach to shape modeling which re-tains some of the attractive features of existing methods and over-comes some of their limitations. Our techniques can be applied to model arbitrarily complex shapes, which include shapes with significant protrusions, and to situations where no a priori as-sumption about the object’s topology is made. A single instance of our model, when presented with an image having more than one object of interest, has the ability to split freely to represent each object. This method is based on the ideas developed by Osher and Sethian to model propagating solidhiquid interfaces with curva-ture-dependent speeds. The interface (front) is a closed, noninter-secting, hypersurface flowing along its gradient field with con-stant speed or a speed that depends on the curvature. It is moved by solving a “Hamilton-Jacob? ’ type equation written for a func-tion in which the interface is a particular level set. A speed term synthesizpd from the image is used to stop the interface in the vi-cinity of object boundaries. The resulting equation of motion is solved by employing entropy-satisfying upwind finite difference schemes. We present a variety of ways of computing evolving front, including narrow bands, reinitializations, and different stopping criteria. The efficacy of the scheme is demonstrated with numerical experiments on some synthesized images and some low contrast medical images. Index Terms- Shape modeling, shape recovery, interface mo-tion, level sets, hyperbolic conservation laws, Hamilton-Jacobi
1232|The Use of Active Shape Models For Locating Structures in Medical Images|This paper describes a technique for building compact models of the shape  and appearance of flexible objects (such as organs) seen in 2-D images. The models  are derived from the statistics of sets of labelled images of examples of the objects. Each model
1233|Realistic Modeling for Facial Animation|A major unsolved problem in computer graphics is the construction and animation of realistic human facial models. Traditionally, facial models have been built painstakingly by manual digitization and animated by ad hoc parametrically controlled facial mesh deformations or kinematic approximation of muscle actions. Fortunately, animators are now able to digitize facial geometries through the use of scanning range sensors and animate them through the dynamic simulation of facial tissues and muscles. However, these techniques require considerableuser input to construct facial models of individuals suitable for animation. In this paper, we present a methodology for automating this challenging task. Starting with a structured facial mesh, we develop algorithms that automatically construct functional models of the heads of human subjects from laser-scanned range and reflectance data. These algorithms automatically insert contractile muscles at anatomically correct positions within a dynamic skin model and root them in an estimated skull structure with a hinged jaw. They also synthesize functional eyes, eyelids, teeth, and a neck and fit them to the final model. The constructed face may be animated via muscle actuations. In this way, we create the most authentic and functional facial models of individuals available to date and demonstrate their use in facial animation.  
1234|Boundary Finding with Parametrically Deformable Models|Introduction  This work describes an approach to finding objects in images based on deformable shape models. Boundary finding in two and three dimensional images is enhanced both by considering the bounding contour or surface as a whole and by using model-based shape information.  Boundary finding using only local information has often been frustrated by poor-contrast boundary regions due to occluding and occluded objects, adverse viewing conditions and noise. Imperfect image data can be augmented with the extrinsic information that a geometric shape model provides. In order to exploit model-based information to the fullest extent, it should be incorporated explicitly, specifically, and early in the analysis. In addition, the bounding curve or surface can be profitably considered as a whole, rather than as curve or surface segments, because it tends to result in a more consistent solution overall.  These models are best suited for objects whose diversity and irregularity of shape make 
1235|Topologically Adaptable Snakes|This paper presents a topologically adaptable snakes model for image segmentation and object representation. The model is embedded in the framework of domain subdivision using simplicial decomposition. This framework extends the geometric and topological adaptability of snakes while retaining all of the features of traditionalsnakes, such as user interaction, and overcoming many of the limitations of traditionalsnakes. By superposing a simplicial grid over the image domain and using this grid to iteratively reparameterize the deforming snakes model, the model is able to flow into complex shapes, even shapes with significant protrusions or branches, and to dynamically change topology as necessitated by the data. Snakes can be created and can split into multiple parts or seamlessly merge into other snakes. The model can also be easily converted to and from the traditional parametric snakes model representation. We apply a 2D model to various synthetic and real images in order to segment ...
1236|Finite Element Methods for Active Contour Models and Balloons for 2D and 3D Images|The use of energy-minimizing curves, known as &#034;snakes&#034; to extract features of interest in images has been introduced by Kass, Witkin and Terzopoulos [23]. A balloon model was introduced in [12] as a way to generalize and solve some of the problems encountered with the original method. We present a 3D generalization of the balloon model as a 3D deformable surface, which evolves in 3D images. It is deformed under the action of internal and external forces attracting the surface toward detected edgels by means of an attraction potential. We also show properties of energy-minimizing surfaces concerning their relationship with 3D edge points. To solve the minimization problem for a surface, two simplified approaches are shown first, defining a 3D surface as a series of 2D planar curves. Then, after comparing Finite Element Method and Finite Difference Method in the 2D problem, we solve the 3D model using the Finite Element Method yielding greater stability and faster convergence. We have a...
1237|Closed-Form Solutions for Physically Based Shape Modeling and Recognition| We present a closed-form, physically based solution for recovering a 3-D solid model from collections of 3-D surface measurements. Given a sufficient number of independent mea-surements, the solution is overconstrained and unique except for rotational symmetries. We then present a physically based object-recognition method that allows simple, closed-form comparisons of recovered 3-D solid models. The performance of these methods is evaluated using both synthetic range data with various signal-to-noise ratios and using laser rangefinder data. 
1238|A Dynamic Finite Element Surface Model for Segmentation and Tracking in Multidimensional Medical Images with Application to Cardiac 4D Image Analysis|This paper presents a physics-based approach to anatomical surface segmentation, reconstruction, and tracking in multidimensional medical images. The approach makes use of a dynamic &#034;balloon&#034; model---a spherical thin-plate under tension surface spline which deforms elastically to fit the image data. The fitting process is mediated by internal forces stemming from the elastic properties of the spline and external forces which are produced from the data. The forces interact in accordance with Lagrangian equations of motion that adjust the model&#039;s deformational degrees of freedom to fit the data. We employ the finite element method to represent the continuous surface in the form of weighted sums of local polynomial basis functions. We use a quintic triangular finite element whose nodal variables include positions as well as the first and second partial derivatives of the surface. We describe a system, implemented on a high performance graphics workstation, which applies the model fitting ...
1239|Volumetric deformable models with parameter functions: A new approach to the 3-D motion analysis|endorsement of any of the University of Pennsylvania&#039;s products or services. Internal or personal use of this material is permitted. However, permission to reprint/republish this material for advertising or promotional purposes or for creating new collective works for resale or redistribution must be obtained from the IEEE by writing to pubs-permissions@ieee.org. By choosing to view this document, you agree to all provisions of the copyright laws protecting it.
1240|An Active Contour Model For Mapping The Cortex|A new active contour model for finding and mapping the outer cortex in brain images is developed. A cross-section of the brain cortex is modeled as a ribbon, and a constant speed mapping of its spine is sought. A variational formulation, an associated force balance condition, and a numerical approach are proposed to achieve this goal. The primary difference between this formulation and that of snakes is in the specification of the external force acting on the active contour. A study of the uniqueness and fidelity of solutions is made through convexity and frequency domain analyses, and a criterion for selection of the regularization coefficient is developed. Examples demonstrating the performance of this method on simulated and real data are provided.  
1241|Tracking Points on Deformable Objects Using Curvature Information|The objective of this paper is to present a significant improvement to the approach of Duncan et al. [1, 8] to analyze the deformations of curves in sequences of 2D images. This approach is based on the paradigm that high curvature points usually possess an anatomical meaning, and are therefore good landmarks to guide the matching process, especially in the absence of a reliable physical or deformable geometric model of the observed structures. As Duncan&#039;s
1242|Image Registration Based on Boundary Mapping|A new two-stage approach for nonlinear brain image registration is proposed. In the first stage, an active contour algorithm is used to establish a homothetic one-to-one map between a set of region boundaries in two images to be registered. This mapping is used in the second step: a two-dimensional transformation which is based on an elastic body deformation. This method is tested by registering magnetic resonance images to atlas images. I. Introduction  Registration of both intra-subject and inter-subject brain images has been the subject of extensive study in the medical imaging literature. The various techniques that have been proposed can be classified into three major categories: polynomial transformations, similarity-based methods, and boundary-based methods. Polynomial transformations [1, 2, 3] apply a polynomial warping and determine the coefficients of the polynomial using linear regression if a sufficient number of landmark points is provided. Numerical instabilities and the ...
1243|Curves and Surfaces for CAGD|This article provides a historical account of the major developments in the area of curves and surfaces as they entered the area of CAGD – Computer Aided Geometric Design – until the middle 1980s. We adopt the definition that CAGD deals with the construction and representation of free-form curves, surfaces, or volumes. 1.
1244|Analysis of Left Ventricular Wall Motion Based on Volumetric Deformable Models and MRI-SPAMM|We present a new approach for the analysis of the left ventricular shape and motion that is based on the development of a new class of volumetric deformable models. We estimate the deformation and complex motion of the left ventricle (LV) in terms of a few parameters that are functions and whose values vary locally across the LV. These parameters capture the radial and longitudinal contraction, the axial twisting, and the long-axis deformation. Using Lagrangian dynamics and the finite element theory, we convert these volumetric primitives into dynamic models that deform due to forces exerted by the datapoints. We present experiments where we used magnetic tagging (MRI-SPAMM) to acquire datapoints from the LV during systole. By applying our method to MRI-SPAMM datapoints, we were able to characterize both locally and globally the 3D shape and motion of the LV in a clinically useful way. In addition, based on the model parameters we were able to extract quantitative differences between n...
1245|A parametric deformable model to fit unstructured 3D data|Recovery of unstructured 3D data with deformable models has been the subject of many studies over the last ten years. In particular, in medical image understanding, deformable models are useful to get a precise representation of anatomical structures. However, general deformable models involve large linear systems to solve when dealing with high resolution 3D images. The advantage of parametric deformable models like superquadrics is their small number of parameters to describe a shape combined with a better robustness in the presence of noise or sparse data. Also, at the expense of a reasonable number of additional parameters, free form deformations provide a much closer fit and a volumetric deformation field. This article introduces such a model to fit unstructured 3D points with a parametric deformable surface based on a superquadric fit followed by a free form deformation to describe the cardiac left ventricle. We present the mathematical and algorithmic details of the method, as wel...
1246|Medical computer vision, virtual reality and robotics|The automated analysis of 3D medical images can improve both diagnosis and therapy significantly. This automation raises a number of new fascinating research problems in the fields of computer vision, graphics and robotics. In this paper, I propose a list of such problems after a review of the current major 3D imaging modalities, and a description of the related medical needs. I then present some of the past and current work done in our research group EPIDAURE * at INRIA, on the following topics: segmentation of 3D images; 3D shape modelling; 3D rigid and nonrigid registration; 3D motion analysis; and 3D simulation of therapy. Most topics are discussed in a synthetic manner, and illustrated by results. Rigid matching is treated more thoroughly as an illustration of a transfer from computer vision towards 3D image processing. The later topics are illustrated by preliminary results, and a number of promising research tracks are suggested.
1247|Model-Based Interpretation of 3D Medical Images|The automatic segmentation and labelling of anatomical structures in 3D medical  imagesis a challenging task of practical importance. We describe amodel- based approach  which allows robust and accurate interpretation using explicit anatomical  knowledge. Our method is based on the extension to 3D of Point Distribution Models  (PDMs) and associated image search algorithms. Acombination of global, GeneticAlgorithm  (GA), and local, Active Shape Model (ASM), search is used. We have  built a 3D PDM of the human brain describing a number of major structures. Using  this model we have obtained automatic interpretations for 30 3D Magnetic Resonance  head images from different individuals. The results have been evaluated  quantitativelyand support our claim of robust and accurate interpretation.
1248|Fast Segmentation, Tracking, and Analysis of Deformable Objects|We present a physically-based deformable model which can be used to track and to analyze non-rigid motion of dynamic structures in time sequences of 2D or 3D medical images. The model considers an object undergoing an elastic deformation as a set of masses linked by springs, where the natural lengths of the springs is set equal to zero, and is replaced by a set of constant equilibrium forces, which characterize the shape of the elastic structure in the absence of external forces. This model has the extremely nice property of yielding dynamic equations which are linear and decoupled for each coordinate, whatever the amplitude of the deformation. It provides a reduced algorithmic complexity, and a sound framework for modal analysis, which allows a compact representation of a general deformation by a reduced number of parameters. The power of the approach to segment, track and analyze 2-D and 3-D images is demonstrated by a set of experimental results on various complex medical images.  1...
1249|Brownian Strings: Segmenting Images with Stochastically Deformable Models|Abstract—This paper describes an image segmentation technique in which an arbitrarily shaped contour was deformed stochastically until it fitted around an object of interest. The evolution of the contour was controlled by a simulated annealing process which caused the contour to settle into the global minimum of an image-derived “energy ” function. The nonparametric energy function was derived from the statistical properties of previously segmented images, thereby incorporating prior experience. Since the method was based on a state space search for the contour with the best global properties, it was stable in the presence of image errors which confound segmentation techniques based on local criteria, such as connectivity. Unlike “snakes ” and other active contour approaches, the new method could handle arbitrarily irregular contours in which each interpixel crack represented an independent degree of freedom. Furthermore, since the contour evolved toward the global minimum of the energy, the method was more suitable for fully automatic applications than the snake algorithm, which frequently has to be reinitialized when the contour becomes trapped in local energy minima. High computational complexity was avoided by efficiently introducing a random local perturbation in a time independent of contour length, providing control over the size of the perturbation, and assuring that resulting shape changes were unbiased. The method was illustrated by using it to find the brain surface in magnetic resonance head images and to track blood vessels in angiograms. Additional information is available from
1250|Three--dimensional medical imaging: Algorithms and computer systems|This paper presents an introduction to the field of three-dimensional medical imaging It presents medical imaging terms and concepts, summarizes the basic operations performed in three-dimensional medical imaging, and describes sample algorithms for accomplishing these operations. The paper contains a synopsis of the architectures and algorithms used in eight machines to render three-dimensional medical images, with particular emphasis paid to their distinctive contributions. It compares the performance of the machines along several dimensions, including image resolution, elapsed time to form an image, imaging algorithms used in the machine, and the degree of parallehsm used in the architecture. The paper concludes with general trends for future developments in this field and references on three-dimensional medical imaging.
1251|Automatic Retrieval of Anatomical Structures in 3D Medical Images|This paper describes a method to automatically generate the mapping between a completely labeled reference image and the 3D medical image of a patient. Toachieve this, we combined three techniques: the extraction of 3D feature lines, their matching using 3D deformable line models, the extension of the deformation to the whole image space using warping techniques. We present experimental results for the segmentation of structures in Magnetic Resonance images of the brain of different patients; the segmentation of the cortical and ventricle structures. We emphasize the advantages of using crest lines deformable models prior to surface based models. This gives a sparser representation of the data, easier to manipulate, and which makes the convergence of the model much less sensitive to initial positionning. In the future, we hope to use this method to generate anatomical atlases, by the automatic interpretation of large sets of 3D medical images.
1252|Superquadrics and free-form deformations : a global model to fit and track 3D medical data|Recovery of 3-D data with simple parametric models has been the subject of many studies over the last ten years. Many have used the notion of superquadrics, introduced for graphics in [4]. It appears, however, that although superquadrics can describe a wide variety of forms, they are too simple to recover and describe complex shapes. This paper describes a method to øt to 3-D points and then track a parametric deformable surface. We suppose that a 3-D image has been segmented to get a set of 3-D points. A ørst estimate consists of our version of a superquadric fit with global tapering. We then apply the technique of free-form deformations, as introduced by [9] in computer graphics to refine the estimate. We present experimental results with real 3-D medical images, where the original points are laid on an iso-surface. This is also applied to give efficient tracking of the deformation of the myocardium
1253|Integration Of Boundary Finding And Regionbased Segmentation Using Game Theory|. Robust segmentation of structures from an image is essential for a variety of applications in biomedical image analysis. Here we propose a method that integrates region based segmentation and gradient based boundary finding using game theory in an effort to form a unified approach that is robust to noise and poor initialization. The novelty of the method is that this is a bi-directional framework whereby the two seperate modules improve their results through mutual information sharing.  Keywords: game theory, boundary finding, region based segmentation, Maximum A posteriori probability  1. Introduction  Precise segmentation of underlying objects in an image is very important especially for biomedical image analysis where it constitutes an important pre-processing step to such tasks as the registration of images obtained from two modalities, quantitative analysis of anatomical structures, the derivation of priors for image reconstruction in another modality and cardiac motion tracking...
1254|Analyzing the Deformation of the Left Ventricle of the Heart with a Parametric Deformable Model|We present a new approach to analyze the deformation of the left ventricle of the heart, based on a parametric model that gives a compact representation of a set of points in a 3D image. We present four different approaches to tracking surfaces in a sequence of 3D cardiac images. Following tracking, we then infer quantitative parameters which are useful for the physician, suchas  the variation of volume and wall thickness during a cardiac cycle, the ejection fraction or the twist component in the deformation of the ventricle. We explicit the computation of these parameters using our model. Experimental results are shown in time sequences of two kinds of medical images, Nuclear Medicine and X-Ray Computed Tomography (CT).
1255|Reconstruction of 3D medical images: A nonlinear interpolation technique for reconstruction of 3D medical images|Three-dimensional medical images reconstructed from a series of two-dimensional images produced by computerized tomography, magnetic resonance imaging, etc., present a valuable tool for modem medicine. Usually, the inter-resolution between two cross sections is less than the intraresolution within each cross section. Therefore, interpolations are required to create a 3D visualization. Many techniques, including voxel-based and patch tiling methods, apply linear interpolations between two cross sections. Although those techniques using linear interpolations are economical in computation, they need much cross-sectional data and are unable to enlarge because of aliasmg. Hence, the techniques that apply two-dimensional nonlinear interpolation functions among cross sections were proposed. In this paper, we introduce the curvature sampling of the contour of a medical object in a CT (computerized tomography) image. Those sampled contour points are the candidates for the control points of Hermite surfaces between each pair of cross sections. Then, a nearest-neighbor mapping of control points between every two cross sections is used for surface formation. The time complexity of our mapping algorithm is O(m + n), where m and II are the numbers of control points of two cross sections. It is much faster than Kehtamavaz and De Figueiredo’s merge method, whose time complexity is O(n’m~). 0 1991 Academic Press, Inc. 1.
1256|A tutorial on particle filters for online nonlinear/non-Gaussian Bayesian tracking|Increasingly, for many application areas, it is becoming important to include elements of nonlinearity and non-Gaussianity in order to model accurately the underlying dynamics of a physical system. Moreover, it is typically crucial to process data on-line as it arrives, both from the point of view of storage costs as well as for rapid adaptation to changing signal characteristics. In this paper, we review both optimal and suboptimal Bayesian algorithms for nonlinear/non-Gaussian tracking problems, with a focus on particle filters. Particle filters are sequential Monte Carlo methods based on point mass (or “particle”) representations of probability densities, which can be applied to any state-space model and which generalize the traditional Kalman filtering methods. Several variants of the particle filter such as SIR, ASIR, and RPF are introduced within a generic framework of the sequential importance sampling (SIS) algorithm. These are discussed and compared with the standard EKF through an illustrative example. 
1258|Kernel independent component analysis|We present a class of algorithms for independent component analysis (ICA) which use contrast functions based on canonical correlations in a reproducing kernel Hilbert space. On the one hand, we show that our contrast functions are related to mutual information and have desirable mathematical properties as measures of statistical dependence. On the other hand, building on recent developments in kernel methods, we show that these criteria can be computed efficiently. Minimizing these criteria leads to flexible and robust algorithms for ICA. We illustrate with simulations involving a wide variety of source distributions, showing that our algorithms outperform many of the presently known algorithms. 1.
1259|Rao-Blackwellised Particle Filtering for Dynamic Bayesian Networks |Particle filters (PFs) are powerful sampling-based inference/learning algorithms for dynamic Bayesian networks (DBNs). They allow us to treat, in a principled way, any type of probability distribution, nonlinearity and non-stationarity. They have appeared in several fields under such names as “condensation”, “sequential Monte Carlo” and “survival of the fittest”. In this paper, we show how we can exploit the structure of the DBN to increase the efficiency of particle filtering, using a technique known as Rao-Blackwellisation. Essentially, this samples some of the variables, and marginalizes out the rest exactly, using the Kalman filter, HMM filter, junction tree algorithm, or any other finite dimensional optimal filter. We show that Rao-Blackwellised particle filters (RBPFs) lead to more accurate estimates than standard PFs. We demonstrate RBPFs on two problems, namely non-stationary online regression with radial basis function networks and robot localization and map building. We also discuss other potential application areas and provide references to some Þnite dimensional optimal filters.  
1260|Algebraic Decision Diagrams and their Applications|In this paper we present theory and experiments on the Algebraic Decision Diagrams (ADD&#039;s). These diagrams extend BDD&#039;s by allowing values from an arbitrary finite domain to be associated with the terminal nodes. We present a treatment founded in boolean algebras and discuss algorithms and results in applications like matrix multiplication and shortest path algorithms. Furthermore, we outline possible applications of ADD&#039;s to logic synthesis, formal verification, and testing of digital systems.
1261|Tractable inference for complex stochastic processes|The monitoring and control of any dynamic system depends crucially on the ability to reason about its current status and its future trajectory. In the case of a stochastic system, these tasks typically involve the use of a belief state—a probability distribution over the state of the process at a given point in time. Unfortunately, the state spaces of complex processes are very large, making an explicit representation of a belief state intractable. Even in dynamic Bayesian networks (DBNs), where the process itself can be represented compactly, the representation of the belief state is intractable. We investigate the idea of maintaining a compact approximation to the true belief state, and analyze the conditions under which the errors due to the approximations taken over the lifetime of the process do not accumulate to make our answers completely irrelevant. We show that the error in a belief state contracts exponentially as the process evolves. Thus, even with multiple approximations, the error in our process remains bounded indefinitely. We show how the additional structure of a DBN can be used to design our approximation scheme, improving its performance significantly. We demonstrate the applicability of our ideas in the context of a monitoring task, showing that orders of magnitude faster inference can be achieved with only a small degradation in accuracy. 1
1262|Operations for Learning with Graphical Models|This paper is a multidisciplinary review of empirical, statistical learning from a graphical model perspective. Well-known examples of graphical models include Bayesian networks, directed graphs representing a Markov chain, and undirected networks representing a Markov field. These graphical models are extended to model data analysis and empirical learning using the notation of plates. Graphical operations for simplifying and manipulating a problem are provided including decomposition, differentiation, and the manipulation of probability models from the exponential family. Two standard algorithm schemas for learning are reviewed in a graphical framework: Gibbs sampling and the expectation maximization algorithm. Using these operations and schemas, some popular algorithms can be synthesized from their graphical specification. This includes versions of linear regression, techniques for feed-forward networks, and learning Gaussian and discrete Bayesian networks from data. The paper conclu...
1263|Mixture Kalman filters|In treating dynamic systems,sequential Monte Carlo methods use discrete samples to represent a complicated probability distribution and use rejection sampling, importance sampling and weighted resampling to complete the on-line `filtering&#039; task. We propose a special sequential Monte Carlo method,the mixture Kalman filter, which uses a random mixture of the Gaussian distributions to approximate a target distribution. It is designed for on-line estimation and prediction of conditional and partial conditional dynamic linear models,which are themselves a class of widely used non-linear systems and also serve to approximate many others. Compared with a few available filtering methods including Monte Carlo methods,the gain in efficiency that is provided by the mixture Kalman filter can be very substantial. Another contribution of the paper is the formulation of many non-linear systems into conditional or partial conditional linear form,to which the mixture Kalman filter can be applied. Examples in target tracking and digital communications are given to demonstrate the procedures proposed.
1264|Semiring-Based Constraint Satisfaction and Optimization|We introduce a general framework for constraint satisfaction and optimization where classical CSPs, fuzzy CSPs, weighted CSPs, partial constraint satisfaction, and others can be easily cast. The framework is based on a semiring structure, where the set of the semiring specifies the values to be associated with each tuple of values of the variable domain, and the two semiring operations (1 and 3) model constraint projection and combination respectively. Local consistency algorithms, as usually used for classical CSPs, can be exploited in this general framework as well, provided that certain conditions on the semiring operations are satisfied. We then show how this framework can be used to model both old and new constraint solving and optimization schemes, thus allowing one to both formally justify many informally taken choices in existing schemes, and to prove that local consistency techniques can be used also in newly defined schemes.
1265|Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables|We discuss Bayesian methods for learning Bayesian networks when data sets are incomplete. In particular, we examine asymptotic approximations for the marginal likelihood of incomplete data given a Bayesian network. We consider the Laplace approximation and the less accurate but more efficient BIC/MDL approximation. We also consider approximations proposed by Draper (1993) and Cheeseman and Stutz (1995). These approximations are as efficient as BIC/MDL, but their accuracy has not been studied in any depth. We compare the accuracy of these approximations under the assumption that the Laplace approximation is the most accurate. In experiments using synthetic data generated from discrete naive-Bayes models having a hidden root node, we find that (1) the BIC/MDL measure is the least accurate, having a bias in favor of simple models, and (2) the Draper and CS measures are the most accurate. 1
1266|Stochastic Dynamic Programming with Factored Representations|Markov decision processes(MDPs) have proven to be popular models for decision-theoretic planning, but standard dynamic programming algorithms for solving MDPs rely on explicit, state-based specifications and computations. To alleviate the combinatorial problems associated with such methods, we propose new representational and computational techniques for MDPs that exploit certain types of problem structure. We use dynamic Bayesian networks (with decision trees representing the local families of conditional probability distributions) to represent stochastic actions in an MDP, together with a decision-tree representation of rewards. Based on this representation, we develop versions of standard dynamic programming algorithms that directly manipulate decision-tree representations of policies and value functions. This generally obviates the need for state-by-state computation, aggregating states at the leaves of these trees and requiring computations only for each aggregate state. The key to these algorithms is a decision-theoretic generalization of classic regression analysis, in which we determine the features relevant to predicting expected value. We demonstrate the method empirically on several planning problems,
1267|Adaptive Probabilistic Networks with Hidden Variables|. Probabilistic networks (also known as Bayesian belief networks) allow a compact description of complex stochastic relationships among several random variables. They are rapidly becoming the tool of choice for uncertain reasoning in artificial intelligence. In this paper, we investigate the problem of learning probabilistic networks with known structure and hidden variables. This is an important problem, because structure is much easier to elicit from experts than numbers, and the world is rarely fully observable. We present a gradient-based algorithmand show that the gradient can be computed locally, using information that is available as a byproduct of standard probabilistic network inference algorithms. Our experimental results demonstrate that using prior knowledge about the structure, even with hidden variables, can significantly improve the learning rate of probabilistic networks. We extend the method to networks in which the conditional probability tables are described using a ...
1268|Learning Equivalence Classes Of Bayesian Network Structures|Approaches to learning Bayesian networks  from data typically combine a scoring metric  with a heuristic search procedure. Given  aBayesian network structure, many of the  scoring metrics derived in the literature return  a score for the entire equivalence class  to which the structure belongs. When using  such a metric, it is appropriate for the heuristic  search algorithm to searchover equivalence  classes of Bayesian networks as opposed  to individual structures. We present the general  formulation of a search space for which  the states of the search correspond to equivalence  classes of structures. Using this space,  anyoneofanumber of heuristic searchalgorithms  can easily be applied. We compare  greedy search performance in the proposed  search space to greedy search performance in  a search space for which the states correspond  to individual Bayesian network structures.  1 
1269|Policy Recognition in the Abstract Hidden Markov Model|In this paper, we present a method for recognising an agent&#039;s behaviour in dynamic, noisy, uncertain domains, and across multiple levels of abstraction. We term this problem on-line plan recognition under uncertainty and view it generally as probabilistic inference on the stochastic process representing the execution of the agent&#039;s plan. Our contributions in this paper are twofold. In terms of probabilistic inference, we introduce the Abstract Hidden Markov Model (AHMM), a novel type of stochastic processes, provide its dynamic Bayesian network (DBN) structure and analyse the properties of this network. We then describe an application of the Rao-Blackwellised Particle Filter to the AHMM which allows us to construct an ecient, hybrid inference method for this model. In terms of plan recognition, we propose a novel plan recognition framework based on the AHMM as the plan execution model. The Rao-Blackwellised hybrid inference for AHMM can take advantage of the independence properties inherent in a model of plan execution, leading to an algorithm for online probabilistic plan recognition that scales well with the number of levels in the plan hierarchy. This illustrates that while stochastic models for plan execution can be complex, they exhibit special structures which, if exploited, can lead to efficient plan recognition algorithms. We demonstrate the usefulness of the AHMM framework via a behaviour recognition system in a complex spatial environment using distributed video surveillance data.
1270|Symbolic Dynamic Programming for First-order MDPs|We present a dynamic programming approach for the solution of first-order Markov decisions processes. This technique uses an MDP whose dynamics is represented in a variant of the situation calculus allowing for stochastic actions. It produces a logical description of the optimal value function and policy by constructing a set of first-order formulae that minimally partition state space according to distinctions made by the value function and policy. This is achieved through the use of an operation known as decision-theoretic regression. In effect, our algorithm performs value iteration without explicit enumeration of either the state or action spaces of the MDP. This allows problems involving relational fluents and quantification to be solved without requiring explicit state space enumeration or conversion to propositional form. 1
1271|Learning Bayesian Networks from Data: An Information-Theory Based Approach|This paper provides algorithms that use an information-theoretic analysis to learn Bayesian network structures from data. Based on our three-phase learning framework, we develop efficient algorithms that can effectively learn Bayesian networks, requiring only polynomial numbers of conditional independence (CI) tests in typical cases. We provide precise conditions that specify when these algorithms are guaranteed to be correct as well as empirical evidence (from real world applications and simulation tests) that demonstrates that these systems work efficiently and reliably in practice.
1272|The graphical models toolkit: An open source software system for speech and time-series processing|This paper describes the Graphical Models Toolkit (GMTK), an open source, publically available toolkit for developing graphical-model based speech recognition and general time series systems. Graphical models are a flexible, concise, and expressive probabilistic modeling framework with which one may rapidly specify a vast collection of statistical models. This paper begins with a brief description of the representational and computational aspects of the framework. Following that is a detailed description of GMTK’s features, including a language for specifying structures and probability distributions, logarithmic space exact training and decoding procedures, the concept of switching parents, and a generalized EM training method which allows arbitrary sub-Gaussian parameter tying. Taken together, these features endow GMTK with a degree of expressiveness and functionality that significantly complements other publically available packages. GMTK was recently used in the 2001 Johns Hopkins Summer Workshop, and experimental results are described in detail both herein and in a companion paper. 1.
1273|Markovian Models for Sequential Data|Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many machine learning applications, especially for speech recognition. Furthermore, in the last few years, many new and promising probabilistic models related to HMMs have been proposed. We first summarize the basics of HMMs, and then review several recent related learning algorithms and extensions of HMMs, including in particular hybrids of HMMs with artificial neural networks, Input-Output HMMs (which are conditional HMMs using neural networks to compute probabilities), weighted transducers, variable-length Markov models and Markov switching state-space models. Finally, we discuss some of the challenges of future research in this very active area. 1 Introduction  Hidden Markov Models (HMMs) are statistical models of sequential data that have been used successfully in many applications in artificial intelligence, pattern recognition, speech recognition, and modeling of biological ...
1274|Input/output hmms for sequence processing|We consider problems of sequence processing and propose a solution based on a discrete state model in order to represent past context. Weintroduce a recurrent connectionist architecture having a modular structure that associates a subnetwork to each state. The model has a statistical interpretation we call Input/Output Hidden Markov Model (IOHMM). It can be trained by the EM or GEM algorithms, considering state trajectories as missing data, which decouples temporal credit assignment and actual parameter estimation. The model presents similarities to hidden Markov models (HMMs), but allows us to map input se-quences to output sequences, using the same processing style as recurrent neural networks. IOHMMs are trained using a more discriminant learning paradigm than HMMs, while potentially taking advantage of the EM algorithm. We demonstrate that IOHMMs are well suited for solving grammatical inference problems on a benchmark problem. Experimental results are presented for the seven Tomita grammars, showing that these adaptive models can attain excellent generalization.
1275|AIS-BN: An Adaptive Importance Sampling Algorithm for Evidential Reasoning in Large Bayesian Networks|Stochastic sampling algorithms, while an attractive alternative to exact algorithms in  very large Bayesian network models, have been observed to perform poorly in evidential  reasoning with extremely unlikely evidence. To address this problem, we propose an adaptive  importance sampling algorithm, AIS-BN, that shows promising convergence rates  even under extreme conditions and seems to outperform the existing sampling algorithms  consistently. Three sources of this performance improvement are (1) two heuristics for  initialization of the importance function that are based on the theoretical properties of importance  sampling in nite-dimensional integrals and the structural advantages of Bayesian  networks, (2) a smooth learning method for the importance function, and (3) a dynamic  weighting function for combining samples from dierent stages of the algorithm.  We tested the performance of the AIS-BN algorithm along with two state of the art  general purpose sampling algorithms, lik...
1276|Using Dirichlet Mixture Priors to Derive Hidden Markov Models for Protein Families|A Bayesian method for estimating the amino acid  distributions in the states of a hidden Markov  model (HMM) for a protein family or the columns  of a multiple alignment of that family is introduced.  This method uses Dirichlet mixture densities  as priors over amino acid distributions. These  mixture densities are determined from examination  of previously constructed HMMs or multiple  alignments. It is shown that this Bayesian method  can improve the quality of HMMs produced from  small training sets. Specific experiments on the  EF-hand motif are reported, for which these priors  are shown to produce HMMs with higher likelihood  on unseen data, and fewer false positives  and false negatives in a database search task.  
1277|A new approach to analyzing gene expression time series data|1 Introduction Principled methods for estimating unobserved time-points,clustering, and aligning microarray gene expression timeseries are needed to make such data useful for detailed anal-ysis. Datasets measuring temporal behavior of thousands of genes offer rich opportunities for computational biologists. For example, Dynamic Bayesian Networks may be usedto build models and try to understand how genetic responses unfold. However, such modeling frameworks need a suf-ficient quantity of data in the appropriate format. Current gene expression time-series data often do not meet these re-quirements, since they may be missing data points, sampled non-uniformly, and measure biological processes that exhibittemporal variation.
1278|Coupled hidden Markov models for modeling interacting processes|We present methods for coupling hidden Markov models (hmms) to model systems of multiple interacting processes. The resulting models have multiple state variables that are temporally coupled via matrices of conditional probabilities. We introduce a deterministic O(T (CN)  2  ) approximation for maximum a posterior (MAP) state estimation which enables fast classification and parameter estimation via expectation maximization. An &#034;N-heads&#034; dynamic programming algorithm samples from the highest probability paths through a compact state trellis, minimizing an upper bound on the cross entropy with the full (combinatoric) dynamic programming problem. The complexity is O(T (CN)  2  ) for C  chains of N states apiece observing T data points, compared with O(TN  2C  ) for naive (Cartesian product), exact (state clustering), and stochastic (Monte Carlo) methods applied to the same inference problem. In several experiments examining training time, model likelihoods, classification accuracy, and ro...
1279|Structure Learning in Conditional Probability Models via an Entropic Prior and Parameter Extinction|We introduce an entropic prior for multinomial parameter estimation problems and solve for its maximum...
1280|Markov Chain Monte Carlo in Conditionally Gaussian State Space Models|Introduction Linear Gaussian state space models are used extensively, with unknown parameters usually estimated by maximum likelihood: Wecker &amp; Ansley (1983), Harvey (1989). However, many time series and nonparametric regression applications, such as change point problems, outlier detection and switching regression, require the full generality of the conditionally Gaussian model: Harrison &amp; Stevens (1976), Shumway &amp; Stoffer (1991), West &amp; Harrison (1989), Gordon &amp; Smith (1990). The presence of a large number of indicator variables makes it difficult to estimate conditionally Gaussian models using maximum likelihood, and a Bayesian approach using Markov chain Monte Carlo appears more tractable. We propose a new sampler, which is used to estimate an unknown function nonparametrically when there are jumps in the function and outliers in the observations; it is also applied to a time series change point problem previously discussed by Gordon &amp; Smith (1990). For the first example th
1281|Markov Chain Monte Carlo Model Determination for Hierarchical and Graphical Log-linear Models|this paper, we will only consider undirected graphical models. For details of Bayesian model selection for directed graphical models see Madigan et al (1995). An (undirected) graphical model is determined by a set of conditional independence constraints of the form `fl 1 is independent of fl 2 conditional on all other fl i 2 C&#039;. Graphical models are so called because they can each be represented as a graph with vertex set C and an edge between each pair fl 1 and fl 2 unless fl 1 and fl 2 are conditionally independent as described above. Darroch, Lauritzen and Speed (1980) show that each graphical log-linear model is hierarchical, with generators given by the cliques (complete subgraphs) of the graph. The total number of possible graphical models is clearly given by 2 (
1282|A Sufficiently Fast Algorithm for Finding Close to Optimal Junction Trees|An algorithm is developed for finding a close  to optimal junction tree of a given graph G.
1283|Dynamic Bayesian Multinets|In this work, dynamic Bayesian multinets are  introduced where a Markov chain state at time  t determines conditional independence patterns  between random variables lying within a local  time window surrounding t. It is shown how  information-theoretic criterion functions can be  used to induce sparse, discriminative, and classconditional  network structures that yield an optimal  approximation to the class posterior probability,  and therefore are useful for the classification  task. Using a new structure learning  heuristic, the resulting models are tested on a  medium-vocabulary isolated-word speech recognition  task. It is demonstrated that these discriminatively  structured dynamic Bayesian multinets,  when trained in a maximum likelihood setting using  EM, can outperform both HMMs and other  dynamic Bayesian networks with a similar number  of parameters.  1 Introduction  While Markov chains are sometimes a useful model for sequences, such simple independence assumptions can lead...
1284|Cascaded Markov Models|This paper presents a new approach to  partial parsing of context-free structures. The approach is based
1285|Natural Statistical Models for Automatic Speech Recognition|  The performance of state-of-the-art speech recognition systems is still far worse than that of humans. This is partly caused by the use of poor statistical models. In a general statistical pattern classification task, the probabilistic models should represent the statistical structure unique to and distinguishing those objects to be classified. In many cases, however, model families are selected without verification of their ability to represent vital discriminative properties. For example, Hidden Markov Models (HMMs) are frequently used in automatic speech recognition systems even though they possess conditional independence properties that might cause inaccuracies when modeling and classifying speech signals. In this work, a new method for automatic speech recognition is developed where the natural statistical properties of speech are used to determine the probabilistic model. Starting from an HMM, new models are created by adding dependencies only if they are not already well captured by the HMM, and only if they increase the
1286|Compositionality, MDL Priors, and Object Recognition|Images are ambiguous at each of many levels of a contextual hierarchy. Nevertheless, the high-level interpretation of most scenes is unambiguous, as evidenced by the superior performance of humans. This observation argues for global vision models, such as deformable templates. Unfortunately, such models are computationally intractable for unconstrained problems. We propose a compositional model in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings. Ambiguity is propagated up the hierarchy in the form of multiple interpretations, which are later resolved by a Bayesian, equivalently minimum-description-length, cost functional. 1 Bayesian decision theory and compositionality  In his Essay on Probability, Laplace (1812) devotes a short chapter---his &#034;Sixth Principle&#034;---to what we call today the Bayesian decision rule. Laplace observes that we interpret a &#034;regular combination,&#034; e.g., an arrangement of objects th...
1287|Learning Motion Patterns of Persons for Mobile Service Robots|We propose a method for learning models of people&#039;s motion behaviors in an indoor environment. As people move through their environments, they do not move randomly. Instead, they often engage in typical motion patterns, related to specific locations that they might be interested in approaching and specific trajectories that they might follow in doing so. Knowledge about such patterns may enable a mobile robot to develop improved people following and obstacle avoidance skills. This paper proposes an algorithm that learns collections of typical trajectories that characterize a person&#039;s motion patterns. Data, recorded by mobile robots equipped with laser range finders, is clustered into different types of motion using the popular expectation maximization algorithm, while simultaneously learning multiple motion patterns. Experimental results, obtained using data collected in a domestic residence and in an office building, illustrate that highly predictive models of human motion patterns can be learned.
1288|A simple constraint-based algorithm for efficiently mining observational databases for causal relationships|Abstract. This paper presents a simple, efficient computer-based method for discovering causal relationships from databases that contain observational data. Observational data is passively observed, as contrasted with experimental data. Most of the databases available for data mining are observational. There is great potential for mining such databases to discover causal relationships. We illustrate how observational data can constrain the causal relationships among measured variables, sometimes to the point that we can conclude that one variable is causing another variable. The presentation here is based on a constraint-based approach to causal discovery. A primary purpose of this paper is to present the constraint-based causal discovery method in the simplest possible fashion in order to (1) readily convey the basic ideas that underlie more complex constraint-based causal discovery techniques, and (2) permit interested readers to rapidly program and apply the method to their own databases, as a start toward using more elaborate causal discovery algorithms.
1289|Tracking and Surveillance in Wide-Area Spatial Environments Using the Abstract Hidden Markov Model|In this paper, we consider the problem of tracking an object and predicting the object future trajectory in a wide-area environment, with complex spatial layout and the use of multiple sensors/cameras. To solve this problem, there is a need for representing the dynamic and noisy data in the tracking tasks, and dealing with them at different levels of detail. We employ the Abstract Hidden Markov Models (AHMM), an extension of the well-known Hidden Markov Model (HMM) and a special type of Dynamic Probabilistic Network (DPN), as our underlying representation framework. The AHMM allows us to explicitly encode the hierarchy of connected spatial locations, making it scalable to the size of the environment being modelled. We describe an application for tracking human movement in a...
1290|Approximate Learning of Dynamic Models|Inference is a key component in learning probabilistic models from partially  observable data. When learning temporal models, each of the many  inference phases requires a complete traversal over a potentially very  long sequence; furthermore, the data structures propagated in this procedure  can be extremely large, making the whole process very demanding.  In [2], we describe an approximate inference algorithm for monitoring  stochastic processes, and prove bounds on its approximation error. In this  paper, we apply this algorithm as an approximate forward propagation  step in an EM algorithm for learning temporal Bayesian networks. We  also provide a related approximation for the backward step, and prove  error bounds for the combined algorithm. We show that EM using our  inference algorithm is much faster than EM using exact inference, with  no degradation of the quality of the learned model. We then extend our  analysis to the online learning task, showing a boundon the error resul...
1291|Discovering the Hidden Structure of Complex Dynamic Systems|Dynamic Bayesian networks provide a compact and natural representation for complex dynamic systems. However, in many cases, there is no expert available from whom a model can be elicited. Learning provides an alternative approach for constructing models of dynamic systems. In this paper, we address some of the crucial computational aspects of learning the structure of dynamic systems, particularly those where some relevant variables are partially observed or even entirely unknown. Our approach is based on the  Structural Expectation Maximization (SEM) algorithm. The main computational cost of the SEM algorithm is the gathering of expected sufficient statistics. We propose a novel approximation scheme that allows these sufficient statistics to be computed efficiently. We also investigate the fundamental problem of discovering the existence of hidden variables without exhaustive and expensive search. Our approach is based on the observation that, in dynamic systems, ignoring a hidden var...
1292|Iterative Algorithms for State Estimation of Jump Markov Linear Systems|Jump Markov linear systems (JMLSs) are linear systems whose parameters evolve with time according to a finite state Markov chain. Given a set of observations, our aim is to estimate the states of the finite state Markov chain and the continuous (in space) states of the linear system.
1293|Exploiting the architecture of dynamic systems|Consider the problem of monitoring the state of a complex dynamic system, and predicting its future evolution. Exact algorithms for this task typically maintain a belief state, or distribution over the states at some point in time. Unfortunately, these algorithms fail when applied to complex processes such as those represented as dynamic Bayesian networks (DBNs), as the representation of the belief state grows exponentially with the size of the process. In (Boyen &amp; Koller 1998), we recently proposed an efficient approximate tracking algorithm that maintains an approximate belief state that has a compact representation as a set of independent factors. Its performance depends on the error introduced by approximating a belief state of this process by a factored one. We informally argued that this error is low if the interaction between variables in the processes is “weak”. In this paper, we give formal information-theoretic definitions for notions such as weak interaction and sparse interaction of processes. We use these notions to analyze the conditions under which the error induced by this type of approximation is small. We demonstrate several cases where our results formally support intuitions about strength of interaction.
1294|Top-down Construction and Repetitive Structures Representation in Bayesian Networks|Bayesian networks for large and complex domains are  dicult to construct and maintain. For example modifying  a small network fragment in a repetitive structure  might be very time consuming. Top-down modelling  may simplify the construction of large Bayesian  networks, but methods (partly) supporting top-down  modelling have only recently been introduced and tools  do not exist. In this paper, we try to take a topdown  approach to constructing Bayesian networks by  using existing object oriented methods. We change  these where they fail to support top-down modeling.  This provides a new framework that allows topdown  methodologies for the construction of Bayesian  networks, provides an ecient class hierarchy and a  compact way of specifying and representing temporal  Bayesian networks. Furthermore, a conceptual simpli-  cation is achieved.  Introduction  Constructing and maintaining Bayesian networks (BNs) can be a time consuming process. Using current methods and tools, a top-down a...
1295|An EM Algorithm for Asynchronous Input/Output Hidden Markov Models|In learning tasks in which input sequences are mapped to output sequences, it is often the case that the input and output sequences are not synchronous. For example, in speech recognition, acoustic sequences are longer than phoneme sequences. Input/Output Hidden Markov Models have already been proposed to represent the distribution of an output sequence given an input sequence of the same length. We extend here this model to the case of asynchronous sequences, and show an Expectation-Maximization algorithm for training such models.
1296|Diffusion of context and credit information in Markovian models|This paper studies the problem of ergodicity of transition probabilitymatricesinMarkovian models, such as hidden Markov models (HMMs), and how itmakes very di cult the task of learning to represent long-term context for sequential data. This phenomenon hurts the forward propagation of long-term context information, as well as learning a hidden state representation to represent long-term context, which depends on propagating credit information backwards in time. Using results from Markov chain theory, weshow that this problem of di usion of context and credit is reduced when the transition probabilities approach 0 or 1, i.e., the transition probability matrices are sparse and the model essentially deterministic. The results found in this paper apply to learning approachesbasedon continuous optimization, such asgradient descent and the Baum-Welch algorithm. 1.
1297|Data-Driven Extensions To Hmm Statistical Dependencies|... HMM conditional independence assumption in a principled way. Without increasing the number of states, the modeling power of an HMM is increased by including only those additional probabilistic dependencies (to the surrounding observation context) that are believed to be both relevant and discriminative. Conditional mutual information is used to determine both relevance and discriminability. Extended Gaussian-mixture HMMs and new EM update equations are introduced. In an isolated word speech database, results show an average 34% word error improvement over an HMM with the same number of states, and a 15% improvement over an HMM with a comparable number of parameters.
1298|Sequential Bayesian Estimation And Model Selection For Dynamic Kernel Machines|In this paper, we address the complex problem of sequential Bayesian estimation and model selection/averaging. This problem does not usually admit any type of closed-form analytical solutions and, as a result, one has to resort to numerical methods. We propose here an original and powerful sequential simulation-based strategy to perform the necessary computations. This strategy is based on Monte Carlo particle methods and model selection/averaging using predictive distributions. It combines sequential importance sampling, Rao-Blackwellisation, a selection procedure and reversible jump MCMC moves. We demonstrate the eectiveness of the method by performing inference and learning on a hybrid model consisting of a dynamic linear model and a dynamic mixture of kernel basis functions. 
1299|Causality and graphical models in time series|analysis
1300|Structural Learning in Object Oriented Domains|When constructing a Bayesian network, it can be advantageous to employ structural learning algorithms to combine knowledge captured in databases with prior information provided by domain experts. Unfortunately, conventional algorithms do not exploit the occurrence of repetitive structures, which are often found in object oriented domains such as fault prediction in computer networks and large pedigrees.
1301|Efficient Inference for Mixed Bayesian Networks|Bayesian network is a compact representation for probabilistic models and inference. They have been used successfully for multisensor fusion and situation assessment. It is well known that, in general, the inference algorithms to compute the exact posterior probability of the target state are either computationally infeasible for dense networks or impossible for mixed discretecontinuous networks. In those cases, one approach is to compute the approximate results using simulation methods. This paper proposes efficient inference methods for those cases. The goal is not to compute the exact or approximate posterior probability of the target state, but to identify the top (most likely) ones in an efficient manner. The approach is to use intelligent simulation techniques where previous samples will be used to guide the future sampling strategy. By focusing the sampling on the &#034;important&#034; space, we are able to sort out the top candidates quickly. Simulation results are included to demonstrate the performances of the algorithms.
1302|Structured arc reversal and simulation of dynamic probabilistic networks|We present an algorithm for arc reversal in Bayesian networks with tree-structured conditional probability tables, and consider some of its advantages, especially for the simulation of dynamic probabilistic networks. In particular, the method allows one to produce CPTs for nodes involved in the reversal that exploit regularities in the conditional distributions. We argue that this approach alleviates some of the overhead associated with arc reversal, plays an important role in evidence integration and can be used to restrict sampling of variables in DPNs. We also provide an algorithm that detects the dynamic irrelevance of state variables in forward simulation. This algorithm exploits the structured CPTs in a reversed network to determine, in a timeindependent fashion, the conditions under which a variable does or does not need to be sampled. 1
1303|Application of Bayesian controllers to dynamic systems|Bayesian networks for the static as well as for the dynamic case have gained an enormous interest in the research community of machine learning and pattern recognition. Although the parallels between dynamic Bayesian networks and Kalman tikers are well-known since many years, Bayesian networks have not been applied to problems in the area of adaptive control of dynamic systems.
1304|A Digital Fountain Approach to Reliable Distribution of Bulk Data|The proliferation of applications that must reliably distribute bulk data to a large number of autonomous clients motivates the design of new multicast and broadcast prot.ocols. We describe an ideal, fully scalable protocol for these applications that we call a digital fountain. A digital fountain allows any number of heterogeneous clients to acquire bulk data with optimal efficiency at times of their choosing. Moreover, no feedback channels are needed to ensure reliable delivery, even in the face of high loss rates. We develop a protocol that closely approximates a digital fountain using a new class of erasure codes that for large block sizes are orders of magnitude faster than standard erasure codes. We provide performance measurements that demonstrate the feasibility of our approach and discuss the design, implementation and performance of an experimental system. 
1305|Dissemination-based Data Delivery Using Broadcast Disks|Mobile computers and wireless networks are emerging technologies which promise to make ubiquitous computing a reality. One challenge that must be met in order to truly realize this potential is that of providing mobile clients with ubiquitous access to data. Mobile clients may often be disconnected from stationary server machines or may have only a low-bandwidth channel for sending messages to servers. Such an environment raises difficulties for supporting data-intensive applications for three reasons: 1) the inability to predict, with 100% accuracy, the future data needs of many applications, 2) limits on storage capacities of mobile machines, and 3) the need to provide clients with new or updated data values. One (and perhaps the only) way to address these challenges is to provide stationary server machines with a relatively high-bandwidth channel over which to broadcast data to a client population in anticipation of the need for that data at the clients. Such a system can be said to...
1306|Some simple effective approximations to the 2-Poisson model for probabilistic weighted retrieval|The 2–Poisson model for term frequencies is used to suggest ways of incorporating certain variables in probabilistic models for information retrieval. The variables concerned are within-document term frequency, document length, and within-query term frequency. Simple weighting functions are developed, and tested on the TREC test collection. Considerable performance improvements (over simple inverse collection frequency weighting) are demonstrated. 1
1307|Models for retrieval with probabilistic indexing|Abstract- in this article three retrieval models for probabilistic indexing are described along with evaluation results for each. First is the binary independence indexing @II) model, which is a generalized version of the Maron and Kuhns indexing model. In this model, the indexing weight of a descriptor in a document is an estimate of the proba-bility of relevance of this document with respect to queries using this descriptor. Sec-ond is the retrieval-with-probabilistic-indexing (RPI) model, which is suited to different kinds of probabilistic indexing. For that we assume that each indexing scheme has its own concept of “correctness ” to which the probabilities relate. In addition to the prob-abilistic indexing weights, the RPI model provides the possibility of reIevance weight-ing of search terms. A third mode1 that is similar was proposed by Croft some years ago as an extension of the binary independence retrieval model but it can be shown that this model is not based on the probabilistic ranking principle. The probabilistic indexing weights required for any of these models can be provided by an application of the Darm-stadt indexing approach (DIA) for indexing with descriptors from a controlled vocabu-Iary. The experimental results show signi~cant improvements over retrieval with binary indexing. Finally, suggestions are made regarding how the DIA can be applied to prob-abilistic indexing with free text terms. 1.
1308|A New Probabilistic Model of Text Classification and Retrieval|This paper introduces the multinomial model of text classification and retrieval. One important feature of the model is that the tf statistic, which usually appears in probabilistic IR models as a heuristic, is an integral part of the model. Another is that the variable length of documents is accounted for, without either making a uniform length assumption or using length normalization. The multinomial model employs independence assumptions which are similar to assumptions made in previous probabilistic models, particularly the binary independence model and the 2-Poisson model. The use of simulation to study the model is described. Performance of the model is evaluated on the TREC-3 routing task. Results are compared with the binary independence model and with the simulation studies.  
1309|A Morphable Model For The Synthesis Of 3D Faces|In this paper, a new technique for modeling textured 3D faces is introduced. 3D faces can either be generated automatically from one or more photographs, or modeled directly through an intuitive user interface. Users are assisted in two key problems of computer aided face modeling. First, new face images or new 3D face models can be registered automatically by computing dense one-to-one correspondence to an internal face model. Second, the approach regulates the naturalness of modeled faces avoiding faces with an &#034;unlikely&#034; appearance. Starting from
1310|Synthesizing Realistic Facial Expressions from Photographs |We present new techniques for creating photorealistic textured 3D facial models from photographs of a human subject, and for creating smooth transitions between different facial expressions by morphing between these different models. Starting from several uncalibrated views of a human subject, we employ a user-assisted technique to recover the camera poses corresponding to the views as well as the 3D coordinates of a sparse set of chosen locations on the subject&#039;s face. A scattered data interpolation technique is then used to deform a generic face mesh to fit the particular geometry of the subject&#039;s face. Having recovered the camera poses and the facial geometry, we extract from the input images one or more texture maps for the model. This process is repeated for several facial expressions of a particular subject. To generate transitions between these facial expressions we use 3D shape morphing between the corresponding face models, while at the same time blending the corresponding tex...
1311|Linear Object Classes and Image Synthesis From a Single Example Image|Abstract—The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, we have recently introduced [1], [2], [3] simpler techniques that are applicable under restricted conditions. The approach exploits image transformations that are specific to the relevant object class, and learnable from example views of other “prototypical ” objects of the same class. In this paper, we introduce such a technique by extending the notion of linear class proposed by Poggio and Vetter. For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views. We demonstrate the approach on artificial objects and then show preliminary evidence that the technique can effectively “rotate ” highresolution face images from a single 2D view. Index Terms—3D object recognition, rotation invariance, deformable templates, image synthesis. 1
1312|Automatic interpretation and coding of face images using flexible models|Abstract—Face images are difficult to interpret because they are highly variable. Sources of variability include individual appearance, 3D pose, facial expression, and lighting. We describe a compact parametrized model of facial appearance which takes into account all these sources of variability. The model represents both shape and gray-level appearance, and is created by performing a statistical analysis over a training set of face images. A robust multiresolution search algorithm is used to fit the model to faces in new images. This allows the main facial features to be located, and a set of shape, and gray-level appearance parameters to be recovered. A good approximation to a given face can be reconstructed using less than 100 of these parameters. This representation can be used for tasks such as image coding, person identification, 3D pose recovery, gender recognition, and expression recognition. Experimental results are presented for a database of 690 face images obtained under widely varying conditions of 3D pose, lighting, and facial expression. The system performs well on all the tasks listed above.
1313|Making Faces|We have created a system for capturing both the three-dimensional geometry and color and shading information for human facial expressions. We use this data to reconstruct photorealistic, 3D animations of the captured expressions. The system uses a large set of sampling points on the face to accurately track the three dimensional deformations of the face. Simultaneously with the tracking of the geometric data, we capture multiple high resolution, registered video images of the face. These images are used to create a texture map sequence for a three dimensional polygonal face model which can then be rendered on standard 3D graphics hardware. The resulting facial animation is surprisingly life-like and looks very much like the original live performance. Separating the capture of the geometry from the texture images eliminates much of the variance in the image data due to motion, which increases compression ratios. Although the primary emphasis of our work is not compression we have investigated the use of a novel method to compress the geometric data based on principal components analysis. The texture sequence is compressed using an MPEG4 video codec. Animations reconstructed from 512x512 pixel textures look good at data rates as low as 240 Kbits per second.
1314|Computer generated animation of faces|This paper describes the representation, animation and data collection techniques that have been used to produce &amp;quot;realistic&amp;quot; computer generated half-tone animated se-quences of a human face changing expres-sion. It was determined that approximating the surface of a face with a polygonal skin containing approximately 250 polygons de-fined by about 400 vertices is sufficient to achieve a realistic face. Animation was accomplished using a cosine interpolation scheme to fill in the intermediate frames between expressions. This approach is good enough to produce realistic facial motion. The three-dimensional data used to describe the expressions of the face was obtained photogrammetrically using pairs of photographs. KEY WORDS AND PHRASES: computer graphics, half-tone rendering, smooth shading, com-puter animation, flexible surfaces, poly-gonal surfaces, facial topology, cosine interpolation, £hree-dimensional data acquisition.
1315|Dynamic NURBS with Geometric Constraints for Interactive Sculpting|This article develops a dynamic generalization of the nonuniform rational B-spline (NURBS) model. NURBS have become a de facto standard in commercial modeling systems because of their power to represent free-form shapes as well as common analytic shapes. To date, however, they have been viewed as purely geometric primitives that require the user to manually adjust multiple control points and associated weights in order to design shapes. Dynamic NURBS, or D-NURBS, are physics-based models that incorporate mass distributions, inertial deformation energies, and other physical quantities into the popular NURBS geometric substrate. Using D-NURBS, a modeler can interactively sculpt curves and surfaces and design complex shapes to required specifications not only in the traditional indirect fashion, by adjusting control points and weights, but also through direct physical manipulation, by applying simulated forces and local and global shape constraints. D-NURBS move and deform in a physically intuitive manner in response to the user&#039;s direct manipulations. Their dynamic behavior results from the numerical integration of a set of nonlinear differential equations that automatically evolve the control points and weights in response to the applied forces and constraints. To derive these equations, we employ Lagrangian mechanics and finite-element-like discretization. Our approach supports the trimming of D-NURBS surfaces using D-NURBS curves. We demonstrate D-NURBS models and constraints in applications including the rounding of solids, optimal surface fitting to unstructured data, surface design from cross-sections, and free-form deformation. We also introduce a new technique for 2D shape metamorphosis using constrained D-NURBS surfaces.
1316|Multidimensional morphable models: A framework for representing and matching object classes|This thesis describes a flexible model for representing images of objects of a certain class, such as faces, and introduces a new algorithm for matching the model to novel images from the class. The model and matching algorithm are very general and can be used for many image analysis tasks. The flexible model, called a multidimensional morphable model, is learned from example images (called prototypes) of objects of a class. In the learning phase, pixelwise correspon-dences between a reference prototype and each of the other prototypes are first computed and then used to obtain shape and texture vectors associated with each prototype. The mor-phable model is then synthesized as a linear combination that spans the linear vector space determined by the prototypical shapes and textures. We next introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Several experiments demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying
1317|A Bootstrapping Algorithm for Learning Linear Models of Object Classes|Flexible models of object classes, based on linear combinations of prototypical images, are capable of matching novel images of the same class and have been shown to be a powerful tool to solve several fundamental vision tasks such as recognition, synthesis and correspondence. The key problem in creating a specific flexible model is the computation of pixelwise correspondence between the prototypes, a task done until now in a semiautomatic way. In this paper we describe an algorithm that automatically bootstraps the correspondence between the prototypes. The algorithm -- which can be used for 2D images as well as for 3D models -- is shown to synthesize successfully a flexible model of frontal face images and a flexible model of handwritten digits. 1 Introduction  In recent papers we have introduced a new type of flexible model for images of objects of a certain class. The idea is to represent images of a certain type -- for instance images of frontal faces -- as the linear combination ...
1318|Constructing Physics-Based Facial Models of Individuals|This paper develops a highly automated approach to constructing realistic, working models of human heads for use in animation. These physics-based models are anatomically accurate and may be made to conform closely to specific individuals. We begin by scanning a person with a laser sensor which circles around the head, acquiring detailed range and reflectance information. Next, an automatic conformation algorithm adapts a triangulated face mesh of predetermined topological structure to these data. The generic mesh, which is reusable with different individuals, reduces the range data to an efficient, polygonal approximation of the facial geometry and supports a high-resolution texture mapping of the skin reflectivity. The conformed polygonal mesh forms the epidermal layer of a new, physics-based model of facial tissue. An automatic algorithm constructs the multilayer synthetic skin and estimates an underlying rigid &#034;skull&#034; substructure with a jointed jaw. Finally, the algorithm inserts synthetic muscles into the deepest layer of the facial tissue. These contractile actuators, which emulate the primary muscles of facial expression, generate forces that deform the synthetic tissue into meaningful expressions. To increase realism, we include constraints to emulate tissue incompressibility and to enable the tissue to slide over the skull substructure without penetrating into it. The resulting animate models appear significantly more realistic than our previous physics-based facial models. Keywords: Physics-Based Facial Modeling, Facial Animation, Cylindrical Facial Scanning, Feature-Based Facial Adaptation, Texture Mapping, Discrete Deformable Models. 1 
1319|Estimating Coloured 3D Face Models from Single Images: An Example Based Approach|Abstract. In this paper we present a method to derive 3D shape and surface texture of a human face from a single image. The method draws on a general flexible 3D face model which is “learned ” from examples of individual 3D-face data (Cyberware-scans). In an analysis-by-synthesis loop, the flexible model is matched to the novel face image. From the coloured 3D model obtained by this procedure, we can generate new images of the face across changes in viewpoint and illumination. Moreover, nonrigid transformations which are represented within the flexible model can be applied, for example changes in facial expression. The key problem for generating a flexible face model is the computation of dense correspondence between all given 3D example faces. A new correspondence algorithm is described which is a generalization of common algorithms for optic flow computation to 3D-face data. 1
1320|Emotion Editing using Finite Elements|This paper describes the prototype of a facial expression editor. In contrast to existing systems the presented  editor takes advantage of both medical data for the simulation and the consideration of facial anatomy during  the definition of muscle groups. The C¹-continuous geometry and the high degree of abstraction for the expression  editing sets this system apart from others. Using finite elements we achieve a better precision in comparison  to particle systems. Furthermore, a precomputing of facial action units enables us to compose facial  expressions by a superposition of facial action geometries in real-time. The presented model is based on a  generic facial model using a thin plate and membrane approach for the surface and elastic springs for facial  tissue modeling. It has been used successfully for performing facial surgery simulation. We illustrate features  of our system with examples from the Visible Human Dataset. 
1321|Design, Transformation and Animation of Human Faces|Creation of new human faces for synthetic actors is a tedious and painful task. The situation may be improved by introducing tools for the creation. Two approaches are discussed in this paper: modification and edition of an existing synthetic actor using local transformations; generation of new synthetic actors obtained by interpolation between two existing actors; creation of a synthetic actor by composition of different parts. This paper also describes the methods used in the facial animation of synthetic actors who change their personalities from one person to another. This means that our purpose is to transform one character into another, and also to transform the animation at the same time. The interpolation must be at several levels: the shape level, the parameter level, the expression level and the script level. For the animation, we introduce three levels of inbetweens: inbetween parameters, inbetween expressions and inbetween scripts. The method has been completely implemented and integrated into the Human Factory software.
1322|A user-friendly texture-fitting methodolgy for virtual humans|This paper describes a methodology for applying textures interactively to a 3D object, taking into account its topological features. A user-friendly interface based on this allows the user to evaluate the consequences on the 3D object of manipulating the 2D texture in real time. As a case study, we have applied this methodology to re-create the terra-cotta texture of virtual soldiers from Xian.
1323|Activity recognition from user-annotated acceleration data|  In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small biaxial accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from 20 subjects without researcher supervision or observation. Subjects were asked to perform a sequence of everyday tasks but not told specifically where or how to do them. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated and several classifiers using these features were tested. Decision tree classifiers showed the best performance recognizing everyday activities with an overall accuracy rate of 84%. The results show that although some activities are recognized well with subject-independent training data, others appear to require subject-specific training data. The results suggest that multiple accelerometers aid in recognition because conjunctions in acceleration feature values can effectively discriminate many activities. With just two biaxial accelerometers – thigh and wrist – the recognition performance dropped only slightly. This is the first work to investigate performance of recognition algorithms with multiple, wire-free accelerometers on 20 activities using datasets annotated by the subjects themselves.  
1324|Activity and location recognition using wearable sensors|Using measured acceleration and angular velocity data gathered through inexpensive, wearable sensors, this dead-reckoning method can determine a user’s location, detect transitions between preselected locations, and recognize and classify sitting, standing, and walking behaviors. Experiments demonstrate the proposed method’s effectiveness.
1325|Multi-Sensor Activity Context Detection for Wearable Computing|For wearable computing applications, human activity is a central part of the user&#039;s context. In order to avoid user annoyance it should be acquired automatically using body-worn sensors. We propose to use multiple acceleration sensors that are distributed over the body, because they are lightweight, small and cheap. Furthermore activity can best be measured where it occurs. We present a hardware platform that we developed for the investigation of this issue and results as to where to place the sensors and how to extract the context information.
1327|What Shall We Teach Our Pants?|If a wearable device can register what the wearer is currently doing, it can anticipate and adjust its behavior to avoid redundant interaction with the user. However, the relevance and properties of the activities that should be recognized depend on both the application and the user. This requires an adaptive recognition of the activities where the user, instead of the designer, can teach the device what he/she is doing. As a case study we connected a pair of pants with accelerometers to a laptop to interpret the raw sensor data. Using a combination of machine learning techniques such as Kohonen maps and probabilistic models, we build a system that is able to learn activities while requiring minimal user attention. This approach to context awareness is more universal since it requires no a priori knowledge about the contexts or the user. 1.
1328|Unsupervised, dynamic identification of physiological and activity context in wearable computing|Context-aware computing describes the situation where a wearable / mobile computer is aware of its user’s state and surroundings and modifies its behavior based on this information. We designed, implemented and evaluated a wearable system which can determine typical user context and context transition probabilities online and without external supervision. The system relies on techniques from machine learning, statistical analysis and graph algorithms. It can be used for online classification and prediction. Our results indicate the power of our method to determine a meaningful user context model while only requiring data from a comfortable physiological sensor device. 1.
1329|Multi-Sensor Context Aware Clothing|Inspired by perception in biological systems, distribution of a massive amount of simple sensing devices is gaining more support in detection applications. A focus on fusion of sensor signals instead of strong analysis algorithms, and a scheme to distribute sensors, results in new issues. Especially in wearable computing, where sensor data continuously changes, and clothing provides an ideal supporting structure for simple sensors, this approach may prove to be favourable. Experiments with a body-distributed sensor system investigate the influence of two factors that affect classification of what has been sensed: an increase in sensors enhances recognition, while adding new classes or contexts depreciates the results. Finally, a wearable computing related scenario is discussed that exploits the presence of many sensors.
1330|Hierarchical recognition of intentional human gestures for sports video annotation |We present a novel technique for the recognition of complex human gestures for video annotation using accelerometers and the hidden Markov model. Our extension to the standard hidden Markov model allows us to consider gestures at different levels of abstraction through a hierarchy of hidden states. Accelerometers in the form of wrist bands are attached to humans performing intentional gestures, such as umpires in sports. Video annotation is then performed by populating the video with time stamps indicating significant events, where a particular gesture occurs. The novelty of the technique lies in the development of a probabilistic hierarchical framework for complex gesture recognition and the use of accelerometers to extract gestures and significant events for video annotation. 1.
1331|Physical Activity Recognition from Acceleration Data under SemiNaturalistic Conditions|Achieving context-aware computer systems requires that computers can automatically recognize what people are doing. In this work, algorithms are developed and evaluated to detect physical activities from data acquired using five small accelerometers worn simultaneously on different parts of the body. Acceleration data was collected from twenty subjects in both laboratory and semi-naturalistic environments. For semi-naturalistic data, subjects were asked to perform a sequence of everyday tasks outside of the laboratory. Mean, energy, frequency-domain entropy, and correlation of acceleration data was calculated over 6.71 s sliding windows. Decision table, nearest neighbor, decision tree, and Naive Bayesian classifiers were tested on these features. Classification results using individual training and leave-one-subject-out validation were compared. Leave-one-subject-out validation with decision tree classifiers
1332|Acquiring In Situ Training Data for Context-Aware Ubiquitous Computing Applications|Ubiquitous, context-aware computer systems may ultimately enable computer applications that naturally and usefully respond to a user&#039;s everyday activity. Although new algorithms that can automatically detect context from wearable and environmental sensor systems show promise, many of the most flexible and robust systems use probabilistic detection algorithms that require extensive libraries of training data with labeled examples. In this paper, we describe the need for such training data and some challenges we have identified when trying to collect it while testing three contextdetection  systems for ubiquitous computing and mobile applications. Author Keywords  Context-aware, ubiquitous, computing, supervised learning, experience sampling, user interface design  ACM Classification Keywords  H5.m Information interfaces and presentation (e.g. HCI): Miscellaneous.
1333|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.