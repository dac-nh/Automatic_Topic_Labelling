ID|Title|Summary
1|Query Optimization|Imagine yourself standing in front of an exquisite buffet filled with numerous delicacies. Your goal is to try them all out, but you need to decide in what order. What exchange of tastes will maximize the overall pleasure of your palate? Although much less pleasurable and subjective, that is the type of problem that query optimizers are called to solve. Given a query, there are many plans that a database management system (DBMS) can follow to process it and produce its answer. All plans are equivalent in terms of their final output but vary in their cost, i.e., the amount of time that they need to run. What is the plan that needs the least amount of time? Such query optimization is absolutely necessary in a DBMS. The cost difference between two alternatives can be enormous. For example, consider the following database schema, which will be...
3|Access path selection in a relational database management system|ABSTRACT: In a high level query and data manipulation language such as SQL, requests are stated non-procedurally, without reference to access paths. This paper describes how System R chooses access paths for both simple (single relation) and complex queries (such as joins), given a user specification of desired data as a boolean expression of predicates. System R is an experimental database management system developed to carry out research on the relational model of data. System R was designed and built by members of the IBM San Jose Research&#039;Laboratory. 1.
4|Query optimization in database systems|Efficient methods of processing unanticipated queries are a crucial prerequisite for the success of generalized database management systems. A wide variety of approaches to improve the performance of query evaluation algorithms have been proposed: logic-based and semantic transformations, fast implementations of basic operations, and combinatorial or heuristic algorithms for generating alternative access plans and choosing among them. These methods are presented in the framework of a general query evaluation procedure using the relational calculus representation of queries. In addition, nonstandard query optimization issues such as higher level query evaluation, query optimization in distributed databases, and use of database machines are addressed. The focus, however, is on query optimization in centralized database systems.
5|The Volcano optimizer generator: Extensibility and efficient search|Emerging database application domains demand not only new functionality but also high performance. To satisfy these two requirements, the Volcano project provides efficient, extensible tools for query and request processing, particularly for object-oriented and scientific database systems. One of these tools is a new optimizer generator. Data model, logical algebra, physical algebra, and optimi-zation rules are translated by the optimizer generalor into optimizer source code. Compared with our earlier EX-ODUS optimizer generator prototype, the search engine is more extensible and powerful; it provides effective support for non-trivial cost models and for physical properties such as sort order. At the same time, it is much more efficient as it combines dynamic programming, which until now had been used only for relational select-project-join optimization, with goal-directed search and branch-and-bound pruning. Compared with other rule-based optimi-zation systems, it provides complete data model indepen-dence and more natural extensibility. 1.
6|PracticaJ selectivity estimation through adaptive sampling|Recently we have proposed an adaptive, random sampling algorithm for general query size estlmatlon In earlier work we analyzed the asymptotic ef’l?clency and accuracy of the algorithm, m this paper we mvestlgate Its practlcahty as applied to selects and Jams First, we extend our previous analysis to provide agmficantly improved bounds on the amount of samplmg necessary for a given level of accuracy Next, we provide “sanity bounds ” to deal with queries for which the underlying data 1s extremely skewed or the query result 1s very small Finally, we report on the performance of the estlmatlon algorithm as amplemented m a host language on a commercial relational system The results are encouraging, even with this loose couplmg between the estlmatlon algorithm and the DBMS 1
7|Dynamic Query Evaluation Plans|Traditional query optimizers assume accurate knowledge of run-time parameters such as selectivities and resource availability during plan optimization, i.e., at compile-time. In reality, however, this assumption is often not justified. Therefore, the “static ” plans produced by traditional optimizers may not be optimal for many of their actual run-time invocations. Instead, we propose a novel optimization model that assigns the bulk of the optimization effort to compile-time and delays carefully selected optimization decisions until run-time. Our previous work defined the run-time primitives, “dynamic plans ” using “choose-plan” operators, for executing such delayed decisions, but did not solve the problem of constructing dynamic plans at compile-time. The present paper introduces techniques that solve this problem. Experience with a working prototype optimizer demonstrates (i) that the additional optimization and start-up overhead of dynamic plans compared to static plans is dominated by their advantage at run-time, (ii) that dynamic plans are as robust as the “brute-force” remedy of run-time optimization, i.e., dynamic plans maintain their optimality even if parameters change between compile-time and run-time, and (iii) that the start-up overhead of dynamic plans is signifmantly less than the time required for complete optimization at run-time. In other words, our proposed techniques are superior to both techniques considered to-date, namely compile-time optimization into a single static plan as well as run-time optimization. Finally, we believe that the concepts and technology described can be transferred to commercial query optimizers in order to improve the performance of embedded queries with host variables in the query predicate and to adapt to run-time system loads unpredictable at compile-time. 1.
8|On the propagation of errors in the size of join results|yannisQcs.wise.edu Query optimizers of current relational database systems use several statistics maintained by the system on the contents of the database to decide on the most efficient access plan for a given query. These statistics contain errors that transitively affect many estimates derived by the optimizer. We present a formal framework based on which the principles of this error propagation can be studied. Within this framework, we obtain several ana-lytic results on how the error propagates in general, as well as in the extreme and average cases. We also pro-vide results on guarantees that the database system can make based on the statistics that it maintains. Finally, we discuss some promising approaches to controlling the error propagation and derive several interesting proper-ties of them. 1
9|Optimization of nonrecursive queries|State-of-the-art optimization approaches for relational database systems, e.g., those used in systems such as OBE, SQL/DS, and commercial INGRES. when used for queries in non-traditional database applications, suffer from two problems. First, the time complexity of their optimization algorithms, being combinatoric, is exponential in the number of relations to be joined in the query. Their cost is therefore prohibitive in situa-tions such as deductive databases and logic oriented languages for knowledge bases, where hundreds of joins may be required. The second problem with the traditional approaches is that, al-beit effective in their specific domain, it is not clear whether they can be generalized to different scenarios (e.g. parallel evalua-tion) since they lack a formal model to define the assumptions and critical factors on which their valiclity depends. This paper
10|Universality of Serial Histograms|Many current relational database systems use some form of histograms to approximate the frequency distribution of values in the attributes of relations and based on them estimate query result sizes and access plan costs. The errors that exist in the histogram approximations directly or transitively affect many estimates derived by the database system. We identify the class of serial histograms and demonstrate that they are optimal for reducing the query result size error for several classes of queries when the actual query result size (and hence the value of that error) reaches some extreme. Specifically, serial histograms are shown to be optimal for arbitrary tree equality-join queries when the query result size is maximized, whether or not the attribute independence assumption holds, and when the query result size is minimized and the attribute independence assumption holds. We also show that the expected error for any such query is always zero under all histograms, and thus argue that histograms should be chosen based on the reduction of the extreme-cases error, since reduction of the expected error is meaningless. 
11|Statistical profile estimation in database systems|A statistical profile summarizes the instances of a database. It describes aspects such as the number of tuples, the number of values, the distribution of values, the correlation between value sets, and the distribution of tuples among secondary storage units. Estimation of database profiles is critical in the problems of query optimization, physical database design, and database performance prediction. This paper describes a model of a database of profile, relates this model to estimating the cost of database operations, and surveys methods of estimating profiles. The operators and objects in the model include build profile, estimate profile, and update profile. The estimate operator is classified by the relational algebra operator (select, project, join), the property to be estimated (cardinality, distribution of values, and other parameters), and the underlying method (parametric, nonparametric, and ad-hoc). The accuracy, overhead, and assumptions of methods are discussed in detail. Relevant research in both the database and the statistics disciplines is incorporated in the detailed discussion.
12|Query Processing in a System for Distributed Databases (SDD-1  (1981) |Thii paper describes the techniques used to optimize relational queries in the SDD-1 distributed database system. Queries are submitted to SDD-1 in a high-level procedural language called Datalan-guage. Optimization begins by translating each Datalanguage query into a relational calculus form called an envelope, which is essentially an aggregate-free QUEL query. This paper is primarily concerned with the optimization of envelopes. Envelopes are processed in two phases. The first phase executes relational operations at various sites of the distributed database in order to delimit a subset of the database that contains all data relevant to the envelope. This subset is called a reduction of the database. The second phase transmits the reduction to one designated site, and the query is executed locally at that site. The critical optimization problem is to perform the reduction phase efficiently. Success depends on designing a good repertoire of operators to use during this phase, and an effective algorithm for deciding which of these operators to use in processing a given envelope against a given database. The principal reduction operator that we employ is called a
13|The Design Of Xprs|This paper presents an overview of the techniques we are using to build a DBMS at Berkeley that will simultaneously provide high performance and high availability in transaction processing environments, in applications with complex ad-hoc queries and in applications with large objects such as images or CAD layouts. We plan to achieve these goals using a general purpose DBMS and operating system and a shared memory multiprocessor. The hardware and software tactics which we are using to accomplish these goals are described in this paper and include a novel &#034;fast path&#034; feature, a special purpose concurrency control scheme, a two-dimensional file system, exploitation of parallelism and a novel method to efficiently mirror disks. 
14|Multi-join optimization for symmetric multiprocessors|This paper looks at the problem of multi-join query optimization for symmetric multiproceasore. Optimizrt-lion algorithms based on dynamic programming and greedy heuristics are described that, unlike traditional methods, include memory resources and pipelining in their cost model. An analytical model is presented and used to compare the quality of plans produced by each optimization algorithm. Experimental results show that, while dynamic programming produces the be &amp; plans, simple heuristics often do nearly as well. The came results are also used to highlight the advan-tages of bushy execution trees over more restricted tree shapes. 1
15|A Genetic Algorithm for Database Query Optimization|Current query optimization techniques are inadequate to support some of the emerging database applications. In this paper, we outline a database query optimization problem and describe the adaptation of a genetic algorithm to the problem. We present a method for encoding arbitrary binary trees as chromosomes and describe several crossover operators for such chromosomes. Preliminary computational comparisons with the current best--known method for query optimization indicate this to be a promising approach. In particular, the output quality and the time needed to produce such solutions is comparable to and in general better than the current method. 1 INTRODUCTION Genetic algorithms [4, 6] are becoming a widely used and accepted method for very difficult optimization problems. In this paper, we describe the implementation of a genetic algorithm (GA) for a problem in database query optimization. In order to give a careful formulation of our GA, we first give a broad outline of this parti...
16|Structure and Complexity of Relational Queries|This paper is an attempt at laying the foundations for the classification of queries on  relational data bases according to their structure and their computational complexity. Using  the operations of composition and fixpoints, a Z--// hierarchy of height w 2, called the  fixpoint query hierarchy, is defined, and its properties investigated. The hierarchy includes  most of the queries considered in the literathre including those of Codd and Aho and Ullman
17|Relational completeness of data base sublanguages|ABSTRACT: In the near future, we can expect a great variety of languages to be proposed for interrogating and updating data bases. This paper attempts to provide a theoretical basis which may be used to determine how complete a selection capability is provided in a proposed data sublanguage independently of any host language in which the sublanguage may be embedded. A relational algebra and a relational calculus are defined. Then, an algorithm is presented for reducing an arbitrary relation-defining expression (based on the calculus) into a semantically equivalent expression of the relational algebra. Finally, some opinions are stated regarding the relative merits of calculus-oriented versus algebra-oriented data sublanguages from the standpoint of optimal search and highly discriminating authorization schemes.
18|Using semi-joins to solve relational queries|ABSTRACT. The semi-join is a relational algebraic operation that selects a set of tuples in one relation that match one or more tuples of another relation on the joining domains. Semi-joins have been used as a basic ingredient in query processing strategies for a number of hardware and software database systems. However, not all queries can be solved entirely using semi-joins. In this paper the exact class of relational queries that can be solved using semi-joins is shown. It is also shown that queries outside of this class may not even be partially solvable using &amp;quot;short &amp;quot; semi-join programs. In addition, a linear-time membership test for this class is presented.
19|EQUIVALENCES AMONG RELATIONAL EXPRESSIONS|Many database queries can be formulated in terms of expressions whose operands represent tables of information (relations) and whose operators are the relational operations select, project, and join. This paper studies the equivalence problem for these relational expressions, with expression optimization in mind. A matrix, called a tableau, is proposed as a natural representative for the value of an expression. It is shown how tableaux can be made to reflect functional dependencies among attributes. A polynomial time algorithm is presented for the equivalence of tableaux that correspond to an important subset of expressions, although the equivalence problem is shown to be NP-complete under slightly more general circumstances.
20|Distributed query processing in a relational database system|ABSTRACT: In this paper we present a new algorithm for retrieving and updating data from a distributed relational data base. Within such. a data base, any number of relations can be distributed over &#039; any number of sites. Moreover, a user supplied distribution criteria can optionally be used to specify what site a tuple belongs to. The algorithm is an efficient way to process any query by f&#039;breaking&#039;l the qual-ification into separate &amp;quot;pieces &amp;quot; using a few simple heuristics. The cost cri-teria considered are minimum response time and minimum communications traffic. In addition, the algorithm can optimize separately for two models of a communi-cation network representing respectively ARPANET and ETHERNET like networks. This algorithm is being implemented as part of the INGRES data base system.
21|Processing Queries Over Generalization Hierarchies in a Multidatabase System|An important task of multidatabase systems is the integration of existing databases. Data-base Integration is achieved primarily through the use of generalization. Hence, it is impor-tant to develop good tactics for processing queries over generalization hierarchies. This paper defines the class of conjunctive generali-zation queries, and it describes four tactics for processing those queries that have boon developed for the MDLTIDASE system. Since query processing tactics are best describe algebraically, the paper shows how to model generalization as a sequence of algebraic operations. Three of the tactics described here are adapted from convon-tional distributed query processing techniqaes. However, it is argued that these tactics are of limited applicability to processing queries over generalization hierarchies. A fourth tactic, semioutorjoin, which is more widely applicable. is introduced. This research was jointly supported by the De-
22|Common Subexpression Isolation in Multiple Query Optimization|Abstract: The simultaneous optimization of multiple queries submitted to a database system may lead to substantial savings over the current approach of optimizing each query separately. Isolating common subexpressions in multiple queries and treating their execution as a sharable resource are important prerequisites. This chapter presents techniques for recognizing, supporting, and exploiting common subexpressions in record-oriented, relational algebra, domain relational calculus, and tuple relational calculus query representations, It also investigates preconditions that transaction management mechanisms must satisfy to make multiple query optimization effective. The joint execution of batches of queries and update operations has been a standard technique in the conventional, record-at-a-time file systems of the sixties and early seventies. However, with the introduction of interactive database systems based on direct access to specific subsets of data, the
23|On Line Processing of Compacted Relations|Most data base machines use some kind of &amp;quot;filter&amp;quot; that performs unary relational operators (selec-tion and projection) on relations Cl to 71. These filters operate &amp;quot;on the fly &amp;quot; that is, at the speed of the disk, while the relation is being transferred into main memory, Processing time being proportional to relation size, it is therefore important to represent data in the most compacted way. In this paper we address the problem of satisfying the two seemingly contra-dictory requirements: i) finding an &amp;quot;optimal &amp;quot; compaction scheme ii) processing optimally compacted relations on
24|On estimating cost of accessing records in blocked database organizations|The estimation of the cost of processing a query using a particular access path under a given physical organization has important applications in integrated database environments. When records in a file are stored in fixed-length physical blocks in secondary storage, and mechanisms are available whereby a query can be resolved without the accessing of all of the records, an important measure of the cost of using a particular access path is the number of blocks that have to be accessed in referencing the records of interest. In this paper, a general formula is derived for the expected number of blocks on which a random sample of r records from a file containing n records (which may be of arbitrary lengths, and which may extend across block boundaries) will reside. The specialization of this formula to the case of fixed-length records is discussed. An approximation to this formula which is highly accurate for a wide range of parameters and which can be computed very efficiently is also provided.
25|Multiple-Query Optimization|Some recently proposed extensions to relational database systems, as well as to deductive database systems, require support for multiple-query processing. For example, in a database system enhanced with inference capabilities, a simple query involving a rule with multiple definitions may expand to more than one actual query that has to be run over the database. It is an interesting problem then to come up with algorithms that process these queries together instead of one query at a time. The main motivation for performing such an interquery optimization lies in the fact that queries may share common data. We examine the problem of multiple-query optimization in this paper. The first major contribution of the paper is a systematic look at the problem, along with the presentation and analysis of algorithms that can be used for multiple-query optimization. The second contribution lies in the presentation of experimental results. Our results show that using multiple-query processing algorithms may reduce execution cost considerably.
26|The design of Postgres|This paper presents the preliminary design of a new database management system, called POSTGRES, that is the successor to the INGRES relational database system. The main design goals of the new system are to: 1) provide better support for complex objects, 2) provide user extendibility for data types, operators and access methods, 3) provide facilities for active databases (i.e., alerters and triggers) and inferencing including forward- and backward-chaining, 4) simplify the DBMS code for crash recovery, 5) produce a design that can take advantage of optical disks, workstations composed of multiple tightly-coupled processors, and custom designed VLSI chips, and 6) make as few changes as possible (preferably none) to the relational model. The paper describes the query language, programming langauge interface, system architecture, query processing strategy, and storage system for the new system. 1.
27|The Design and Implementation of INGRES|The currently operational (March 1976) version of the INGRES database management system is described. This multiuser system gives a relational view of data, supports two high level nonprocedural data sublanguages, and runs as a collection of user processes on top of the UNIX operating system for Digital Equipment Corporation PDP 11/40, 11/45, and 11/70 computers. Emphasis is on the design decisions and tradeoffs related to (1) structuring the system into processes, (2) embedding one command language in a general purpose programming language, (3) the algorithms implemented to process interactions, (4) the access methods implemented, (5) the concurrency and recovery control currently provided, and (6) the data structures used for system catalogs and the role of the database administrator. 
Also discussed are (1) support for integrity constraints (which is only partly operational), (2) the not yet supported features concerning views and protection, and (3) future plans concerning the system.
28|System R: Relational Approach to Database Management|System R is a database management system which provides a high level relational data interface. The system provides a high level of data independence by isolating the end user as much as possible from underlying storage structures. The system permits definition of a variety of relational views on common underlying data. Data control features are provided, including authorization, integrity assertions, triggered transactions, a logging and recovery subsystem, and facilities for maintaining data consistency in a shared-update environment. This paper contains a description of the overall architecture and design of the system. At the present time the system is being implemented and the design evaluated. We emphasize that System R is a vehicle for research in database architecture, and is not planned as a product.
29|Decomposition - a strategy for query processing|Strategy for processing multivariable queries in the database management system INGRES is considered. The general procedure is to decompose the query into a sequence of one-variable queries by alternating between (a) reduction: breaking off components of the query which are joined to it by a single variable, and (b) tuple substitution: substituting for one of the variables a tuple at a time. Algorithms for reduction and for choosing the variable to be substituted are given. In most cases the latter decision depends on estimation of costs; heuristic procedures for making such estimates are outlined.
30|An optimizing PROLOG front-end to a relational query system|An optimizing translation mechanism for the dynamic interaction between a logic-based expert system written in PROLOG and a re-lational database accessible through SQL is presented. The mechanism makes use of an intermediate language that decomposes the optimization problem and makes the proposed approach target-language independent. It can either facilitate expert system-database interaction, e.g., when integrating expert systems into business systems, or augment existing database with (external) deductive capabilities.
31|Query Optimization for XML|XML is an emerging standard for data representation  and exchange on the World-Wide Web. Due to  the nature of information on the Web and the inherent  flexibility of XML, we expect that much of the  data encoded in XML will be semistructured:the  data may be irregular or incomplete, and its structure  may change rapidly or unpredictably. This paper  describes the query processor of Lore,aDBMS  for XML-based data supporting an expressive query  language. We focus primarily on Lore&#039;s cost-based  query optimizer. While all of the usual problems  associated with cost-based query optimization apply  to XML-based query languages, a number of additional  problems arise, such as new kinds of indexing,  more complicated notions of database statistics, and  vastly different query execution strategies for different  databases. We define appropriate logical and  physical query plans, database statistics, and a cost  model, and we describe plan enumeration including  heuristics for reducing the large search space. Our  optimizer is fully implemented in Lore and preliminary  performance results are reported.
32|Query evaluation techniques for large databases|Database management systems will continue to manage large data volumes. Thus, efficient algorithms for accessing and manipulating large sets and sequences will be required to provide acceptable performance. The advent of object-oriented and extensible database systems will not solve this problem. On the contrary, modern data models exacerbate it: In order to manipulate large sets of complex objects as efficiently as today’s database systems manipulate simple records, query processing algorithms and software will become more complex, and a solid understanding of algorithm and architectural issues is essential for the designer of database management software. This survey provides a foundation for the design and implementation of query execution facilities in new database management systems. It describes a wide array of practical query evaluation techniques for both relational and post-relational database systems, including iterative execution of complex query evaluation plans, the duality of sort- and hash-based set matching algorithms, types of parallel query execution and their implementation, and special operators for emerging database application domains.
33|The Lorel Query Language for Semistructured Data|We present the Lorel language, designed for querying semistructured data. Semistructured data is becoming more and more prevalent, e.g., in structured documents such as HTML and when performing simple integration of data from multiple sources. Traditional data models and query languages are inappropriate, since semistructured data often is irregular, some data is missing, similar concepts are represented using different types, heterogeneous sets are present, or object structure is not fully known. Lorel is a user-friendly language in the SQL/OQL style for querying such data effectively. For wide applicability, the simple object model underlying Lorel can be viewed as an extension of ODMG and the language as an extension of OQL. The main novelties of the Lorel language are: (i) extensive use of coercion to relieve the user from the strict typing of OQL, which is inappropriate for semistructured data
34|DataGuides: Enabling Query Formulation and Optimization in Semistructured Databases|In semistructured databases there is no schema fixed  in advance. To provide the benefits of a schema in  such environments, we introduce DataGuides:  concise and accurate structural summaries of  semistructured databases. DataGuides serve as  dynamic schemas, generated from the database; they  are useful for browsing database structure,  formulating queries, storing information such as  statistics and sample values, and enabling query  optimization. This paper presents the theoretical  foundations of DataGuides along with an algorithm  for their creation and an overview of incremental  maintenance. We provide performance results based  on our implementation of DataGuides in the Lore  DBMS for semistructured data. We also describe the  use of DataGuides in Lore, both in the user interface  to enable structure browsing and query formulation,  and as a means of guiding the query processor and  optimizing query execution.
35|Querying Semi-Structured Data|

36|Object exchange across heterogeneous information sources|We address the problem of providing integrated access to diverse and dynamic information sources. We explain how this problem differs from the traditional database integration problem and we focus on one aspect of the information integration problem, namely information exchange. We define an object-based information exchange model and a corresponding query language that we believe are well suited for integration of diverse information sources. We describe how, the model and language have been used to integrate heterogeneous bibliographic information sources. We also describe two general-purpose libraries we have implemented for object exchange between clients and servers.
37|A Query Language and Optimization Techniques for Unstructured Data|A new kind of data model has recently emerged in which the database is not constrained by a conventional schema. Systems like ACeDB, which has become very popular with biologists, and the recent Tsimmis proposal for data integration organize data in tree-like structures whose components can be used equally well to represent sets and tuples. Such structures allow great flexibility in data representation  What query language is appropriate for such structures? Here we propose a simple language UnQL for querying data organized as a rooted, edge-labeled graph. In this model, relational data may be represented as fixed-depth trees, and on such trees UnQL is equivalent to the relational algebra. The novelty of UnQL consists in its programming constructs for arbitrarily deep data and for cyclic structures. While strictly more powerful than query languages with path expressions like XSQL, UnQL can still be efficiently evaluated. We describe new optimization techniques for the deep or &#034;vertical&#034; dimension of UnQL queries. Furthermore, we show that known optimization techniques for operators on flat relations apply to the &#034;horizontal&#034; dimension of UnQL.  
38|A Query Language for XML|An important application of XML is the interchange of electronic data (EDI) between multiple data sources on the Web. As XML data proliferates on the Web, applications will need to integrate and aggregate data from multiple source and clean and transform data to facilitate exchange. Data extraction, conversion, transformation, and integration are all well-understood database problems, and their solutions rely on a query language. We present a query language for XML, called XML-QL, which we argue is suitable for performing the above tasks. XML-QL is a declarative, &#034;relational complete&#034; query language and is simple enough that it can be optimized. XML-QL can extract data from existing XML documents and construct new XML documents. Keywords: XML, query languages, electronic-data interchange (EDI) 1. Introduction The goal of XML is to provide many of SGML&#039;s benefits not available in HTML and to provide them in a language that is easier to learn and use than complete SGML. These benefits...
39|Lore: A database management system for semistructured data|Lore (for Lightweight Object Repository) is a DBMS designed specifically for managing semistructured information. Implementing Lore has required rethinking all aspects of a DBMS, including storage management, indexing, query processing and optimization, and user interfaces. This paper provides an overview of these aspects of the Lore system, as well as other novel features such as dynamic structural summaries and seamless access to data from external sources. 
40|Semistructured data|In semistructured data, the information that is normally as-sociated with a schema is contained within the data, which is sometimes called “self-describing”. In some forms of semi-structured data there is no separate schema, in others it exists but only places loose constraints on the data. Semi-structured data has recently emerged as an important topic of study for a variety of reasons. First, there are data sources such as the Web, which we would like to treat as databases but which cannot be constrained by a schema. Second, it may be desirable to have an extremely flexible format for data exchange between disparate databases. Third, even when dealing with structured data, it may be helpful to view it. as semistructured for the purposes of browsing. This tu-torial will cover a number of issues surrounding such data: finding a concise formulation, building a sufficiently expres-sive language for querying and transformation, and opti-mizat,ion problems. 1 The motivation The topic of semistructured data (also called unstructured data) is relatively recent, and a tutorial on the topic may well be premature. It represents, if anything, the conver-gence of a number of lines of thinking about new ways to represent and query data that do not completely fit with conventional data models. The purpose of this tutorial is to to describe this motivation and to suggest areas in which further research may be fruitful. For a similar exposition, the reader is referred to Serge Abiteboul’s recent survey pa-per PI. The slides for this tutorial will be made available from a section of the Penn database home page
41|Catching the Boat with Strudel: Experiences with a Web-Site Management System|The Strudel system applies concepts from database management systems to the process of building Web sites. Strudel&#039;s key idea is separating the management of the site&#039;s data, the creation and management of the site&#039;s structure, and the visual presentation of the site&#039;s pages. First, the site builder creates a uniform model of all data available at the site. Second, the builder uses this model to declaratively define the Web site&#039;s structure by applying a &#034;site-definition query&#034; to the underlying data. The result of evaluating this query is a &#034;site graph&#034;, which represents both the site&#039;s content and structure. Third, the builder specifies the visual presentation of pages in Strudel&#039;s HTML-template language. The data model underlying Strudel is a semi-structured model of labeled directed graphs. We describe Strudel&#039;s key characteristics, report on our experiences using Strudel, and present the technical problems that arose from our experience. We describe our experience constructing sev...
42|Optimizing Regular Path Expressions Using Graph Schemas|Query languages for data with irregular structure use regular path expressions for navigation. This feature is useful for querying data where parts of the structure is either unknown, unavailable to the user, or changes frequently. Naive execution of regular path expressions is inefficient, however, because it ignores any structure in the data. We describe two optimization techniques for queries with regular path expressions. Both rely on graph schemas for specifying partial knowledge about the data&#039;s structure. Query pruning uses this structure to restrict navigation to only a fragment of the data; we give an efficient algorithm for rewriting any regular path expression query into a pruned one. Query rewriting using state extents  can eliminate or reduce navigation altogether; it is reminiscent of optimizing relational queries using indices. There may be several ways to optimize a query using state extents; we give a polynomial-space algorithm that finds all such optimizations. For re...
43|Querying Semistructured Heterogeneous Information|. Semistructured data has no absolute schema fixed in advance and its structure may be irregular or incomplete. Such data commonly arises in sources that do not impose a rigid structure (such as the World-Wide Web) and when data is combined from several heterogeneous sources. Data models and query languages designed for well structured data are inappropriate in such environments. Starting with a &#034;lightweight&#034; object model adopted for the TSIMMIS project at Stanford, in this paper we describe a query language and object repository designed specifically for semistructured data. Our language provides meaningful query results in cases where conventional models and languages do not: when some data is absent, when data does not have regular structure, when similar concepts are represented using different types, when heterogeneous sets are present, and when object structure is not fully known. This paper motivates the key concepts behind our approach, describes the language through a series o...
44|A Query Language for a Web-Site Management System|&#034;, &#034;Author&#034;. In addition it constructs a special node Authors() and connects it to all pages corresponding to &#034;Author&#034;s. The output graph is called SiteGraph. One way to write this in StruQL is:  input DataGraph  where Root(x); x ! ! y; y ! l ! z; l in f&#034;Paper&#034;, &#034;TechReport&#034;, &#034;Title&#034;, &#034;Abstract&#034;, &#034;Author&#034;g  create Authors(); Page(y); Page(z)  link Page(y) ! l ! Page(z)  where x ! ! y1; y1 ! &#034;Author&#034; ! z1  link Authors() ! &#034;Author&#034; ! Page(z1)  output SiteGraph  2  In order to integrate information from several source, we allow multiple input graphs. When multiple input graphs are present, every occurrence of a collection needs to be preceded by a graph name. For clarity of presentation however, we focus on queries with only one input graph.  Intermixing the where; create; link clauses makes the query easier to read. This is nothing more than syntactic convenience, since the meaning is the same as that of the query in which all clauses are joined together:  input DataGraph  where Root(x...
45|Evaluating Queries with Generalized Path Expressions|In the past few years, query languages featuring generalized path expressions have been proposed. These languages allow the interrogation of both data and structure. They are powerful and essential for a number of applications. However, until now, their evaluation has relied on a rather naive and inefficient algorithm. In this paper, we extend an object algebra with two new operators and present some interesting rewriting techniques for queries featuring generalized path expressions. We also show how a query optimizer can integrate the new techniques.  1 Introduction  In the past few years there has been a growing interest in query languages featuring generalized path expressions (GPE) [BRG88, KKS92, CACS94, QRS  +  95]. With these languages, one may issue queries on data without exact knowledge of its structure. A GPE queries data and structure at the same time. Although very useful for standard database applications, these languages are vital for new applications dedicated, for insta...
46|Indexing Semistructured Data|This paper describes techniques for building and exploiting indexes on semistructured data: data that may not have a fixed schema and that may be irregular or incomplete. We first present a general framework for indexing values in the presence of automatic type coercion. Then based on  Lore, a DBMS for semistructured data, we introduce four types of indexes and illustrate how they are used during query processing. Our techniques and indexing structures are fully implemented and integrated into the Lore prototype. 1 Introduction  We call data that is irregular or that exhibits type and structural heterogeneity semistructured,  since it may not conform to a rigid, predefined schema. Such data arises frequently on the Web, or when integrating information from heterogeneous sources. In general, semistructured data can be neither stored nor queried in relational or object-oriented database management systems easily and efficiently. We are developing Lore  1  , a database management system d...
47|Semantic Query Optimization for Object Databases|We present a technique for semantic query optimization (SQO) for object databases. We use the ODMG-93 standard ODL and OQL languages. The ODL object schema and the OQL object query are translated into a DATALOG representation. Semantic knowledge about the object model and the particular application is expressed as integrity constraints. This is an extension of the ODMG-93 standard. SQO is performed in the DATALOG representation and an equivalent logic query, and subsequently an equivalent OQL object query, are obtained. SQO is based on the residue technique of [3]. We show that our technique generalizes previous research on SQO for object databases. It can be applied to queries with structure constructors and method application. It utilizes integrity constraints about keys, methods, and knowledge of access support relations, to produce equivalent queries, which may have more efficient evaluation plans.  1. 
48|Cost-based Selection of Path Expression Processing Algorithms in Object-Oriented Databases|An object query can include a path expression to traverse a number of related collections. The order of collection traversals given by the path expression may not be the most efficient to process the query. This generates a critical problem for an object query optimizer to select the best execution plan. This paper studies the different algorithms to process path expressions with predicates, including depth first navigation, forward and reverse joins. Using a cost model, it then compares their performances in different cases, according to memory size, selectivity of predicates, fan out between collections, etc.. It also presents a heuristic-based algorithm to find profitable n-ary operators for traversing collections, thus reducing the search space of query plans to process a query with a qualified path expression. An implementation based on the O2 system demonstrates the validity of the results. 
49|Query Optimization for Semistructured Data|With the emerging prevalence of semistructured data -- data that may be irregular or incomplete -- it is important to develop efficient query processing techniques for such data. This paper describes the query processor of Lore, a DBMS for semistructured data, and focuses particularly on the cost-based query optimization techniques we have developed and implemented for a semistructured environment. While all of the usual problems associated with cost-based query optimization apply to semistructured data as well, a number of additional problems arise, suchasvastly different query execution strategies for different semistructured databases, more complicated notions of database statistics, and novel uses of indexing. Weintroduce very flexible logical query plans that can be transformed into a wide varietyofphysical plans, define appropriate database statistics and a cost model, and describe plan enumeration including heuristics for reducing the search space. Our optimizer is fully implemented for most of the Lore query language, and preliminary performance results are reported.
50|Query Optimization and Execution Plan Generation in Object-Oriented Data Management Systems|The generation of execution plans for object-oriented database queries is a new and challenging area of study. Unlike the relational algebra, a common set of object algebra operators has not been defined. Similarly, a standardized object manager interface analogous to storage manager interfaces of relational subsystems does not exist. We define the interface to an object manager whose operations are the executable elements of query execution plans. Parameters to the object manager interface are streams of tuples of object identifiers. The object manager can apply methods and simple predicates to the objects identified in a tuple. Two algorithms for generating such execution plans for queries expressed in an object algebra are presented. The first algorithm runs quickly but may produce inefficient plans. The second algorithm enumerates all possible execution plans and presents them in an efficient, compact representation.  Keywords: object-oriented databases, query processing, query opt...
51|Compile-Time Path Expansion in Lore|Semistructured data usually is modeled as labeled directed graphs, and query languages  are based on declarative path expressions that specify traversals through the graphs. Regular  (or generalized) path expressions use regular expression operators to specify traversal patterns.  Regular path expressions typically are evaluated at run-time by exploring the database graph.  However, if the database includes a structural summary such as a DataGuide, then an alternative  approach is to expand regular path expressions at compile-time using the structural summary,  reducing the run-time overhead of database exploration. This paper describes algorithms for  compile-time regular path expression expansion in the context of the Lorel query language  for semistructured data, and reports on performance results conducted on the Lore system  illustrating the benefits of compile-time expansion.  1 Introduction  Efficiently storing and querying semistructured data---data that need not adhere to a fi...
52|An Extensible Query Optimizer for an Objectbase Management System|We describe an extensible query optimizer for objectbase management systems. Since these systems are expected to serve data management needs of a wide range of application domains with possibly different query optimization requirements, extensibility is essential. Our work is conducted within the context of TIGUKAT, which is a uniform behavioral system that models every system component as a first-class object. Consistent with this philosophy, we model every component of the optimizer as a first-class object, providing ultimate extensibility. We describe the optimizer architecture and how the optimizer components are modeled as extensions of a uniform type system.  
53|Query Optimization for Parallel Execution|The decreasing cost of computing makes it economically viable to reduce the response time of decision support queries by using parallel execution to exploit inexpen-sive resources. This goal poses the following query op-timization problem: Mzntmzze response ttme subject to constraints on throughput, which we motivate as the dual of the traditional DBMS problem, We address this novel problem in the context of Select-Project-Join queries by extending the execution space, cost model and search al-gorithm that are widely used in commercial DBItlSs. We incorporate the sources and deterrents of parallelism in the traditional execution space. We show that a cost model can predict response time while accounting for the new aspects due to parallelism, We observe that the response time optimization metric violates a fundamen-tal assumption in the dynamic programming algorithm that is the linchpin in the optimizers of most commer-cial DBMSS. We extend dynamic programming and show how optimization metrics which correctly predict response time may be designed. 1
54|The Gamma database machine project|This paper describes the design of the Gamma database machine and the techniques employed in its implementation. Gamma is a relational database machine currently operating on an Intel iPSC/2 hypercube with 32 processors and 32 disk drives. Gamma employs three key technical ideas which enable the architecture to be scaled to 100s of processors. First, all relations are horizontally partitioned across multiple disk drives enabling relations to be scanned in parallel. Second, novel parallel algorithms based on hashing are used to implement the complex relational operators such as join and aggregate functions. Third, dataflow scheduling techniques are used to coordinate multioperator queries. By using these techniques it is possible to control the execution of very complex queries with minimal coordination- a necessity for configurations involving a very large number of processors. In addition to describing the design of the Gamma software, a thorough performance evaluation of the iPSC/2 hypercube version of Gamma is also presented. In addition to measuring the effect of relation size and indices on the response time for selection, join, aggregation, and update queries, we also analyze the performance of Gamma relative to the number of processors employed when the sizes of the input relations are kept constant (speedup) and when the sizes of the input relations are increased proportionally to the number of processors (scaleup). The speedup results obtained for both selection and join queries are linear; thus, doubling the number of processors
55|Parallel Database Systems: The Future of Database Processing or a Passing Fad?|Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems.
56|The Cascades Framework for Query Optimization|This paper describes a new extensible query optimization framework that resolves many of the shortcomings of the EXODUS and Volcano optimizer generators. In addition to extensibility, dynamic programming, and memorization based on and extended from the EXODUS and Volcano prototypes, this new optimizer provides (i) manipulation of operator arguments using rules or functions, (ii) operators that are both logical and physical for predicates etc., (iii) schema-specific rules for materialized views, (iv) rules to insert ”enforcers ” or ”glue operators, ” (v) rule-specific guidance, permitting grouping of rules, (vi) basic facilities that will later permit parallel search, partially ordered cost measures, and dynamic plans, (vii) extensive tracing support, and (viii) a clean interface and implementation making full use of the abstraction mechanisms of C++. We describe and justify our design choices for each of these issues. The optimizer system described here is operational and will serve as the foundation for new query optimizers in Tandem’s NonStop SQL product and in Microsoft’s SQL Server product. 1
57|Algebraic optimization of computations over scientific databases|Although scientific data analysis increasingly requires access and manipulation of large quantities of data, current database technology fails to meet the needs of scientific processing in a number of areas. To overcome acceptance problems among scientific users, database systems must provide performance and functionality comparable to current combinations of scientific programs and file systems. Therefore, we propose extending the concept of a database query to include numeric computation over scientific databases. In this paper, we examine the specification of an integrated algebra that includes traditional database operators for pattern matching and search as well as numeric operators for scientific data sets. Through the use of a single integrated algebra, we can perform automatic optimization on scientific computations, realizing all of the traditional benefits of optimization. We have experimented with a prototype optimizer which integrates sets, time series and spectra data types and operators on those types. Our results demonstrate that scientific database computations using numeric operators on multiple data types can’be effectively optimized and permit performance gains that could not be realized without the integration, This research has been performed in collaboration with the Space Grant College at the University of Colorado at Boulder, where the results are being applied to the analysis of experimental data from satellite observations. 1.
58|An overview of query optimization in relational systems|There has been extensive work in query optimization since the early ‘70s. It is hard to capture the breadth and depth of this large body of work in a short article. Therefore, I have decided to focus primarily on the optimization of SQL queries in relational database systems and present my biased and incomplete view of this field. The goal of this article is not to be comprehensive, but rather to explain the foundations and present samplings of significant work in this area. I would like to apologize to the many contributors in this area whose work I have failed to explicitly acknowledge due to oversight or lack of space. I take the liberty of trading technical precision for ease of presentation. 2.
59|Data cube: A relational aggregation operator generalizing group-by, cross-tab, and sub-totals|Abstract. Data analysis applications typically aggregate data across many dimensions looking for anomalies or unusual patterns. The SQL aggregate functions and the GROUP BY operator produce zero-dimensional or one-dimensional aggregates. Applications need the N-dimensional generalization of these operators. This paper defines that operator, called the data cube or simply cube. The cube operator generalizes the histogram, crosstabulation, roll-up, drill-down, and sub-total constructs found in most report writers. The novelty is that cubes are relations. Consequently, the cube operator can be imbedded in more complex non-procedural data analysis programs. The cube operator treats each of the N aggregation attributes as a dimension of N-space. The aggregate of a particular set of attribute values is a point in this space. The set of points forms an N-dimensional cube. Super-aggregates are computed by aggregating the N-cube to lower dimensional spaces. This paper (1) explains the cube and roll-up operators, (2) shows how they fit in SQL, (3) explains how users can define new aggregate functions for cubes, and (4) discusses efficient techniques to compute the cube. Many of these features are being added to the SQL Standard.
60|Combining fuzzy information from multiple systems (Extended Abstract)  (1996) |In a traditional database system, the result of a query is a set of values (those values that satisfy the query). In other data servers, such as a system with queries baaed on image content, or many text retrieval systems, the result of a query is a sorted list. For example, in the case of a system with queries based on image content, the query might aak for objects that are a particular shade of red, and the result of the query would be a sorted list of objects in the database, sorted by how well the color of the object matches that given in the query. A multimedia system must somehow synthesize both types of queries (those whose result is a set, and those whose result is a sorted list) in a consistent manner. In this paper we discuss the solution adopted by Garlic, a multimedia information system being developed at
61|Improved Histograms for Selectivity Estimation of Range Predicates|Many commercial database systems maintain histograms to summarize the contents of relations and permit efficient estimation of query result sizes and access plan costs. Although several types of histograms have been proposed in the past, there has never been a systematic study of all histogram aspects, the available choices for each aspect, and the impact of such choices on histogram effectiveness. In this paper, we provide a taxonomy of histograms that captures all previously proposed histogram types and indicates many new possibilities. We introduce novel choices for several of the taxonomy dimensions, and derive new histogram types by combining choices in effective ways. We also show how sampling techniques can be used to reduce the cost of histogram construction. Finally, we present results from an empirical study of the proposed histogram types used in selectivity estimation of range predicates and identify the histogram types that have the best overall performance. 1 Introduction...
62|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
63|Selectivity Estimation Without the Attribute Value Independence Assumption|The result size of a query that involves multiple attributes from the same relation depends on these attributes’joinr data distribution, i.e., the frequencies of all combinations of attribute values. To simplify the estimation of that size, most commercial systems make the artribute value independenceassumption and maintain statistics (typically histograms) on individual attributes only. In reality, this assumption is almost always wrong and the resulting estimations tend to be highly inaccurate. In this paper, we propose two main alternatives to effectively approximate (multi-dimensional) joint data distributions. (a) Using a multi-dimensional histogram, (b) Using the Singular Value Decomposition (SVD) technique from linear algebra. An extensive set of experiments demonstrates the advantages and disadvantages of the two approaches and the benefits of both compared to the independence assumption. 1
64|Predicate Migration: Optimizing Queries with Expensive Predicates|. The traditional focus of relational query optimization schemes has been on the choice of join methods and join orders. Restrictions have typically been handled in query optimizers by &#034;predicate pushdown&#034; rules, which apply restrictions in some random order before as many joins as possible. These rules work under the assumption that restriction is essentially a zero-time operation. However, today&#039;s extensible and object-oriented database systems allow users to define time-consuming functions, which may be used in a query&#039;s restriction and join predicates. Furthermore, SQL has long supported subquery predicates, which may be arbitrarily time-consuming to check. Thus restrictions should not be considered zero-time operations, and the model of query optimization must be enhanced. In this paper we develop a theory for moving expensive predicates in a query plan so that the total cost of the plan --- including the costs of both joins and restrictions --- is minimal. We present an algorithm to implement the theory, as well as results of our implementation in POSTGRES. Our experience with the newly enhanced POSTGRES query optimizer demonstrates that correctly optimizing queries with expensive predicates often produces plans that are orders of magnitude faster than plans generated by a traditional query optimizer. The additional complexity of considering expensive predicates during optimization is found to be manageably small. 1
65|Extensible/Rule Based Query Rewrite Optimization in Starburst|This paper describes the Query Rewrite facility of the Starburst extensible database system, a novel phase of query optimization. We present a suite of rewrite rules used in Starburst to transform queries into equivalent queries for faster execution, and also describe the production rule engine which is used by Starburst to choose and execute these rules. Examples are provided demonstrating that these Query Rewrite transformations lead to query execution time improvements of orders of magnitude, suggesting that Query Rewrite in general --- and these rewrite rules in particular --- are an essential step in query optimization for modern database systems.  1 Introduction  In traditional database systems, query optimization typically consists of a single phase of processing in which access methods, join orders and join methods are chosen to provide an efficient plan for executing a user&#039;s declarative query. We refer to this phase as plan optimization. In this paper we present a distinct ph...
66|Random Sampling for Histogram Construction: How much is enough?|Random sampling is a standard technique for constructing (approximate) histograms for query optimization. However, any real implementation in commercial products requires solving the hard problem of determining &#034;How much sampling is enough?&#034; We address this critical question in the context of equi-height histograms used in many commercial products, including Microsoft SQL Server. We introduce a  conservative error metric capturing the intuition that for an approximate histogram to have low error, the error must be small in all regions of the histogram. We then present a result establishing an optimal bound on the amount of sampling required for pre-specified error bounds. We also describe an adaptive page sampling algorithm which achieves greater efficiency by using all values in a sampled page but adjusts the amount of sampling depending on clustering of values in pages. Next, we establish that the problem of estimating the number of distinct values is provably difficult, but propose ...
67|Including Group-By in Query Optimization|In existing relational database systems, processing of group-by and computation of aggregate functions are always postponed until all joins are performed. In this paper, we present transformations that make it possible to push group-by operation past one or more joins and can potentially reduce the cost of processing a query significantly. Therefore, the placement of group-by should be decided based on cost estimation. We explain how the traditional System-R style optimizers can be modified by incorporating the greedy conservative heuristic that we developed. We prove that applications of greedy conservative heuristic produce plans that are better (or no worse) than the plans generated by a traditional optimizer. Our experimental study shows that the extent of improvement in the quality of plans is significant with only a modest increase in optimization cost. Our technique also applies to optimization of Select Distinct queries by pushing down duplicate elimination in a cost-based fas...
68|Optimization of queries with user-defined predicates|Relational databases provide the ability to store user-defined functions and predicates which can be invoked in SQL queries. When evaluation of a user-defined predicate is relatively expensive, the traditional method of evaluating predicates as early as possible is no longer a sound heuristic. There are two previous approaches for optimizing such queries. However, neither is able to guarantee the optimal plan over the desired execution space. We present efficient techniques that are able to guarantee the choice of an optimal plan over the desired execution space. The naive optimization algorithm is very general, and therefore is most widely applicable. The optimization algorithm with complete rank-ordering improves upon the naive optimization algorithm by exploiting the nature of the cost formulas for join methods and is polynomial in the number of user-defined predicates (for a given number of relations). We also propose pruning rules that significantly reduce the cost of searching the execution space for both the naive algorithm as well as for the optimization algorithm with complete rank-ordering, without compromising optimality. We also propose a conservative local heuristic that is simpler and has low optimization overhead. Although it is not always guaranteed to find the optimal plans, it produces close to optimal plans in most cases. We discuss how, depending on application requirements, to determine the algorithm of choice. It should be emphasized that our optimization algorithms handle user-defined selections as well as user-defined join predicates uniformly. We present complexity analysis and experimental comparison of the algorithms.
69|Optimizing Queries over Multimedia Repositories|Multimedia repositories and applications that retrieve multimedia information are becoming increasingly popular. In this paper, we study the problem of selecting objects from multimedia repositories, and show how this problem relates to the processing and optimization of selection queries in other contexts, e.g., when some of the selection conditions are expensive user-defined predicates. We find that the problem has unique characteristics that lead to interesting new research questions and results. This article presents an overview of the results in [1]. An expanded version of that paper is in preparation [2]. 1 Query Model  In this section we first describe the model that we use for querying multimedia repositories. Then, we briefly review related models for querying text and image repositories.  1.1 Our Query Model  In our model, a multimedia repository consists of a set of multimedia objects, each with a distinct object identity. Each multimedia object has a set of attributes, like...
70|Eager Aggregation and Lazy Aggregation|Efficient processing of aggregation queries is essential for decision support applications. This paper describes a class of query transformations, called eager aggregation and lazy aggregation, that allows a query optimizer to move group-by operations up and down the query tree. Eager aggregation partially pushes a group-by past a join. After a group-by is partially pushed down, we still need to perform the original group-by in the upper query block. Eager aggregation reduces the number of input rows to the join and thus may result in a better overall plan. The reverse transformation, lazy aggregation, pulls a group-by above a join and combines two group-by operations into one. This transformation is typically of interest when an aggregation query references a grouped view (a view containing a group-by). Experimental results show that the technique is very beneficial for queries in the TPC-D benchmark. 1 Introduction Aggregation is widely used in decision support systems. All queries...
71|Query optimization by predicate move-around|levyQresearch.att.com mumickQresearch.att.com A new type of optimization, called predicate move-around, ia introduced. It is shown how this optimization ‘considerably improvea the efficiency of evaluating SQL queries that have query graphs with a large number of query blocks (which ie a typical situation when queries are defined in terms of multiple views and subqueries). Predicate move-around works by moving predicates across query blocks (in the query graph) that cannot be merged into one block. Predicate move-around is a generalization of and has many advantages over the traditional predicate pushdotin. One key advantage arises from the fact that predicate move-around precedes pushdown by pulling predicates up the query graph. As a result, predicates that appear in the query in one part of the graph can be moved around the graph and applied alao in other parts of graph. Moreover, predicate move-around optimization can move a wider class of predicates in a wider class of queries aa compared to the standard predicate pushdown techniques. In addition to the usual comparison and arithmetic predicates, other predicates that can be moved around are the EXISTS and HOT EXISTS clauses, the EXCEPT clause, and functional dependencies. The proposed optimization can also move predicates through aggregation. Moreover, the method can also infer new predicates when existing predicates are moved through aggregation or when certain functional dependencies are known to hold. Finally, the predicate move-around algorithm is easy to implement on top of existing query optimizers. 1
72|Improved unnesting algorithms for join aggregate SQL queries|Abstract: The SQL language allows users to express queries that have nested subqueries in them. Optimi-zation of nested queries has received considerable attention over the last few years [Kim82, Ganski87, Daya187, Murali891. As pointed out in [Ganski87], the solution presented in [Kim821 for JA type queries has the COUNT bug. In order to avoid the COUNT bug, the more general strategy described in [Gan-ski871 and [Dayal87] is used. In this paper, we modify Kim’s algorithm so that it avoids the COUNT bug. The modified algorithm may be used when it is more efficient than the general strategy. In addition, we present a couple of enhancements that precompute aggregates and evaluate joins and outer joins in a top-down order. These enhancements eliminate Cartesian products when certain correlation predicates are ab-sent and enable us to employ Kim’s method for more blocks. Finally, we incorporate the above improve-merits into a new unnesting algorithm. 1.
73|Magic is Relevant|We define the magic-sets transformation for traditional relational systems (with duplicates, aggregation and grouping), as well as for relational systems extended with recursion. We compare the magic-sets rewriting to traditional optimization techniques for nonrecursive queries, and use performance experiments to argue that the magic-sets transformation is often a better optimization technique.  1 Introduction  &#034;Magic-sets&#034; is the name of a query transformation algorithm ([BMSU86]) (and now a class of algorithms    Part of this work was done at the IBM Almaden Research Center. Work at Stanford was supported by an NSF grant IRI87 -22886, an Air Force grant AFOSR-88-0266, and a grant of IBM Corporation.  y  Author&#039;s current affiliation: Tandem Computers.  z  Part of this work was done while the author was visiting IBM Almaden Research Center. Work at Wisconsin was supported by an IBM Faculty Development Award and an NSF grant IRI-8804319.   --- Generalized Magic-sets of [BR87], Magic Tem...
74|Towards an Open Architecture for LDL|We extend LDL to allow programs to call external procedures and vice versa. This extension allows the modularization of LDL, since external predicates are equivalent to external procedures written in LDL. External predicates are viewed as infinite relations so that the traditional semantics of logic programs remain applicable. To avoid computing infinite relations, wellformedness conditions for programs in extended LDL are given. The traditional optimization framework can still be used; it is only necessary to define a new set of cost functions capable of handling the infinite relations. The problem of interfacing LDL programs with external procedures---exchanging complex objects and returning multiple solutions---is discussed. Thus, we provide a general framework to allow logic programs to interact with external procedures without sacrificing amenities such as optimization, safety, etc. This approach forms the basis for the implementation of externals and modules in the LDL ...
76|Implementation of Magic-sets in a Relational Database System|We describe the implementation of the magic-sets transformation in the Starburst extensible relational database system. To our knowledge this is the first implementation of the magic-sets transformation in a relational database system. The Starburst implementation has many novel features that make our implementation especially interesting to database practitioners (in addition to database researchers). (1) We use a cost-based heuristic for determining join orders  (sips) before applying magic. (2) We push all equality and non-equality predicates using magic, replacing traditional predicate pushdown optimizations. (3) We apply magic to full SQL with duplicates, aggregation, null values, and subqueries. (4) We integrate magic with other relational optimization techniques. (5) The implementation is extensible.   Our implementation demonstrates the feasibility of the magic-sets transformation for commercial relational systems, and provides a mechanism to implement magic as an integral part...
77|Parallel Query Processing Using Shared Memory Multiprocessors and Disk Arrays| my research. I will also remember that it was from them that I learned how to appreciate a good beer and enjoy a good party. I would like to thank my fellow students Yongdong Wang and Chuen-tsai Sun for their valuable friendship and for all their help. I also would like to thank Guangrui Zhu and Yan Wei for being two special friends and making my life more interesting. Many thanks  v also go to my college friends Yuzheng Ding and Jiyang Liu. Our communications have always been an inspiring source in my life. Although my parents and my sister are an ocean away, they have offered me their constant love and encouragement throughout my study. I would like to take this opportunity to thank them for everything they have done for me. Last, but the most, I would like to thank my dear wife, Nanyan Xiong. Without her love, understanding and support throughout my Ph.D. program, this thesis would not have been possible. This thesis is dedicated to her as a small token of my deep appreciation.  
78|Query optimization in heterogeneous DBMS|We propose a query optimization strategy for heterogeneous DBMSs that extends the traditional optimizer strategy widely used in commercial DBMSs to allow execution of queries over both known (i.e., proprietary) DBMSs and foreign vendor DBMSs that conform to some standard such as providing the usual relational database statistics. We assume that participating DBMSs are autonomous and may not be able, even if willing, to provide the cost model parameters. The novelty of the strategy is to deduce the necessary information by calibrating a given DBMS. As the calibration has to be done as a user, not as a system administrator, it poses unpredictability problems such as inferring the access methods used by the DBMS, idiosyncrasies of the storage subsystem and coincidental clustering of data. In this paper we propose a calibrating database which is synthetically created so as to make the process of deducing the cost model coefficients reasonably devoid of the unpredictability problems. Using this procedure, we calibrate three commercial DBMSs, namely Allbase, DB2, and Informix, and observe that in 80 % of the cases the estimate is quite accurate. 1 Mot ivat ion Heterogeneity of databases and database management systems (DBMSs) h ave gained its due recognition as a result of the advent of open systems. Typically this heterogeneity may result from semantic discrepancies in the data, multiple data models, different systems, etc. All of these are consequences of the need for independent database systems to interoperate while
79|Global Query Optimization|novel hybrid algorithm with marriage of particle swarm
80|Rank-aware query optimization|Ranking is an important property that needs to be fully supported by current relational query engines. Recently, several rank-join query operators have been proposed based on rank aggregation algorithms. Rank-join operators progressively rank the join results while performing the join operation. The new operators have a direct impact on traditional query processing and optimization. We introduce a rank-aware query optimization framework that fully integrates rank-join operators into relational query engines. The framework is based on extending the System R dynamic programming algorithm in both enumeration and pruning. We define ranking as an interesting property that triggers the generation of rank-aware query plans. Unlike traditional join operators, optimizing for rank-join operators depends on estimating the input cardinality of these operators. We introduce a probabilistic model for estimating the input cardinality, and hence the cost of a rank-join operator. To our knowledge, this paper is the first effort in estimating the needed input size for optimal rank aggregation algorithms. Costing ranking plans, although challenging, is key to the full integration of rank-join operators in real-world query processing engines. We experimentally evaluate our framework by modifying the query optimizer of an open-source database management system. The experiments show the validity of our framework and the accuracy of the proposed estimation model. 1.
81|Rank Aggregation Methods for the Web|We consider the problem of combining ranking results from various sources. In the context of the Web, the main applications include building meta-search engines, combining ranking functions, selecting documents based on multiple criteria, and improving search precision through word associations. Wedevelop a set of techniques for the rank aggregation problem and compare their performance to that of well-known methods. A primary goal of our work is to design rank aggregation techniques that can effectively combat &#034;spam,&#034; a serious problem in Web searches. Experiments show that our methods are simple, efficient, and effective.  
82|Evaluating Top-k Queries over Web-Accessible Databases|... In this article, we study how to process top-k queries efficiently in this setting, where the attributes for which users specify target values might be handled by external, autonomous sources with a variety of access interfaces. We present a sequential algorithm for processing such queries, but observe that any sequential top-k query processing strategy is bound to require unnecessarily long query processing times, since web accesses exhibit high and variable latency. Fortunately, web sources can be probed in parallel, and each source can typically process concurrent requests, although sources may impose some restrictions on the type and number of probes that they are willing to accept. We adapt our sequential query processing technique and introduce an efficient algorithm that maximizes sourceaccess parallelism to minimize query response time, while satisfying source-access constraints. 
83|Dataflow query execution in a parallel main-memory environment|Abstract. In this paper, the performance and characteristics of the execution of various join-trees on a parallel DBMS are studied. The results of this study are a step into the direction of the design of a query optimization strategy that is fit for parallel execution of complex queries. Among others, synchronization issues are identified to limit the performance gain from parallelism. A new hash-join algorithm is introduced that has fewer synchronization constraints han the known hash-join algorithms. Also, the behavior of individual join operations in a join-tree is studied in a simulation experiment. The results how that the introduced Pipelining hash-join algorithm yields a better performance for multi-join queries. The format of the optimal join-tree appears to depend on the size of the operands of the join: A multi-join between small operands performs best with a bushy schedule; larger operands are better off with a linear schedule. The results from the simulation study are confirmed with an analytic model for dataflow query execution. Ke~,ords: parallel query processing, multi-join queries, simulation, analytical modeling 1.
84| Ripple Joins for Online Aggregation |We present a new family of join algorithms, called ripple joins, for online processing of multi-table aggregation queries in a relational database management system (dbms). Such queries arise naturally in interactive exploratory decision-support applications. Traditional offline join algorithms are designed to minimize the time to completion of the query. In contrast, ripple joins are designed to minimize the time until an acceptably precise estimate of the query result is available, as measured by the length of a confidence interval. Ripple joins are adaptive, adjusting their behavior during processing in accordance with the statistical properties of the data. Ripple joins also permit the user to dynamically trade off the two key performance factors of online aggregation: the time between successive updates of the running aggregate, and the amount by which the confidence-interval length decreases at each update. We show how ripple joins can be implemented in an existing dbms using iterators, and we give an overview of the methods used to compute confidence intervals and to adaptively optimize the ripple join &#034;aspect-ratio&#034; parameters. In experiments with an initial implementation of our algorithms in the postgres dbms, the time required to produce reasonably precise online estimates was up to two orders of magnitude smaller than the time required for the best offline join algorithms to produce exact answers. 
85|Prefer: A system for the efficient execution of multi-parametric ranked queries|Users often need to optimize the selection of objects by appropriately weighting the importance of multiple object attributes. Such optimization problems appear often in operations’ research and applied mathematics as well as everyday life; e.g., a buyer may select a home as a weighted function of a number of attributes like its distance from office, its price, its area, etc. We capture such queries in our definition of preference queries that use a weight function over a relation’s attributes to derive a score for each tuple. Database systems cannot efficiently produce the top results of a preference query because they need to evaluate the weight function over all tuples of the relation. PREFER answers preference queries efficiently by using materialized views that have been preprocessed
86|Supporting top-k join queries in relational databases|Abstract. Ranking queries, also known as top-k queries, produce results that are ordered on some computed score. Typically, these queries involve joins, where users are usually interested only in the top-k join results. Top-k queries are dominant in many emerging applications, e.g., multimedia retrieval by content, Web databases, data mining, middlewares, and most information retrieval applications. Current relational query processors do not handle ranking queries efficiently, especially when joins are involved. In this paper, we address supporting top-k join queries in relational query processors. We introduce a new rank-join algorithm that makes use of the individual orders of its inputs to produce join results ordered on a user-specified scoring function. The idea is to rank the join results progressively during the join operation. We introduce two physical query operators based on variants of ripple join that implement the rank-join algorithm. The operators are nonblocking and can be integrated into pipelined execution plans. We also propose an efficient heuristic designed to optimize a top-k join query by choosing the best join order. We address several practical issues and optimization heuristics to integrate the new join operators in practical query processors. We implement the new operators inside a prototype database engine based on PREDATOR. The experimental evaluation of our approach compares recent algorithms for joining ranked inputs and shows superior performance. Keywords: Ranking – Top-k queries – Rank aggregarion – Query operators
87|Top-k selection queries over relational databases: Mapping strategies and performance evaluation|In many applications, users specify target values for certain attributes, without requiring exact matches to these values in return. Instead, the result to such queries is typically a rank of the “top k” tuples that best match the given attribute values. In this paper, we study the advantages and limitations of processing a top-k query by translating it into a single range query that a traditional relational database management system (RDBMS) can process efficiently. In particular, we study how to determine a range query to evaluate a top-k query by exploiting the statistics available to an RDBMS, and the impact of the quality of these statistics on the retrieval efficiency of the resulting scheme. We also report the first experimental evaluation of the mapping strategies over a real RDBMS, namely over Microsoft’s SQL Server 7.0. The experiments show that our new techniques are robust and significantly more efficient than previously known strategies requiring at least one sequential scan of the data sets.
88|Reducing the braking distance of an SQL query engine|In a recent paper, we proposed adding a STOP AFTER clause to SQL to permit the cardinality of a query result to be explicitly limited by query writers and query tools. We demonstrated the usefulness of having this clause, showed how to extend a traditional cost-based query optimizer to accommodate it, and demonstrated via DB2-based simulations that large performance gains are possible when STOP AFTER queries are explicitly supported by the database engine. In this paper, we present several new strategies for efficiently processing STOP AFTER queries. These strategies, based largely on the use of range partitioning techniques, offer significant additional savings for handling STOP AFTER queries that yield sizeable result sets. We describe classes of queries where such savings would indeed arise and present experimental measurements that show the benefits and tradeoffs associated with the new processing strategies. 1
89|J.S.: Supporting incremental join queries on ranked inputs|This paper investigates the problem of incremental joins of multiple ranked data sets when the join condition is a list of arbitrary user-defined predicates on the input tuples. This problem arises in many important applications dealing with ordered inputs and multiple ranked data sets, and requiring the top k solutions. We use multimedia applications as the motivating examples but the problem is equally applicable to traditional database applications involving optimal resource allocation, scheduling, decision making, ranking, etc. We propose an algorithm J that enables querying of ordered data sets by imposing arbitrary userdefined join predicates. The basic version of the algorithm does not use any random access but a JPA variation can exploit available indexes for efficient random access based on the join predicates. A special case includes the join scenario considered by Fagin [1] for joins based on identical keys, and in that case, our algorithms perform as efficiently as Fagin’s. Our main contribution, however, is the generalization to join scenarios that were previously unsupported, including cases where random access in the algorithm is not possible due to lack of unique keys. In addition, J can support multiple join levels, or nested join hierarchies, which are the norm for modeling multimedia data. We also give-approximation versions of both of the above algorithms. Finally, we give strong optimality results for some of the proposed algorithms, and we study their performance empirically.
90|Tree Pattern Relaxation|Tree patterns are fundamental to querying tree-structured  data like XML. Because of the heterogeneity of XML data, it is often  more appropriate to permit approximate query matching and return  ranked answers, in the spirit of Information Retrieval, than to return only  exact answers. In this paper, we study the problem of approximate XML  query matching, based on tree pattern relaxations, and devise efficient  algorithms to evaluate relaxed tree patterns. We consider weighted tree  patterns, where exact and relaxed weights, associated with nodes and  edges of the tree pattern, are used to compute the scores of query answers. We are
91|Joining Ranked Inputs in Practice|Joining ranked inputs is an essential requirement for many database applications, such as ranking search results from multiple search engines and answering multi-feature queries for multimedia retrieval systems. We introduce a new practical pipelined query operator, termed NRA-RJ, that produces a global rank from input ranked streams based on a score function. The output of NRA-RJ can serve as a valid input to other NRA-RJ operators in the query pipeline. Hence, the NRA-RJ operator can support a hierarchy of join operations and can be easily integrated in query processing engines of commercial database systems.
92|Towards Efficient Multi-Feature Queries in Heterogeneous Environments|Applications like multimedia databases or enterprisewide information management systems have to meet the challenge of efficiently retrieving best matching objects from vast collections of data. We present a new algorithm Stream-Combine for processing multi-feature queries on heterogeneous data sources. Stream-Combine is selfadapting to different data distributions and to the specific kind of the combining function. Furthermore we present a new retrieval strategy that will essentially speed up the output of relevant objects.
93|Optimizing top-k selection queries over multimedia repositories|Repositories of multimedia objects having multiple types of attributes (e.g., image, text) are becoming increasingly common. A query on these attributes will typically request not just a set of objects, as in the traditional relational query model (filtering), but also a grade of match associated with  each object, which indicates how well the object matches the selection condition (ranking). Further-  more, unlike in the relational model, users may just want the k top-ranked objects for their selection  queries, for a relatively small k. In addition to the differences in the query model, another peculiarity  of multimedia repositories is that they may allow access to the attributes of each object only through  indexes. In this paper, we investigate how to optimize the processing of top-k selection queries over  multimedia repositories. The access characteristics of the repositories and the above query model lead  to novel issues in query optimization. In particular, the choice of the indexes used to search the repos-  itory strongly influences the cost of processing the filtering condition. We define an execution space  that is search-minimal, i.e., the set of indexes searched is minimal. Although the general problem  of picking an optimal plan in the search-minimal execution space is NP-hard, we present an efficient  algorithm that solves the problem optimally with respect to our cost model and execution space when  the predicates in the query are independent. We also show that the problem of optimizing top-k selection queries can be viewed, in many cases, as that of evaluating more traditional selection conditions. Thus,
94|Query Optimization in Database Grid |Abstract. DarGrid  is an implemented database gird system whose goal is to provide a semantic solution for integrating database resources on the web. Although many algorithms have been proposed for optimizing query-processing in order to minimize costs and/or response time, associated with obtaining the answer to query in a distributed database system, database grid query optimization problem is fundamentally different from distributed query optimization. These differences are shown to be the consequences of autonomy and heterogeneity of databases in database grid. Therefore, more challenges have arisen for query optimization in database grid than traditional distributed database. Following this observation, we present the design of a query optimizer in DartGrid  , and a heuristic, dynamic, and parallel query optimization approach for processing query in database grid is proposed. 1
95|Heuristic and Randomized Optimization for the Join Ordering Problem|Recent developments in database technology, such as deductive database systems, have given rise to the demand for new, cost-effective optimization techniques for join expressions. In this paper many different algorithms that compute approximate solutions for optimizing join orders are studied since traditional dynamic programming techniques are not appropriate for complex problems. First, two possible solution spaces, the space of left-deep and bushy processing trees, respectively, are evaluated from a statistical point of view. The result is that the common limitation to leftdeep processing trees is only advisable for certain join graph types. Basically, optimizers from three classes are analysed: heuristic, randomized and genetic algorithms. Each one is extensively scrutinized with respect to its working principle and its fitness for the desired application. It turns out that randomized and genetic algorithms are well suited for optimizing join expressions. They generate solutions of...
96|Iterative Dynamic Programming: A New Class of Query Optimization Algorithms|The query optimizer is one of the most important components of a database system. Most commercial query optimizers today are based on a dynamic-programming algorithm, as proposed in [SAC+79]. While this algorithm produces good optimization results (i.e., good plans), its high complexity can be prohibitive if complex queries need to be processed, new query execution techniques need to be integrated, or in certain programming environments (e.g., distributed database systems). In this paper, we present and thoroughly evaluate a new class of query optimization algorithms that are based on a principle that we call iterative dynamic programming, or IDP for short. IDP has several important advantages: First, IDP-algorithms produce the best plans of all known algorithms in situations in which dynamic programming is not viable because of its high complexity. Second, some IDP variants are adaptive and produce as good plans as dynamic programming if dynamic programming is viable an...
97|Feng L, Query Routing in a Peer-to-Peer Semantic Link Network, in|A semantic link peer-to-peer (P2P) network specifies and manages semantic relationships between peers ’ data schemas and can be used as the semantic layer of a scalable Knowledge Grid. The proposed approach consists of an automatic semantic link discovery method, a tool for building and maintaining P2P semantic link networks (P2PSLNs), a semantic-based peer similarity measurement for efficient query routing, and the schema mapping algorithms for query reformulation and heterogeneous data integration. The proposed approach has three important aspects. First, it uses semantic links to enrich the relationships between peers ’ data schemas. Second, it considers not only nodes but also the XML structure in measuring the similarity between schemas to efficiently and accurately forward queries to relevant peers. Third, it copes with semantic and structural heterogeneity and data inconsistency so that peers can exchange and translate heterogeneous information within a uniform view.
98|Query Optimization With One Parameter|Query Optimization refers to the process of selecting an efficient plan to execute a given query from among the various ways available. Each of the alternatives available to the optimizer have an associated cost estimate. Usually, either all the components of a cost estimate are known at compile time or their values are assumed and taken as constant. Plans selected using such assumptions could prove to be sub-optimal if cost parameters change significantly between compile time and run time. &#034;Parametric Query Optimization&#034; attempts query optimization even where some of the factors involved in cost calculation are not known till the query is executed nor can they be accurately estimated.
99|Design and Analysis of Parametric Query Optimization Algorithms|Query optimizers normally compile queries into one optimal plan by assuming complete knowledge of all cost parameters such as selec-tivity and resource availability. The execution of such plans could be sub-optimal when cost parameters are either unknown at compile time or change significantly between compile time and runtime [Loh89, GrW89]. Paramet-ric query optimization [INS+92, CG94, GK94] optimizes a query into a number of candidate plans, each optimal for some region of the pa-rameter space. In this paper, we present para-metric query optimization algorithms. Our approach is based on the property that for linear cost functions, each parametric optimal plan is optimal in a convex polyhedral region of the parameter space. This property is used to optimize linear and non-linear cost func-tions. We also analyze the expected sizes of the parametric optimal set of plans and the number of plans produced by the Cole and Graefe algorithm [CG94]. 1
100|Pipelining in multi-query optimization|Database systems frequently have to execute a set of related queries, which share several common subexpressions. Multi-query optimization exploits this, by finding evaluation plans that share common results. Current approaches to multi-query optimization assume that common subexpressions are materialized. Significant performance benefits can be had if common subexpressions are pipelined to their uses, without being materialized. However, plans with pipelining may not always be realizable with limited buffer space, as we show. We present a general model for schedules with pipelining, and present a necessary and sufficient condition for determining validity of a schedule under our model. We show that finding a valid schedule with minimum cost is NP-hard. We present a greedy heuristic for finding good schedules. Finally, we present a performance study that shows the benefit of our algorithms on batches of queries from the TPCD benchmark. 1.
101|Efficient and Extensible Algorithms for Multi Query Optimization|Complex queries are becoming commonplace, with the growing use of decision support systems.  These complex queries often have a lot of common sub-expressions, either within a single query, or  across multiple such queries run as a batch. Multi-query optimization aims at exploiting common subexpressions  to reduce evaluation cost. Multi-query optimization has hither-to been viewed as impractical,  since earlier algorithms were exhaustive, and explore a doubly exponential search space.  In this paper we demonstrate that multi-query optimization using heuristics is practical, and provides  significant benefits. We propose three cost-based heuristic algorithms: Volcano-SH and Volcano-RU,  which are based on simple modifications to the Volcano search strategy, and a greedy heuristic. Our  greedy heuristic incorporates novel optimizations that improve efficiency greatly. Our algorithms are  designed to be easily added to existing optimizers. We present a performance study comparing the  algo...
102|Simultaneous Optimization and Evaluation of Multiple Dimensional Queries|Database researchers have made significant progress on several research issues related to multidimensional data analysis, including the development of fast cubing algorithms, efficient schemes for creating and maintaining precomputed group-bys, and the design of efficient storage structures for multidimensional data. However, to date there has been little or no work on multidimensional query optimization. Recently, Microsoft has proposed &#034;OLE DB for OLAP&#034; as a standard multidimensional interface for databases. OLE DB for OLAP defines Multi-Dimensional Expressions (MDX), which have the interesting and challenging feature of allowing clients to ask several related dimensional queries in a single MDX expression. In this paper, we present three algorithms to optimize multiple related dimensional queries. Two of the algorithms focus on how to generate a global plan from several related local plans. The third algorithm focuses on generating a good global plan without first generating local p...
103|The implementation of a heuristic algorithm for the multiple-query optimization problem|Multiple-query processing has received a lot of attention recently. The problem arises in many areas, such as extended relational database systems and deductive database systems. In this paper we describe a heuristic search algorithm for this problem. This algorithm uses an improved heuristic function that enables it to expand only a small fraction of the nodes expanded by an algorithm that has been proposed in the past. In addition, it handles implied relationships without increasing the size of the search space or the number of nodes generated in this space. We include both theoretical analysis and experimental results to demonstrate the utility of the algorithm.
104|Scheduling Problems in Parallel Query Optimization|We introduce a class of novel multiprocessor scheduling problems that arise in the optimization of SQL queries for parallel machines. These consist of scheduling a tree of interdependent communicating operators while exploiting both inter-operator and intra-operator parallelism. We develop algorithms for the specific problem of scheduling a Pipelined Operator Tree in which all operators run in parallel using inter-operator parallelism. Weights associated with nodes and edges represent respectively the cost of operators and communication. Communication cost is incurred only if adjacent operators are assigned different processors. The optimization problem is to assign operators to processors so as to minimize the maximum processor load. We develop two approximation algorithms for this NP-hard problem. The faster algorithm has a performance ratio of 3.56 while the slower algorithm has a ratio of 2.87.  1 Introduction  Exploiting parallel execution [DG92, Val93] to speed up database querie...
105|On Tuning and Optimization for Multiple Queries in Databases|On Tuning and Optimization for Multiple Queries in Databases  by  Kevin O&#039;Gorman  Multiple concurrent queries occur in many database settings. This dissertation explores two such settings, one in which the system being examined generates the queries and one in which the system processes multiple incoming queries. In the former case, we originally set out to explore the performance of a particular algorithm for incremental maintenance of a materialized view in a data warehouse. In the process, we discovered that at a realistic database size, unexpected issues with the query and update processing had to be understood and dealt with before the results would be meaningful. The development of this understanding is as much to the point as the particular experimental results because they illuminate the signi  cance of the experimental environment for research. The experiment proper validates that incremental maintenance is feasible over a wide range of update sizes (granularities), and that in all cases a cursor-based version of the algorithm performs the best.
106|On Multi-Query Optimization|In some key database applications, such as data mining, a sequence of interdependent queries may be posed  simultaneously to the DBMS. The optimization of such sequences is called multi-query optimization,  and it attempts to exploit these dependencies in the derivation of a query evaluation plan (qep). Although  it has been observed and demonstrated by several researchers that exploitation of dependencies speed up the  query processing, limited research has been reported how to benefit from multi-query optimization, taking the  capabilities of existing query optimizers into account. This is exactly the topic of this paper. Since existing  optimizers are able to optimize queries in which a restricted number of basic operations appears, e.g., number  of joins is limited to 10, and the optimization of a query is relatively expensive, we attempt to profit from multi  query optimization under the condition that queries are passed only once and separately to the optimizer. We  propose a two...
107|Physical database design for relational databases|This paper describes the concepts used in the implementation of DBDSGN, an experimental physical design tool for relational databases developed at the IBM San Jose Research Laboratory. Given a workload for System R (consisting of a set of SQL statements and their execution frequencies), DBDSGN suggests physical configurations for efficient performance. Each configuration consists of a set of indices and an ordering for each table. Workload statements are evaluated only for atomic configurations of indices, which have only one index per table. Costs for any configuration can be obtained from those of the atomic configurations. DBDSGN uses information supplied by the System R optimizer both to determine which columns might be worth indexing and to obtain estimates of the cost of executing statements in different configurations. The tool finds efficient solutions to the index-selection problem; if we assume the cost estimates supplied by the optimizer are the actual execution costs, it finds the optimal solution. Optionally, heuristics can be used to reduce execution time. The approach taken by DBDSGN in solving the index-selection problem for multiple-table statements significantly reduces the complexity of the problem. DBDSGN’s principles were used in the Relational Design Tool (RDT), an IBM product based on DBDSGN, which performs design for SQL/DS, a relational system based on System R. System R actually uses DBDSGN’s suggested solutions as the tool expects because cost estimates and other necessary information can be obtained from System R using a new SQL statement, the EXPLAIN statement. This illustrates how a system can export a model of its internal assumptions and behavior so that other systems (such as tools) can share this model.
108|Automating Physical Database Design: An Extensible Approach|In a high-level query language such as SQL, queries yield the same result no matter how the logical schema is physically implemented. Nevertheless, a query&#039;s cost can vary by orders of magnitude among di erentphysical implementations of the same logical schema, even with the most modern query optimizers. Therefore, design-ing a low-cost physical implementation is an important pragmatic problem|one that requires a sophisticated understanding of physical design options and query strategies, and that involves estimating query costs, a tedious and error-prone process when done manually. We have devised a simple framework for automating physical design in rela-tional or post-relational DBMSs and in database programming languages. Within this framework, design options are uniformly represented as \features&amp;quot;, and de-signs are represented by \con ict&amp;quot;-free sets of features. (Mutually exclusive fea-tures con ict. An example would be two primary indexes on the same table.) The uniform representation of design options as features accommodates a greater vari-
109|Semantic Query Optimization for Methods . . .|Although the main difference between the relational and the object-oriented data model is the possibility to define object behavior, query optimization techniques in objectoriented database systems are mainly based on the structural part of objects. We claim that the optimization potential emerging from methods has been strongly underestimated so far. In this paper we concentrate on the question of how semantic knowledge about methods can be considered in query optimization. We rely on the algebraic and rulebased approach for query optimization and present a framework that allows to integrate schema-specific knowledge by tailoring the query optimizer according to the particular application&#039;s needs. We sketch an implementation of our concepts within the OODBMS VODAK using the Volcano optimizer generator.
110|Queries and query processing in object-oriented database systems|One of the basic functionalities of database management systems (DBMSs) is to be able to process declarative user queries. The first generation of object-oriented DBMSs did not provide declarative query capabilities. However, the last decade has seen significant research in defining query models (including calculi, algebra and user languages) and in techniques for processing and optimizing them. Many of the current commercial systems provide at least rudimentary query capabilities. In this chapter we discuss the techniques that have been developed for processing object-oriented queries. Our particular emphasis is on extensible query processing architectures and techniques. The other chapters in this book on query languages and optimization techniques complement this chapter.  
111|Nested Queries in Object Bases|Many declarative query languages for object-oriented databases allow nested subqueries. This paper  contains the first (to our knowledge) proposal to optimize them. A two-phase approach is used  to optimize nested queries in the object-oriented context. The first phase---called dependency-based  optimization---transforms queries at the query language level in order to treat common subexpressions  and independent subqueries more efficiently. The transformed queries are translated to nested  algebraic expressions. These entail nested loop evaluation which may be very inefficient. Hence, the  second phase unnests nested algebraic expressions to allow for more efficient evaluation.  1 Introduction  Many declarative query languages for object-oriented database management systems have been proposed in the last few years (e.g. [3, 5, 2, 18, 14]). To express complex conditions, access nested structure, or produce nested results, an essential feature found in these languages is the nesting of q...
113|Object-Oriented Queries: Equivalence And Optimization|data types can also define equality operations. Such operations are typespecific and will not be considered in this paper. Objects can be represented graphically, where nodes are objects or atomic values, arcs connect collection type objects to all objects that are members of the collection, and arcs connect non-collection type objects to the values of all properties of that object (similarly to [11] for example). Nodes representing atomic values are leaves and are labelled with the atomic value. Nodes representing non-atomic objects are labelled with an (artificial) object identifier as well as the object type. Types can be labelled symbolically, using ? for set types, !? for tuple types, and ffl for other abstract data types. In Figure 1 three set objects are represented graphically. Note that graphs b and c represent 3-equal set objects: C 2 and C 1 are 1-equal, making t 3 and t 5 2-equal (t 1  and t 4 are 1-equal, therefore 2-equal), thus S 2 and S 3 are 3-equal. Object S1 (graph a...
114|The Prospects of Publishing Using Advanced Database Concepts|this article, these classes are referred to as element-type classes.The
115|Deterministic Semantics of Set-Oriented Update Sequences|An iterator is proposed that allows to apply sequences of update operations in a set-oriented way with deterministic semantics. Because the mechanism is independent of a particular model, it can be used in the relational and in object-oriented ones. Thus, the deterministic semantics of embedded SQL cursors, and of triggers that are applied after (set-oriented) SQL updates can be checked. Furthermore, the iterator can be used to apply object-oriented methods, which are usually update sequences defined on a single object, also to sets in a deterministic way. It turns out that the criteria that guarantee determinism are also used in semantic or multi-level concurrency control. 1 Introduction  The paper deals with the general problem of defining update languages that are comparable in expressive power to typical query languages. When designing such an update language, one should pursue the following objectives:  ffl genericity: update operations should be applicable to all types of objects...
116|Object-Oriented Query Processing: The Impact of Methods on Language, Architecture and Optimization|Although nearly all object-oriented data models proposed so far include behavioral aspects, most object-oriented query languages, algebras and query optimization strategies simply adapt relational concepts since they focus on the complex structures of objects and neglect the behavior. We claim that this approach is not sufficient since it does not reflect the much richer semantics methods can carry which have to be taken into account for really efficient query processing. The quite straightforward approach we consider is to integrate methods in an algebraic framework for query processing and to make there partial knowledge about methods available in the form of equivalences. We examine two important questions which emerge from taking this approach. First, how is it possible to integrate algebraic set operators with methods defined in database schemas within an object-oriented data model? Second, what is the impact on the architecture of the query processor when the algebra becomes an ex...
117|Feedback-Directed Query Optimization|Current database systems employ static heuristics for estimating the access time of a particular query. These heuristics are based on several parameters, such as relation size and number of tuples. Yet these parameters are only updated intermittently, and the heuristics themselves are hand-tuned. As trends in database systems aim toward self-tuning systems, we can apply the experience of the feedback-directed compiler world to provide robust, self-tuning query optimizers. This paper presents the design and evaluation of a feedback-directed query optimization infrastructure. Using trace-driven simulation, we conclude that dynamic feedback can be quite e#ective at improving the accuracy of a query optimizer, and adapting to predictable query overhead.
118|Eddies: Continuously Adaptive Query Processing|In large federated and shared-nothing databases, resources can exhibit widely fluctuating characteristics. Assumptions made at the time a query is submitted will rarely hold throughout the duration of query processing. As a result, traditional static query optimization and execution techniques are ineffective in these environments.  In this paper we introduce a query processing mechanism called an eddy, which continuously reorders operators in a query plan as it runs. We characterize the moments of symmetry  during which pipelined joins can be easily reordered, and the synchronization barriers that require inputs from different sources to be coordinated. By combining eddies with appropriate join algorithms, we merge the optimization and execution phases of query processing, allowing each tuple to have a flexible ordering of the query operators. This flexibility is controlled by a combination of fluid dynamics and a simple learning algorithm. Our initial implementation demonstrates prom...
119|Parallel Query Processing|With relations growing larger and queries becoming more complex, parallel query processing is an increasingly attractive option for improving the performance of database systems. The objective of this paper is to examine the various issues encountered in parallel query processing and the techniques available for addressing these issues. The focus of the paper is on the join operation with both sort-merge join and hash joins being considered. Three types of parallelism can be exploited, namely intra-operator, inter-operator, and inter-query parallelism. In intra-operator parallelism the major issue is task creation, and the objective is to split a join operation into tasks in a manner such that the load can be spread evenly across a given number of processors. This is a challenge when the values on the join attribute are not uniformly distributed. Inter-operator parallelism can be achieved either through parallel execution of independent operations or through pipelining. In either case,...
120|Cost-based Query Scrambling for Initial Delays|Remote data access from disparate sources across a wide-area network such as the Internet is problematic due to the unpredictable nature of the communications medium and the lack of knowledge about the load and potential delays at remote sites. Traditional, static, query processing approaches break down in this environment because they are unable to adapt in response to unexpected delays. Query scrambling has been proposed to address this problem. Scrambling modifies query execution plans on-the-fly when delays are encountered during runtime. In its original formulation, scrambling was based on simple heuristics, which although providing good performance in many cases, were also shown to be susceptible to problems resulting from bad scrambling decisions. In this paper we address these shortcomings by investigating ways to exploit query optimization technology to aid in making intelligent scrambling choices. We propose three different approaches to using query optimization for scramblin...
121|A Piggyback Method to Collect Statistics for Query Optimization in Database Management Systems|A database management system (DBMS) usually performs query optimization based on statistical information about data in the underlying database. Out-of-date statistics may lead to inefficient query processing in the system. Existing solutions to this problem have some drawbacks such as heavy administrative burden, high system load, and tardy updates. To overcome these drawbacks, our new approach, called the piggyback method, is proposed in this paper. The key idea is to piggyback some additional retrievals during the processing of a user query in order to collect more up-to-date statistics. The collected statistics are used to optimize the processing of subsequent queries. To specify the piggybacked queries, basic piggybacking operators are defined in this paper. Using the operators, several types of piggybacking such as vertical, horizontal, mixed vertical and horizontal, and multi-query piggybacking are introduced. Statistics that can be obtained from different access methods...
122|Multiple-Granularity Interleaving for Piggyback Query Processing|Piggyback query processing is a new technique, described in [24], intended to perform additional useful computation (e.g., database statistics collection) during normal query processing, taking full advantage of data resident in main memory. Different types of beneficial piggybacking have been identified and studied, but how to efficiently integrate piggyback operations with a given user query is still an open issue. In this paper, we propose a technique of multiplegranularity interleaving to efficiently integrate multiple piggyback operations with a given query at different levels of data granularity. We introduce an algebraic notation to capture the main characteristics of data ows in a database management system (DBMS), facilitating the study of piggybacking and enabling the automated integration of piggyback operations and user queries in a DBMS supporting the piggyback method. Various integration techniques are introduced to facilitate multiple-granularity interleaving including merging sh...
123|Incremental Maintenance for Materialized Views over Semistructured Data|Semistructured data is not strictly typed like relational or object-oriented data and may be irregular or incomplete. It often arises in practice, e.g., when heterogeneous data sources are integrated or data is taken from the World Wide Web. Views over semistructured data can be used to filter the data and to restructure (or provide structure to) it. To achieve fast query response time, these views are often materialized. This paper studies incremental maintenance techniques for materialized views over semistructured data. We use the graph-based data model OEM and the query language Lorel, developed at Stanford, as the framework for our work. We propose a new algorithm that produces a set of queries that compute the changes to the view based upon a change to the source. We develop an analytic cost model and compare the cost of executing our incremental maintenance algorithm to that of recomputing the view. We show that for nearly all types of database updates, it is more efficient to a...
124|ARQ Static Query Optimizer|semantic web, SPARQL, query optimization In this paper we describe the architecture of ARQo, a first approach for SPARQL static query optimization in ARQ. Specifically, we focus on static optimization of BasicGraphPattern (BGP) for in-memory models. Static query optimization is intended as a query rewriting process where the set of triple patterns defined for a BGP are rewritten according to a specific order. We propose a rewriting process according to the estimated execution cost of joined triple patterns in increasing order. Specifically, the estimated execution cost is a function of multiple parameters such as the estimated selectivity of joined triple patterns, the availability of indexes or pre-calculated result sets.
125|LUBM: A benchmark for OWL knowledge base systems|We describe our method for benchmarking Semantic Web knowledge base systems with respect to use in large OWL applications. We present the Lehigh University Benchmark (LUBM) as an example of how to design such benchmarks. The LUBM features an ontology for the university domain, synthetic OWL data scalable to an arbitrary size, fourteen extensional queries representing a variety of properties, and several performance metrics. The LUBM can be used to evaluate systems with different reasoning capabilities and storage mechanisms. We demonstrate this with an evaluation of two memorybased systems and two systems with persistent storage.
126|Optimized Index Structures for Querying RDF from the Web|Storing and querying Resource Description Framework (RDF) data is one of the basic tasks within any Semantic Web application. A number of storage systems provide assistance for this task. However, current RDF database systems do not use optimized indexes, which results in a poor performance behavior for querying RDF. In this paper we describe optimized index structures for RDF, show how to process and evaluate queries based on the index structure, describe a lightweight adaptable implementation in Java, and provide a performance comparison with existing RDF databases.
127|OptARQ: A SPARQL Optimization Approach based on Triple Pattern Selectivity Estimation|Abstract Query engines for ontological data based on graph models mostly execute user queries without considering any optimization. Especially for large ontologies, optimization techniques are required to ensure that query results are delivered within reasonable time. OptARQ is a first prototype for SPARQL query optimization based on the concept of triple pattern selectivity estimation. The evaluation we conduct demonstrates how triple pattern reordering according to their selectivity affects the query execution performance.
128|Intensional Query Optimization|We have introduced a new query optimization framework called intensional query optimization  (IQO), which enables existing optimization techniques to be applied to queries that use views. In particular, we consider that view definitions may employ unions. Advanced database technologies and applications---such as federation and mediation over heterogeneous database sources---lead to such complex view definitions, and to the need to handle complex, expensive queries. Query rewriting techniques have been proposed which exploit semantic query caches, materialized views, and semantic knowledge about the database domain to optimize query evaluation. These can augment syntactic optimization to reduce evaluation costs further. Such techniques include semantic query caching, query folding, and semantic query optimization. However, most proposed rewrite techniques ignore views in queries; that is, the views are considered as other tables. The IQO framework enables rewrites to be applied to vario...
129|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
130|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
131|Semantic Data Caching and Replacement|We propose a semantic model for client-side caching and replacement in a client-server database system and compare this approach to page caching and tuple caching strategies. Our caching model is based on, and derives its advantages from, three key ideas. First, the client maintains a semantic description of the data in its cache,which allows for a compact specification, as a remainder query, of the tuples needed to answer a query that are not available in the cache. Second, usage information for replacement policies is maintained in an adaptive fashion for semantic regions, which are associated with collections of tuples. This avoids the high overheads of tuple caching and, unlike page caching, is insensitive to bad clustering. Third, maintaining a semantic description of cached data enables the use of sophisticated value functions that incorporate semantic notions of locality, not just LRU or MRU, for cache replacement. We validate these ideas with a detailed performance study that i...
132|A predicate-based caching scheme for client-server database architectures|Abstract. We propose a new client-side data-caching scheme for relational databases with a central server and multiple clients. Data are loaded into each client cache based on queries executed on the central database at the server. These queries are used to form predicates that describe the cache contents. A subsequent query at the client may be satisfied in its local cache if we can determine that the query result is entirely contained in the cache. This issue is called cache completeness. A separate issue, cache currency, deals with the effect on client caches of updates committed at the central database. We examine the various performance tradeoffs and optimization issues involved in addressing the questions of cache currency and completeness using predicate descriptions and suggest solutions that promote good dynamic behavior. Lower query-response times, reduced message traffic, higher server throughput, and better scalability are some of the expected benefits of our approach over commonly used relational server-side and object ID-based or page-based client-side caching.
133|Query Folding|Query folding refers to the activity of determining if and how a query can be answered using a given set of resources, which might be materialized views, cached results of previous queries, or queries answerable by another database. We investigate query folding in the context where queries and resources are conjunctive queries. We develop an exponential-time algorithm that finds all foldings, and a polynomial-time algorithm for the subclass of acyclic queries. Our results can be applied to query optimization in centralized databases, to query processing in distributed databases, and to query answering in federated databases. 1 Introduction  Query folding refers to the activity of determining if and how a query can be answered using a given set of resources. These resources might be materialized views, cached results of previous queries, or even queries answerable by another database. Query folding is important because the base relations referred to in a query might be stored remotely a...
134|An Overview of Cooperative Answering|Databases and information systems are often hard to use because they do not explicitly attempt to cooperate with their users. Direct answers to database and knowledge base queries may not always be the best answers. Instead, an answer with extra or alternative information may be more useful and less misleading to a user. This paper surveys foundational work that has been done toward endowing intelligent information systems with the ability to exhibit cooperative behavior. Grice&#039;s maxims of cooperative conversation, which provided a starting point for the field of cooperative answering, are presented along with relevant work in natural language dialogue systems, database query answering systems, and logic programming and deductive databases. The paper gives a detailed account of cooperative techniques that have been developed for considering users&#039; beliefs and expecations, presuppositions, and misconceptions. Also, work in intensional answering and generalizing queries and answers is co...
135|The implementation and performance evaluation of the ADMS query optimizer: Integrating query result caching and matching| In this paper, we describe the design and implementation of the ADMS query optimizer. This optimizer integrates query matching into optimization and generates more e cient query plans using cached results. It features data caching and pointer caching, alternative cache replacement strategies, and di erent cache update methods. A comprehensive set of experiments were conducted using a benchmark database and synthetic queries. The results showed that pointer caching and dynamic cache update strategies substantially saved query execution time and, thus, increased query throughput under situations with fair query correlation and update load. The requirement of the disk cache space is relatively small, and the extra optimization overhead introduced is more than o set by the time saved in query evaluation.  
136|Semantic Query Optimization in Datalog Programs (Extended Abstract)  (1995) |) Alon Y. Levy  AT&amp;T Bell Laboratories levy@research.att.com  Yehoshua Sagiv   Hebrew University, Jerusalem sagiv@cs.huji.ac.il Abstract  Semantic query optimization refers to the process of using integrity constraints (ic&#039;s) in order to optimize the evaluation of queries. The process is well understood in the case of unions of select-project-join queries (i.e., nonrecursive datalog). For arbitrary datalog programs, however, the issue has largely remained an unsolved problem. This paper studies this problem and shows when semantic query optimization can be completely done in recursive rules provided that order constraints and negated EDB subgoals appear only in the recursive rules, but not in the ic&#039;s. If either order constraints or negated EDB subgoals are introduced in ic&#039;s, then the problem of semantic query optimization becomes undecidable. Since semantic query optimization is closely related to the containment problem of a datalog program in a union of conjunctive queries, our res...
137|An Architecture for a Cooperative Database System|. Database systems can be difficult to use. Part of the problem is that systems do not, for the most part, help a user when a query fails or fails to evaluate as the user expects. Schema and semantics of databases are often complex, and are rarely understood in entirety by the lay user. As a consequence, queries a lay user casts may not make sense with respect to a database&#039;s semantics. A system which returns informative responses beyond a query&#039;s answer set itself can elucidate the schema and semantics of the database, which can greatly help the user to cast the queries intended. Such a database system is to be called a cooperative database system (CDBS). A number of cooperative behaviors and methods have been introduced to be incorporated into information systems to make them more informative and, hence, easier to use. We have identified a fundamental set of such cooperative techniques that we believe relational and deductive systems should be extended to include. We present an archi...
138|A Cooperative Answering System|Introduction  We seek to improve human-computer communication by enabling a computer to respond to questions in a manner that accounts for a user&#039;s misconceptions, expectations, desires, and interests. We have developed a prototype system that gives cooperative answers, corrects misconceptions, and attempts to meet users&#039; needs. The system, described in Fig. 1, uses semantic information about the database (DB) to formulate coherent and informative answers. The work has two main thrusts: 1) the construction of a logic formula which embodies the content of a cooperative answer; and 2) the presentation of the logic formula to the user in a natural language form. Information available in a deductive database (DDB) system for building cooperative answers includes integrity constraints, user constraints, the search tree for answers to the query, and false presuppositions present in the query. The work of Gal and Minker [3] forms the basis of the&lt;F9.216
139|Semantic Query Optimization for Bottom-Up Evaluation|Semantic query optimization uses semantic knowledge in databases (represented in the form of integrity constraints) to rewrite queries and logic programs for the purpose of more efficient query evaluation. Much work has been done to develop various techniques for optimization. Most of it, however, is only applicable to top-down query evaluation strategies. Moreover, little attention has been paid to the cost of the optimization itself. In this paper, we address the issue of semantic query optimization for bottom-up query evaluation strategies with an emphasis on overall efficiency. We restrict our attention to a single optimization technique, join elimination. We discuss various factors that influence the cost of semantic optimization, and present two abstract algorithms for different optimization approaches. The first one pre-processes a query statically before it is evaluated; the second approach combines query evaluation with semantic optimization using heuristics to achieve the lar...
140|Multidatabase Query Optimization|. A multidatabase system (MDBS) allows the users to simultaneously access heterogeneous, and autonomous databases using an integrated schema and a single global query language. The query optimization problem in MDBSs is quite different from the query optimization problem in distributed homogeneous databases due to schema heterogeneity and autonomy of local database systems. In this work, we consider the optimization of query distribution in case of data replication and the optimization of intersite joins, that is, the join of the results returned by the local sites in response to the global subqueries. The algorithms presented for the optimization of intersite joins try to maximize the parallelism in execution and take the federated nature of the problem into account. It has also been shown through a comparative performance study that the proposed intersite join optimization algorithms are efficient. The approach presented can easily be generalized to any operation required for intersi...
141|Parallel Evaluation of Multi-Join Queries|A number of execution strategies for parallel evaluation of multi-join queries have been proposed in the literature; their performance was evaluated by simulation. In this paper we give a comparative performance evaluation of four execution strategies by implementing all of them on the same parallel database system, PRISMA/DB. Experiments have been done up to 80 processors. The basic strategy is to first determine an execution schedule with minimum total cost and then parallelize this schedule with one of the four execution strategies. These strategies, coming from the literature, are named: Sequential Parallel, Synchronous Execution, Segmented Right-Deep, and Full Parallel. Based on the experiments clear guidelines are given when to use which strategy. 1 
142|Dynamic Query Optimization on a Distributed Object Management Platform|A Distributed Object Management (DOM) architecture, when used as the infrastructure of a multidatabase system, not only enables easy and flexible interoperation of DBMSs, but also facilitates interoperation of the multidatabase system with other repositories that do not have DBMS capabilities. This is an important advantage, since most of data still resides on repositories that do not have DBMS capabilities. In this paper, we describe a dynamic query optimization technique for a multidatabase system, namely MIND, implemented on a DOM environment. Dynamic query optimization, which schedules intersite operations at runtime, fits better to such an environment since it benefits from location transparency provided by the DOM framework. In this way, the dynamic changes in the configuration of system resources such as a relocated DBMS or a new mirror to an existing DBMS, do not affect the optimized query execution in the system. Furthermore, the uncertainty in estimating the appearance times ...
143|Building Interoperable Databases on Distributed Object Management Platforms|A common characteristic of today&#039;s information systems is the distribution of data among a number of autonomous and heterogeneous repositories. Increasingly, these repositories are database management systems (DBMS), but there is still a very large volume of data that is stored in le systems, spreadsheets and others. A fundamental challenge in building next generation information systems is to provide interoperability among these autonomous and potentially heterogeneous repositories. The commercial state-of-the-art in addressing the interoperability of DBMSs is to build gateways. This approach is quite restricted and provides only a partial solution. None of these systems properly deal with semantic or structural heterogeneity of the stored data. An alternative approach toachieving interoperability among DBMSs is the multidatabase approach[11]. Amultidatabase system resides unabtrusively on top of existing database systems and presents a single database illusion to its users. In particular, a multidatabase system maintains a single global database schema against which its users issue queries and updates. It is suggested that this is the approach that should be preferred over gateways [5]. One restriction of multidatabase systems has been that they cannot handle repositories which donothave DBMS capabilities. The introduction of object-oriented technology into data management [3] has lifted this restriction. Object-orientation, with its encapsulation and abstraction capabilities, enable the development of wrappers which encapsulate a particular repository and provide
144|Query Decomposition And Processing In Multidatabase Systems|A multidatabase system allows its users to simultaneously access heterogeneous, and autonomous databases using an integrated schema and a single global query language. An important problem in multidatabase systems is processing of the global queries. In this paper, we describe a global query processing scheme as it is implemented in a multidatabase environment, namely MIND. Since multidatabase query processing is very much dependent on the way schema integration is realized, the underlying MIND schema integration is also described. The details of both query decomposition process and the necessary post-processing to combine the partial results coming from local databases, are provided. 1 INTRODUCTION  Various types of database systems are currently in use today. These information resources could be immediately made available for many users through existing computer systems. However, since these systems often use different data models and different query languages, users of one system ca...
145|A heuristic approach for optimization of path expressions|Abstract. The object-oriented database management systems store references to objects (implicit joins, precomputed joins), and use path expressions in query languages. One way of executing path expressions is pointer chasing of precomputed joins. However it has been previously shown that converting implicit joins to explicit joins during the optimization phase may yield better execution plans. A path expression is a linear query, therefore, considering all possible join sequences within a path expression is polynomial in the number of classes involved. Yet, when the implicit joins are converted to explicit joins in a query involving multiple path expressions bound to the same bind variable, the query becomes a star query and thus considering all possible joins is exponential in the numberofpathsinvolved. This implies that there is a need for improvement by using heuristic in optimizing queries involving multiple path expressions. A heuristic based approach for optimizing queries involving multiple path expressions is described in this paper. First, given the cost and the selectivities of path expressions by considering a path expression as a unit of processing, we provide an algorithm that gives the optimum execution order of multiple path expressions bound to the same bind variable. For this purpose, we derive the formulas for the selectivities of path expressions. Then by using this ordering as a basis we provide a general heuristic approach for optimizing queries involving multiple path expressions. Two optimizers are developed to compare the performance of the heuristic based approach suggested in this paper with the performance of an optimizer based on an exhaustive search strategy. The exhaustive optimizer is generated through Volcano Optimizer Generator (VOG). The results of the experiments indicate that the heuristic based optimizer has a superior performance with the increasing number of path expressions. 1
146|A Cost Model for Path Expressions in Object-Oriented Queries|Query processing remains one of the important challenges of Object-Oriented Database Management Systems. Cost based query optimization involves creating alternative executing plans for a given query and executing the least costly one within a cost model framework. In Object-Oriented Database Management Systems (OODBMSs) objects may store references to other objects (precomputed joins), and path expressions are used in query languages. Although the cost fomulas for explicit joins and the selectivities of attributes and joins are well-known in the relational model, there is no similar work involving path expressions for OODBMSs. However in order to optimize object-oriented queries involving path expressions, a cost model is essential. This information is necessary for deciding whether to use pointer chasing or to convert the path expressions into explicit joins and also for deciding the execution order of path expressions. In this paper, we provide a cost model that includes the formulas...
147|Implementing data cubes efficiently|Decision support applications involve complex queries on very large databases. Since response times should be small, query optimization is critical. Users typically view the data as multidimensional data cubes. Each cell of the data cube is a view consisting of an aggregation of interest, like total sales. The values of many of these cells are dependent on the values of other cells in the data cube..A common and powerful query optimization technique is to materialize some or all of these cells rather than compute them from raw data each time. Commercial systems differ mainly in their approach to materializing the data cube. In this paper, we investigate the issue of which cells (views) to materialize when it is too expensive to materialize all views. A lattice framework is used to express dependencies among views. We present greedy algorithms that work off this lattice and determine a good set of views to materialize. The greedy algorithm performs within a small constant factor of optimal under a variety of models. We then consider the most common case of the hypercube lattice and examine the choice of materialized views for hypercubes in detail, giving some good tradeoffs between the space used and the average time to answer a query. 1
148|Selection of Views to Materialize in a Data Warehouse|. A data warehouse stores materialized views of data from one or more sources, with the purpose of efficiently implementing decisionsupport or OLAP queries. One of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse. The goal is to select an appropriate set of views that minimizes total query response time and the cost of maintaining the selected views, given a limited amount of resource, e.g., materialization time, storage space etc. In this article, we develop a theoretical framework for the general problem of selection of views in a data warehouse. We present competitive polynomial-time heuristics for selection of views to optimize total query response time, for some important special cases of the general data warehouse scenario, viz.: (i) an AND view graph, where each query/view has a unique evaluation, and (ii) an OR view graph, in which any view can be computed from any one of its related views, e.g.,...
149|An efficient cost-driven index selection tool for Microsoft SQL Server|In this paper we describe novel techniques that make it possible to build an industrial-strength tool for automating the choice of indexes in the physical design of a SQL database. The tool takes as input a workload of SQL queries, and suggests a set of suitable indexes. We ensure that the indexes chosen are effective in reducing the cost of the workload by keeping the index selection tool and the query optimizer &amp;quot;in step&amp;quot;. The number of index sets that must be evaluated to find the optimal configuration is very large. We reduce the complexity of this problem using three techniques. First, we remove a large number of spurious indexes from consideration by taking into account both query syntax and cost information. Second, we introduce optimizations that make it possible to cheaply evaluate the “goodness ” of an index set. Third, we describe an iterative approach to handle the complexity arising from multicolumn indexes. The tool has been implemented on Microsoft SQL Server 7.0. We performed extensive experiments over a range of workloads, including TPC-D. The results indicate that the tool is efficient and its choices are close to optimal. 1.
150|Algorithms for materialized view design in data warehousing environment|Selecting views to materialize is one of the most important decisions in designing a data warehouse. In this paper, we present a frame-work for analyzing the issues in selecting views to materialize so as to achieve the best com-bination of good query performance and low view maintenance. We first develop a heuristic algorithm which can provide a feasible solu-tion based on individual optimal query plans. We also map the materialized view design problem as O-l integer programming problem, whose solution can guarantee an optimal so-lution. 1
151|Materialized view maintenance and integrity constraint checking: Trading space for time|Abstract We investigate the problem of incremental maintenance of an SQL view in the face of database updates, and show that it is possible to reduce the total time cost of view maintenance by materializing (and maintaining) additional views. We formulate the problem of determining the optimal set of additional views to materialize as an optimization problem over the space of possible view sets (which includes the empty set). The optimization problem is harder than query optimization since it has to deal with multiple view sets, updates of multiple relations, and multiple ways of maintaining each view set for each updated relation. We develop a memoing solution for the problem; the solution can be implemented using the expression DAG representation used in rule-based optimizers such as Volcano. We demonstrate that global optimization cannot, in general, be achieved by locally optimizing each materialized subview, because common subexpressions between different materialized subviews can allow nonoptimal local plans to be combined into an optimal global plan. We identify conditions on materialized subviews in the expression DAG when local optimization is possible. Finally, we provide a systematic space of heuristics that can be used to efficiently determine a useful set of additional views to materialize. Our results are particularly important for the efficient checking of assertions (complex integrity constraints) in the SQL-92 standard, since the incremental checking of such integrity constraints is known to be essentially equivalent to the view maintenance problem.
152|Query optimization in the presence of limited access patterns|We consider the problem of query optimization in the presence of limitations on access patterns to the data (i.e., when one must provide values for one of the attributes of a relation in order to obtain tuples). We show that in the presence of limited access patterns we must search a space of annotated query plans, where the annotations describe the inputs that must be given to the plan. We describe a theoretical and experimental analysis of the resulting search space and a novel query optimization algorithm that is designed to perform well under the di erent conditions that may arise. The algorithm searches the set of annotated query plans, pruning invalid and non-viable plans as early as possible in the search space, and it also uses a best- rst search strategy in order to produce a rst complete plan early in the search. We describe experiments to illustrate the performance of our algorithm. 1
153|DynaMat: A Dynamic View Management System for Data Warehouses|Pre-computation and materialization of views with aggregate functions is a common technique in Data Warehouses. Due to the complex structure of the warehouse and the different profiles of the users who submit queries, there is need for tools that will automate the selection and management of the materialized data. In this paper we present DynaMat, a system that dynamically materializes information at multiple levels of granularity in order to match the demand (workload) but also takes into account the maintenance restrictions for the warehouse, such as down time to update the views and space availability. DynaMat unifies the view selection and the view maintenance problems under a single framework using a novel “goodness ” measure for the materialized views. DynaMat constantly monitors incoming queries and materializes the best set of views subject to the space constraints. During updates, DynaMat reconciles the current materialized view selection and refreshes the most beneficial subset of it within a given maintenance window. We compare DynaMat against a system that is given all queries in advance and the pre-computed optimal static view selection. The comparison is made based on a new metric, the Detailed Cost Savings Ratio introduced for quantifying the benefits of view materialization against incoming queries. These experiments show that DynaMat’s dynamic view selection outperforms the optimal static view selection and thus, any sub-optimal static algorithm that has appeared in the literature. 1
154|Physical Database Design for Data Warehouses|wilburt,quass,adelberg¢ e-mail: ¡ Data warehouses collect copies of information from remote sources into a single database. Since the remote data is cached at the warehouse, it appears as local relations to the users of the warehouse. To improve query response time, the warehouse administrator will often materialize views defined on the local relations to support common or complicated queries. Unfortunately, the requirement to keep the views consistent with the local relations creates additional overhead when the remote sources change. The warehouse is often kept only loosely consistent with the sources: it is periodically refreshed with changes sent from the source. When this happens, the warehouse is taken off-line until the local relations and materialized views can be updated. Clearly, the users would prefer as little down time as possible. Often the down time can be reduced by adding carefully selected materialized views or indexes to the physical schema. This paper studies how to select the sets of supporting views and of indexes to materialize to minimize the down time. We call this the view index selection (VIS) problem. We present an A * search based solution to the problem as well as rules of thumb. We also perform additional experiments to understand the space-time tradeoff as it applies to data warehouses. 1.
155|The Complexity of Transformation-Based Join Enumeration|Query optimizers that explore a search space exhaustively using transformation rules usually apply all possible rules on each alternative, and stop when no new information is produced. A memoizing structure was proposed in [McK93] to improve the re-use of common subexpression, thus improving the efficiency of the search considerably. However, a question that remained open is, what is the complexity of the transformation-based enumeration process ? In particular, with n the number of relations, does it achieve the O(3  n  ) lower bound established by [OL90]? In this paper we examine the problem of duplicates, in transformation-based enumeration. In general, different sequences of transformation rules may end up deriving the same element, and the optimizer must detect and discard these duplicate elements generated by multiple paths. We show that the usual commutativity/associativity rules for joins generate O(4  n  ) duplicate opera-  Permission to copy without fee all or part of this ma...
156|Reusing Invariants: A New Strategy for Correlated Queries|Correlated queries are very common and important in decision support systems. Traditional nested iteration evaluation methods for such queries can be very time consuming. When they apply, query rewriting techniques have been shown to be much more efficient. But query rewriting is not always possible. When query rewriting does not apply, can we do something better than the traditional nested iteration methods? In this paper, we propose a new invariant technique to evaluate correlated queries efficiently. The basic idea is to recognize the part of the subquery that is not related to the outer references and cache the result of that part after its first execution. Later, we can reuse the result and combine it with the result of the rest of the subquery that is changing for each iteration. Our technique applies to arbitrary correlated subqueries. This paper introduces algorithms to recognize the invariant part of a data flow tree, and to restructure the evaluation plan to reuse the stored ...
157|Query Optimization Strategies i |Abstract- The query optimization problem in large-scale distributed databases is NP nature and difficult to solve. The complexity of the optimizer increases as the number of relations and number of joins in a query increases. being carried out to find an appropriate algorithm to seek an optimal solution especially when the size of the database increases. Various Optimization Strategies have been reviewed in this paper and the studies show that the performance of distributed query optimization is improved when Ant Colony Optimization Algorithm is integrated with other optimization algorithms.
158|Pragmatic Approach to Query Optimization |One of the most critical functional requirements of a DBMS is its ability to process queries in a timely manner. This is particularly true for very large, applications such as weather forecasting, banking systems and aeronautical applications, which can contain millions and even trillions of records. The need for faster and faster, and immediate results never ceases. Thus, a great deal of research and resources is required on creating smarter, highly efficient query optimization techniques. Some of the basic techniques of query processing and optimization have been presented in this paper. This paper highlights the basic concepts of query processing and query optimization in the relational database domain. The results of the experiment presented have been verified using Query Analyzer.
159|XPath query containment |Consider an XML publish-subscribe scenario with hundreds of subscribers and tens of thousands of XML documents to be delivered per day. Subscribers specify the documents in which they are interested in
160|Buffering Database Operations for Enhanced Instruction Cache Performance|As more and more query processing work can be done in main memory, memory access is becoming a signicant cost component of database operations. Recent database re-search has shown that most of the memory stalls are due to second-level cache data misses and rst-level instruction cache misses. While a lot of research has focused on re-ducing the data cache misses, relatively little research has been done on improving the instruction cache performance of database systems. We rst answer the question \Why does a database system incur so many instruction cache misses? &#034; We demonstrate that current demand-pull pipelined query execution engines suer from signicant instruction cache thrashing between dierent operators. We propose techniques to buer database operations during query execution to avoid instruction cache thrashing. We implement a new light-weight \buer &#034; oper-ator and study various factors which may aect the cache performance. We also introduce a plan renement algorithm that considers the query plan and decides whether it is ben-e cial to add additional \buer &#034; operators and where to put them. The benet is mainly from better instruction locality and better hardware branch prediction. Our techniques can be easily integrated into current database systems without signi cant changes. Our experiments in a memory-resident PostgreSQL database system show that buering techniques can reduce the number of instruction cache misses by up to 80 % and improve query performance by up to 15%. 1.
161|An Optimal Algorithm for Querying Tree Structures and its Applications in Bioinformatics |Trees and graphs are widely used to model biological databases. Providing efficient algorithms to support tree-based or graph-based querying is therefore an important issue. In this paper, we propose an optimal algorithm which can answer the following question: “Where do the root-to-leaf paths of a rooted labeled tree Q occur in another rooted labeled tree T? ” in time O(m + Occ), where m is the size of Q and Occ is the output size. We also show the problem of querying a general graph is NP-complete and not approximable within n k for any k &lt; 1, where n is the number of nodes in the queried graph, unless P = NP. 1
162|Energy Efficient Query Optimization|Recently, a number of small yet powerful mobile computers have appeared on the market. Endowed with large disk capacity and network hardware, such systems are capable of holding large amounts of data, as well as of exchanging information with other hosts on a distributed system. These features suggest the need for the design and implementation of a distributed database system for mobile computers. Such mobile database systems present a host of technical challenges. One of them is dealing with the limited battery capacity of current mobile machines. We investigate the possibility of increasing the effective battery life of these computers by selecting energy-efficient query plans during query optimization. In this paper we review the capabilities of mobile computers and describe the power management issues in such systems. We examine the case of database systems on stand-alone mobile computers, and conclude that only a small modification is needed in standard query optimizers. However, ...
163|IP-based Protocols for Mobile Internetworking|We consider the problem of providing network access to hosts whose physical location changes with time. Such hosts cannot depend on traditional forms of network connectivity and routing because their location, and hence the route to reach them, cannot be deduced from their network address. In this paper, we explore the concept of providing continuous network access to mobile computers, and present a set of IP-based protocols that achieve that goal. They are primarily targeted at supporting a campus environment with mobile computers, but also extend gracefully to accommodate hosts moving between different networks. The key feature is the dependence on ancillary machines, the Mobile Support Stations (MSSs), to track the location of the Mobile Hosts. Using a combination of caching, forwarding pointers, and timeouts, a minimal amount of state is kept in each MSS. The state information is kept in a distributed fashion; the system scales well, reacts quickly to changing topologies, and does ...
164|Querying in highly mobile distributed environments|New challenges to the database field brought about by the rapidly growing area of wireless personal communication are presented. Preliminary solutions to two of the major problems, control of update volume and integration of query processing and data acquisition, are discussed along with the simulation results. 1
165|Replication and Mobility|this paper we address some of the above questions, by proposing an adaptive replication scheme for mobile data. Before doing this we review some basic facts about possible architectures for PCN. 2 Architectures Location Server Base Station Mobile Unit Radio Link Public Access Network Figure 1: Architecture of Cellular Network Today, there is no consensus about the future architecture for PCN. There are different proposals in US, Europe and Japan but most of them are
166|Progressive Parametric Query Optimization|Abstract—Commercial applications usually rely on precompiled parameterized procedures to interact with a database. Unfortunately, executing a procedure with a set of parameters different from those used at compilation time may be arbitrarily suboptimal. Parametric query optimization (PQO) attempts to solve this problem by exhaustively determining the optimal plans at each point of the parameter space at compile time. However, PQO is likely not cost-effective if the query is executed infrequently or if it is executed with values only within a subset of the parameter space. In this paper, we propose instead to progressively explore the parameter space and build a parametric plan during several executions of the same query. We introduce algorithms that, as parametric plans are populated, are able to frequently bypass the optimizer but still execute optimal or near-optimal plans. Index Terms—Parametric query optimization, adaptive optimization, selectivity estimation. Ç 1
168|AniPQO: Almost Non-intrusive Parametric Query Optimization for Nonlinear Cost Functions|The cost of a query plan depends on many parameters,  such as predicate selectivities and  available memory, whose values may not be  known at optimization time. Parametric  query optimization (PQO) optimizes a query  into a number of candidate plans, each optimal  for some region of the parameter space.
169|Parametric Query Optimization for Linear and Piecewise Linear Cost Functions|The cost of a query plan depends on many parameters,  such as predicate selectivities and  available memory, whose values may not be  known at optimization time. Parametric  query optimization (PQO) optimizes a query  into a number of candidate plans, each optimal  for some region of the parameter space.
170|Parametric Query Optimization: A Non-Geometric Approach|Query Optimizers normally compile queries into one optimal plan by assuming complete knowledge of all cost parameters such as selectivity and resource availability. The problem of Parametric Query Optimization refers to optimizing queries with parameters whose values are known only at execution time. We present a non-geometric approach to find the set of parametric optimal plans for 2 parameter queries. This set may contain spurious plans. We measure the average number of non-geometric optimal plans as a function of number of query relations. The average number of nongeometric plans for linear queries are `(n    ) and `(n    ) for different relative CPU to disk weighting factors. The average number of non-geometric optimal plans for star queries are `(n    ) for both the weighting factors, Where n is the number of query relations. We also measure the distribution of non-geometric optimal plans along axes and near the origin.
171|Parametric Query Optimization: A Geometric Approach|Traditional query optimizers assume accurate knowledge of many parameters such as unknown selectivities and resource availability to compute the cost of a plan. As the values of some of these parameters might change over time, the statically computed plan may not be optimal at run time. Parametric Query Optimization refers to the strategies and techniques used to optimize queries involving unknown parameters whose values will be known only at run time. In this thesis we develop the geometric approach initiated in [Gan98], to binary non-linear cost functions and linear cost functions in 3 parameters. The strategy is to compute a set of parametric optimal plans, each member of which is optimal for some region of the parameter space. We study the structural properties and characteristics of optimality regions and develop algorithms to solve the parametric query optimization problem with binary non-linear cost functions. The problem of degeneracy is analyzed and techniques are developed to handle the problem. We
172|Datacubes and Query Optimizations Query optimization can be decomposed|Abstract. The view size estimation plays an important role in query optimization. It has been observed that many data follow a power law distribution. In this paper, we consider the balls in bins problem where we place balls into N bins when the bin selection probabilities follow a power law distribution. As a generalization to the coupon collector’s problem, we address the problem of determining the expected number of balls that need to be thrown in order to have at least one ball in each of the N bins. We prove that T ( Na lnN c a N) balls are needed to achieve this where aistheparameterofthepowerlawdistributionandc a N = a-1 a-Na-1 for a ? = 1 and c a N = 1 for a = 1. Next, when fixing the number of balls lnN that are thrown to T, we provide closed form upper and lower bounds on the expected number of bins that have at least one occupant. For n
173|Probabilistic Counting Algorithms for Data Base Applications|This paper introduces a class of probabilistic counting lgorithms with which one can  estimate the number of distinct elements in a large collection of data (typically a large file  stored on disk) in a single pass using only a small additional storage (typically less than a  hundred binary words) and only a few operations per element scanned. The algorithms are  based on statistical observations made on bits of hashed values of records. They are by con-  struction totally insensitive to the replicafive structure of elements in the file; they can be used  in the context of distributed systems without any degradation of performances and prove  especially useful in the context of data bases query optimisation. ; 1985 Academic Press, Inc
174|An Optimal Algorithm for the Distinct Elements Problem |We give the first optimal algorithm for estimating the number of distinct elements in a data stream, closing a long line of theoretical research on this problem begun by Flajolet and Martin in their seminal paper in FOCS 1983. This problem has applications to query optimization, Internet routing, network topology, and data mining. For a stream of indices in {1,..., n}, our algorithm computes a (1 ± e)approximation using an optimal O(e -2 +log(n)) bits of space with 2/3 success probability, where 0 &lt; e &lt; 1 is given. This probability can be amplified by independent repetition. Furthermore, our algorithm processes each stream update in O(1) worst-case time, and can report an estimate at any point midstream in O(1) worst-case time, thus settling both the space and time complexities simultaneously.
175|Modeling Skewed Distribution Using Multifractals and the `80-20&#039; Law|law&#039;
176|A comparison of five probabilistic view-size estimation techniques in OLAP|A data warehouse cannot materialize all possible views, hence we must estimate quickly, accurately, and reliably the size of views to determine the best candidates for materialization. Many available techniques for view-size estimation make particular statistical assumptions and their error can be large. Comparatively, unassuming probabilistic techniques are slower, but they estimate accurately and reliability very large view sizes using little memory. We compare five unassuming hashing-based view-size estimation techniques including Stochastic Probabilistic Counting and LOGLOG Probabilistic Counting. Our experiments show that only Generalized Counting, Gibbons-Tirthapura, and Adaptive Counting provide universally tight estimates irrespective of the size of the view; of those, only Adaptive Counting remains constantly fast as we increase the memory budget.
177|Fast and accurate traffic matrix measurement using adaptive cardinality counting|The spatial and temporal properties of traffic matrix (TM) can be used to diagnose various network anomalies [6]. To identify anomalies quickly, TM needs to be measured in a fast and accurate manner. Besides byte-level TM (BTM) and packet-level TM (PTM), flowlevel
178|Distinct Values Estimators for Power Law Distributions|The number of distinct values in a relation is an important statistic for database query optimization. As databases have grown in size, scalability of distinct values estimators has become extremely important, since a naïve linear scan through the data is no longer feasible. An approach that scales very well involves taking a sample of the data, and performing the estimate on the sample. Unfortunately, it has been shown that obtaining estimators with guaranteed small error bounds requires an extremely large sample size in the worst case. On the other hand, it is typically the case that the data is not worst-case, but follows some form of a Power Law or Zipfian distribution. We exploit data distribution assumptions to devise distinct-values estimators with analytic error guarantees for Zipfian distributions. Our estimators are the first to have the required number of samples depend only on the number of distinct values present, D, and not the database size, n. This allows the estimators to scale well with the size of the database, particularly if the growth is due to multiple copies of the data. In addition to theoretical analysis, we also provide experimental evidence of the effectiveness of our estimators by benchmarking their performance against previously best known heuristic and analytic estimators on both synthetic and real-world datasets. 
179|Polynomial heuristics for query optimization|Abstract — Research on query optimization has traditionally focused on exhaustive enumeration of an exponential number of candidate plans. Alternatively, heuristics for query optimization are restricted in several ways, such as by either focusing on join predicates only, ignoring the availability of indexes, or in general having high-degree polynomial complexity. In this paper we propose a heuristic approach to very efficiently obtain execution plans for complex queries, which takes into account the presence of indexes and goes beyond simple join reordering. We also introduce a realistic workload generator and validate our approach using both synthetic and real data. I.
180|Rapid Bushy Join-order Optimization with Cartesian Products|Query optimizers often limit the search space for join orderings, for example by excluding Cartesian products in subplans or by restricting plan trees to left-deep vines. Such exclusions are widely assumed to reduce optimization effort while minimally affecting plan quality. However, we show that searching the complete space of plans is more affordable than has been previously recognized, and that the common exclusions may be of little benefit. We start by presenting a Cartesian product optimizer that requires at most a few seconds of workstation time to search the space of bushy plans for products of up to 15 relations. Building on this result, we present a join-order optimizer that achieves a similar level of performance, and retains the ability to include Cartesian products in subplans wherever appropriate. The main contribution of the paper is in fully separating join-order enumeration from predicate analysis, and in showing that the former problem in particular can be solved swift...
181|Practical Predicate Placement|. Recent work in query optimization has addressed the issue of placing expensive predicates in a query plan. In this paper we explore the predicate placement options considered in the Montage DBMS, presenting a family of algorithms that form successively more complex and effective optimization solutions. Through analysis and performance measurements of Montage SQL queries, we classify queries and highlight the simplest solution that will optimize each class correctly. We demonstrate limitations of previously published algorithms, and discuss the challenges and feasibility of implementing the various algorithms in a commercial-grade system.  1 Introduction  Relational Database Management Systems have begun to allow user-defined data types and operators to be utilized in ad-hoc queries. Simultaneously, ObjectOriented DBMSs have begun to offer ad-hoc query facilities, allowing declarative access to objects and methods that were previously only accessible through handcoded, imperative appl...
182|On the Complexity of Generating Optimal Left-Deep Processing Trees with Cross Products|. Producing optimal left-deep trees is known to be NP-complete for general join graphs and a quite complex cost function counting disk accesses for a special block-wise nested-loop join [2]. Independent of any cost function is the dynamic programming approach to join ordering. The number of alternatives this approach generates is known as well [5]. Further, it is known that for some cost functions --- those fulfilling the ASI property [4] --- the problem can be solved in polynomial time for acyclic query graph, i.e., tree queries [2, 3]. Unfortunately, some cost functions like sort merge could not be treated so far. We do so by a slight detour showing that this cost function (and others too) are optimized if and only if the sum of the intermediate result sizes is minimized. This validates the database folklore that minimizing intermediate result sizes is a good heuristic. Then we show that summarizing the intermediate result sizes has the ASI property. It further motivates us to restri...
183|Dynamic programming strikes back|Two highly efficient algorithms are known for optimally ordering joins while avoiding cross products: DPccp, which is based on dynamic programming, and Top-Down Partition Search, based on memoization. Both have two severe limitations: They handle only (1) simple (binary) join predicates and (2) inner joins. However, real queries may contain complex join predicates, involving more than two relations, and outer joins as well as other non-inner joins. Taking the most efficient known join-ordering algorithm, DPccp, as a starting point, we first develop a new algorithm, DPhyp, which is capable to handle complex join predicates efficiently. We do so by modeling the query graph as a (variant of a) hypergraph and then reason about its connected subgraphs. Then, we present a technique to exploit this capability to efficiently handle the widest class of non-inner joins dealt with so far. Our experimental results show that this reformulation of non-inner joins as complex predicates can improve optimization time by orders of magnitude, compared to known algorithms dealing with complex join predicates and non-inner joins. Once again, this gives dynamic programming a distinct advantage over current memoization techniques.
184|A New Heuristic for Optimizing Large Queries|There is a number of OODB optimization techniques proposed recently, such as the translation of path expressions into joins and query unnesting, that may generate a large number of implicit joins even for simple queries. Unfortunately, most current commercial query optimizers are still based on the dynamic programming approach of System R, and cannot handle queries of more than ten tables. There is a number of recent proposals that advocate the use of combinatorial optimization techniques, such as iterative improvement and simulated annealing, to deal with the complexity of this problem. These techniques, though, fail to take advantage of the rich semantic information inherent in the query specification, such as the information available in query graphs, which gives a good handle to choose which relations to join each time. This paper presents a polynomial-time algorithm that generates a good quality order of relational joins. It can also be used with minor modifications to sort OODB a...
185|Parallelizing Query Optimization |Many commercial RDBMSs employ cost-based query optimization exploiting dynamic programming (DP) to efficiently generate the optimal query execution plan. However, optimization time increases rapidly for queries joining more than 10 tables. Randomized or heuristic search algorithms reduce query optimization time for large join queries by considering fewer plans, sacrificing plan optimality. Though commercial systems executing query plans in parallel have existed for over a decade, the optimization of such plans still occurs serially. While modern microprocessors employ multiple cores to accelerate computations, parallelizing query optimization to exploit multi-core parallelism is not as straightforward as it may seem. The DP used in join enumeration belongs to the challenging nonserial polyadic DP class because of its non-uniform data dependencies. In this paper, we propose a comprehensive and practical solution for parallelizing query optimization in the multi-core processor architecture, including a parallel join enumeration algorithm and several alternative ways to allocate work to threads to balance their load. We also introduce a novel data structure called skip vector array to significantly reduce the generation of join partitions that are infeasible. This solution has been prototyped in PostgreSQL. Extensive experiments using various query graph topologies confirm that our algorithms allocate the work evenly, thereby achieving almost linear speed-up. Our parallel join enumeration algorithm enhanced with our skip vector array outperforms the conventional generate-and-filter DP algorithm by up to two orders of magnitude for star queries–linear speedup due to parallelism and an order of magnitude performance improvement due to the skip vector array. 1.
186|Analysis of two existing and one new dynamic programming algorithm for the generation of optimal bushy join trees without cross products|Two approaches to derive dynamic programming algorithms for constructing join trees are described in the literature. We show analytically and experimentally that these two variants exhibit vastly diverging runtime behaviors for different query graphs. More specifically, each variant is superior to the other for one kind of query graph (chain or clique), but fails for the other. Moreover, neither of them handles star queries well. This motivates us to derive an algorithm that is superior to the two existing algorithms because it adapts to the search space implied by the query graph. 1.
187|Load balancing in a parallel dynamic programming multi-method applied to the 0-1 knapsack problem |The 0-1 knapsack problem is considered. A parallel dynamic programming multi-method using dominance technique and processor cooperation is proposed. Different load balancing approaches are studied. Computational experiments carried out on an Origin 3800 supercomputer are reported and analyzed. 1
188|Automatic Partitioning of Parallel Loops with Parallelepiped-Shaped Tiles|In this paper, an efficient algorithm to implement loop partitioning is introduced and evaluated. We start from results of  Agarwal et al. [1] whose aim is to minimize the number of accessed data throughout the computation of a tile; this number is called the  cumulative footprint of the tile. We improve these results along several directions. First, we derive a new formulation of the cumulative  footprint, allowing for an analytical solution of the optimization problem stated in [1]. Second, we deal with arbitrary parallelepipedshaped  tiles, as opposed to rectangular tiles in [1]. We design an efficient heuristic to determine the optimal tile shape in this general  setting and we show its usefulness using both examples from [1] and a large collection of randomly generated data.
189|Abstract Parametric Query Optimization |In most database systems, the values of many impor-tant run-time parameters of the system, the data, or the query are unknown at query optimization time. Parametric query optimization attempts to identify several execution plans, each one of which is optimal for a subset of all possible values of the run-time pa-rameters. We present a general formulation of this problem and study it primarily for the buffer size pa-rameter. We adopt randomized algorithms as the main approach to this style of optimization and enhance them with a sideways information passing feature that increases their effectiveness in the new task. Experi-mental results of these enhanced algorithms show that they optimize queries for large numbers of buffer sizes in the same time needed by their conventional versions l PartiaIIy supported by NSF under PYI Grant IRI-9157368 and by grams from DEC, HP, and AT&amp;T.
190|Left-deep vs. bushy trees: An analysis of strategy spaces and its implications for query optimization|We present a combination of analytical and experimental results that shed some light into the shape of the cost function of the strategy spaces that query optimizers must deal with. These are the space that includes only left-deep trees and the space that includes koth deep and bushy trees. We conclude that the cost functions of both spaces essentially form a “well ” but of a distinctly different quality. Based on this result, we discuss how Iterative Improvement, Simulated Annealing, and Two Phase Optimization perform on these spaces. We conclude that the space of both deep and bushy trees is easier to optimize than the space of left-deep trees alone. 1.
191|Query Optimization by Stored Queries|A stored query is a pair &lt;query,response&gt;, where &amp;quot;respons&amp; &#039; is the- query meaning for the current database state. When a collection of stored queries is available responses to sane queries may be obtained easily. Stored queries give a possibility of improvement of database sys tern response time regardless of the complexity of user request and the data model assumed. The method is a generalization of methods based on indices. Its main properties and problems are outlined, particularly the problem of updating stored queries. The presented solutions are based on detecting whether the response associated with a query is influenced by a database update, and on correcting the response after an update. The methods concern NETUL, a user-friendly query language, with the power of programming languages, for network/semantic data models. 1.
192|Adding structure to unstructured data|We develop a new schema for unstructured data. Traditional schemas resemble the type systems of programming languages. For unstructured data, however, the underlying type may be much less constrained and hence an alternative way of expressing constraints on the data is needed. Here, we propose that both data and schema be represented as edge-labeled graphs. We develop notions of conformance between a graph database and a graph schema and show that there is a natural and e ciently computable ordering on graph schemas. We then examine certain subclasses of schemas and show that schemas are closed under query applications. Finally, we discuss how they may be used in query decomposition and optimization. 1
193|Representative Objects: Concise Representations of Semistructured Hierarchical Data|In this paper we introduce the representative object, which uncovers the inherent schema(s) in semistructured, hierarchical data sources and provides a concise description of the structure of the data. Semistructured data, unlike data stored in typical relational or object-oriented databases, does not have fixed schema that is known in advance and stored separately from the data. Withthe rapid growth of the World Wide Web, semistructured hierarchical data sources are becoming widely available to the casual user. The lack of external schema information currently makes browsing and querying these data sources inefficient at best, and impossible at worst. We show how representative objects make schema discovery efficient and facilitate the generation of meaningful queries over the data. 1.
195|PESTO: An Integrated Query/Browser for Object Databases|This paper describes the design and implementation of PESTO (Portable Explorer of  STructured Objects), a user interface that supports browsing and querying of object databases. PESTO allows users to navigate the relationships that exist among objects. In addition, users can formulate complex object queries through an integrated query paradigm (&#034;query-in-place&#034;) that presents querying as a natural extension of browsing. PESTO is designed to be portable to any object database system that supports a high-level query language; in addition, PESTO is extensible, providing hooks for specialized predicate formation and object display tools for new data types (e.g., images or text).  1 Introduction  The Garlic project at the IBM Almaden Research Center [Care95] is developing a system and associated tools for managing large quantities of heterogeneous multimedia information. The goal of Garlic is to permit both traditional and multimedia data residing in a variety of existing data repositories ...
196|OdeView: The Graphical Interface to Ode|OdeView is the graphical front end for Ode, an object-oriented database system and environment. Ode&#039;s data model supports data encapsulation, type inheritance, and complex objects. OdeView provides facilities for examining the database schema (i.e., the object type or class hierarchy), examining class definitions, browsing objects, following chains of references starting from an object, synchronized browsing, displaying selected portions of objects (projection), and retrieving objects with specific characteristics (selection).  OdeView does not need to know about the internals of Ode objects. Consequently, the internals of specific classes are not hardwired into OdeView and new classes can be added to the Ode database without requiring any changes to or recompilation of OdeView. Just as OdeView does not know about the object internals, class functions (methods) for displaying objects are written without knowing about the specifics of the windowing software used by OdeView or the graphi...
197|Access Support Relations: An Indexing Method for Object Bases|In this work access support relations are introduced as a means for optimizing query processing in object-oriented database systems. The general idea is to maintain separate structures (dissociated from the object representation) to redundantly store those object references that are frequently traversed in database queries. The proposed access support relation technique is no longer restricted to relate an object (tuple) to an atomic (,alue (attribute value) as in conventional indexing. Rather, access support relations relate objects with each other and can span over reference chains which may contain collection-valued components in order to support queries involving path expressions. We present several alternative extensions and decompositions of access support relations for a given path expression, the best of which has to be determined according to the application-specific database usage profile. An analytical performance analysis of access support relations is developed. This analytical cost model is, in particular, used to determine the best access support relation extension and decomposition with respect to specific database configuration and usage characteristics.
198|On Index Selection Schemes for Nested Object Hierarchies|In this paper we address the problem of devising a set of indexes for a nested object hierarchy in an object-oriented database to improve the overall system performance. It is noted that the effects of two indexes could be entangled in that the inclusion of one index might affect the benefit achievable by the other index. Such a phenomenon is termed index interaction. Clearly, the effect of index interaction needs to be taken into consideration when a set of indexes is being built. The index selection problem is first formulated and four index selection algorithms are evaluated via simulation. The effects of different objective functions, which guide the search in the index selection algorithms, are also investigated. It is shown by simulation results that the greedy algorithm which is devised in light of the phenomenon of index interaction performs fairly well in most cases. Sensitivity analysis for various database parameters is conducted. Index Term: Object-oriented databases, indexing, nested object hierarchy, index interaction.
199|Exploiting Uniqueness in Query Optimization|Consider an sql query that specifies duplicate elimination via a Distinct clause. Because duplicate elimination often requires an expensive sort of the query result, it is often worthwhile to identify unnecessary Distinct  clauses and avoid the sort altogether. We prove a necessary and sufficient condition for deciding if a query requires duplicate elimination. The condition exploits knowledge about keys, table constraints, and query predicates. Because the condition cannot always be tested efficiently, we offer a practical algorithm that tests a simpler, sufficient condition. We consider applications of this condition for various types of queries, and show that we can exploit this condition in both relational and nonrelational database systems.  1 Introduction  sql queries that contain Distinct are common enough to warrant special consideration by commercial query optimizers because duplicate elimination often requires an expensive sort of the query result. It is worthwhile, then, for...
200|Deriving Production Rules for Incremental View Maintenance|. It is widely recognized that production rules in database systems can be used to automatically maintain derived data such as views. However, writing a correct set of rules for efficiently maintaining a given view can be a difficult and ad-hoc process. We provide a facility whereby a user defines a view as an SQL select expression, from which the system automatically derives set-oriented production rules that maintain a materialization of that view. The maintenance rules are triggered by operations on the view&#039;s base tables. Generally, the rules perform incremental maintenance: the materialized view is modified according to the sets of changes made to the base tables, which are accessible through logical tables provided by the rule language. However, for some operations substantial recomputation may be required. We give algorithms that, based on key information, perform syntactic analysis on a view definition to determine when efficient maintenance is possible.  1 Introduction  In rel...
201|A Survey:  Query Optimization |Abstract-Almost all applications use database and Information Retrieval system for storing and retrieving operational data i.e. query processing. But in current time most of the users on internet are not interested in huge amount of results. They look for specific type of information with in short time that can be retrieving after resolving cardinality estimation. The new challenge requires several changes that vary from introducing new query language constructs to enhancing the query processing and optimization engines with new query operators. This paper presented a survey of query optimization.
202|Query Optimization over Web Services|Web services are becoming a standard method of sharing data and functionality among loosely-coupled systems. We propose a generalpurpose Web Service Management System (WSMS) that enables querying multiple web services in a transparent and integrated fashion. This paper tackles a first basic WSMS problem: query optimization for Select-Project-Join queries spanning multiple web services. Our main result is an algorithm for arranging a query’s web service calls into a pipelined execution plan that optimally exploits parallelism among web services to minimize the query’s total running time. Surprisingly, the optimal plan can be found in polynomial time even in the presence of arbitrary precedence constraints among web services, in contrast to traditional query optimization where the analogous problem is NP-hard. We also give an algorithm for determining the optimal granularity of data “chunks ” to be used for each web service call. Experiments with an initial prototype indicate that our algorithms can lead to significant performance improvement over more straightforward techniques. 1.
203|Distributed Database Systems |this article, we discuss the fundamentals of distributed DBMS technology. We address the data distribution and architectural design issues as well as the algorithms that need to be implemented to provide the basic DBMS functions such as query processing, concurrency control, reliability, and replication control.
204|Don&#039;t Scrap It, Wrap It! A Wrapper Architecture for Legacy Data Sources|Garlic is a middleware system that provides an in-tegrated view of a variety of legacy data sources, without changing how or where data is stored. In this paper, we describe our architecture for wrap-pers, key components of Garlic that encapsulate data sources and mediate between them and the middleware. Garlic wrappers model legacy data as objects, participate in query planning, and provide standard interfaces for method invocation and query execution. To date, we have built wrappers for 10 data sources. Our experience shows that Garlic wrappers can be written quickly and that our architecture is flexible enough to accommo-date data sources with a variety of data models and a broad range of traditional and non-tradition-al query processing capabilities. 1
205|XJoin: A Reactively-Scheduled Pipelined Join Operator|Wide-area distribution raises significant performance problems for traditional query processing techniques as data access becomes less predictable due to link congestion, load imbalances, and temporary outages. Pipelined query execution is a promising approach to coping with unpredictability in such environments as it allows scheduling to adjust to the arrival properties of the data. We have developed a non-blocking join operator, called XJoin, which has a small memory footprint, allowing many such operators to be active in parallel. XJoin is optimized to produce initial results quickly and can hide intermittent delays in data arrival by reactively scheduling background processing. We show that XJoin is an effective solution for providing fast query responses to users even in the presence of slow and bursty remote sources.  
207|WSQ/DSQ: A Practical Approach for Combined Querying of Databases and the Web|We present WSQ/DSQ (pronounced &#034;wisk-disk&#034;), a new approach for combining the query facilities  of traditional databases with existing search engines on the Web. WSQ, for Web-Supported (Database)  Queries, leverages results from Web searches to enhance SQL queries over a relational database. DSQ,  for Database-Supported (Web) Queries, uses information stored in the database to enhance and explain  Web searches. This paper focuses primarily on WSQ, describing a simple, low-overhead way to support  WSQ in a relational DBMS, and demonstrating the utility of WSQ with a number of interesting  queries and results. The queries supported by WSQ are enabled by two virtual tables, whose tuples  represent Web search results generated dynamically during query execution. WSQ query execution may  involve many high-latency calls to one or more search engines, during which the query processor is  idle. We present a lightweight technique called asynchronous iteration that can be integrated easily into  a standard sequential query processor to enable concurrency between query processing and multiple  Web search requests. Asynchronous iteration has broader applications than WSQ alone, and it opens up  many interesting query optimization issues. We have developed a prototype implementation of WSQ by  extending a DBMS with virtual tables and asynchronous iteration; performance results are reported.  1 
208|Tuple Routing Strategies for Distributed Eddies|Many applications that consist of streams of data  are inherently distributed. Since input stream  rates and other system parameters such as the  amount of available computing resources can  fluctuate significantly, a stream query plan must  be able to adapt to these changes. Routing tuples  between operators of a distributed stream query  plan is used in several data stream management  systems as an adaptive query optimization  technique. The routing policy used can have a  significant impact on system performance. In this  paper, we use a queuing network to model a  distributed stream query plan and define  performance metrics for response time and  system throughput. We also propose and  evaluate several practical routing policies for a  distributed stream management system. The  performance results of these policies are  compared using a discrete event simulator.
209|Efficient Access to Web Services|For Web services to expand across the Internet, users need to be able to efficiently access and share Web services. The authors present a query infrastructure that treats Web services as first-class objects. It evaluates queries through the invocations of different Web service operations. Because efficiency plays a central role in such evaluations,the authors propose a query optimization model based on aggregating the quality of Web service (QoWS) parameters of different Web services.The model adjusts QoWS through a dynamic rating scheme and multilevel matching in which the rating provides an assessment of Web services ’ behavior.Multilevel matching allows the expansion of the solution space by enabling similar and partial answers. The Web brought connectivity to a wealth of hitherto-inaccessible information sources. Although powerful search engines help us sift through the information glut, the ever-increasing
210|Adapting to Source Properties in Processing Data Integration Queries|An effective query optimizer finds a query plan that exploits the characteristics of the source data. In data integration, little is known in advance about sources&#039; properties, which necessitates the use of adaptive query processing techniques to adjust query processing on-the-fly. Prior work in adaptive query processing has focused on compensating for delays and adjusting for mis-estimated cardinality or selectivity values. In this paper, we present a generalized architecture for adaptive query processing and introduce a new technique, called adaptive data partitioning (ADP), which is based on the idea of dividing the source data into regions, each executed by different, complementary plans. We show how this model can be applied in novel ways to not only correct for underestimated selectivity and cardinality values, but also to discover and exploit order in the source data, and to detect and exploit source data that can be effectively pre-aggregated. We experimentally compare a number of alternative strategies and show that our approach is effective.
211|XL: A Platform for Web Services|This papers presents XL, a new platform for  Web services. We have designed XL with three  main goals in mind: (a) increase application  developers productivity via high-level programming  constructs for Web Services routine programming  patterns, (b) achieve high scalability,  security, and availability for Web services  and (c) compliance with all W3C standards  (e.g., XML, SOAP, WSDL) such that  XL Web services can interact with any other  Web services written in, say, Java or C#. We  hope to achieve these objectives by providing  the new XL programming model based on a  simple core programming &#034;algebra&#034; that extends  Milner&#039;s PI-calculus [21]. To optimize  XL programs, we employ techniques from the  design of database systems, compiler construction,  and data flow machines, as well as techniques  specially designed for Web Services. A  demo of the platform has been shown at [14].
212|Query Processing and Optimization on the Web|The advent of the Internet and the Web and their subsequent ubiquity have brought forth opportunities to connect information sources across all types of boundaries (local, regional, organizational, etc.). Examples of such information sources include databases, XML documents, and other unstructured sources. Uniformly querying those information sources has been extensively investigated. A major challenge relates to query optimization. Indeed, querying multiple information sources scattered on the Web raises several barriers for achieving efficiency. This is due to the characteristics of Web information sources that include volatility, heterogeneity, and autonomy. Those characteristics impede a straightforward application of classical query optimization techniques. They add new dimensions to the optimization problem such as the choice of objective function, selection of relevant information sources, limited query capabilities, and unpredictable events. In this paper, we survey the current research on fundamental problems to efficiently process queries over Web data integration systems. We also outline a classification for optimization techniques and a framework for evaluating them.
213|Efficient querying of distributed resources in mediator systems| This work investigates the integration of heterogeneous resources, such as data and programs, in a fully distributed peer-to-peer mediation architecture. The challenge in making such a system succeed at a large scale is twofold. First, we need a simple concept for modeling resources. Second, we need efficient operators for distributed query execution, capable of handling well costly computations and large data transfers. To model heterogeneous resources, we use the model of table with binding patterns. To exploit a resource with restricted binding patterns, we propose an efficient BindJoin operator, optimized for minimizing large data transfers and costly computations. Furthermore, the proposed BindJoin operator delivers most of its output in the early stages of the execution, which is an important asset in a system meant for human interaction. Our experimental evaluation validates the proposed BindJoin algorithm on queries involving expensive programs.
214|Abstract Query Optimization on the Web |The increasing availability of the Internet has allowed tremendous amounts of data to be stored and accessed by the users of the Web. This, in turn, has brought up an expectation to access data widely distributed in nature in an efficient manner. The type of access to such data, however, is currently in the form of non-database facilities. It is conjectured in this paper that advances in heterogeneous and semi-structured databases taking into account the intrinsic properties and the problems inherent in the Internet will overcome this problem, will the Internet eventually be the medium allowing for the querying of information required easily and efficiently. The contributions of this paper is twofold: Initially, it addresses the issues for the design and implementation of an efficient Web Information Retrieval System (WIRS) by paying attention to inherent characteristics of Internet applications and surveying the present related state of the art work in the literature; Finally, a novel architecture for WIRS and ideas for methods of efficient information retrieval on the web in the framework of query optimization are presented. Keywords: Mobile and ubiquitous work, optimization of queries on the web, Software Architecture. 1.
215|NiagaraCQ: A Scalable Continuous Query System for Internet Databases|Continuous queries are persistent queries that allow users to receive new results when they become available. While continuous query systems can transform a passive web into an active environment, they need to be able to support millions of queries due to the scale of the Internet. No existing systems have achieved this level of scalability. NiagaraCQ addresses this problem by grouping continuous queries based on the observation that many web queries share similar structures. Grouped queries can share the common computation, tend to fit in memory and can reduce the I/O cost significantly. Furthermore, grouping on selection predicates can eliminate a large number of unnecessary query invocations. Our grouping technique is distinguished from previous group optimization approaches in the following ways. First, we use an incremental group optimization strategy with dynamic re-grouping. New queries are added to existing query groups, without having to regroup already installed queries. Second, we use a query-split scheme that requires minimal changes to a general-purpose query engine. Third, NiagaraCQ groups both change-based and timer-based queries in a uniform way. To insure that NiagaraCQ is scalable, we have also employed other techniques including incremental evaluation of continuous queries, use of both pull and push models for detecting heterogeneous data source changes, and memory caching. This paper presents the design of NiagaraCQ system and gives some experimental results on the system’s performance and scalability. 1.
216|Optimizing Queries across Diverse Data Sources|Businesses today need to interrelate data stored in diverse systems with differing capabilities, ideally via a single high-level query interface. We present the design of a query optimizer for Gar- lic [C+95], a middleware system designed to integrate data from a broad range of data sources with very different query capabilities. Garlic&#039;s optimizer extends the rule-based approach of [Loh88 ] to work in a heterogeneous environment, by defining generic rules for the middleware and using wrapper-provided rules to encapsulate the capabilities of each data source. This approach offers great advantages in terms of plan quality, extensibility to new sources, incremental implementation of rules for new sources, and the ability to express the capabilities of a diverse set of sources. We describe the design and implementation of this optimizer, and illustrate its actions through an example.
217|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
218|A Promising Genetic Algorithm Approach to Job-Shop Scheduling, Rescheduling, and Open-Shop Scheduling Problems|The general job-shop scheduling problem is known to be extremely hard. We describe a GA approach which produces reasonably good results very quickly on standard benchmark job-shop scheduling problems, better than previous efforts using genetic algorithms for this task, and comparable to existing conventional search-based methods. The representation used is a variant of one known to work moderately well for the traveling salesman problem. It has the considerable merit that crossover will always produce legal schedules. A novel method for performance enhancement is examined based on dynamic sampling of the convergence rates in different parts of the genome. Our approach also promises to effectively address the open-shop scheduling problem and the job-shop rescheduling problem. 
219|Describing and using query capabilities of heterogeneous sources|Information integration systems have to cope with the different and limited query interfaces of the underlying information sources. First, the integration systems need descriptions of the query capabilities of each source, i.e., the set of queries supported by each source. Second, the integration systems need algo-rithms for deciding how a query can be an-swered given the capabilities of the sources. Third, they need to translate a query into the format that the source understands. We present two languages suitable for descrip-tions of query capabilities of sources and com-pare their expressive power. We also de-scribe algorithms for deciding whether a query “matches ” the description and show their ap-plication to the problem of translating user queries into source-specific queries and com-mands. Finally, we propose new improved al-gorithms for the problem of answering queries using these descriptions. 1
220|Capabilities-based Query Rewriting in Mediator Systems|Users today are struggling to integrate a broad range of information sources providing different levels of query capabilities. Currently, data sources with different and limited capabilities are accessed either by writing rich functional wrappers for the more primitive sources, or by dealing with all sources at a “lowest common denominator”. This paper explores a third approach, in which a mediator ensures that sources receive queries they can handle, while still taking advantage of all of the query power of the source. We propose an architecture that enables this, and identify a key component of that architecture, the Capabilities-Based Rewriter (CBR). The CBR takes as input a description of the capabilities of a data source, and a query targeted for that data source. From these, the CBR determines component queries to be sent to the sources, commensurate with their abilities, and computes a plan for combining their results using joins, unions, selections, and projections. We provide a language to describe the query capability of data sources and a plan generation algorithm. Our description language and plan generation algorithm are schema independent and handle SPJ queries.
221|Optimizing Large Join Queries in Mediation Systems|. In data integration systems, queries posed to a mediator need to be translated into a sequence of queries to the underlying data sources. In a heterogeneous environment, with sources of diverse and limited query capabilities, not all the translations are feasible. In this paper, we study the problem of finding feasible and efficient query plans for mediator systems. We consider conjunctive queries on mediators and model the source capabilities through attribute-binding adornments. We use a simple cost model that focuses on the major costs in mediation systems, those involved with sending queries to sources and getting answers back. Under this metric, we develop two algorithms for source query sequencing -- one based on a simple greedy strategy and another based on a partitioning scheme. The first algorithm produces optimal plans in some scenarios, and we show a linear bound on its worst case performance when it misses optimal plans. The second algorithm generates optimal plans in mor...
222|Dynamic Query Operator Scheduling for Wide-Area Remote Access|Distributed databases operating over wide-area networks such as the Internet, must deal with the unpredictable nature of the performance of communication. The response times of accessing remote sources can vary widely due to network congestion, link failure, and other problems. In such an unpredictable environment, the traditional iterator-based query execution model performs poorly. We have developed a class of methods, called query scrambling, for dealing explicitly with the problem of unpredictable response times. Query scrambling dynamically modifies query execution plans on-the-fly in reaction to unexpected delays in data access. In this paper we focus on the dynamic scheduling of query operators in the context of query scrambling. We explore various choices for dynamic scheduling and examine, through a detailed simulation, the effects of these choices. Our experimental environment considers pipelined and non-pipelined join processing in a client with multiple remote data sources ...
223|On Answering Queries in the Presence of Limited Access Patterns|. In information-integration systems, source relations often  have limitations on access patterns to their data; i.e., when one must  provide values for certain attributes of a relation in order to retrieve its  tuples. In this paper we consider the following fundamental problem: can  we compute the complete answer to a query by accessing the relations  with legal patterns? The complete answer to a query is the answer that  we could compute if we could retrieve all the tuples from the relations.  We give algorithms for solving the problem for various classes of queries,  including conjunctive queries, unions of conjunctive queries, and conjunctive  queries with arithmetic comparisons. We prove the problem is  undecidable for datalog queries. If the complete answer to a query cannot  be computed, we often need to compute its maximal answer. The  second problem we study is, given two conjunctive queries on relations  with limited access patterns, how to test whether the maximal answer to...
224|IRO-DB - A Distributed System Federating Object and Relational Databases|this paper adopts the federated approach to database interoperability [SL90] to overcome these limitations. Unlike homogeneous distributed database management systems, a federated database management system adds layers of software to pre-existing heterogeneous database management systems without privileging one system. These layers provide for syntactically uniform export schemas and data manipulation languages and also for semantically integrated schemas with global transaction management and concurrency control over multiple sites. Most importantly, this approach does not violate the autonomy of the pre-existing databases; that is, database providers participating in a federation do not lose control over their data and ideally do not have to re-engineer their DBMSs and applications to allow interoperability with other DBMSs. Several federated database systems have already been prototyped [TTC
225|Using Knowledge of Redundancy for Query Optimization in Mediators|this paper. We will focus on what needs to be done after such inferences have been made. A set of source queries has already been divided into equivalence classes; at least one query from each class needs to be executed to obtain a complete
226|Capability-Sensitive Query Processing on Internet Sources|On the Internet, the limited query-processing capabilities of sources make answering even the simplest queries challenging. In this paper, we present a scheme called  GenCompact for generating capability-sensitive plans for queries on Internet sources. The query plans generated by  GenCompact have the followingadvantages over those generated by existing query-processing systems: (1) the sources are guaranteed to support the query plans; (2) the plans take advantage of the source capabilities; and (3) the plans are more efficient since a larger space of plans is examined. 1. Introduction  Data sources on the Internet have a wide range of queryprocessing capabilities. Processing queries on such sources poses interesting challenges, as illustrated by the following examples.  EXAMPLE 1.1 (Bookstore): Consider the Internet bookstore BarnesAndNoble (http://barnesandnoble.com). Suppose we are looking for books written by Sigmund Freud or Carl Jung on the topic of dreams. The BarnesAndNoble qu...
227|Query Optimization: On the Ordering of Rules|This paper describes the conceptual development of the rule-based component of the CROQUE query rewrite and optimization system. We present different heuristics that may increase the efficiency of the logical term rewriting, especially in the CROQUE project. Therefore, several approaches to optimization are presented and some problems inherent to these approaches are sketched. Based on this, a variety of heuristics is discussed resulting in the most prominent one, an ordering of the rules present in our term rewriting system according to their &#034;optimization potential&#034;. This heuristic may indeed be used for any other rule-based optimizer, too. The major contribution of our approach is a combination of three ideas: (1) limit the search space of query optimization by grouping and ordering rules (for rule-based optimizers), (2) use of &#034;offline&#034; pre-optimization ordering instead of dynamic ordering during the optimization process, (3) taking into consideration more than one (&#034;n-best&#034;...
228|Rule Languages and Internal Algebras for Rule-Based Optimizers|Rule-based optimizers and optimizer generators use rules to specify query transformations. Rules act directly on query representations, which typically are based on query algebras. But most algebras complicate rule formulation, and rules over these algebras must often resort to calling to externally defined bodies of code. Code makes rules difficult to formulate, prove correct and reason about, and therefore compromises the effectiveness of rule-based systems. In this paper we present KOLA; a combinator-based algebra designed to simplify rule formulation. KOLA is not a user language, and KOLA&#039;s variable-free queries are difficult for humans to read. But KOLA is an effective internal algebra because its combinatorstyle makes queries manipulable and structurally revealing. As a result, rules over KOLA queries are easily expressed without the need for supplemental code. We illustrate this point, first by showing some transformations that despite their simplicity, require head and body rou...
229|Changing the Rules: Transformations for Rule-Based Optimizers|Rule-based optimizers are extensible because they consist of modifiable sets of rules. For modification to be straightforward, rules must be easily reasonedabout (i.e., understood and verified). At the same time, rules must be expressive and efficient (to fire) for rulebased optimizers to be practical. Production-style rules (as in [15]) are expressed with code and are hard to reason about. Pure rewrite rules (as in [1]) lack code, but cannot atomically express complex transformations (e.g., normalizations). Some systems allow rules to be grouped, but sacrifice efficiency by providing limited control over their firing. Therefore, none of these approaches succeeds in making rules expressive, efficient and understandable. We propose a language (COKO) for expressing an alternative form of input to a rule-based optimizer. A COKO transformation  consists of a set of declarative (KOLA) rewrite rules and a (firing) algorithm that specifies their firing. It is straightforward to reason about C...
230|A Formalization of ODMG Queries|The ODMG proposal has helped to focus the work on object-oriented databases (OODBs) onto a common object model and query language. Nevertheless there are several shortcomings of the current proposal stemming from the adaption of concepts of object-oriented programming and a lack of formalization. In this paper we present a formalization of the ODMG model and the OQL query language that is used in the CROQUE project as a basis for query optimization. An essential part is a complete, formally sound type system that allows us to reason about the types of intermediate query results and gives rise to fully orthogonal queries, including useful extensions of projections and set operations. 1 INTRODUCTION  For a long time, the evolution of OODB seemed to disperse in quite different directions: there were rather distinct object-oriented database models (OODMs) either based on (nested) relational formalisms or OOPL-like notions, and hardly any consensus about the structure and formalization of q...
231|Query optimization - The CROQUE project|This paper describes parts of a concept for the evaluation and optimization of ODMGOQL  queries. We present a logical object algebra for the internal representation of OQL  queries. Algebraic expressions are also represented as query trees. Different optimization  techniques are sketched: factorization of common subexpressions, dependency-based optimization  and query rewriting. Afterwards, execution plan generation from query trees is  presented. The transformation of physical queries into ObjectStore DML code and the developed  cost model for the calculation of the costs of the execution plans are not considered  here.  Keywords: OQL, logical and physical algebra, optimization of object-oriented queries.  1 Introduction  In the framework of the CROQUE project  1  we focus on the development of optimization techniques for object databases. Central topics of our work are examinations about rule-based rewriting of algebraic queries, cost-based selection of evaluation mechanisms for obje...
232|Performing Group-By before Join|Assume that we have an SQL query containing joins and a group-by. The standard way of evaluating this type of query is to first perform all the joins and then the group-by operation. However, it may be possible to perform the group-by early, that is, to push the groupby operation past one or more joins. Early grouping may reduce the query processing cost by reducing the amount of data participating in joins. We formally define the problem, adhering strictly to the semantics of NULL and duplicate elimination in SQL2, and prove necessary and sufficient conditions for deciding when this transformation is valid. In practice, it may be expensive or even impossible to test whether the conditions are satisfied. Therefore, we also present a more practical algorithm that tests a simpler, sufficient condition. This algorithm is fast and detects a large subclass of transformable queries. 1 Introduction SQL queries containing joins and group-by are fairly common. The standard way of evaluating su...
233|Query Optimization for Sensor Networks|This paper proposes a new approach to obtain optimality in distributed query processing in a wireless sensor network environment. The goal of this paper is to design a scheme which supports multiple data acquisition and aggregation queries and minimizes the amount of radio transmissions, average transmission time, and energy consumption. Co-related queries will be automatically rewritten into synthetic queries to minimize communication and computation costs. The query propagation and data retrieval process will also be optimized by applying dynamic route selection algorithms to individual node to decide how, when and to whom it should propagate process and transmit data. This paper provides a solution which combines various schemes applied at different phases of sensor network query optimization. The proposed approach provides significant insight into techniques which can reduce power consumption on sensor devices. traversed as the query propagates downwards into the network. However, most of these queries are issued by multiple users simultaneously while current research has focused on optimization model for a single long-running query. The problem lies in identifying what data should be collected from sensor nodes and at what frequency should it be collected. Most recent development has lead to using in-network optimization to improve performance in many-to-many aggregation for sensor streams used in controlled applications [9]. Most of the query processing sensor networks define, analyze and discuss the various aspects related to data modeling, query algebra and query optimization by attaching cost function to query [13].
234|A Survey on Sensor Networks|Recent advancement in wireless communica- tions and electronics has enabled the develop- ment of low-cost sensor networks. The sensor networks can be used for various application areas (e.g., health, military, home). For different application areas, there are different technical issues that researchers are currently resolving. The current state of the art of sensor networks is captured in this article, where solutions are discussed under their related protocol stack layer sections. This article also points out the open research issues and intends to spark new interests and developments in this field.
235|Model-Driven Data Acquisition in Sensor Networks|Declarative queries are proving to be an attractive paradigm for interacting with networks of wireless sensors. The metaphor that &#034;the sensornet is a database&#034; is problematic, however, because sensors do not exhaustively represent the data in the real world. In order to map the raw sensor readings onto physical reality, a model of that reality is required to complement the readings. In this paper, we enrich interactive sensor querying with statistical modeling techniques. We demonstrate that such models can help provide answers that are both more meaningful, and, by introducing approximations with probabilistic confidences, significantly more efficient to compute in both time and energy. Utilizing the combination of a model and live data acquisition raises the challenging optimization problem of selecting the best sensor readings to acquire, balancing the increase in the confidence of our answer against the communication and data acquisition costs in the network. We describe an exponential time algorithm for finding the optimal solution to this optimization problem, and a polynomial-time heuristic for identifying solutions that perform well in practice. We evaluate our approach on several real-world sensor-network data sets, taking into account the real measured data and communication quality, demonstrating that our model-based approach provides a high-fidelity representation of the real phenomena and leads to significant performance gains versus traditional data acquisition techniques.
236|Fjording the Stream: An Architecture for Queries over Streaming Sensor Data|If industry visionaries are correct, our lives will soon be full of sensors, connected together in loose conglomerations via wireless networks, each monitoring and collecting data about the environment at large. These sensors behave very differently from traditional database sources: they have intermittent connectivity, are limited by severe power constraints, and typically sample periodically and push immediately, keeping no record of historical information. These limitations make traditional database systems inappropriate for queries over sensors. We present the Fjords architecture for managing multiple queries over many sensors, and show how it can be used to limit sensor resource demands while maintaining high query throughput. We evaluate our architecture using traces from a network of traffic sensors deployed on Interstate 80 near Berkeley and present performance results that show how query throughput, communication costs, and power consumption are necessarily coupled in sensor environments.
237|Multi-query optimization for sensor networks|Abstract. The widespread dissemination of small-scale sensor nodes has sparked interest in a powerful new database abstraction for sensor networks: Clients “program” the sensors through queries in a high-level declarative language permitting the system to perform the low-level optimizations necessary for energy-efficient query processing. In this paper we consider multi-query optimization for aggregate queries on sensor networks. We develop a set of distributed algorithms for processing multiple queries that incur minimum communication while observing the computational limitations of the sensor nodes. Our algorithms support incremental changes to the set of active queries and allow for local repairs to routes in response to node failures. A thorough experimental analysis shows that our approach results in significant energy savings, compared to previous work. 1
238|Optimization of Sequence Queries in Database Systems| The need to search for complex and recurring patterns in database sequences is shared by many applications. In this work, we discuss how to express and support efficiently sophisticated sequential pattern queries in relational database systems. Thus, we first
239|Two-tier multiple query optimization for sensor networks|When there are multiple queries posed to the resourceconstrained wireless sensor network, it is critical to process them efficiently. In this paper, we propose a Two-Tier Multiple Query Optimization (TTMQO) scheme. The first tier, called base station optimization, adopts a cost-based approach to rewrite a set of queries into an optimized set that shares the commonality and eliminates the redundancy among the queries in the original set. The optimized queries are then injected into the wireless sensor network. In the second tier, called in-network optimization, our scheme efficiently delivers query results by taking advantage of the broadcast nature of the radio channel and sharing the sensor readings among similar queries over time and space at a finer granularity. Our experimental results indicate that our proposed TTMQO scheme offers significant improvements over the traditional single query optimization technique. 1.
240|XPath Whole Query Optimization |Previous work reports about SXSI, a fast XPath engine which executes tree automata over compressed XML indexes. Here, reasons are investigated why SXSI is so fast. It is shown that tree automata can be used as a general framework for fine grained XML query optimization. We define the “relevant nodes ” of a query as those nodes that a minimal automaton must touch in order to answer the query. This notion allows to skip many subtrees during execution, and, with the help of particular tree indexes, even allows to skip internal nodes of the tree. We efficiently approximate runs over relevant nodes by means of on-the-fly removal of alternation and non-determinism of (alternating) tree automata. We also introduce many implementation techniques which allows us to efficiently evaluate tree automata, even in the absence of special indexes. Through extensive experiments, we demonstrate the impact of the different optimization techniques. 1.
241|Efficient algorithms for processing XPath queries|Our experimental analysis of several popular XPath processors reveals a striking fact: Query evaluation in each of the systems requires time exponential in the size of queries in the worst case. We show that XPath can be processed much more efficiently, and propose main-memory algorithms for this problem with polynomial-time combined query evaluation complexity. Moreover, we present two fragments of XPath for which linear-time query processing algorithms exist. 1
242|Processing XML Streams with deterministic automata|Abstract. We consider the problem of evaluating a large number of XPath expressions on an XML stream. Our main contribution consists in showing that Deterministic Finite Automata (DFA) can be used effectively for this problem: in our experiments we achieve a throughput of about 5.4MB/s, independent of the number of XPath expressions (up to 1,000,000 in our tests). The major problem we face is that of the size of the DFA. Since the number of states grows exponentially with the number of XPath expressions, it was previously believed that DFAs cannot be used to process large sets of expressions. We make a theoretical analysis of the number of states in the DFA resulting from XPath expressions, and consider both the case when it is constructed eagerly, and when it is constructed lazily. Our analysis indicates that, when the automaton is constructed lazily, and under certain assumptions about the structure of the input XML data, the number of states in the lazy DFA is manageable. We also validate experimentally our findings, on both synthetic and real XML data sets. 1
243|Staircase Join: Teach a Relational DBMS to Watch its (Axis) Steps  (2003) |Relational query processors derive much of their effectiveness from the awareness of specific table properties like sort order, size, or absence of duplicate tuples. This text applies (and adapts) this successful principle to database-supported XML and XPath processing: the relational system is made tree aware, i.e., tree properties like subtree size, intersection of paths, inclusion or disjointness of subtrees are made explicit. We propose a local change to the database kernel, the staircase join, which encapsulates the necessary tree knowledge needed to improve XPath performance. Staircase join
244|Fast Text Searching for Regular Expressions or Automaton Searching on Tries |  We present algorithms for efficient searching of regular expressions on preprocessed text, using a Patricia tree as a logical model for the index. We obtain searching algorithms that run in logarithmic expected time in the size of the text for a wide subclass of regular expressions, and in sublinear expected time for any regular expression. This is the first such algorithm to be found with this complexity.
245|Locating Matches of Tree Patterns in Forests|. We deal with matching and locating of patterns in forests of variable arity. A pattern consists of a structural and a contextual condition for subtrees of a forest, both of which are given as tree or forest regular languages. We use the notation of constraint systems to uniformly specify both kinds of conditions. In order to implement pattern matching we introduce the class of pushdown forest automata. We identify a special class of contexts such that not only pattern matching but also locating all of a forest&#039;s subtrees matching in context can be performed in a single traversal. We also give a method for computing the reachable states of an automaton in order to minimize the size of transition tables. 1 Introduction  In Standard Generalized Markup Language (SGML) [Gol90] documents are represented as trees. A node in a document tree may have arbitrarily many children, independent of the symbol at that node. A sequence of documents or subdocuments is called a forest. A main task in do...
246|XPathMark: an XPath benchmark for the XMark generated data |Abstract. We propose XPathMark, an XPath benchmark on top of the XMark generated data. It consists of a set of queries which covers the main aspects of the language XPath 1.0. These queries have been designed for XML documents generated under XMark, a popular bench-mark for XML data management. We suggest a methodology to evaluate the XPathMark on a given XML engine and, by way of example, we eval-uate two popular XML engines using the proposed benchmark. 1
247|Efficient Processing of Expressive Node-Selecting Queries on XML Data in Secondary Storage: A Tree Automata-based Approach|We propose a new, highly scalable and efficient technique for evaluating node-selecting queries on XML trees which is based on recent advances in the theory of tree automata. Our query
248|The complexity of XPath query evaluation and XML typing|We study the complexity of two central XML processing problems. The rst is XPath 1.0 query processing, which has been shown to be in PTime in previous work. We prove that both the data complexity and the query complexity of XPath 1.0 fall into lower (highly parallelizable) complexity classes, while the combined complexity is PTime-hard. Subsequently, we study the sources of this hardness and identify a large and practically important fragment of XPath 1.0 for which the combined complexity is LogCFL-complete and, therefore, in the highly parallelizable complexity class NC2. The second problem is the complexity of validating XML documents against various typing schemes like Document Type Denitions (DTDs), XML Schema Denitions (XSDs), and tree automata, both with respect to data and to combined complexity. For data complexity, we prove that validation is in LogSpace and depends crucially on how XML data is represented. For the combined complexity, we show that the complexity ranges from LogSpace to LogCFL, depending on the typing scheme.
249|Fully-functional static and dynamic succinct trees|We propose new succinct representations of ordinal trees, which have been studied extensively. It is known that any n-node static tree can be represented in 2n + o(n) bits and various operations on the tree can be supported in constant time under the word-RAM model. However the data structures are complicated and difficult to dynamize. We propose a simple and flexible data structure, called the range min-max tree, that reduces the large number of relevant tree operations considered in the literature, to a few primitives that are carried out in constant time on sufficiently small trees. The result is extended to trees of arbitrary size, achieving 2n + O(n/polylog(n)) bits of space. The redundancy is significantly lower than any previous proposal. For the dynamic case, where insertion/deletion of nodes is allowed, the existing data structures support very limited operations. Our data structure builds on the range min-max tree to achieve 2n + O(n / log n) bits of space and O(log n) time for all the operations. We also propose an improved data structure using 2n+O(n loglog n / logn) bits and improving the time to O(log n / loglog n) for most operations. 
250|N–ary Queries by Tree Automata|  We investigate n-ary node selection queries in trees by successful runs of tree au-tomata. We show that run-based n-ary queries capture MSO, contribute algorithms for enumerating answers of n-ary queries, and study the complexity of the problem. We investigate the subclass of run-based n-ary queries by unambiguous tree automata.
251|Regular Tree Language Recognition with Static Information|This paper presents our compilation strategy to produce efficient code for pattern matching in the CDuce compiler, taking into account static information provided by the type system. Indeed, this information allows in many cases to compute the result (that is, to decide which branch to consider) by looking only at a small fragment of the tree. Formally, we introduce a new kind of deterministic tree automata that can efficiently recognize regular tree languages with static information about the trees and we propose a compilation algorithm to produce these automata.
252|Fast In-Memory XPath Search using Compressed Indexes| A large fraction of an XML document typically consists of text data. The XPath query language allows text search via the equal, contains, and starts-with predicates. Such predicates can be efficiently implemented using a compressed self-index of the document’s text nodes. Most queries, however, contain some parts querying the text of the document, plus some parts querying the tree structure. It is therefore a challenge to choose an appropriate evaluation order for a given query, which optimally leverages the execution speeds of the text and tree indexes. Here the SXSI system is introduced. It stores the tree structure of an XML document using a bit array of opening and closing brackets plus a sequence of labels, and stores the text nodes of the document using a global compressed self-index. On top of these indexes sits an XPath query engine that is based on tree automata. The engine uses fast counting queries of the text index in order to dynamically determine whether to evaluate top-down or bottom-up with respect to the tree structure. The resulting system has several advantages over existing systems: (1) on puretree queries(withouttext search) suchas the XPathMark queries, the SXSI system performs on par or better than the fastest known systems MonetDB and Qizx, (2) on queries that use text search, SXSI outperforms the existing systems by 1–3 orders of magnitude (depending on the size of the result set), and (3) with respect to memory consumption, SXSI outperforms all other systems for counting-only queries.  
253|Tinydb: An acquisitional query processing system for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. Categories and Subject Descriptors: H.2.3 [Database Management]: Languages—Query languages; H.2.4 [Database Management]: Systems—Distributed databases; query processing
254|Directed Diffusion: A scalable and robust communication paradigm for sensor networks|Advances in processor, memory and radio technology will enable small and cheap nodes capable of sensing, communication and computation. Networks of such nodes can coordinate to perform distributed sensing of environmental phenomena. In this paper, we explore the directed diffusion paradigm for such coordination. Directed diffusion is data-centric in that all communication is for named data. All nodes in a directed diffusion-based network are application-aware. This enables diffusion to achieve energy savings by selecting empirically good paths and by caching and processing data in-network. We explore and evaluate the use of directed diffusion for a simple remote-surveillance sensor network.
255|System architecture directions for networked sensors|Technological progress in integrated, low-power, CMOS communication devices and sensors makes a rich design space of networked sensors viable. They can be deeply embedded in the physical world or spread throughout our environment. The missing elements are an overall system architecture and a methodology for systematic advance. To this end, we identify key requirements, develop a small device that is representative of the class, design a tiny event-driven operating system, and show that it provides support for efficient modularity and concurrency-intensive operation. Our operating system fits in 178 bytes of memory, propagates events in the time it takes to copy 1.25 bytes of memory, context switches in the time it takes to copy 6 bytes of memory and supports two level scheduling. The analysis lays a groundwork for future architectural advances. 
256|ANALYSIS OF WIRELESS SENSOR NETWORKS FOR HABITAT MONITORING|  We provide an in-depth study of applying wireless sensor networks (WSNs) to real-world habitat monitoring. A set of system design requirements were developed that cover the hardware design of the nodes, the sensor network software, protective enclosures, and system architecture to meet the requirements of biologists. In the summer of 2002, 43 nodes were deployed on a small island off the coast of Maine streaming useful live data onto the web. Although researchers anticipate some challenges arising in real-world deployments of WSNs, many problems can only be discovered through experience. We present a set of experiences from a four month long deployment on a remote island. We analyze the environmental and node health data to evaluate system performance. The close integration of WSNs with their environment provides environmental data at densities previously impossible. We show that the sensor data is also useful for predicting system operation and network failures. Based on over one million 2 Polastre et. al. data readings, we analyze the node and network design and develop network reliability profiles and failure models.
258|The Cricket Location-Support System|This paper presents the design, implementation, and evaluation of Cricket, a location-support system for in-building, mobile, locationdependent applications. It allows applications running on mobile and static nodes to learn their physical location by using listeners that hear and analyze information from beacons spread throughout the building. Cricket is the result of several design goals, including user privacy, decentralized administration, network heterogeneity, and low cost. Rather than explicitly tracking user location, Cricket helps devices learn where they are and lets them decide whom to advertise this information to; it does not rely on any centralized management or control and there is no explicit coordination between beacons; it provides information to devices regardless of their type of network connectivity; and each Cricket device is made from off-the-shelf components and costs less than U.S. $10. We describe the randomized algorithm used by beacons to transmit information, the use of concurrent radio and ultrasonic signals to infer distance, the listener inference algorithms to overcome multipath and interference, and practical beacon configuration and positioning techniques that improve accuracy. Our experience with Cricket shows that several location-dependent applications such as in-building active maps and device control can be developed with little effort or manual configuration.  1 
259|The nesC language: A holistic approach to networked embedded systems|We present nesC, a programming language for networked embedded systems that represent a new design space for application developers. An example of a networked embedded system is a sensor network, which consists of (potentially) thousands of tiny, lowpower “motes, ” each of which execute concurrent, reactive programs that must operate with severe memory and power constraints. nesC’s contribution is to support the special needs of this domain by exposing a programming model that incorporates event-driven execution, a flexible concurrency model, and component-oriented application design. Restrictions on the programming model allow the nesC compiler to perform whole-program analyses, including data-race detection (which improves reliability) and aggressive function inlining (which reduces resource consumption). nesC has been used to implement TinyOS, a small operating system for sensor networks, as well as several significant sensor applications. nesC and TinyOS have been adopted by a large number of sensor network research groups, and our experience and evaluation of the language shows that it is effective at supporting the complex, concurrent programming style demanded by this new class of deeply networked systems.
260|Timing-Sync Protocol for Sensor Networks|Wireless ad-hoc sensor networks have emerged as an interesting and important research area in the last few years. The applications envisioned for such networks require collaborative execution of a distributed task amongst a large set of sensor nodes. This is realized by exchanging messages that are timestamped using the local clocks on the nodes. Therefore, time synchronization becomes an indispensable piece of infrastructure in such systems. For years, protocols such as NTP have kept the clocks of networked systems in perfect synchrony. However, this new class of networks has a large density of nodes and very limited energy resource at every node; this leads to scalability requirements while limiting the resources that can be used to achieve them. A new approach to time synchronization is needed for sensor networks.
261|TelegraphCQ: Continuous Dataflow Processing for an Uncertan World|Increasingly pervasive networks are leading towards a world where data is constantly in motion. In such a world, conventional techniques for query processing, which were developed under the assumption of a far more static and predictable computational environment, will not be sufficient. Instead, query processors based on adaptive dataflow will be necessary. The Telegraph project has developed a suite of novel technologies for continuously adaptive query processing. The next generation Telegraph system, called TelegraphCQ, is focused on meeting the challenges that arise in handling large streams of continuous queries over high-volume, highly-variable data streams. In this paper, we describe the system architecture and its underlying technology, and report on our ongoing implementation effort, which leverages the PostgreSQL open source code base. We also discuss open issues and our research agenda.
262|The Cougar Approach to In-Network Query Processing in Sensor Networks|The widespread distribution and availability of smallscale sensors, actuators, and embedded processors is transforming the physical world into a computing platform. One such example is a sensor network consisting of a large number of sensor nodes that combine physical sensing capabilities such as temperature, light, or seismic sensors with networking and computation capabilities. Applications range from environmental control, warehouse inventory, and health care to military environments. Existing sensor networks assume that the sensors are preprogrammed and send data to a central frontend where the data is aggregated and stored for offline querying and analysis. This approach has two major drawbacks. First, the user cannot change the behavior of the system on the fly. Second, conservation of battery power is a major design factor, but a central system cannot make use of in-network programming, which trades costly communication for cheap local computation.
263|A Transmission Control Scheme for Media Access in Sensor Networks|We study the problem of media access control in the novel regime of sensor networks, where unique application behavior and tight constraints in computation power, storage, energy resources, and radio technology have shaped this design space to be very different from that found in traditional mobile computing regime. Media access control in sensor networks must not only be energy efficient but should also allow fair bandwidth allocation to the infrastructure for all nodes in a multihop network. We propose an adaptive rate control mechanism aiming to support these two goals and find that such a scheme is most effective in achieving our fairness goal while being energy effcient for both low and high duty cycle of network traffic.
264|Routing indices for peer-to-peer systems|Finding information in a peer-to-peer system currently requires either a costly and vulnerable central index, or ooding the network with queries. In this paper we introduce the concept of Routing Indices (RIs), which allow nodes to forward queries to neighbors that are more likely to have answers. If a node cannot answer a query, it forwards the query to a subset of its neighbors, based on its local RI, rather than by selecting neighbors at random or by ooding the network by forwarding the query to all neighbors. We present three RI schemes: the compound, the hop-count, and the exponential routing indices. We evaluate their performance via simulations, and nd that RIs can improve performance by one or two orders of magnitude vs. a ooding-based system, and by up to 100 % vs. a random forwarding system. We also discuss the tradeo s between the di erent RIschemes and highlight the e ects of key design variables on system performance.
266|An Adaptive Query Execution System for Data Integration|Query processing in data integration occurs over networkbound, autonomous data sources. This requires extensions to traditional optimization and execution techniques for three reasons: there is an absence of quality statistics about the data, data transfer rates are unpredictable and bursty, and slow or unavailable data sources can often be replaced by overlapping or mirrored sources. This paper presents the  Tukwila data integration system, designed to support adaptivity at its core using a two-pronged approach. Interleaved planning and execution with partial optimization allows Tukwila  to quickly recover from decisions based on inaccurate estimates. During execution, Tukwila uses adaptive query operators such as the double pipelined hash join, which produces answers quickly, and the dynamic collector, which robustly and efficiently computes unions across overlapping data sources. We demonstrate that the Tukwila architecture extends previous innovations in adaptive execution (such as...
267|Approximate Query Processing Using Wavelets|Abstract. Approximate query processing has emerged as a cost-effective approach for dealing with the huge data volumes and stringent response-time requirements of today’s decision support systems (DSS). Most work in this area, however, has so far been limited in its query processing scope, typically focusing on specific forms of aggregate queries. Furthermore, conventional approaches based on sampling or histograms appear to be inherently limited when it comes to approximating the results of complex queries over high-dimensional DSS data sets. In this paper, we propose the use of multi-dimensional wavelets as an effective tool for general-purpose approximate query processing in modern, high-dimensional applications. Our approach is based on building wavelet-coefficient synopses of the data and using these synopses to provide approximate answers to queries. We develop novel query processing
268|Query Processing, Approximation, and Resource Management In a Data Stream Management System|This paper describes our ongoing work developing the Stanford Stream Data Manager (STREAM), a system for executing continuous queries over multiple continuous data streams. The STREAM system supports a declarative query language, and it copes with high data rates and query workloads by providing approximate answers when resources are limited. This paper describes specific contributions made so far and enumerates our next steps in developing a general-purpose Data Stream Management System.
269|Adaptive Query Processing: Technology in Evolution|As query engines are scaled and federated, they must cope with highly unpredictable and changeable environments. In the Telegraph project, we are attempting to architect and implement a continuously adaptive query engine suitable for global-area systems, massive parallelism, and sensor networks. To set the stage for our research, we present a survey of prior work on adaptive query processing, focusing on three characterizations of adaptivity: the frequency of adaptivity, the effects of adaptivity, and the extent of adaptivity. Given this survey, we sketch directions for research in the Telegraph project.  
270|Database System Issues in Nomadic Computing|Mobile computers and wireless networks are emerging technologies that will soon be available to a  wide variety of computer users. Unlike earlier generations of laptop computers, the new generation  of mobile computers can be an integrated part of a distributed computing environment, one in  which users change physical location frequently. The result is a new computing paradigm, nomadic  computing. This paradigm will affect the design of much of our current systems software, including  that of database systems.  This paper discusses in some detail the impact of nomadic computing on a number of traditional  database system concepts. In particular, we point out how the reliance on short-lived batteries  changes the cost assumptions underlying query processing. In these systems, power consumption  competes with resource utilization in the definition of cost metrics. We also discuss how the likelihood  of temporary disconnection forces consideration of alternative transaction processing pr...
271|Optimization techniques for queries with expensive methods|Object-Relational database management systems allow knowledgeable users to de ne new data types, as well as new methods (operators) for the types. This exibility produces an attendant complexity, which must be handled in new ways for an Object-Relational database management system to be e cient. In this paper we study techniques for optimizing queries that contain time-consuming methods. The focus of traditional query optimizers has been on the choice of join methods and orders; selections have been handled by \pushdown &#034; rules. These rules apply selections in an arbitrary order before as many joins as possible, using the assumption that selection takes no time. However, users of Object-Relational systems can embed complex methods in selections. Thus selections may take signi cant amounts of time, and the query optimization model must be enhanced. In this paper, we carefully de ne a query cost framework that incorporates both selectivity and cost estimates for selections. We develop an algorithm called Predicate Migration, and prove that it produces optimal plans for queries with expensive methods. We then describe our implementation of Predicate Migration in the commercial Object-Relational database management system Illustra, and discuss practical issues that a ect our earlier assumptions. We compare Predicate Migration to a variety of simpler optimization techniques, and demonstrate that Predicate Migration is the best general solution to date. The alternative techniques we presentmaybe useful for constrained workloads.
272|Adaptive Parallel Aggregation Algorithms|Aggregation and duplicate removal are common in SQL queries. However, in the parallel query processing literature, aggregate processing has received surprisingly little attention; furthermore, for each of the traditional parallel aggregation algorithms, there is a range of grouping selectivities where the algorithm performs poorly. In this work, we propose new algorithms that dynamically adapt, at query evaluation time, in response to observed grouping selectivities. Performance analysis via analytical modeling and an implementation on a workstation-cluster shows that the proposed algorithms are able to perform well for all grouping selectivities. Finally, we study the effect of data skew and show that for certain data sets the proposed algorithms can even outperform the best of traditional approaches. 1 Introduction  SQL queries are replete with aggregate and duplicate elimination operations. One measure of the perceived importance of aggregation is that in the proposed TPCD benchmark...
274|Bluetooth and Sensor Networks: A Reality Check|The current generation of sensor nodes rely on commodity components. The choice of the radio is particularly important as it impacts not only energy consumption but also software design (e.g., network self-assembly, multihop routing and in-network processing). Bluetooth is one of the most popular commodity radios for wireless devices. As a representative of the frequency hopping spread spectrum radios, it is a natural alternative to broadcast radios in the context of sensor networks. The question is whether Bluetooth can be a viable alternative in practice. In this paper, we report our experience using Bluetooth for the sensor network regime. We describe our tiny Bluetooth stack that allows TinyOS applications to run on Bluetooth-based sensor nodes, we present a multihop network assembly procedure that leverages Bluetooth&#039;s device discovery protocol, and we discuss how Bluetooth favorably impacts in-network query processing. Our results show that despite obvious limitations the Bluetooth sensor nodes we studied exhibit interesting properties, such as a good energy per bit sent ratio. This reality check underlies the limitations and some promises of Bluetooth for the sensor network regime.
275|Aggregation and Relevance in Deductive Databases|In this paper we present a technique to optimize queries on deductive databases that use aggregate operations such as min, max, and “largest Ic values. ” Our approach is based on an extended notion of relevance of facts to queries that takes aggregate operations into account. The approach has two parts: a rewriting part that labels predica.tes with “ag-gregate selections, ” and an evaluat,ion part. t.hat, makes use of “aggregate selections ” to detect that facts are irrelevant and discards them. The rewriting complements standard rewrit-ing algorithms like Magic sets, and the evaluation essentially refines Semi-Naive evaluation. 1
276|Query Optimization In Compressed Database Systems|Over the lastd ecad es, improvements in CPU speed have outpaced improvements in main memory and d isk access rates by ord ers of magnitud , enabling the use ofd ata compression techniques to improve the performance ofd atabase systems. Previous work d scribes the benefits of compression for numerical attributes, whered8 a is stored in compressed  format ond isk. Despite the abund3&amp; e of stringvalued  attributes in relational schemas there is little work on compression for string attributes in ad atabase context. Moreover, none of the previous work suitablyad2 esses the role of the query optimizer: During query execution, dD a is either eagerly d compressed when it is read into main memory, or dD a lazily stays compressed in main memory and is d compressed ond emand only. In this paper, we present an e#ective approach for dD abase compression based on lightweight, attribute-level compression techniques. We propose a Hierarchical ictionary Encod  ing strategy that intelligently selects the most e#ective compression method for string-valued attributes. We show that eager and lazy d compression strategies prod1 e suboptimal plans for queries involving compressed string attributes. We then formalize the problem of compressionaware query optimizationand propose one provably optimal  and two fast heuristic algorithms for selecting a query plan for relational schemas with compressed attributes; our algorithms can easily be integrated into existing cost-based query optimizers. Experiments using TPC-Hd atad emonstrate the impact of our string compression method s and show the importance of compression-aware query optimization. Our approach results in up to an or d r speed up over existing approaches.  1. 
277|The Design and Implementation of the Ariel Active Database Rule System|This paper describes the design and implementation of the Ariel DBMS and it&#039;s tightlycoupled forward-chaining rule system. The query language of Ariel is a subset of POSTQUEL, extended with a new production-rule sublanguage. Ariel supports traditional relational database query and update operations efficiently, using a System Rlike query processing strategy. In addition, the Ariel rule system is tightly coupled with query and update processing. Ariel rules can have conditions based on a mix of patterns, events, and transitions. For testing rule conditions, Ariel makes use of a discrimination network composed of a special data structure for testing single-relation selection conditions efficiently, and a modified version of the TREAT algorithm, called A-TREAT, for testing join conditions. The key modification to TREAT (which could also be used in the Rete algorithm) is the use of virtual ff-memory nodes which save storage since they contain only the predicate associated with the memory n...
278|DOMINO: Databases fOr MovINg Objects tracking|Consider a database that represents information about moving objects and their location. For example, for a database representing the location of taxi-cabs a typical query may be: retrieve the free cabs that are currently within 1 mile of 33 N. Michigan Ave., Chicago (to pick-up a customer); or for a trucking company database a typical query may be: retrieve the trucks that are currently within 1 mile of truck ABT312 (which needs assistance); or for a database representing the current location of objects in a battlefield a typical query may be: retrieve the friendly helicopters that are in a given region, or, retrieve the friendly helicopters that are expected to enter the region within the next 10 minutes. The queries may originate from the moving objects, or from stationary users. We will refer to applications with the above characteristics as moving-objects-database (MOD) applications, and to queries as the ones mentioned above as MOD queries.
279|From ethnography to design in a vineyard, in|This paper summarizes the process from ethnographic study of a vineyard to concept development and interaction design for a ubiquitous computing solution. It provides examples of vineyard interfaces and the lessons learned that could be generally applied to the interaction design of ubiquitous systems. These are: design for multiple perspectives on data, design for multiple access points, and design for varying levels of attention.
280|Query Optimization in Mobile Environments|We consider the issue of optimizing queries for a distributed processing in mobile environment. An interesting characteristic of mobile machines is that they depend on battery as a source of energy which may not be substantial enough. Hence, the appropriate optimization criterion in a mobile environment considers both resource utilization and energy consumption at the mobile client. In this scenario, the optimal plan for a query depends on the residual battery level of the mobile client and the load at the server. We approach this problem by compiling a query into a sequence of candidate plans, such that for any state of the client-server system, the optimal plan is one of the candidate plans. A general solution is proposed by adapting the partial order dynamic programming search algorithm [17, 18] (p.o dp) such that the coverset of the query is the set of candidate plans. We propose two novel algorithms, namely, the linear combinations algorithm and the linearset algorithm (referred t...
281|Online Dynamic Reordering|We present a mechanism for providing dynamic user control during long running, data-intensive operations. Users  can see partial results and dynamically direct the processing to suit their interests, thereby enhancing the interactivity  in several contexts such as online aggregation and large-scale spreadsheets.  In this paper, we develop a pipelining, dynamically tunable reorder operator for providing user control. Users  specify preferences for different data items based on prior results, so that data of interest is prioritized for early  processing. The reordering mechanism is efficient and non-blocking, and can be used over arbitrary data streams  from files and indexes, as well as continuous data feeds. We also investigate several policies for the reordering  based on the performance goals of various typical applications. We present performance results for reordering in the  context of an Online Aggregation implementation in Informix, and in the context of sorting and scrolling in...
282|Novel Distributed Query Optimization Model and Hybrid Query Optimization Algorithm ABSTRACT |Query optimization is the most critical phase in query processing. Query optimization in distributed databases explicitly needed in many aspects of the optimization process, this is not only increases the cost of optimization, but also changes the trade-offs involved in the optimization process significantly.This paper describes the synthetically evolution of query optimization methods from uniprocessor relational database systems to parallel database systems. We point out a set of parameters to characterize and compare query optimization methods, mainly: (i) type of algorithm (static or dynamic), (ii) working environments (re-optimization or re-scheduling) and (iii) level of modification. The major contributions of this paper are: (I) Understanding the mechanisms of query optimization methods with respect to the considered environments and their constraints (e.g. parallelism, distribution, heterogeneity, large scale, dynamicity of nodes). (ii) Study the problem of query optimization particular in term of heterogeneously environment and pointing out their main characteristics, which allow comparing them and help to Implement new query optimization algorithm and model. These contributions is led to performance enhancement of query optimization in distributed database system through classify by different QEPs and minimize the response time.
283|Efficient Optimization of a Class of Relational Expressions|The design of several database query languages has been influenced by Codd’s relational algebra. This paper discusses the difficulty of optimizing queries based on the relational algebra operations select, project, and join. A matrix, called a tableau, is proposed as a useful device for representing the value of a query, and optimization of queries is couched in terms of finding a minimal tableau equivalent to a given one. Functional dependencies can be used to imply additional equivalences among tableaux. Although the optimization problem is NP-complete, a polynomial time algorithm exists to optimize tableaux that correspond to an important subclass of queries.
284|Mariposa: A new architecture for distributed data|We describe the design of Mariposa, an experimental distributed data management system that provides high performance in an environment of high data mobility and heterogeneous host capabilities. The Mariposa design unifies the approaches taken by distributed file systems and distributed databases. In addition, Mariposa provides a general, flexible platform for the development of new algorithms for distributed query optimization, storage management, and scalable data storage structures. This flexibility is primarily due to a unique rule-based design that permits autonomous, local-knowledge decisions to be made regarding data placement, query execution location, and storage management. 1.
285|Distributed Database Systems: Where Are We Now|Distributed database technology is expected to have a significant impact on data processing in the upcoming years. With the introduction of commercial products, expectations are that distributed database management systems will by and large replace centralized ones within the next decade. In this paper, we reflect on the promises of distributed database technology, take stock of where we are, and discuss the issues that remain to be solved. We also highlight new research issues that arise with the introduction of new technology and the subsequent relaxation of some of the assumptions underlying current systems.
286|Green Query Optimization: Taming Query Optimization Overheads through Plan Recycling|PLASTIC [1] is a recently-proposed tool to help  query optimizers significantly amortize optimization  overheads through a technique of plan recycling.
287|X-Diff: A Fast Change Detection Algorithm for XMLDocuments|Over the next several years XML is likely to replace HTML as the standard web publishing  language and data transportation format. Since online information changes frequently, being able  to quickly detect changes in XML documents is important to Internet query systems, search  engines, and continuous query systems. Previous work in change detection on XML or other  hierarchically structured documents used an ordered tree model, in which left-to-right order  among siblings is important and it affects the change result. In this paper, we argue that an  unordered model (only ancestor relationships are significant) is more suitable for most database  applications. Using an unordered model, change detection is substantially harder than using  ordered model, but the change result that it generates is more accurate. We propose X-Diff, a fast  algorithm that integrates key XML structure characteristics with standard tree-to-tree correction  techniques. We also analyze the algorithm and study its performance.  1. 
288|PLASTIC: Reducing Query Optimization Overheads through Plan Recycling|INTRODUCTION  Query optimization is a computationally intensive process, especially for the complex queries that are typical in current data warehousing and mining applications. The inherent overheads of query optimization are compounded by the fact that a new query is typically optimized afresh, providing no opportunity to amortize these overheads over prior optimizations. While current commercial query optimizers do provide facilities for reusing execution plans generated for earlier queries (e.g. &#034;stored outlines&#034; in Oracle 9i), the query matching is extremely restrictive -- only if the incoming query has a close textual resemblance with one of the stored queries is the associated plan re-used to execute the new query.  Recently, in [1], we proposed a tool called PLASTIC (Plan Selection Through Incremental Clustering) to significantly increase the scope of plan reuse. The tool is based on the observation that even queries which differ in projection, selection and join predicates, as
289|The design of an acquisitional query processor for sensor networks|We discuss the design of an acquisitional query processor for data collection in sensor networks. Acquisitional issues are those that pertain to where, when, and how often data is physically acquired (sampled) and delivered to query processing operators. By focusing on the locations and costs of acquiring data, we are able to significantly reduce power consumption over traditional passive systems that assume the a priori existence of data. We discuss simple extensions to SQL for controlling data acquisition, and show how acquisitional issues influence query optimization, dissemination, and execution. We evaluate these issues in the context of TinyDB, a distributed query processor for smart sensor devices, and show how acquisitional techniques can provide significant reductions in power consumption on our sensor devices. 1.
290|Fine-grained network time synchronization using reference broadcasts|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
291|Continuously Adaptive Continuous Queries over Streams|We present a continuously adaptive, continuous query (CACQ) implementation based on the eddy query processing framework. We show that our design provides significant performance benefits over existing approaches to evaluating continuous queries, not only because of its adaptivity, but also because of the aggressive crossquery sharing of work and space that it enables. By breaking the abstraction of shared relational algebra expressions, our Telegraph CACQ implementation is able to share physical operators -- both selections and join state -- at a very fine grain. We augment these features with a grouped-filter index to simultaneously evaluate multiple selection predicates. We include measurements of the performance of our core system, along with a comparison to existing continuous query approaches.
292|Distributed Query Optimization by Query Trading |Large-scale distributed environments, where each node is completely autonomous and offers services to its peers through external communication, pose significant challenges to query processing and optimization. Autonomy is the main source of the problem, as it results in lack of knowledge about any particular node with respect to the information it can produce and its characteristics, e.g., cost of production or quality of produced results. Inter-node competition is another source of the problem, as it results in potentially inconsistent behavior of the nodes at different times. In this paper, inspired by e-commerce technology, we recognize queries (and query answers) as commodities and model query optimization as a trading negotiation process. Query parts (and their answers) are traded between nodes until deals are struck with some nodes for all of them. We identify the key parameters of this framework and suggest several potential alternatives for each one. In comparison to trading negotiations for e-commerce, query optimization faces unique new challenges that stem primarily from the complex structure of queries. We address these challenges through a particular instantiation of our framework focusing on the negotiation strategy, the contents of the negotiation messages, the evaluation metrics of the queries, and the optimization algorithm run on a “buying ” node. Finally, we conclude with some experiments that demonstrate the scalability and performance characteristics of our approach compared to those of traditional query optimization. 1
293|Improved algorithms for optimal winner determination in combinatorial auctions and generalizations|Combinatorial auctions can be used to reach efficient resource and task allocations in multiagent systems where the items are complementary. Determining the winners is NP-complete and inapproximable, but it was recently shown that optimal search algorithms do very well on average. This paper presents a more sophisticated search algorithm for optimal (and anytime) winner determination, including structural improvements that reduce search tree size, faster data structures, and optimizations at search nodes based on driving toward, identifying and solving tractable special cases. We also uncover a more general tractable special case, and design algorithms for solving it as well as for solving known tractable special cases substantially faster. We generalize combinatorial auctions to multiple units of each item, to reserve prices on singletons as well as combinations, and to combinatorial exchanges -- all allowing for substitutability. Finally, we present algorithms for determining the winners in these generalizations.
294|Answering Queries Using Views: A Survey|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously defined materialized views over the database, rather than accessing the database relations. The problem has recently received significant attention because of its relevance to a wide variety of data management problems. In query optimization, finding a rewriting of a query using a set of materialized views can yield a more efficient query execution plan. To support the separation of the logical and physical views of data, a storage schema can be described using views over the logical schema. As a result, finding a query execution plan that accesses the storage amounts to solving the problem of answering queries using views. Finally, the problem arises in data integration systems, where data sources can be described as precomputed views over a mediated schema. This article surveys the state of the art on the problem of answering queries using views, and synthesizes the disparate works into a coherent framework. We describe the different applications of the problem, the algorithms proposed to solve it and the relevant theoretical results.
295|Issues in Automated Negotiation and Electronic Commerce: Extending the Contract Net Framework|In this paper we discuss a number of previously  unaddressed issues that arise in automated negotiation  among self-interested agents whose  rationality is bounded by computational complexity.  These issues are presented in the context  of iterative task allocation negotiations.  First, the reasons why such agents need to  be able to choose the stage and level of commitment  dynamically are identified. A protocol  that allows such choices through conditional  commitment breaking penalties is presented.  Next, the implications of bounded rationality  are analyzed. Several tradeoffs between  allocated computation and negotiation  benefits and risk are enumerated, and the necessity  of explicit local deliberation control is  substantiated. Techniques for linking negotiation  items and multiagent contracts are presented  as methods for escaping local optima in  the task allocation process. Implementing both  methods among self-interested bounded rational  agents is discussed. Finally, the ...
296|An economic paradigm for query processing and data migration|Many new database applications require very large volumes of data. Mariposa is a data base system under construction at Berkeley responding to this need. Mariposa objects can be stored over thousands of autonomous sites and on memory hierarchies with very large capacity. This scale of the system leads to complex query execution and storage management issues, unsolvable in practice with traditional techniques. We propose an economic paradigm as the solution. A query receives a budget which itspends to obtain the answers. Each site attempts to maximize income by buying and selling storage objects, and processing queries for locally stored objects. We present the protocols which underlie the Mariposa economy. 1.
297|Manufacturing Experience with the Contract Net|We are implementing a control system for a discrete manufacturing environment that partitions tasks using a negotiation protocol like the contract net described by Smith and Davis [24,25,26,3]. The application domain differs in interesting ways from those to which contract nets have previously been applied. This report
298|Answering complex SQL queries using automatic summary tables|We investigate the problem of using materialized views to answer SQL queries. We focus on modern decision-support queries, which involve joins, arithmetic operations and other (possibly user-defined) functions, aggregation (often along multiple dimensions), and nested subqueries. Given the complexity of such queries, the vast amounts of data upon which they operate, and the requirement for interactive response times, the use of materialized views (MVs) of similar complexity is often mandatory for acceptable performance. We present a novel algorithm that is able to rewrite a user query so that it will access one or more of the available MVs instead of the base tables. The algorithm extends prior work by addressing the new sources of complexity mentioned above, that is, complex expressions, multidimensional aggregation, and nested subqueries. It does so by relying on a graphical representation of queries and a bottomup, pair-wise matching of nodes from the query and MV graphs. This approach offers great modularity and extensibility, allowing for the rewriting of a large class of queries. 1.
299|Multi-Attribute Auctions for Electronic Procurement|Auctions are a fundamental mechanism to automate negotiations in electronic commerce. We consider a class of multi-lateral negotiation situations, which requires negotiation on multiple attributes of a deal. These situations are typical for corporate procurement. Current auction implementations do not support these negotiations adequately. In the paper we propose multi-attribute auctions, an economic mechanism, which supports negotiation on multiple attributes of a deal. In this approach we combine decision analysis techniques and single -sided auction mechanisms to allocate tasks. We outline the theoretical questions associated with multiattribute auctions and describe an implementation of the proposed mechanism. In this context, we tackle several important issues concerning the design of electronic market places.
302|Semantic XML query optimization |The objective of my research is to carry out query optimization of XML queries based on the content of the documents. Now eXtensible Markup Language (XML) [1] is widely accepted by website constructors and programmers because of its capability and its flexibility in storing and transferring semistructured data. Query transformation is the method used to improve the query execution time of the database system. Some research has been done with XML query transformation. To date, these works concentrate on the structure of XML documents and little work has been done that takes into account the content of XML documents. With XML documents there is a need to concentrate more on the content because XML is more flexible than other databases and XML documents do not typically have a schema. One of the disadvantages of the flexibility is that the structure of XML documents is inconsistent and unpredictable. Web Ontology Language (OWL) [4] is an ontology language that can formally describe the meaning of terminology used in XML documents. In contrast to other ontology languages, such as RDF (Resource Description Framework) [6], OWL provides more meaningful terms to describe the relationships of elements of the XML document. With these description methods, the elements in XML document can be grouped and described based on the features of their contents. In this research, we will undertake work in the content analysis of XML documents and how the content analysis can help XML query transformation. All the elements in an XML document will be analyzed and classified with their content features using an On Line Analytical Processing (OLAP) [10] system. We choose the OLAP system as the analysis tool because it has two important features: a multidimensional cube can help us to analyze the XML documents from different dimensions and the hierarchy provides a method to classify the elements with their content. The generated groups will be described with their features in an OWL document. With the generated OWL documents, the input XML query can be transformed to a more efficient one, which has the same function as the original query. Currently little work on building OLAP systems for XML databases has been done because of XML’s unpredictable structure. The existing methods, which can present
303|Heuristic Query Optimization|This report explains the implementation of an algorithm to optimize a QT with heuristic optimization rules. These rules were taken from [1] chapter 16 and [2] chapter 11. Heuristic optimization rules are based on properties of operations as mathematical operations in the relational algebra. Summaries of these properties can be found both in [1] and [2] also. These properties give the following heuristic rules for query optimization: 1. Perform SELECT operations as far down the tree as possible. This has the effect of reducing the number of tuples in later binary operations which are highly expensive. 2. Perform PROJECT operations as far down the tree as possible. This has the effect of reducing the number of attributes in each tuple and reduces the memory requirements, attempting to cut down on secondary storage usage. 3. Combine successive SELECT operations into one composite SELECT operation and successive PROJECT operations into one composite PROJECT operation. 4. Combine a PRODUCT followed by a SELECT into a JOIN with the selection condition in the  SELECT. Also, combine a JOIN followed by a SELECT into a new JOIN which incorporates the selection condition. The remainder of this report describes an implementation of an algorithm which, given a suitable representation of a QT, optimizes according to the above heuristic rules.  2 Implementation The implementation of the heuristic optimization is done here in multiple passes. This modularity between the steps of the algorithm makes it easy to replace one or more of the existing routines with a user-supplied routine, such as might be done in an academic setting. The command line interface to the program is as follows: % Query !input file? !output file base? !input file? is the name of the input file containing the QT ...
304|The TSIMMIS Project: Integration of Heterogeneous Information Sources |The goal of the Tsimmis Project is to develop tools that facilitate the rapid integration of heterogeneous information sources that may include both structured and unstructured data. This paper gives an overview of the project, describing components that extract properties from unstructured objects, that translate information into a common object model, that combine information from several sources, that allow browsing of information, and that manage constraints across heterogeneous sites. Tsimmis is a joint project between Stanford and the IBM Almaden Research Center.
305|Relational Databases for Querying XML Documents:  Limitations and Opportunities|XML is fast emerging as the dominant standard for representing data in the World Wide Web. Sophisticated query engines that allow users to effectively tap the data stored in XML documents will be crucial to exploiting the full power of XML. While there has been a great deal of activity recently proposing new semistructured data models and query languages for this purpose, this paper explores the more conservative approach of using traditional relational database engines for processing XML documents conforming to Document Type Descriptors (DTDs). To this end, we have developed algorithms and implemented a prototype system that converts XML documents to relational tuples, translates semi-structured queries over XML documents to SQL queries over tables, and converts the results to XML. We have qualitatively evaluated this approach using several real DTDs drawn from diverse domains. It turns out that the relational approach can handle most (but not all) of the semantics of semi-structured queries over XML data, but is likely to be effective only in some cases. We identify the causes for these limitations and propose certain extensions to the relational 
306|Complexity of answering queries using materialized views|WC study the complexity of the problem of answering queries using materinlized views, This problem has attracted a lot of attention re-cently because of its relevance in data integration. Previous work considered only conjunctive view definitions. We examine the con-sequences of allowing more expressive view definition languages. Tl~olanguagcsweconsiderforviewdefinitionsanduserqueriesare: conjunctive qucrics with inequality, positive queries, datalog, and first-order logic. We show that the complexity of the problem de-pcnds on whether views are assumed to store all the tuples that sat-isfy the view definition, or only a subset of it. Finally, we apply the results to the view consistency and view self-maintainability prob-lems which nrise in data warehousing. 1
307|A scalable algorithm for answering queries using views|The problem of answering queries using views is to find efficient methods of answering a query using a set of previously materialized views over the database, rather than accessing the database relations. The problem has received significant attention because of its relevance to a wide variety of data management problems, such as data integration, query optimization, and the maintenance of physical data independence. To date, the performance of proposed algorithms has received very little attention, and in particular, their scale up in the presence of a large number of views is unknown. We first analyze two previous algorithms, the bucket algorithm and the inverse-rules algorithm, and show their deficiencies. We then describe the MiniCon algorithm, a novel algorithm for finding the maximally-contained rewriting of a conjunctive query using a set of conjunctive views. We present the first experimental study of algorithms for answering queries using views. The study shows that the MiniCon algorithm scales up well and significantly outperforms the previous algorithms. Finally, we describe an extension of the MiniCon algorithm to handle comparison predicates, and show its performance experimentally.
308|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
309|Quilt: An XML Query Language for Heterogeneous Data Sources|The World Wide Web promises to transform human society by  making virtually all types of information instantly available  everywhere. Two prerequisites for this promise to be realized are  a universal markup language and a universal query language. The  power and flexibility of XML make it the leading candidate for a  universal markup language. XML provides a way to label  information from diverse data sources including structured and  semi-structured documents, relational databases, and object  repositories. Several XML-based query languages have been  proposed, each oriented toward a specific category of information. Quilt is a new proposal that attempts to unify concepts from  several of these query languages, resulting in a new language that  exploits the full versatility of XML. The name Quilt suggests both  the way in which features from several languages were assembled  to make a new query language, and the way in which Quilt queries can combine information from diverse data sou...
310|Answering Recursive Queries Using Views|We consider the problem of answering datalog queries using materialized views. The ability to answer queries using views is crucial in the context of information integration. Previous work on answering queries using views restricted queries to being conjunctive. We extend this work to general recursive queries: Given a datalog program P and a set of views, is it possible to find a datalog program that is equivalent to P and only uses views as EDB predicates? In this paper, we show that the problem of whether a datalog program can be rewritten into an equivalent program that only uses views is undecidable. On the other hand, we prove that a datalog program P can be effectively rewritten into a program that only uses views, that is contained in P,  and that contains all programs that only use views and are contained in P. As a consequence, if there exists a program equivalent to P that only uses views, then our construction is guaranteed to yield a program equivalent to P.  1 Introductio...
311|Description Logics in Data Management|Description logics and reasoners, which are descendants of the kl-one language, have been studied in depth in Artificial Intelligence. After a brief introduction, we survey in this paper their application to the problems of information management, using the framework of an abstract information server equipped with several operations -- each involving one or more languages. Specifically, we indicate how one can achieve enhanced access to data and knowledge by using descriptions in languages for schema design and integration, queries, answers, updates, rules, and constraints.
312|Index Selection for OLAP|On-line analytical processing (OLAP) is a recent and important application of database systems. Typically, OLAP data is presented as a multidimensional &#034;data cube.&#034; OLAP queries are complex and can take many hours or even days to run, if executed directly on the raw data. The most common method of reducing execution time is to precompute some of the queries into summary tables (subcubes of the data cube) and then to build indexes on these summary tables. In most commercial OLAP systems today, the summary tables that are to be precomputed are picked first, followed by the selection of the appropriate indexes on them. A trial-and-error approach is used to divide the space available between the summary tables and the indexes. This two-step process can perform very poorly. Since both summary tables and indexes consume the same resource -- space -- their selection should be done together for the most efficient use of space. In this paper, we give algorithms that automate the selection of summary tables and indexes. In particular, we present a family of algorithms of increasing time complexities, and prove strong performance bounds for them. The algorithms with higher complexities have better performance bounds. However, the increase in the performance bound is diminishing, and we show that an algorithm of moderate complexity can perform fairly close to the optimal.
313|Materialized View Selection in a Multidimensional Database|A multidimensional database is a data repository that supports the efficient execution of complex business decision queries. Query response can be significantly improved by storing an appropriate set of materialized views. These views are selected from the multidimensional lattice whose elements represent the solution space of the problem. Several techniques have been proposed in the past to perform the selection of materialized views for databases with a reduced number of dimensions. When the number and complexity of dimensions increase, the proposed techniques do not scale well. The technique we are proposing reduces the solution space by considering only the relevant elements of the multidimensional lattice. An additional statistical analysis allows a further reduction of the solution space.  1 Introduction  A multidimensional database (MDDB) is a data repository that provides an integrated environment for decision support queries that require complex aggregations on huge amounts of...
314|Representing and Using Interschema Knowledge in Cooperative Information Systems|Managing interschema knowledge is an essential task when dealing with cooperative information systems. We propose a logical approach to the problem of both expressing interschema knowledge, and reasoning about it. In particular, we set up a structured representation language for expressing semantic interdependencies between classes belonging to different database schemas, and present a method for reasoning over such interdependencies. The language and the associated reasoning technique makes it possible to build a logic-based module that can draw useful inferences whenever the need arises of both comparing and combining the knowledge represented in the various schemas. Notable examples of such inferences include checking the coherence of interschema knowledge, and providing integrated access to a cooperative information system.
315|Tableau Techniques For Querying Information Sources Through Global Schemas|. The foundational homomorphism techniques introduced by Chandra and Merlin for testing containment of conjunctive queries have recently attracted renewed interest due to their central role in information integration applications. We show that generalizations of the classical tableau representation of conjunctive queries are useful for computing query answers in information integration systems where information sources are modeled as views defined on a virtual global schema. We consider a general situation where sources may or may not be known to be correct and complete. We characterize the set of answers to a global query and give algorithms to compute a finite representation of this possibly infinite set, as well as its certain and possible approximations. We show how to rewrite a global query in terms of the sources in two special cases, and show that one of these is equivalent to the Information Manifold rewriting of Levy et al. 1 Introduction Information Integration systems [Ull...
316|Recursive Query Plans for Data Integration|Generating query-answering plans for data integration systems requires to translate a user query, formulated in terms of a mediated schema, to a query that uses relations that are actually stored in data sources. Previous solutions to the translation problem produced sets of conjunctive plans, and were therefore limited in their ability to handle recursive queries and to exploit data sources with binding-pattern limitations and functional dependencies that are known to hold in the mediated schema. As a result, these plans were incomplete w.r.t. sources encountered in practice (i.e., produced only a subset of the possible answers). We describe the novel class of recursive query answering plans, which enables us to settle three open problems. First, we describe an algorithm for finding a query plan that produces the maximal set of answers from the sources for arbitrary recursive queries. Second, we extend this algorithm to use the presence of functional and full dependencies in the media...
317|Logic-Based Techniques In Data Integration|The data integration problem is to provide uniform access to multiple heterogeneous information sources available online (e.g., databases on the WWW). This problem has recently received considerable attention from researchers in the fields of Artificial Intelligence and Database Systems. The data integration problem is complicated by the facts that (1) sources contain closely related and overlapping data, (2) data is stored in multiple data models and schemas, and (3) data sources have differing query processing capabilities. A key element in a data integration system is the language used to describe the contents and capabilities of the data sources. While such a language needs to be as expressive as possible, it should also enable to efficiently address the main inference problem that arises in this context: to translate a user query that is formulated over a mediated schema into a query on the local schemas. This paper describes several lanaguages for describing contents of data sources, ...
318|Optimizing queries using materialized views: A practical, scalable solution|Materialized views can provide massive improvements in query processing time, especially for aggregation queries over large tables. To realize this potential, the query optimizer must know how and when to exploit materialized views. This paper presents a fast and scalable algorithm for determining whether part or all of a query can be computed from materialized views and describes how it can be incorporated in transformation-based optimizers. The current version handles views composed of selections, joins and a final group-by. Optimization remains fully cost based, that is, a single “best ” rewrite is not selected by heuristic rules but multiple rewrites are generated and the optimizer chooses the best alternative in the normal way. Experimental results based on an implementation in Microsoft SQL Server show outstanding performance and scalability. Optimization time increases slowly with the number of views but remains low even up to a thousand.
319|Selection of Views to Materialize Under a Maintenance Cost Constraint|. A data warehouse stores materialized views derived from one or more sources for the purpose of efficiently implementing decisionsupport or OLAP queries. One of the most important decisions in designing a data warehouse is the selection of materialized views to be maintained at the warehouse. The goal is to select an appropriate set of views that minimizes total query response time and/or the cost of maintaining the selected views, given a limited amount of resource such as materialization time, storage space, or total view maintenance time. In this article, we develop algorithms to select a set of views to materialize in a data warehouse in order to minimize the total query response time under the constraint of a given total view maintenance time. As the above maintenance-cost view-selection problem is extremely intractable, we tackle some special cases and design approximation algorithms. First, we design an approximation greedy algorithm for the maintenance-cost view-selection prob...
320|Sound and efficient closed-world reasoning for planning|Closed-world inference is the process of determining that a logical sentence is false based on its absence from a knowledge base, or the inability to derive it. This process is essential for planning with incomplete information. We describe a novel method for closed-world inference and update over the first-order theories of action used by planning algorithms such as NONLIN, TWEAK, and UCPOP. We show the method to be sound and efficient, but incomplete. In our experiments, closed-world inference consistently averaged about 2 milliseconds, while updates averaged approximately 1.2 milliseconds. We incorporated the method into the XII planner, which supports our Internet Softbot (software robot). The method cut the number of actions executed by the Softbot bya factor of one hundred, and resulted in a corresponding speedup to XII. 1
321|On the equivalence of recursive and nonrecursive Datalog programs|vardi Abstract: We study the problem of determining whether a given recursive Datalog program is equivalent to a given nonrecursive Datalog program. Since nonrecursive Dat-alog programs are equivalent to unions of conjunctive queries, we study also the problem of determining whether a given recursive Datalog program is contained in a union of con-junctive queries. For this problem, we prove doubly exponential upper and lower time bounds. For the equivalence problem, we prove triply exponential upper and lower time bounds. 1
322|Using probabilistic information in data integration|The goal of a mediator system is to provide users a uniform interface to the multitude of informa-tion sources. To translate user queries, given in a mediated schema, to queries on the data sources, mediators rely on explicit mappings between the contents of the data sources and the meanings of the relations in the mediated schema. Thus far, contents of data sources were described qualitatively. In this paper we describe the use of quantitative information in the form of proba-bilistic knowledge in mediator systems. We con-sider several kinds of probabilistic information: information about overlap between collections in the mediated schema, coverage of the information sources, and degrees of overlap between informa-tion sources. We address the problem of ordering accesses to multiple information sources, in order to maximize the likelihood of obtaining answers as early as possible. We describe a declarative for-malism for specifying these kinds of probabilistic information, and we propose algorithms for order-ing the information sources. Finally, we discuss a preliminary experimental evaluation of these al-gorithms on the domain of bibliographic sources available on the WWW. 1
323|Query Rewriting for Semistructured Data  | We address the problem of query rewriting for TSL, a language for querying semistructured data. We develop and present an algorithm that, given a semistructured query q and a set of semistructured views V, finds rewriting queries, i.e., queries that access the views and produce the same result as q. Our algorithm is based on appropriately generalizing containment mappings, the chase, and query composition- techniques that were developed for structured, relational data. We also develop an algorithm for equivalence checking of TSL queries. We show that the algorithm is sound and complete for TSL, i.e., it always finds every non-trivial TSL rewriting query of q, and we discuss its complexity. We extend the rewriting algorithm to use some forms of structural constraints (such as DTDs) and find more opportunities for query rewriting.
324|Rewriting Aggregate Queries Using Views|We investigate the problem of rewriting queries with aggregate operators using views that mayormay not contain aggregate operators. A rewriting of a query is a second query that uses view predicates such that evaluating first the views and then the rewriting yields the same result as evaluating the original query. In this sense, the original query and the rewriting are equivalent modulo the view definitions. The queries and views we consider correspond to unnested SQL queries, possibly with union, that employ the operators min, max, count, and sum. Our approach is based on syntactic characterizations of the equivalence of aggregate queries. One contribution of this paper are characterizations of the equivalence of disjunctive aggregate queries, which generalize our previous results for the conjunctive case. For each operator &amp;alpha;, we introduce several types of queries using views as candidates for rewritings. We unfold such a candidate by replacing each occurrence of a view predicate with ...
325|Using Schematically Heterogeneous Structures|Schematic heterogeneity arises when information that is represented as data under one schema, is represented within the schema (as metadata) in another. Schematic heterogeneity is an important class of heterogeneity that arises frequently in integrating legacy data in federated or data warehousing applications. Traditional query languages and view mechanisms are insufficient for reconciling and translating data between schematically heterogeneous schemas. Higher order query languages, that permit quantification over schema labels, have been proposed to permit querying and restructuring of data between schematically disparate schemas. We extend this work by considering how these languages can be used in practice. Specifically, we consider a restricted class of higher order views and show the power of these views in integrating legacy structures. Our results provide insights into the properties of restructuring transformations required to resolve schematic discrepancies. In addition, we ...
326|Query Planning and Optimization in Information Integration|Information integration systems, also knows as mediators, information brokers, or information gathering agents, provide uniform user interfaces to varieties of different information sources. With corporate databases getting connected by intranets, and vast amounts of information becoming available over the Internet, the need for information integration systems is increasing steadily. Our work focusses on query planning in such systems...
327|A Formal Perspective on the View Selection Problem|The view selection problem is to choose a set of views to materialize over a database schema, such that the cost of evaluating a set of workload queries is minimized and such that the views t into a prespeci ed storage constraint. The two main applications of the view selection problem are materializing views in a database to speed up query processing, and selecting views to materialize in a data warehouse to answer decision support queries. We describe several fundamental results concerning the view selection problem. We consider the problem for views and workloads that consist of equalityselection, project and join queries, and show that the complexity of the problem depends crucially on the quality of the estimates that a query optimizer has on the size of the views it is considering to materialize. When a query optimizer has good estimates of the sizes of the views, we show that an optimal choice of views may involve a number of views that is exponential in the size of the database schema. On the other hand, when an optimizer uses standard estimation heuristics, we show that the number of necessary views and the expression size of each view are polynomially bounded. 1
328|Query Planning in Infomaster|Infomaster is an information integration system. It provides integrated access to distributed, heterogeneous information sources, thus giving its users the illusion of a centralized, homogeneous information system. Infomaster is the first such system that is able to handle arbitrary positive relational algebra user queries and database descriptions. It is able efficiently to use integrity constraints and local completeness information for optimization. The system has been deployed in a wide variety of application areas, including engineering, logistics, and electronic commerce. This article provides a much requested overview of the query processing method used by Infomaster.  1 Introduction  In recent years, there has been a dramatic growth in the number of publicly accessible databases on the Internet, and all indicators suggest that this growth should continue in the years to come. Unfortunately, retrieving information from these databases is not easy for several reasons. The first c...
329|Query containment for data integration systems|The problem of query containment is fundamental to many aspects of database systems,including query optimization,determining independence of queries from updates,and rewriting queries using views. In the data-integration framework,however,the standard notion of query containment does not suffice. We define relative containment,which formalizes the notion of query containment relative to the sources available to the data-integration system. First,we provide optimal bounds for relative containment for several important classes of datalog queries,including the common case of conjunctive queries. Next,we provide bounds for the case when sources enforce access restrictions in the form of binding pattern constraints. Surprisingly,we show that relative containment for conjunctive queries is still decidable in this case,even though it is known that finding all answers to such queries may require a recursive datalog program over the sources. Finally,we provide tight bounds for variants of relative containment when the queries and source descriptions may contain comparison predicates.
330|Optimizing Recursive Information Gathering Plans|In this paper we describe two optimization techniques that are specially tailored for information gathering. The first is a greedy minimization algorithm that minimizes an information gathering plan by removing redundant and overlapping information sources without loss of completeness. We then discuss a set of...
331|Physical Data Independence, Constraints, and Optimization with Universal Plans|We present an optimization method and algorithm designed for three objectives: physical data independence, semantic optimization, and generalized tableau minimization. The method relies on generalized forms of chase and &#034;backchase&#034; with constraints (dependencies). By using dictionaries (finite functions) in physical schemas we can capture with constraints useful access structures such as indexes, materialized views, source capabilities, access support relations, gmaps, etc. The search space for query plans is de ned and enumerated in a novel manner: the chase phase rewrites the original query into a &#034;universal&#034; plan that integrates all the access structures and alternative pathways that are allowed by applicable constraints. Then, the backchase phase produces optimal plans by eliminating various combinations of redundancies, again according to constraints. This method is applicable (sound) to a large class of queries, physical access structures, and semantic constraints. We prove that it is in fact complete for &#034;path-conjunctive&#034; queries and views with complex objects, classes and dictionaries, going beyond previous theoretical work on processing queries using materialized views. 
332|An Equational Chase for Path-Conjunctive Queries, Constraints, and Views|We consider the class of path-conjunctive queries and constraints (dependencies) defined over complex values with dictionaries.
333|Query Folding with Inclusion Dependencies|Query folding is a technique for determining how a query may be answered using a given set of resources, which may include materialized views, cached results of previous queries, or queries answerable by other databases. The power of query folding can be considerably enhanced by taking into account integrity constraints that are known to hold on base relations. This paper describes an extension of query folding that utilizes inclusion dependencies to find foldings of queries that would otherwise be overlooked. We describe a complete strategy for finding foldings in the presence of inclusion dependencies and present a basic algorithm that implements that strategy. We also describe extensions to this algorithm when both inclusion and functional dependencies are considered.
334|A Chase Too Far?|In a previous paper we proposed a novel method for generating alternative query plans that uses chasing (and back-chasing) with logical constraints. The method brings together use of indexes, use of materialized views, semantic optimization and join elimination (minimization). Each of these techniques is known separately to be beneficial to query optimization. The novelty of our approach is in allowing these techniques to interact systematically, eg. nontrivial use of indexes and materialized views may be enabled only by semantic constraints.
335|Generating Efficient Plans for Queries Using Views|We study the problem of generating efficient, equivalent rewritings using views to compute the answer to a query. We take the closed-world assumption, in which views are materialized from base relations, rather than views describing sources in terms of abstract predicates, as is common when the open-world assumption is used. In the closedworld model, there can be an infinite number of different rewritings that compute the same answer, yet have quite different performance. Query optimizers take a logical plan (a rewriting of the query) as an input, and generate efficient physical plans to compute the answer. Thus our goal is to generate a small subset of the possible logical plans without missing an optimal physical plan.
336|Answering queries using views in description logics|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of views. This problem is relevant in several fields, such as information integration, data warehousing, query optimization, etc. In this paper we address the problem of query answering using views for nonrecursive datalog queries embedded in a Description Logics (equipped with n-ary relations) knowledge base. We present the following results. Query answering using views is decidable in all cases. Specifically, if the set of all objects in the knowledge base coincides with the set of objects stored in the views (closed domain assumption), the problem is coNP complete, whereas if the knowledge base may contain additional objects (open domain assumption) it is solvable in double exponential time.  
337|Query Optimization Using Local Completeness|We consider the problem of query plan optimization in information brokers. Information brokers are programs that facilitate access to collections of information sources by hiding source-speci c peculiarities and presenting uniform query interfaces. It is unrealistic to assume that data stored by information sources is complete. Therefore, current implementations of information brokers query all possibly relevant information sources in order not to miss any answers. This approach isvery costly. We show how a weaker form of completeness, local completeness, can be used to minimize the number of accesses to information sources.
338|Efficiently Ordering Query Plans for Data Integration|interface to a multitude of data sources. Given a user query formulated in this interface, the system translates it into a set of query plans. Each plan is a query formulated over the data sources, and specifies a way to access sources and combine data to answer the user query.
339|Speeding Up Inferences Using Relevance Reasoning: A Formalism and Algorithms|Irrelevance reasoning refers to the process in which a system reasons about which parts  of its knowledge are relevant (or irrelevant) to a specific query. Aside from its importance  in speeding up inferences from large knowledge bases, relevance reasoning is crucial in advanced  applications such as modeling complex physical devices and information gathering in  distributed heterogeneous systems. This article presents a novel framework for studying the  various kinds of irrelevance that arise in inference and efficient algorithms for relevance reasoning. We present a 
340|Linearly Bounded Reformulations of Conjunctive Databases (Extended Abstract)  (2000) |Database reformulation is the process of rewriting the data  and rules of a deductive database in a functionally equivalent manner.
341|The Identification of Missing Information Resources through the Query Difference Operator|In this paper we consider the processing of queries posed over multiple information resources that advertise their contents in terms of globally available, domain-specific ontologies. We describe a technique to identify the exact portion of a user&#039;s query that may not be answered by the set of available information agents. This is achieved by reasoning over the advertisements of the agents relative to the user&#039;s query. Our technique is based on the realization that the set difference of the queries q 1 and q 2 may be computed as a syntactic manipulation of the expressions q 1 and q 2 for a well defined subset of the relational algebra over a restricted class of relational schemas. That is to say, one may, without materializing data, take the expressions for q 1 and q 2 , apply the query difference formula to yield q 3 , and be guaranteed that q 3 is logically equivalent to q 1 , q 2 . With this Query Difference operator defined, the ability to compute query intersection, subsumption and equivalence follow. These claims are formally defined and proven and an example from an online movie guide domain is provided.  In addition to the identification of missing resource agents, we anticipate a number of other applications of the Query Difference operator. This includes, but is not limited to, limiting the generality of dynamically constructed user queries, efficient query planning, and monitoring and controlling access to sensitive information.   
342|Rule-Based Query Optimization, Revisited|We present an overview and initial performance assessment of a rule-based query optimizer written in VenusDB. VenusDB is an active-database rule language embedded in C++. Following the developments in extensible database query optimizers, first in rule-based form followed by optimizers written as object-oriented programs, the VenusDB optimizer avails the advantages of both. To date, development of rule-based query optimizers have included the definition and implementation of custom rule languages. Thus, extensibility required detailed understanding and often further development of the underlying search mechanism of the rule system. Objectoriented query optimizers appear to have achieved their goals with respect to a clear organization and encapsulation  of an optimizer&#039;s elements. They do not, however, provide for the concise, declarative expression of domain specific heuristics. Our experience demonstrates that a rule-based query optimizer developed in VenusDB can be well structured, ...
344|On the performance of lazy matching in production systems|Production systems are an established method for encoding knowledge in an expert system. The semantics of produc-tion system languages and the concomitant algorithms for their evaluation, RETE and TREAT, enumerate the set of rule instantiations and then apply a strategy that selects a single instantiation for firing. Often rule instantiations are calculated and never fired. In a sense, the time and space re-quired to eagerly compute these unfired instantiations is wasted. This paper presents preliminary results about a new match technique, lazy matching. The lazy match algorithm folds the selection strategy into the search for instantiations, such that only one instantiation is computed per cycle. The algorithm improves the worst-case asymptotic space com-plexity of incremental matching. Moreover, empirical and
345|OPT++: An Object-Oriented Implementation for Extensible Database Query Optimization|Paper Number 323 In this paper we describe the design and implementation of OPT++, a tool for Extensible Database Query Optimization that uses an object-oriented design to simplify the task of implementing, extending, and modifying an optimizer. Building an optimizer using OPT++ makes it easy to extend the query algebra (to add new query algebra operators and physical implementation algorithms to the system), easy to change the search space explored, and also easy to change the search strategy used. Furthermore, OPT++ comes equipped with a number of search strategies that are available for use by an Optimizer{ Implementor. OPT++ considerably simpli es both, the task of implementing an optimizer for a new database system, and the task of experimenting with various optimization techniques and strategies to decide what techniques are best suited for that database system. We present the results of performance studies which validate our design and show that, in spite of its exibility, OPT++ can be used to build e cient optimizers. 1
346|Processing Queries for First-Few Answers|Special support for quickly finding the first-few answers of a query is already appearing in commercial database systems. This support is useful in active databases, when dealing with potentially unmanageable query results, and as a declarative alternative to navigational techniques. In this paper, we discuss query processing techniques for first-answer queries. We provide a method for predicting the cost of a first-answer query plan under an execution model that attempts to reduce wasted effort in join pipelining. We define new statistics necessary for accurate cost prediction, and discuss techniques for obtaining the statistics through traditional statistical measures (e.g. selectivity) and semantic data properties commonly specified through modern OODB and relational schemas. The proposed techniques also apply to all-answer query processing when optimizing for fast delivery of the initial query results. 1 Introduction Traditional methods for query processing, primarily those based ...
347|Prairie: A rule specification framework for query optimizers|From our experience, current rule-based query optimizers do not provide a very intuitive and well-defined framework to define rules and actions. To remedy this situation, we propose an extensible and structured algebraic framework called Prairie for specifying rules. Prairie facilitates rule-writing by enabling a user to write rules and actions more quickly, correctly and in an easy-to-understand and easy-to-debug manner. Query optimizers consist of three major parts: a search space, a cost model and a search strategy. The approach we take is only to develop the algebra which defines the search space and the cost model; we do not propose a search engine (i.e., search strategy) to drive the rules. We have chosen the Volcano optimizer generator as our search engine, because it is publicly available, and also because it has an efficient branch-and-bound search strategy. Using Prairie as a front-end, we translate Prairie rules to Volcano to validate our claim that Prairie makes it easier to write rules. We describe our algebra and present experimental results which show that using a high-level framework like Prairie to design large-scale optimizers does not sacrifice efficiency. 1
348|A Case Study of Venus and a Declarative Bases for Rule Modules|The Venus Rule Language introduced a declarative basis for structured rule-based programming (as opposed to pro cedural encapsulation). The method is closely related to the nested transaction model for concurrency control and otherwise the language has been designed to serve as the basis of both main-memory and hard (or expert) activedatabase systems. We present a quantitative evaluation of the impact on development costs of the Venus language on rule-based programs. The basis of the study is a reimplementation of ALEXSYS, an active-database program originally developed in OPS5 and currently in use in the financial securities industry. The measurements indicate substantially improved code, suggesting substantially reduced development and life-cycle costs 1 Introduction  Forward-chaining rule systems, or production systems, are in endemic use as a method of knowledge representation and the basis of expert-system programs[4]. The paradigm has also gained notoriety by serving as the basis...
349|Selective Indexing Speeds Production Systems|In this paper 1 we present performance results for a production system environment, CLIPS++, that demonstrate the advantage of selectively building and applying simple index structures. We contrast this to the extensive body of work on matching which over the years has evolved increasingly complex compositions of index structurs which are then uniformly applied to all the data types and rules in a production system program. Over a set of benchmark programs, the fastest executions are always attained by carefully selecting a good mixture of indexes rather than universal
350|An Overview of the VenusDB Active Multidatabase System|VenusDB is a C++ embedded, forward-chaining rule language and compiler that includes linguistic elements and runtime support for accessing multiple databases across multiple platforms. Multidatabase access was a natural evolutionary step for Venus. Evaluation of Venus using an expert-database application revealed the need for explicit syntax for the expression of event conditions. Thus, VenusDB provides for both eventcondition -action (ECA) rules typical of active-database systems and condition action rules typical of expert systems and expert-database systems. The Venus compiler is readily extended by virtue of an abstract interface, the AMI, that encapsulates the details of data access. Although middleware elements can be amorphous, the AMI forms a well defined interface for the encapsulation of databases and their integration with a forward-chaining inference engine. 
351|A New Approach to Modularity in Rule-Based Programming|In this paper we describe a purely declarative method for introducing modularity into forwardchaining, rule-based languages and its embodiment in the Venus rule language. The method is enforced by the syntax of the language and includes the ability to parameterize the rule groups. Drawing from two of three Venus applications developed to date, we illustrate how this form of modularity contributes directly to the resolution of certain software engineering problems associated with rule languages. 1 Introduction  It is well established that flat monolithic systems, both procedural and rule-based, lead to difficult application development and even more difficult longterm maintenance[17, 13]. The problem is more vexing in rule-based environments since the scope of free variables in a rule&#039;s predicate encompasses the entire working memory. Thus, side-effects of program updates are not limited to the section of code containing the update. In this paper we describe a declarative method for int...
352|Evaluating Triggers Using Decision Trees|This paper presents an algorithm for implementing rule filtering in active and trigger enabled databases. The algorithm generates one or more decision trees that determine what rules or triggers might be enabled by an individual database element, reducing the number of rules or triggers that must be evaluated. The algorithm operates by symbolically representing the space of database elements and subdividing the space based on rule predicates. Regions of the state space represent particular combinations of enabled rules. Decision trees are then generated based on the subdivided state space. The trees have the important property that no individual test is repeated. The ordered binary decision diagram (BDD) data structure is used to represent and manipulate the state space. 1. Introduction  Modern database systems increasingly support active behavior through rules. This support ranges from simple database triggers to complete active database functionality. Rules typically follow  the even...
353|Alamo: An Architecture for Integrating Heterogeneous Data Sources|We are developing an architecture, Alamo, that addresses both the semantic and physical aspects of data integration. The Alamo architecture permits the interoperability of both data sources and semantic components. As a collection, the supported semantic components capture most basic forms of knowledge representation. Since the semantic integration of heterogeneous data sources requires some representation of the semantic content of the data source, the Alamo architecture forms an infrastructure for the development and possible integration of different forms of semantic integration of heterogeneous data sources. Central to the Alamo architecture is a CORBA compliant software bus called the Abstract Search Machine (ASM). The ASM augments a simple cursor class with methods that can be used to implement the marking, memoing and learning schemes exploited by clever search algorithms. The broad claim is that high performance implementations of se-    This research is partially funded by DAR...
354|On Semantic Query Optimization In Deductive Databases|The focus of this paper is semantic query optimization in the presence of integrity constraints (ICs) such as inclusion dependencies (INDs) and context dependencies (CDs). INDs are well known to arise naturally in many applications. CDs, introduced earlier in a different context, can capture natural semantic constraints that cannot be expressed using INDs. Besides, some CDs can also be inferred from given INDs, and further have the advantage of being more directly geared toward semantic query optimization than INDs. We provide an inference mechanism for reasoning with CDs and develop efficient algorithms for semantic query optimization using them. The contributions of this paper are sufficient conditions and algorithms for the detection of redundant atoms and rules in a class of linear recursive programs, arising in deductive databases. We take a program transformation approach to semantic query optimization. As a consequence, our approach has the following advantages: (i) our techniqu...
355|What you always wanted to know about Datalog (and never dared to ask)  (1989) |Datalog is a database query language based on the logic programming paradigm; it has been designed and intensively studied over the last five years. We present the syntax and semantics of Datalog and its use for querying a relational database. Then, we classify optimization methods for achieving efficient evaluations of Datalog queries, and present the most relevant methods. Finally, we discuss various exhancements of Datalog, currently under study, and indicate what is still needed in order to extend Datalog’s applicability to the solution of real-life problems. The aim of this paper is to provide a survey of research performed on Datalog, also addressed to those members of the database community who are not too familiar with logic programming concepts. 
356|Structural Query Optimization --- A Uniform Framework For Semantic Query Optimization In Deductive Databases|this paper we propose the factoring technique as a general technique which can detect opportunities for making the recursion less &#034;intensive&#034;. For example, this technique can detect that certain subgoals need only be examined a bounded number of times in certain subtrees of the proof trees of the query predicate. More precisely, given a program and a query predicate (the recursive predicate), the factoring technique can determine when it is possible to limit the number of occurrences of a subgoal in selected subtrees of the proof trees of the query predicate. We call this property of subgoals  proof tree removability. Our technique can also exploit the knowledge about proof tree removability in transforming a program into an equivalent program such that the proof trees constructed using the transformed program always contain a limited number of occurrences of the subgoal in selected subtrees. Proof tree removability is thus a notion that generalizes the notion of &#034;recursively redundant&#034; introduced by Naughton [N1, N2] in the sense that a subgoal a need not be recursively redundant w.r.t. a predicate and yet its repeated occurrences in certain subtrees of the query predicate may well be redundant. (See Section 9 for examples.) Factoring is one type of proof tree transformation, (e.g. see [RSUV, Sar]), only that it is achieved at the level of rules. In principle it may be possible to start with Naughton and others&#039; characterization of recursively redundant predicates and then try to generalize the conditions to a larger class of rules to capture the notion of &#034;subtree redundancy of predicates&#034; above.  Such a generalization is not at all obvious and we find it convenient to start with simple syntactic criteria for predicates to be &#034;factored&#034; out of linear sirups and then ...
357|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
358|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
359|An Energy-Efficient MAC Protocol for Wireless Sensor Networks|This paper proposes S-MAC, a medium-access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with individual nodes remaining largely inactive for long periods of time, but then becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in almost every way: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses three novel techniques to reduce energy consumption and support self-configuration. To reduce energy consumption in listening to an idle channel, nodes periodically sleep. Neighboring nodes form virtual clusters to auto-synchronize on sleep schedules. Inspired by PAMAS, S-MAC also sets the radio to sleep during transmissions of other nodes. Unlike PAMAS, it only uses in-channel signaling. Finally, S-MAC applies message passing to reduce contention latency for sensor-network applications that require store-andforward processing as data move through the network. We evaluate our implementation of S-MAC over a sample sensor node, the Mote, developed at University of California, Berkeley. The experiment results show that, on a source node, an 802.11-like MAC consumes 2--6 times more energy than S-MAC for traffic load with messages sent every 1-10s.
360|Geography-informed Energy Conservation for Ad Hoc Routing|We introduce a geographical adaptive fidelity (GAF) algorithm that reduces energy consumption in ad hoc wireless networks. GAF conserves energy by identifying nodes that are equivalent from a routing perspective and then turning off unnecessary nodes, keeping a constant level of routing fidelity. GAF moderates this policy using application- and system-level information; nodes that source or sink data remain on and intermediate nodes monitor and balance energy use. GAF is independent of the underlying ad hoc routing protocol; we simulate GAF over unmodified AODV and DSR. Analysis and simulation studies of GAF show that it can consume 40% to 60% less energy than an unmodified ad hoc routing protocol. Moreover, simulations of GAF suggest that network lifetime increases proportionally to node density; in one example, a four-fold increase in node density leads to network lifetime increase for 3 to 6 times (depending on the mobility pattern). More generally, GAF is an example of adaptive fidelity, a technique proposed for extending the lifetime of self-configuring systems by exploiting redundancy to conserve energy while maintaining application fidelity.   
361|Medium Access Control with Coordinated Adaptive Sleeping for Wireless Sensor Networks|This paper proposes S-MAC, a medium access control (MAC) protocol designed for wireless sensor networks. Wireless sensor networks use battery-operated computing and sensing devices. A network of these devices will collaborate for a common application such as environmental monitoring. We expect sensor networks to be deployed in an ad hoc fashion, with nodes remaining largely inactive for long time, but becoming suddenly active when something is detected. These characteristics of sensor networks and applications motivate a MAC that is different from traditional wireless MACs such as IEEE 802.11 in several ways: energy conservation and self-configuration are primary goals, while per-node fairness and latency are less important. S-MAC uses a few novel techniques to reduce energy consumption and support self-configuration. It enables low-duty-cycle operation in a multihop network. Nodes form virtual clusters based on common sleep schedules to reduce control overhead and enable traffic-adaptive wake-up. S-MAC uses in-channel signaling to avoid overhearing unnecessary traffic. Finally, S-MAC applies message passing to reduce contention latency for applications that require in-network data processing. The paper presents measurement results of S-MAC performance on a sample sensor node, the UC Berkeley Mote, and reveals fundamental tradeoffs on energy, latency and throughput. Results show that S-MAC obtains significant energy savings compared with an 802.11-like MAC without sleeping.
362|Query Processing for Sensor Networks|Hardware for sensor nodes that combine physical sensors, actuators, embedded processors, and communication components has advanced significantly over the last decade, and made the large-scale deployment of such sensors a reality. Applications range from monitoring applications such as inventory maintenance over health care to military applications.
363|Approximate aggregation techniques for sensor databases|In the emerging area of sensor-based systems, a significant challenge is to develop scalable, fault-tolerant methods to extract useful information from the data the sensors collect. An approach to this data management problem is the use of sensor database systems, exemplified by TinyDB and Cougar, which allow users to perform aggregation queries such as MIN, COUNT and AVG on a sensor network. Due to power and range constraints, centralized approaches are generally impractical, so most systems use in-network aggregation to reduce network traffic. Also, aggregation strategies must provide fault-tolerance to address the issues of packet loss and node failures inherent in such a system. An unfortunate consequence of standard methods is that they typically introduce duplicate values, which must be accounted for to compute aggregates correctly. Another consequence of loss in the network is that exact aggregation is not possible in general. With this in mind, we investigate the use of approximate in-network aggregation using small sketches. Our contributions are as follows: 1) we generalize well known duplicateinsensitive sketches for approximating COUNT to handle SUM (and by extension, AVG and other aggregates), 2) we present and analyze methods for using sketches to produce accurate results with low communication and computation overhead (even on low-powered CPUs with little storage and no floating point operations), and 3) we present an extensive experimental validation of our methods. 1
365|Building Efficient Wireless Sensor Networks with Low-Level Naming|In most distributed systems, naming of nodes for low-level communication leverages topological location (such as node addresses)  and is independent of any application. In this paper, we investigate an emerging class of distributed systems where low-level communication does not rely on network topological location. Rather, low-level communication is based on attributes that are external to the network topology and relevant to the application. When combined with dense deployment of nodes, this kind of named data enables in-network processing for data aggregation, collaborative signal processing, and similar problems. These approaches are essential for emerging applications such as sensor networks where resources such as bandwidth and energy are limited. This paper is the first description of the software architecture that supports  named data and in-network processing in an operational, multi-application sensor-network. We show that approaches such as in-network aggregation and nested queries can significantly affect network traffic. In one experiment aggregation reduces traffic by up to 42% and nested queries reduce loss rates by 30%. Although aggregation has been previously studied in simulation, this paper demonstrates nested queries as another form of in-network processing, and it presents the first evaluation of these approaches over an operational testbed.  
366|Simultaneous Optimization for Concave Costs: Single Sink Aggregation or Single Source Buy-at-Bulk|We consider the problem of finding efficient trees to send information from k sources to  a single sink in a network where information can be aggregated at intermediate nodes in the  tree. Specifically, we assume that if information from j sources is traveling over a link, the  total information that needs to be transmitted is f(j). One natural and important (though not  necessarily comprehensive) class of functions is those which are concave, non-decreasing, and  satisfy f(0) = 0. Our goal is to find a tree which is a good approximation simultaneously to  the optimum trees for all such functions. This problem is motivated by aggregation in sensor  networks, as well as by buy-at-bulk network design.
367|Topology Control Protocols to Conserve Energy in Wireless Ad Hoc Networks|In wireless ad hoc networks and sensor networks, energy use is in many cases the most important constraint since it corresponds directly to operational lifetime. This paper presents two topology control protocols that extend the lifetime of dense ad hoc networks while preserving connectivity, the ability for nodes to reach each other. Our protocols conserve energy by identifying redundant nodes and turning their radios off. Geographic Adaptive Fidelity (GAF) identifies redundant nodes by their physical location and a conservative estimate of radio range. Cluster-based Energy Conservation (CEC) directly observes radio connectivity to determine redundancy and so can be more aggressive at identifying duplication and more robust to radio fading. We evaluate these protocols through  
368|Diffusion Filters as a Flexible Architecture for Event Notification in Wireless Sensor Networks|Wireless sensor networks represent an increasingly important example of distributed event systems. Unlike Internet-based distributed event systems, sensor networks are very bandwidth constrained and use sensor nodes that are often dedicated to the network and controlled by a single organization. Bandwidth constraints require, and administrative homogeneity allows, sensor networks to employ in-network processing, where application-specific code is used in the network to optimize data movement. The contribution of this paper is to describe the diffusion filter architecture, a software structure for a distributed event system that allows user-supplied software to interact with event routing. Sensor network nodes will span a wide range of capabilities, from tiny single-address space embedded processors to to desktop-class 32-bit computers. A second contribution of our architecture that it scales from 16- to 32-bit computers with OS support for single or multiple address spaces. We describe what software approaches facilitate this flexibility and quantify the performance differences.
369|Genetic Programming in Database Query Optimization|Database query optimization is a hard research problem. Exhaustive techniques are adequate for trivial instances only, while combinatorial optimization techniques are vulnerable to the peculiarities of specific instances. We propose a model based on genetic programming to address this problem, motivated by its robustness and efficiency in a wide area of search problems. We adapt the genetic programming paradigm to the requirements of the query optimization problem, showing that the nature of the problem makes genetic programming a particularly attractive approach to it. 1 Introduction  Genetic programming enjoys an increasing popularity in the mastering of difficult optimization problems. We propose a methodology applying genetic programming to the query optimization problem in relational databases. Query optimization has been the subject of active research for more than 20 years. Recently, research has turned towards techniques based on combinatorial optimization. It has been observed...
370|Optimizing Join Orders|Recent developments in database technology, such as deductive database  systems, have given rise to the demand for new, cost-effective optimization  techniques for join expressions. In this paper many different algorithms  that compute approximative solutions for optimizing join orders are studied  since traditional dynamic programming techniques are not appropriate  for complex problems. First, two possible solution spaces, the space of  left-deep and bushy processing trees, respectively, are evaluated from a  statistical point of view. The result is that the common limitation to  left-deep processing trees is, from a purely statistical point of view, only  advisable for certain cost models. Basically, optimizers from three classes  are analysed: heuristic, randomized and genetic algorithms. Each one is  extensively scrutinized with respect to its working principle and its fitness  for the desired application. It turns out that randomized and genetic algorithms  are well suited for op...
371|Maintenance of Materialized Views: Problems, Techniques, and Applications|In this paper we motivate and describe materialized views, their applications, and the problems  and techniques for their maintenance. We present a taxonomy of view maintenanceproblems  basedupon the class of views considered, upon the resources used to maintain the view, upon the types of modi#cations to the base data that areconsidered during maintenance, and whether the technique works for all instances of databases and modi#cations. We describe some of the view maintenancetechniques proposed in the literature in terms of our taxonomy. Finally, we consider new and promising application domains that are likely to drive work in materialized  views and view maintenance.  1 Introduction  What is a view? A view is a derived relation de#ned in terms of base #stored# relations. A view thus de#nes a function from a set of base tables to a derived table; this function is typically recomputed every time the view is referenced.  What is a materialized view? A view can be materialized by storin...
372|Objects and Views|Object-oriented databases have been introduced primarily to ease the development of database applications. However, the difficulties encountered when, for instance, trying to restructure data or integrate databases demonstrate that the models being used still lack flexibility. We claim that the natural way to overcome these shortcomings is to introduce a sophisticated view mechanism. This paper presents such a mechanism, one which allows a programmer to restructure the class hierarchy and modify the behavior and structure of objects. The mechanism allows a programmer to specify attribute values implicitly, rather than storing them. It also allows him to introduce new classes into the class hierarchy. These virtual classes are populated by selecting existing objects from other classes and by creating new objects. Fixing the identify of new objects during database updates introduces subtle issues into view design. Our presentation, mostly informal, leans on a number of illustrative examples meant to emphasize the simplicity of our mechanism. 1
373|Object fusion in mediator systems|One of the main tasks of mediators is to fuse information from heterogeneous information sources. This may involve, for example, removing redundancies, and resolving inconsistencies in favor of the most reliable source. The problem becomes harder when the sources are unstructured/semistructured and we do not have complete knowledge of their contents and structure. In this paper we show how many common fusion operations can be specified non-procedurally and succinctly. The key to our approach is to assign semantically meaningful object ids to objects as they are &amp;quot;imported &amp;quot; into the mediator.
374|Regular Path Queries with Constraints|The evaluation of path expression queries on semistructured data in a distributed asynchronous environment is considered. The focus is on the use of local information expressed in the form of path constraints in the optimization of path expression queries. In particular, decidability and complexity results on the implication problem for path constraints are established.
375|Supporting Views in Object-Oriented Databases|  Relational database systems provide a powerful abstraction mechanism: any query, since it returns a relation, can be used to define a view, that becomes a derived (or virtual) relation. Views are defined by a statement such as &#034;define view &lt;name&gt; as &lt;query&gt;.&#034; Views can be used to tailor the global database schema.  . Query formulation is simplified, if frequent subexpressions are predefined.  . Application programs are insulated from changes to the underlying schema.  . Information can be restructured to better suit an application programs&#039; requirements.  . Derived information is kept consistent with base data.  . Access restrictions (for authorization) can be enforced by hiding data.  In contrast to base relations, views are typically not stored permanently, but rather computed on demand. Queries to view relations are modified by query substitution so as to operate on the underlying b
376|MultiView: A Methodology for Supporting Multiple Views in Object-Oriented Databases|A view in object-oriented databases (OODB) corresponds to virtual schema graph with possibly restructured generalization and decomposition hierarchies. We propose a methodology, called MultiView, for supporting multiple such view schemata. MultiView represents a simple yet powerful approach achieved by breaking view specification into independent tasks: class derivation, global schema integration, view class selection, and view schema generation. Novel features of MultiView include an object algebra for class customization; an algorithm for the integration of virtual classes into the global schema; a view definition language for view class selection, and the automatic generation of a view class hierarchy. In addition, we present algorithms that verify the closure property of a view and, if found to be incomplete, transform it into a closed, yet minimal, view. Lastly, we introduce the fundamental concept of view independence and show  MultiView to be view independent.  
377|The GMAP: a versatile tool for physical data independence|.&lt;F3.733e+05&gt; Physical data independence is touted as a central feature of modern database systems. It allows users to frame queries in terms of the logical structure of the data, letting a query processor automatically translate them into optimal plans that access physical storage structures. Both relational and object-oriented systems, however, force users to frame their queries in terms of a logical schema that is directly tied to physical structures. We present an approach that eliminates this dependence. All storage structures are defined in a declarative language based on relational algebra as functions of a logical schema. We present an algorithm, integrated with a conventional query optimizer, that translates queries over this logical schema into plans that access the storage structures. We also show how to compile update requests into plans that update all relevant storage structures consistently and optimally. Finally, we report on experiments with a prototype implementation ...
378|Virtual Schemas and Bases|We propose the notions of virtual schemas and virtual bases as a coherent way of integrating various features in OODB views. A virtual schema is defined based on some existing (real) schema. A virtual base is obtained when a (real) base is attached to a virtual schema. We study the consequences of this simple assumption. In particular, we observe the differences between a real schema and a virtual one. We also consider an extension (that we call generic schemas) where it is necessary to specify several real bases to attach data to a virtual schema. We show how the flexibility provided by virtual schemas can be used to cope with various dynamic features of database systems. 1 Introduction Views are intended to increase the flexibility of database systems and their definition in the object-oriented database (OODB) context comes as a natural extension of the original paradigm. The yet relatively young research on this topic has introduced a large variety of indispensable new features. H...
379|Query Optimization for Selections using Bitmaps|Bitmaps are popular indexes for data warehouse (DW) applications and most database management systems offer them today. This paper proposes query optimization strategies for selections using bitmaps. Both continuous and discrete selection criteria are considered. Query optimization strategies are categorized into static and dynamic. Static optimization strategies discussed are the optimal design of bitmaps, and algorithms based on tree and logical reduction. The dynamic optimization discussed is the approach of inclusion and exclusion for both bit-sliced indexes and encoded bitmap indexes.  
380|Graph-Based Algorithms for Boolean Function Manipulation|In this paper we present a new data structure for representing Boolean functions and an associated set of manipulation algorithms. Functions are represented by directed, acyclic graphs in a manner similar to the representations introduced by Lee [1] and Akers [2], but with further restrictions on the ordering of decision variables in the graph. Although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical applications have a more reasonable representation. Our algorithms have time complexity proportional to the sizes of the graphs being operated on, and hence are quite efficient as long as the graphs do not grow too large. We present experimental results from applying these algorithms to problems in logic design verification that demonstrate the practicality of our approach.
381|Symbolic Boolean manipulation with ordered binary-decision diagrams|Ordered Binary-Decision Diagrams (OBDDS) represent Boolean functions as directed acyclic graphs. They form a canonical representation, making testing of functional properties such as satmfiability and equivalence straightforward. A number of operations on Boolean functions can be implemented as graph algorithms on OBDD
382|The ubiquitous B-tree|B-trees have become, de facto, a standard for file organization. File indexes of users, dedicated database systems, and general-purpose access methods have all been proposed and nnplemented using B-trees This paper reviews B-trees and shows why they have been so successful It discusses the major variations of the B-tree, especially the B+-tree,
383|Improved query performance with variant indexes|Abstract: The read-mostly environment of data warehousing makes it possible to use more complex indexes to speed up queries than in situations where concurrent updates are present. The current paper presents a short review of current indexing technology, including row-set representation by Bitmaps, and then introduces two approaches we call Bit-Sliced indexing and Projection indexing. A Projection index materializes all values of a column in RID order, and a Bit-Sliced index essentially takes an orthogonal bit-by-bit view of the same data. While some of these concepts started with the MODEL 204 product, and both Bit-Sliced and Projection indexing are now fully realized in Sybase IQ, this is the first rigorous examination of such index-ing capabilities in the literature. We compare algorithms that become feasible with these variant index types against algo-rithms using more conventional indexes. The analysis demon-strates important performance advantages for variant indexes in some types of SQL aggregation, predicate evaluation, and group-ing. The paper concludes by introducing a new method whereby multi-dimensional group-by queries, reminiscent of OLAP/Datacube queries but with more flexibility, can be very ef-ficiently performed. 1.
384|A universal algorithm for sequential data compression|A universal algorithm for sequential data compression is presented. Its performance is investigated with respect to a nonprobabilistic model of constrained sources. The compression ratio achieved by the proposed universal code uniformly approaches the lower bounds on the compression ratios attainable by block-to-variable codes and variable-to-block codes designed to match a completely specified source.
385|Data Compression Using Adaptive Coding and Partial String Matching|The recently developed technique of arithmetic coding, in conjunction with a Markov model of the source, is a powerful method of data compression in situations where a linear treatment is inappropriate. Adaptive coding allows the model to be constructed dynamically by both encoder and decoder during the course of the transmission, and has been shown to incur a smaller coding overhead than explicit transmission of the model&#039;s statistics. But there is a basic conflict between the desire to use high-order Markov models and the need to have them formed quickly as the initial part of the message is sent. This paper describes how the conflict can be resolved with partial string matching, and reports experimental results which show that mixed-case English text can be coded in as little as 2.2 bits/ character with no prior knowledge of the source.
386|XMill: an Efficient Compressor for XML Data|We describe a tool for compressing XML data, with applications in data exchange and archiving, which usually achieves about twice the compression ratio of gzip at roughly the same speed. The compressor, called XMill, incorporates and combines existing compressors in order to apply them to heterogeneous XML data: it uses zlib, the library function for gzip, a collection of datatype specific compressors for simple data types, and, possibly, user defined compressors for application specific data types. 1 Introduction  We have implemented a compressor/decompressor for XML data, to be used in data exchange and archiving, that achieves about twice the compression rate of general-purpose compressors (gzip), at about the same speed. The tool can be downloaded from www.research.att.com/sw/tools/xmill/.  XML is now being adopted by many organizations and industry groups, like the healthcare, banking, chemical, and telecommunications industries. The attraction in XML is that it is a self-describi...
387|Database architecture optimized for the new bottleneck: Memory access|In the past decade, advances in speed of commodity CPUs have far out-paced advances in memory latency. Main-memory access is therefore increasingly a performance bottleneck for many computer applications, including database systems. In this article, we use a simple scan test to show the severe impact of this bottleneck. The insights gained are translated into guidelines for database architecture; in terms of both data structures and algorithms. We discuss how vertically fragmented data structures optimize cache performance on sequential data access. We then focus on equi-join, typically a random-access operation, and introduce radix algorithms for partitioned hash-join. The performance of these algorithms is quantified using a detailed analytical model that incorporates memory access cost. Experiments that validate this model were performed on the Monet database system. We obtained exact statistics on events like TLB misses, L1 and L2 cache misses, by using hardware performance counters found in modern CPUs. Using our cost model, we show how the carefully tuned memory access pattern of our radix algorithms make them perform well, which is confirmed by experimental results. **This work was carried out when the author was at the
388|Compressing Relations and Indexes|We propose a new compression algorithm that is tailored to database applications. It can be applied to a collection of records, and is especially effective for records with many low to medium cardinality fields. In addition, this new technique supports very fast decompression. (Gzip typically takes 10 times the CPU time to decompress relations, while achieving about the same compression ratio.) Our algorithm achieves compression on numeric fields only; while low and medium cardinality fields of other types can be handled by mapping values to a small range of integers, our algorithms must be complemented by other compression techniques for general non-numeric (text, image) fields. Promising application domains include decision support systems (DSS), since `fact tables&#039;, which are by far the largest tables in these applications, contain many low and medium cardinality fields and typically no text fields [Kim96]. Further, our decompression rates are faster than typical disk throughputs fo...
389|Data Compression and Database Performance|Data compression is widely used in data management to save storage space and network bandwidth. In this report, we outline the performance improvements that can be achieved by exploiting data compression in query processing. The novel idea is to leave data in compressed state as long as possible, and to only uncompress data when absolutely necessary. We will show that many query processing algorithms can manipulate compressed data just as well as decompressed data, and that processing compressed data can speed query processing by a factor much larger than the compression factor.
390|Optimizing Queries On Compressed Bitmaps|Bitmap indices are used by DBMS&#039;s to accelerate decision support queries. A significant advantage of  bitmap indices is that complex logical selection operations can be performed very quickly, by performing  bit-wise AND, OR, and NOT operators. Although bitmap indices can be space inefficient for high  cardinality attributes, the space use of compressed bitmaps compares well to other indexing methods.  Oracle and Sybase IQ are two commercial products that make extensive use of compressed bitmap indices.  Our recent research showed that there are several fast algorithms for evaluating Boolean operators  on compressed bitmaps. Depending on the nature of the operand bitmaps (their format, density and  clusterdness) and the operation to be performed (AND, NOT, ...), these algorithms can have execution  times that are orders of magnitude different. Choosing an algorithm for performing a Boolean operation  has global effects in the Boolean query expression, requiring global optimization. We present a linear time  dynamic programming search strategy based on a cost model to optimize query expression evaluation  plans. We also present rewriting heuristics that rewrite the query expression to an equivalent one to  encourage better algorithms assignments. Our performance results show that the optimizer requires a  negligible amount of time to execute, and that optimized complex queries can execute up to three times  faster than unoptimized queries on real data.  1 
391|Data Compression Support in Databases|Computers running database management applications often manage large amounts of data. Typically, the price of the I/O sub-system is a considerable portion of the com-puting hardware. Fierce price competition demands every possible savings. Lossless data compression methods, when appropri-ately integrated with the dbms, yield sig-niflcant savings. Roughly speaking, a slight increase in cpu cycles is more than offset by savings in I/O subsystem. Various de-sign issues arise in the use of data compres-sion in the dbms- from the choice of algo-rithm, statistics collection, hardware ver-sus software based compression, location of the compression function in the overall computer system architecture, unit of com-pression, update in place, and the applica-tion of log ’ to compressed data. These are methodic &amp; y examined and trade-offs dis-cussed in the context of choices made for IBM’s DB2 dbms product. 1
392|Database compression|D espite the fact that computer memory costs have decreased ramatically over the past few years, data storage still remains, and will probably always remain, an important cost factor for many large scale database applications. Compressing data in a data-base system is attractive for two reasons: data storage reduction and performance improvement. Storage re-duction is a direct and obvious benefit, while perfor-mance improves because smaller amounts of physical data need to be moved for any particular operation on the database. We address everal aspects of reversible data com-pression and compression techniques: [] general concepts of data compression; [] a number of compression techniques; [] a comparison of the effects of compression on
393|Database Compression: A Performance Enhancement Tool|Compression is typically used for databases that have grown large enough to create a strain on system storage capacity. We argue here that database compression is attractive from a query processing viewpoint also and should therefore be implemented even when disk storage is plentiful. We study the compression ratio and query processing performance of a variety of compression algorithms, for different compression granularities, on a set of relations drawn from real world databases. Our study shows that attribute level compression is the best from a query processing perspective but has poor compression ratio. We then present a modified attribute level compression algorithm, based on non-adaptive arithmetic compression, called COLA, which simultaneously provides good query processing and reasonable compression ratios. We also analyze, for a range of relational queries, the performance benefits that COLA could be expected to provide. 1 Introduction  Many database management systems provide...
394|Daytona And The Fourth Generation Language Cymbal|The Daytona ™ data management system is used by AT&amp;T to solve a wide spectrum of data management problems. For example, Daytona is managing a 4 terabyte data warehouse whose largest table contains over 10 billion rows. Daytona’s architecture is based on translating its high-level query language Cymbal ™ (which includes SQL as a subset) completely into C and then compiling that C into object code. The system resulting from this architecture is fast, powerful, easy to use and administer, reliable and open to UNIX ™ tools. In particular, two forms of data compression plus robust horizontal partitioning enable Daytona to handle terabytes with ease. 1. Daytona The Daytona ™ data management system is used by AT&amp;T to solve a wide spectrum of data management problems. On the tiny end, Daytona provided the data manager for the DACS VI switch which only had 64MB of memory at the time. Since DACS VI used a real-time UNIX operating system, virtual memory could not be paged to swap disk. Consequently, the entire application, including the 15 % that was allocated to the database, had to fit into the rather small amount of physical memory at all times. As another example, all of AT&amp;T’s (phone) call detail data (which represents most of the company’s
395|An algebraic compression framework for query results|Decision-support applications in emerging environments require that SQL query results or intermediate results be shipped to clients for further analysis and presentation. These clients may use low bandwidth connections or have severe storage restrictions. Consequently, there is a need to compress the results of a query for efficient transfer and client-side access. This paper explores a variety of techniques that address this issue. Instead of using a fixed method, we choose a combination of compression methods that use statistical and semantic information of the query results to enhance the effect of compression. To represent such a combination, we present a framework of “compression plans ” formed by composing primitive compression operators. We also present optimization algorithms that enumerate valid compression plans and choose an optimal plan. Our experiments show that our techniques achieve significant performance improvement over standard compression tools like WinZip. 1.
396|Aggregation Algorithms for Very Large Compressed Data Warehouses|Many efficient algorithms to compute  multidimensional aggregation and Cube for  relational OLAP have been developed. However,  to our knowledge, there is nothing to date in the  literature on aggregation algorithms on  compressed data warehouses for  multidimensional OLAP. This paper presents a  set of aggregation algorithms on very large  compressed data warehouses for  multidimensional OLAP. These algorithms  operate directly on compressed datasets without  the need to first decompress them. They are  applicable to data warehouses that are  compressed using variety of data compression  methods. The algorithms have different  performance behavior as a function of dataset  parameters, sizes of outputs and main memory  availability. The analysis and experimental  results show that the algorithms have better  performance than the traditional aggregation  algorithms.
397|New Algorithms for Lexical Query Optimization |Abstract. New algorithms for query modifications are proposed. These algorithms involve lexical optimization based on mathematical transformations that have never been used for query optimization before.
398|Magic Sets and their Application to Data Integration|Abstract. We propose a generalization of the well-known Magic Sets technique to Datalog ¬ programs with (possibly unstratified) negation under stable model semantics. Our technique produces a new program whose evaluation is generally more efficient (due to a smaller instantiation), while preserving soundness under cautious reasoning. Importantly, if the original program is consistent, then full query-equivalence is guaranteed for both brave and cautious reasoning, which turn out to be sound and complete. In order to formally prove the correctness of our Magic Sets transformation, we introduce a novel notion of modularity for Datalog ¬ under the stable model semantics, which is relevant per se. We prove that a module can be evaluated independently from the rest of the program, while preserving soundness under cautious reasoning. For consistent programs, both soundness and completeness are guaranteed for brave reasoning and cautious reasoning as well. Our Magic Sets optimization constitutes an effective method for enhancing the performance of data-integration systems in which query-answering is carried out by means of cautious reasoning over Datalog ¬ programs. In fact, preliminary results of experiments in the EU project INFOMIX, show that Magic Sets are fundamental for the scalability of the system. 1
399|Multidatabase Query Optimization: Issues and Solutions|Despite the interest in multidatabase systems, research in multidatabase query optimization (MQO) has been scarce. Many researchers perceive the difficulty to be the lack of reliable cost estimates for autonomous component databases. Consequently, some have suggested that this problem degenerates to a distributed query optimization (DQO) problem once cost model coefficients for component databases are found. In this paper, we argue that autonomy and heterogeneity of component databases give rise to a number of issues which make MQO a distinct problem from DQO. Consequently, existing solutions for DQO need to be reevaluated in the light of these issues. The design of a multidatabase query optimizer, which accounts for the issues highlighted, is discussed. This provides a framework within which further research in MQO can be carried out. 1 Introduction  Multidatabase management systems (MDBMS) [5] enable data sharing among heterogeneous local databases (called  component databases) and t...
400|A metadata approach to resolving semantic conflicts|Semantic reconciliation is an important step in determining logical connectivity between a data source (database) and a data receiver (application). Semantic reconciliation is used to determine if the semantics of the data provided by the source is meaningful to the receiver. In this paper we describe a rule-based approach to semantic specification and demonstrate how this specification can be used to establish semantic agreement between a source and receiver. We describe query processing techniques that use these specifications along with conversion routines and query modification to guarantee correct data semantics. In addition, this work examines the effect of changing data semantics. These changes may occur at the source of the data or they may be changes in the specifications of the data semantics for the application. Methods are described for detecting these changes and for determining if the database can continue to supply meaningful data to the application. Though described in terms of the source-receiver model, these techniques can also be used for semantic reconciliation and schema integration for multidatabase systems. Keywords[data dictionaries, metadata, query modification, schema integration, semantic conflicts] 1
401|The Picasso Database Query Optimizer Visualizer |Modern database systems employ a query optimizer module to automatically identify the most efficient strategies for executing the declarative SQL queries submitted by users. The efficiency of these strategies, called “plans”, is measured in terms of “costs ” that are
402|Least expected cost query optimization: An exercise in utility|We identify two unreasonable, though standard, assumptions made by database query optimizers that can adversely affect the quality of the chosen evaluation plans. One assumption is that it is enough to optimize for the expected case—that is, the case where various parameters (like available memory) take on their expected value. The other assumption is that the parameters are constant throughout the execution of the query. We present an algorithm based on the “System R”-style query optimization algorithm that does not rely on these assumptions. The algorithm we present chooses the plan of the least expected cost instead of the plan of least cost given some fixed value of the parameters. In execution environments that exhibit a high degree of variability, our techniques should result in better performance. 1
403|DB2’s LEarning Optimizer|Most modern DBMS optimizers rely upon a cost model to choose the best query execution plan (QEP) for any given query. Cost estimates are heavily dependent upon the optimizer’s estimates for the number of rows that will result at each step of the QEP for complex queries involving many predicates and/or operations. These estimates, in turn, rely upon statistics on the database and modeling assumptions that may or may not be true for a given database. In this paper we introduce LEO, DB2&#039;s LEarning Optimizer, as a comprehensive way to repair incorrect statistics and cardinality estimates of a query execution plan. By monitoring previously executed queries, LEO compares the optimizer’s estimates with actuals at each step in a QEP, and computes adjustments to cost estimates and statistics that may be used during future query optimizations. This analysis can be done either on-line or off-line on a separate system, and either incrementally or in batches. In this way, LEO introduces a feedback loop to query optimization that enhances the available information on the database where the most queries have occurred, allowing the optimizer to actually learn from its past mistakes. Our technique is general and can be applied to any operation in a QEP (not just selection predicates on base tables), including joins, derived results after several predicates have been applied, and even to DISTINCT and GROUP-BY operators. As shown by performance measurements on a 10 GB TPC-H data set, the runtime overhead of LEO’s monitoring is insignificant, whereas the potential benefit to response time from more accurate cardinality and cost estimates can be orders of magnitude.
404|Efficiently Approximating Query Optimizer Plan Diagrams|Given a parametrized n-dimensional SQL query template and a choice of query optimizer, a plan diagram is a color-coded pictorial enumeration of the execution plan choices of the optimizer over the query parameter space. These diagrams have proved to be a powerful metaphor for the analysis and redesign of modern optimizers, and are gaining currency in diverse industrial and academic institutions. However, their utility is adversely impacted by the impractically large computational overheads incurred when standard bruteforce exhaustive approaches are used for producing fine-grained diagrams on high-dimensional query templates. In this paper, we investigate strategies for efficiently producing close approximations to complex plan diagrams. Our techniques are customized to the features available in the optimizer’s API, ranging from the generic optimizers that provide only the optimal plan for a query, to those that also support costing of sub-optimal plans and enumerating rank-ordered lists of plans. The techniques collectively feature both random and grid sampling, as well as inference techniques based on nearest-neighbor classifiers, parametric query optimization and plan cost monotonicity. Extensive experimentation with a representative set of TPC-H and TPC-DS-based query templates on industrial-strength optimizers indicates that our techniques are capable of delivering 90 % accurate diagrams while incurring less than 15 % of the computational overheads of the exhaustive approach. In fact, for full-featured optimizers, we can guarantee zero error with less than 10 % overheads. These approximation techniques have been implemented in the publicly available Picasso optimizer visualization tool.  
405|Efficient Generation of Approximate Plan Diagrams|Given a parametrized n-dimensional SQL query template and a choice of query optimizer, a plan diagram is a color-coded pictorial enumeration of the execution plan choices of the optimizer over the query parameter space. These diagrams have proved to be a powerful metaphor for the analysis and redesign of modern optimizers, and are gaining currency in diverse industrial and academic institutions. However, their utility is adversely impacted by the impractically large computational overheads incurred when standard brute-force exhaustive approaches are used for producing high-dimension and high-resolution diagrams. In this paper, we investigate strategies for efficiently producing high-quality approximate plan diagrams that have low plan-identity and plan-location errors. Our techniques are customized to the features available in the optimizer’s API, ranging from the generic optimizers that provide only the optimal plan for a query to those which also support costing of sub-optimal plans and enumerating rank-ordered lists of plans. The techniques collectively feature both random and grid sampling, as well as interpolation techniques based on kNN classifiers, parametric query optimization and plan cost monotonicity. Extensive experimentation with a representative set of TPC-H and TPC-DS-based query tem-plates on industrial-strength optimizers indicates that our techniques are capable of meeting identity and location error bounds as low as 10 % while incurring less than 15 % of the computational over-heads of the exhaustive approach. In fact, for full-featured optimizers, we can guarantee zero error with overheads of less than 10%. These approximation techniques have been implemented in the publicly available Picasso optimizer visualization tool. 1
406|Rate-Based Query Optimization for Streaming Information Sources|Relational query optimizers have traditionally relied upon table cardinalities when estimating the cost of the query plans they consider. While this approach has been and continues to be successful, the advent of the Internet and the need to execute queries over streaming sources requires a different approach, since for streaming inputs the cardinality may not be known or may not even be knowable (as is the case for an unbounded stream.) In view of this, we propose shifting from a cardinality-based approach to a rate-based approach, and give an optimization framework that aims at maximizing the output rate of query evaluation plans. This approach can be applied to cases where the cardinality-based approach cannot be used. It may also be useful for cases where cardinalities are known, because by focusing on rates we are able not only to optimize the time at which the last result tuple appears, but also to optimize for the number of answers computed at any specified time after the query evaluation commences. We present a preliminary validation of our rate-based optimization framework on a prototype XML query engine, though it is generic enough to be used in other database contexts. The results show that rate-based optimization is feasible and can indeed yield correct decisions.
407|Data networks|a b s t r a c t In this paper we illustrate the core technologies at the basis of the European SPADnet project (www. spadnet.eu), and present the corresponding first results. SPADnet is aimed at a new generation of MRI-compatible, scalable large area image sensors, based on CMOS technology, that are networked to perform gamma-ray detection and coincidence to be used primarily in (Time-of-Flight) Positron Emission Tomography (PET). The project innovates in several areas of PET systems, from optical coupling to single-photon sensor architectures, from intelligent ring networks to reconstruction algorithms. In addition, SPADnet introduced the first computational model enabling study of the full chain from gamma photons to network coincidence detection through scintillation events, optical coupling, etc. &amp; 2013 Elsevier B.V. All rights reserved. 1.
408|Characterizing Memory Requirements for Queries over Continuous Data|This paper deals with continuous conjunctive queries with arithmetic comparisons and optional aggregation over multiple data streams. An algorithm is presented for determining whether or not any given query can be evaluated using a bounded amount of memory for all possible instances of the data streams. For queries that can be evaluated using bounded memory, an execution strategy based on constant-sized synopses of the data streams is proposed. For queries that cannot be evaluated using bounded memory, data stream scenarios are identified in which evaluating the queries requires memory linear in the size of the unbounded streams
411|Dynamic Query Re-Optimization|Very long-running queries in database systems are not uncommon in non-traditional application domains such as image processing or data warehousing analysis. Query optimization, therefore, is important. However, estimates of the query characteristics before query execution are usually inaccurate. Further, system configuration and resource availability may change during long evaluation period. As a result, queries are often evaluated with sub-optimal plan configurations. To remedy this situation, we have designed a novel approach to re-optimize suboptimal query plan configurations onthe -fly with Conquest --- an extensible and distributed query processing system. A dynamic optimizer considers reconfiguration cost as well as execution cost in determining the best query plan configuration. Experimental results are presented. 1 Introduction Parallelism is important in today&#039;s database query processing. Very long-running queries require parallel processing to deliver reasonable performance ...
412|Querying object-oriented databases|We present a novel language for querying object-oriented databases. The language is built around the idea of extended path expressions that substantially generalize [ZAN83], and on an adaptation of the first-order formalization of object-oriented languages from [KW89, KLW90, KW92]. The language incorporates features not found in earlier proposals; it is easier to use and has greater expressive power. Some of the salient features of our language are: ffl Precise model-theoretic semantics. ffl A very expressive form of path expressions that not only can do joins, selections and unnesting, but can also be used to explore the database schema. ffl Views can be defined and manipulated in a much more uniform way than in other proposals. ffl Database schema can be explored in the very same language that is used to retrieve data. Unlike in relational languages, the user needs not know anything about the system tables that store schema information. ffl The notions of a type and type-correctness have precise meaning. It accommodates a wide variety of queries that might be deemed well- or ill-typed under different circumstances. In particular, we show that there is more than one way of settling the issue of type correctness. For expository purposes and due to space limitation, we chose to make a number of simplifying assumptions and left some features out. A more complete account can be found in [KSK92].
413|Logical foundations of object-oriented and frame-based languages|We propose a novel formalism, called Frame Logic (abbr., F-logic), that accounts in a clean and declarative fashion for most of the structural aspects of object-oriented and frame-based languages. These features include object identity, complex objects, inheritance, polymorphic types, query methods, encapsulation, and others. In a sense, F-logic stands in the same relationship to the objectoriented paradigm as classical predicate calculus stands to relational programming. F-logic has a model-theoretic semantics and a sound and complete resolution-based proof theory. A small number of fundamental concepts that come from object-oriented programming have direct representation in F-logic; other, secondary aspects of this paradigm are easily modeled as well. The paper also discusses semantic issues pertaining to programming with a deductive object-oriented language based on a subset of F-logic.  
414|F-Logic: A Higher-Order Language for Reasoning about Objects, Inheritance and Scheme|All in-text	references	underlined	in	blue	are	linked	to	publications	on	ResearchGate, letting you	access	and	read	them	immediately.
415|A clash of intuitions: The current state of nonmonotonic multiple inheritance systems|Abstract: Early attempts at combining multiple inheritance with nonmonotonic reasoning were based on straight forward extensions of tree-structured inheritance systems, and were theoretically unsound. In The Mathematics of Inheritance Systems, or TMOIS, Touretzky described two problems these systems cannot handle: reasoning in the presence of true but redundant assertions, and coping with ambiguity. TMOIS provided a definition and analysis of a theoretically sound multiple inheritance system, accompanied by inference algorithms. Other definitions for inheritance have since been proposed that are equally sound and intuitive, but do not always agree with TMOIS. At the heart of the controversy is a clash of intuitions about certain fundamental issues such as skepticism versus credulity, the direction in which inheritance paths are extended, and classical versus intuitive notions of consistency. Just as there are alternative logics, there may be no single &#034;best&#034; approach to nonmonotonic multiple inheritance. 1.
416|The logic of inheritance in frame systems|This paper shows how the semantics of frames with exceptions can be described logically. We define a simple (purely declarative) frame language allowing for multiple inheritance and meta classes (i.e. the instances of a class may be classes themselves). Expressions of this language are translated into first order formulas. Circumscription of a certain predicate in the resulting theory yields the desired semantics. Our approach allows the intuition that subclasses should override superclasses to be represented in a very natural way. Inheritance systems have a long tradition in AI. They allow the description of hierarchies of objects and
417|A First-Order Theory of Types and Polymorphism in Logic Programming|We describe a new logic called typed predicate calculus (T PC) that gives declarative meaning to logic programs with type declarations and type inference. T PC supports all popular types of polymorphism, such as parametric, inclusion, and ad hoc polymorphism. The proper interaction between parametric and inclusion varieties of polymorphism is achieved through a new construct, called type dependency, which is reminiscent of implication types of [PR89] but yields more natural and succinct specifications. Unlike other proposals where typing has extra-logical status, in T PC the notion of type-correctness has precise model-theoretic meaning that is independent of any specific type-checking or type-inference procedure. Moreover, many different approaches to typing that were proposed in the past can be studied and compared within the framework of our logic. As an illustration, we apply T PC to interpret and compare the results reported in [MO84, Smo88, HT90, Mis84, XW88]. Another novel featu...
418|Analysis of Execution Plans in Query Optimization |Abstract — The sequence in which the source tables are accessed during query execution is called a query execution plan. The process of selecting one execution plan from potentially many possible plans is referred to as query optimization. The query optimizer is one of the most important components of a query processor. The input to the optimizer consists of the query, the database schema (table and index definitions), and the database statistics. The output of the optimizer is a query execution plan, sometimes referred to as a query plan or just a plan. The goal is to eliminate as many unneeded tuples, or rows as possible. The paper describes the design of query optimizer w ith four basic phases such as Query Analysis, Index Selection Join Selection and Plan Selection. Index Terms — Execution plan, index selection, join selection, query analysis, query optimization.
420|A General Framework for the Optimization of Object-Oriented Queries|The goal of this work is to integrate in a general framework the different query optimization techniques that have been proposed in the object-oriented context. As a first step, we focus essentially on the logical aspect of query optimization. In this paper, we propose a formalism (i) that unifies different rewriting formalisms, (ii) that allows easy and exhaustive factorization of duplicated subqueries, and (iii) that supports heuristics in order to reduce the optimization rewriting phase.  1 Introduction  Many declarative query languages for object-oriented database management systems have been proposed in the last few years (e.g. [4, 5, 9]). Currently, a very serious concern is their optimization. Interesting techniques have been imported from other environments or developed for this context. However, although complementary, these techniques are often supported by distinct formalisms. The implementation of a query optimizer is thus still an awkward and delicate operation. Clearly, i...
421|Towards an Effective Calculus for Object Query Languages|We define a standard of effectiveness for a database calculus relative to a query language. Effectiveness judges suitability to serve as a processing framework for the query language, and comprises aspects of coverage, manipulability and efficient evaluation. We present the monoid calculus, and argue its effectiveness for object-oriented query languages, exemplified by OQL of ODMG-93. The monoid calculus readily captures such features as multiple collection types, aggregations, arbitrary composition of type constructors and nested query expressions. We also show how to extend the monoid calculus to deal with vectors and arrays in more expressive ways than current query languages do, and illustrate how it can handle identity and updates. 1 Introduction  A much-touted advantage of the relational data model is the existence of a formal calculus and algebra to model database queries. In practice, these formalisms fail to model many of the features present in commercial query languages (e.g...
422|Comprehensions, a Query Notation for DBPLs|This paper argues that comprehensions, a construct found in some programming languages, are a good query notation for DBPLs. It is shown that, like many other query notations, comprehensions can be smoothly integrated into DBPLs and allow queries to be expressed clearly, concisely and efficiently. More significantly, two advantages of comprehensions are demonstrated. The first advantage is that, unlike conventional notations, comprehension queries combine computational power with ease of optimisation. That is, not only can comprehension queries express both recursion and computation, but equivalent comprehension transformations exist for all of the major conventional optimisations. The second advantage is that comprehensions provide a uniform notation for expressing and performing some optimisation on queries over several bulk data types. The bulk types that comprehensions can be defined over include sets, relations, bags and lists. A DBPL can also be automatically extended to provide and partially optimise comprehension queries over new bulk types constructed by the application programmer, providing that the new type has some well-defined properties. 1
423|Improving List Comprehension Database Queries|The task of increasing the efficiency of database queries has recieved considerable attention. In this paper we describe the improvement of queries expressed as list comprehensions in a lazy functional language. The database literature identifies four algebraic and two implementation-based improvement strategies. For each strategy we show an equivalent improvement for queries expressed as list comprehensions. This means that welldeveloped database algorithms that improve queries using several of these strategies can be emulated to improve comprehension queries. We are also able to improve queries which require greater power than that provided by the relational algebra. Most of the improvements entail transforming a simple, inefficient query into a more complex, but more efficient form. We illustrate each improvement using examples drawn from the database literature.  1 Introduction The functional programming community is often accused of being too inward looking. Functional languages a...
424|Optimization of Nested Queries in a Complex Object Model|. Transformation of nested SQL queries into join queries is advantageous because a nested SQL query can be looked upon as a nested-loop join, which is just one of the several join implementations that may be available in a relational DBMS. In join queries, dangling (unmatched) operand tuples are lost, which causes a problem in transforming nested queries having the aggregate function COUNT between query blocks--a problem that has become well-known as the COUNT bug. In the relational context, the outerjoin has been employed to solve the COUNT bug. In complex object models supporting an SQL-like query language, transformation of nested queries into join queries is an important optimization issue as well. The COUNT bug turns out to be a special case of a general problem being revealed in a complex object model. To solve the more general problem, we introduce the nest join operator, which is a generalization of the outerjoin for complex objects. 1 Introduction  Currently, at the University...
425|Translating OQL into Monoid Comprehensions -- Stuck with Nested Loops?|This work tries to employ the monoid comprehension calculus --- which has proven to be an adequate  framework to capture the semantics of modern object query languages featuring a family of collection  types like sets, bags, and lists --- in a twofold manner: First, serving as a target language for the translation  of ODMG OQL queries. We review work done in this field and also give comprehension calculus  equivalents for the recently introduced OQL 1.2 concepts.  Second, we use monoid comprehensions as the formalism in which we try to find efficient execution  methods working on a rich set of physical structures (including indices, vertical and horizontal decomposition,  etc.). The main problem coming up here is the &#034;nested-loop nature&#034; of the calculus expressions.  While these loop-based semantics for evaluating comprehensions at least provide a way for executing  OQL queries, their execution is almost always much less efficient than alternative physical algorithms of  the database e...
426|Specifying Rule-based Query Optimizers in a Reflective Framework|. Numerous structures for database query optimizers have been proposed. Many of those proposals aimed at automating the construction of query optimizers from some kind of specification of optimizer behavior. These specification frameworks do a good job of partitioning and modularizing the kinds of information needed to generate a query optimizer. Most of them represent at least part of this information in a rule-like form. Nevertheless, large portions of these specifications still take a procedural form. The contributions of this work are threefold. We present a language for specifying optimizers that captures a larger portion of the necessary information in a declarative manner. This language is in turn based on a model of query rewriting where query expressions carry annotations that are propagated during query transformation and planning. This framework is reminiscent of inherited and synthesized attributes for attribute grammars, and we believe it is expressive of a wide range of i...
427|A Semantics of Compile-time Reflection|A new language incorporating both ML-style type checking and a limited form of reflection is defined by giving an interpreter and showing how this interpreter may be interpreted as a compositional denotational semantics. The resulting language has a partial function as a compiler, but if the compiler terminates without a type error there will be no type-errors at runtime. Typing issues and first-class environments are discussed as well. 1 Introduction  Traditionally reflection and strong typing are found on different sides of the schism that divides the lisp and functional programming communities. This paper introduces Compile-time Reflective ML (CRML), a reflective language with compile-time type checking and run-time type safety[14]. CRML provides a bridge between two rich cultures. When developing CRML, the goal was to create a system that was both expressive and useful, supported higher abstraction mechanisms, and that could be completely type checked at compile-time. The implement...
428|A Functional Object Database Language|The language BCOOL is formally defined using a denotational semantics  approach. BCOOL is a functional object database language with a  very flexible, yet strong and statically checked, type system. Its main  source of flexibility is its support for object evolution, that is, dynamic  type changes of existing objects. Originally, BCOOLwas used as a formal  basis for a more traditional (relational algebra-style) database language,  COOL. In this paper, though, BCOOL is presented on its own. The  purpose being to compare with other functional languages and discuss  the virtues and limitations that BCOOL and these functional languages  have w.r.t. each other in terms of (i) the above-mentioned flexibility  in the type system, which we consider essential for objects and (ii) the  orthogonality of the language.  1 Introduction  COOL is an object database query language developed in the COCOON project [20, 21]. In a nutshell, COOL is an object-flavored extension of a (nested) relational alge...
429|Investigation of Algebraic Query Optimisation for Database Programming Languages|alexQdcs.kcl.ac.uk A major challenge still facing the designers and implementors of database programming languages (DBPLs) is that of query optimisa-tion. We investigate algebraic query optimi-sation techniques for DBPLs in the context of a purely declarative functional language that supports sets as first-class objects. Since the language is computationally complete issues such as non-termination of expressions and construction of infinite data structures can be investigated, whilst its declarative nature al-lows the issue of side effects to be avoided and a richer set of equivalences to be developed. The support of a set bulk data type enables much prior work on the optimisation of rela-tional languages to be utilised. Finally, the language has a well-defined semantics which permits us to reason formally about the prop erties of expressions, such as their equivalence with other expressions and their termination. 1
430|The CROQUE-Model: Formalization of the Data Model and Query Language|The ODMG proposal has helped to focus the work on object-oriented databases (OODBs) onto a common object model and query language. Nevertheless there are several shortcomings of the current proposal stemming from the adaption of concepts of object-oriented programming and a lack of formalization. In this paper we present a formalization of the ODMG model and the OQL query language that is used in the CROQUE project as a basis for query optimization. An essential part is a complete, formally sound type system that allows us to reason about the types of intermediate query results and gives rise to fully orthogonal queries, including useful extensions of projections and set operations. 1 Introduction  For a long time, the evolution of OODB seemed to disperse in quite different directions: there were rather distinct object-oriented database models (OODMs) either based on (nested) relational formalisms or OOPL-like notions, and hardly any consensus about the structure and formalization of q...
431|Query Optimization using SQL Transformations |Query optimization is of great importance for the performance of a relational database, especially for the execution of complex SQL statements. A query optimizer determines the best strategy for performing each query. The query optimizer chooses, for example, whether or not to use indexes for a given query, and which join techniques to use when joining multiple tables. These decisions have a tremendous effect on SQL performance, and query optimization is a key technology for every application, from operational systems to data warehouse and analysis systems to content-management systems. Oracle’s optimizer consists of four major components: SQL transformations, Execution plan selection, Cost model and statistics and Dynamic runtime optimization. We will discuss SQL transformations in this paper. 1.
432|Power Hints for Query Optimization|Abstract — Commercial database systems expose query hints to address situations in which the optimizer chooses a poor plan for a given query. However, current query hints are not flexible enough to deal with a variety of non-trivial scenarios. In this paper, we introduce a hinting framework that enables the specification of rich constraints to influence the optimizer to pick better plans. We show that while our framework unifies previous approaches, it goes considerably beyond existing hinting mechanisms, and can be implemented efficiently with moderate changes to current optimizers. I.
433|The Skyline Operator|We propose to extend database systems by a Skyline operation. This operation filters out a set of interesting points from a potentially large set of data points. A point is interesting if it is not dominated by any other point. For example, a hotel might be interesting for somebody traveling to Nassau if no other hotel is both cheaper and closer to the beach. We show how SQL can be extended to pose Skyline queries, present and evaluate alternative algorithms to implement the Skyline operation, and show how this operation can be combined with other database operations (e.g., join and Top N).
434|An Efficient Framework for Order Optimization|Since the introduction of cost-based query optimization, the performance-critical role of interesting orders has been recognized. Some algebraic operators change interesting orders (e.g. sort and select), while others exploit interesting orders (e.g. merge join). The two operations performed by any query optimizer during plan generation are 1) computing the resulting order given an input order and an algebraic operator and 2) determining the compatibility between a given input order and the required order a given algebraic operator can beneficially exploit. Since these two operations are called millions of times during plan generation, they are highly performance-critical. The third crucial parameter is the space requirement for annotating every plan node with its output order.
435|Configuration-parametric query optimization for physical design tuning|Automated physical design tuning for database systems has recently become an active area of research and development. Existing tuning tools explore the space of feasible solutions by repeatedly optimizing queries in the input workload for several candidate configurations. This general approach, while scalable, often results in tuning sessions waiting for results from the query optimizer over 90 % of the time. In this paper we introduce a novel approach, called Configuration-Parametric Query Optimization, that drastically improves the performance of current tuning tools. By issuing a single optimization call per query, we are able to generate a compact representation of the optimization space that can then produce very efficiently execution plans for the input query under arbitrary configurations. Our experiments show that our technique speedsup query optimization by 30x to over 450x with virtually no loss in quality, and effectively eliminates the optimization bottleneck in existing tuning tools. Our techniques open the door for new, more sophisticated optimization strategies by eliminating the main bottleneck of current tuning tools.
436|Sparql query optimization on top of dhts|Abstract. We study the problem of SPARQL query optimization on top of distributed hash tables. Existing works on SPARQL query processing in such environments have never been implemented in a real system, or do not utilize any optimization techniques and thus exhibit poor perfor-mance. Our goal in this paper is to propose efficient and scalable algo-rithms for optimizing SPARQL basic graph pattern queries. We augment a known distributed query processing algorithm with query optimization strategies that improve performance in terms of query response time and bandwidth usage. We implement our techniques in the system Atlas and study their performance experimentally in a local cluster. 1
437|Handling Churn in a DHT|This paper addresses the problem of churn---the continuous process of node arrival and departure---in distributed hash tables (DHTs). We argue that DHTs should perform lookups quickly and consistently under churn rates at least as high as those observed in deployed P2P systems such as Kazaa. We then show through experiments on an emulated network that current DHT implementations cannot handle such churn rates. Next, we identify and explore three factors affecting DHT performance under churn: reactive versus periodic failure recovery, message timeout calculation, and proximity neighbor selection. We work in the context of a mature DHT implementation called Bamboo, using the ModelNet network emulator, which models in-network queuing, cross-traffic, and packet loss. These factors are typically missing in earlier simulationbased DHT studies, and we show that careful attention to them in Bamboo&#039;s design allows it to function effectively at churn rates at or higher than that observed in P2P file-sharing applications, while using lower maintenance bandwidth than other DHT implementations.
438|Hexastore: Sextuple Indexing for Semantic Web Data Management|Despite the intense interest towards realizing the Semantic Web vision, most existing RDF data management schemes are constrained in terms of efficiency and scalability. Still, the growing popularity of the RDF format arguably calls for an effort to offset these drawbacks. Viewed from a relationaldatabase perspective, these constraints are derived from the very nature of the RDF data model, which is based on a triple format. Recent research has attempted to address these constraints using a vertical-partitioning approach, in which separate two-column tables are constructed for each property. However, as we show, this approach suffers from similar scalability drawbacks on queries that are not bound by RDF property value. In this paper, we propose an RDF storage scheme that uses the triple nature of RDF as an asset. This scheme enhances the vertical partitioning idea and takes it to its logical conclusion. RDF data is indexed in six possible ways, one for each possible ordering of the three RDF elements. Each instance of an RDF element is associated with two vectors; each such vector gathers elements of one of the other types, along with lists of the third-type resources attached to each vector element. Hence, a sextupleindexing scheme emerges. This format allows for quick and scalable general-purpose query processing; it confers significant advantages (up to five orders of magnitude) compared to previous approaches for RDF data management, at the price of a worst-case five-fold increase in index space. We experimentally document the advantages of our approach on real-world and synthetic data sets with practical queries.
439|RDF-3X: a RISC-style Engine for RDF|RDF is a data representation format for schema-free structured information that is gaining momentum in the context of Semantic-Web corpora, life sciences, and also Web 2.0 platforms. The “pay-as-you-go ” nature of RDF and the flexible pattern-matching capabilities of its query language SPARQL entail efficiency and scalability challenges for complex queries including long join paths. This paper presents the RDF-3X engine, an implementation of SPARQL that achieves excellent performance by pursuing a RISC-style architecture with a streamlined architecture and carefully designed, puristic data structures and operations. The salient points of RDF-3X are: 1) a generic solution for storing and indexing RDF triples that completely eliminates the need for physical-design tuning, 2) a powerful yet simple query processor that leverages fast merge joins to the largest possible extent, and 3) a query optimizer for choosing optimal join orders using a cost model based on statistical synopses for entire join paths. The performance of RDF-3X, in comparison to the previously best state-of-the-art systems, has been measured on several large-scale datasets with more than 50 million RDF triples and benchmark queries that include pattern matching and long join paths in the underlying data graphs.
440|YARS2: A federated repository for querying graph structured data from the Web|Abstract. We present the architecture of an end-to-end semantic search engine that uses a graph data model to enable interactive query answer-ing over structured and interlinked data collected from many disparate sources on the Web. In particular, we study distributed indexing meth-ods for graph-structured data and parallel query evaluation methods on a cluster of computers. We evaluate the system on a dataset with 430 million statements collected from the Web, and provide scale-up experi-ments on 7 billion synthetically generated statements. 1
441|SP 2 Bench: A SPARQL performance benchmark|Abstract — Recently, the SPARQL query language for RDF has reached the W3C recommendation status. In response to this emerging standard, the database community is currently exploring efficient storage techniques for RDF data and evaluation strategies for SPARQL queries. A meaningful analysis and comparison of these approaches necessitates a comprehensive and universal benchmark platform. To this end, we have developed SP 2 Bench, a publicly available, language-specific SPARQL performance benchmark. SP 2 Bench is settled in the DBLP scenario and comprises both a data generator for creating arbitrarily large DBLP-like documents and a set of carefully designed benchmark queries. The generated documents mirror key characteristics and social-world distributions encountered in the original DBLP data set, while the queries implement meaningful requests on top of this data, covering a variety of SPARQL operator constellations and RDF access patterns. As a proof of concept, we apply SP 2 Bench to existing engines and discuss their strengths and weaknesses that follow immediately from the benchmark results. I.
442|SPARQL Basic Graph Pattern Optimization Using Selectivity Estimation |In this paper, we formalize the problem of Basic Graph Pattern (BGP) optimization for SPARQL queries and main memory graph implementations of RDF data. We define and analyze the characteristics of heuristics for selectivitybased static BGP optimization. The heuristics range from simple triple pattern variable counting to more sophisticated selectivity estimation techniques. Customized summary statistics for RDF data enable the selectivity estimation of joined triple patterns and the development of efficient heuristics. Using the Lehigh University Benchmark (LUBM), we evaluate the performance of the heuristics for the queries provided by the LUBM and discuss some of them in more details.
443|A Subscribable Peer-to-Peer RDF Repository for Distributed Metadata Management|In this paper, we present a scalable Peer-to-Peer RDF repository, named RDF-Peers, which stores each triple in a multi-attribute addressable network by applying globally known hash functions. Queries can be efficiently routed to the nodes that store matching triples. RDFPeers also supports users to selectively subscribe to RDF content. In RDFPeers, both the neighbors per node and the routing hops for triple insertion, most query resolution and triple subscription are logarithmic to the network size. Our experiments with real-world RDF data demonstrated that the triple-storing load among nodes in RDFPeers differs by less than an order of magnitude.
444|Evaluating Conjunctive Triple Pattern Queries over Large Structured Overlay Networks |Abstract. We study the problem of evaluating conjunctive queries composed of triple patterns over RDF data stored in distributed hash tables. Our goal is to develop algorithms that scale to large amounts of RDF data, distribute the query processing load evenly and incur little network traffic. We present and evaluate two novel query processing algorithms with these possibly conflicting goals in mind. We discuss the various tradeoffs that occur in our setting through a detailed experimental evaluation of the proposed algorithms. 1
445|Towards web scale RDF|Abstract. We are witnessing the first stages of the document web be-coming a data web, with the implied new opportunities for discovering, re-purposing, ”meshing up ” and analyzing linked data. There is an in-creasing volume of linked open data and the first data web search engines are taking shape. Dealing with queries against the nascent data web may easily add two orders of magnitude in computing power requirements on top of what a text search engine faces. Queries may involve arbitrary joining, aggregation, filtering and so forth, compounded by the need for inference and on the fly schema mapping. This is the environment for which Virtuoso Cluster Edition is intended. This paper presents the main challenges encountered and solutions ar-rived at during the development of this software product. We present adaptations of RDF load and query execution and query planning suited for distributed memory platforms, with special emphasis on dealing with message latency and the special operations required by RDF. 1
446|Distributed Hash Sketches: Scalable, Efficient, and Accurate Cardinality Estimation for Distributed Multisets|Counting items in a distributed system, and estimating the cardinality of multisets in particular, is important for a large variety of applications and a fundamental building block for emerging Internet-scale information systems. Examples of such applications range from optimizing query access plans in peer-to-peer data sharing, to computing the significance (rank/score) of data items in distributed information retrieval. The general formal problem addressed in this article is comput-ing the network-wide distinct number of items with some property (e.g., distinct files with file name containing “spiderman”) where each node in the network holds an arbitrary subset, possibly over-lapping the subsets of other nodes. The key requirements that a viable approach must satisfy are: (1) scalability towards very large network size, (2) efficiency regarding messaging overhead, (3) load balance of storage and access, (4) accuracy of the cardinality estimation, and (5) simplicity and easy integration in applications. This article contributes the DHS (Distributed Hash Sketches) method for this problem setting: a distributed, scalable, efficient, and accurate multiset cardinality estima-tor. DHS is based on hash sketches for probabilistic counting, but distributes the bits of each counter across network nodes in a judicious manner based on principles of Distributed Hash Tables, paying careful attention to fast access and aggregation as well as update costs. The article discusses various
447|Query Processing in a DHT-Based Universal Storage - The World as a Peer-toPeer Database|von / by
448|Query Execution Techniques for Caching Expensive Methods|. Object-Relational and Object-Oriented DBMSs allow users to invoke time-consuming (&#034;expensive&#034;) methods in their queries. When queries containing these expensive methods are run on data with duplicate values, time is wasted redundantly computing methods on the same value. This problem has been studied in the context of programming languages, where &#034;memoization&#034; is the standard solution. In the database literature, sorting has been proposed to deal with this problem. We compare these approachesalong with a third solution, a variant of unary hybrid hashing which we call Hybrid Cache. We demonstrate that Hybrid Cache always dominates memoization, and significantly outperforms sorting in many instances. This provides new insights into the tradeoff between hashing and sorting for unary operations. Additionally, our Hybrid Cache algorithm includes some new optimizations for unary hybrid hashing, which can be used for other applications such as grouping and duplicate elimination. We conclude...
449| Computing Capabilities of Mediators |Existing data-integration systems based on the mediation architecture employ avariety of mechanisms to describe the query-processing capabilities of sources. However, these systems do not compute the capabilities of the mediators based on the capabilities of the sources they integrate. In this paper, we propose a framework to capture a rich variety of query-processing capabilities of data sources and mediators. We present algorithms to compute the set of supported queries of a mediator, based on the capability limitations of its sources. Our algorithms take into consideration a variety of query-processing techniques employed by mediators to enhance the set of supported queries.
450|Join Queries with External Text Sources: Execution and Optimization Techniques|Text is a pervasive information type, and many applications require querying over text sources in addition to structured data. This paper studies the problem of query processing in a system that loosely integrates an extensible database system and a text retrieval system. We focus on a class of conjunctive queries that include joins between text and structured data, in addition to selections over these two types of data. We adapt techniques from distributed query processing and introduce a novel class of join methods based on probing that is especially useful for joins with text systems, and we present a cost model for the various alternative query processing methods. Experimental results confirm the utility of these methods. The space of query plans is extended due to the additional techniques, and we describe an optimization algorithm for searching this extended space. The techniques we describe in this paper are applicable to other types of external data managers loosely integrated ...
451|Cost-Based Optimization for Magic: Algebra and Implementation|Magic sets rewriting is a well-known optimization heuristic for complex decision-support queries. There can be many variants of this rewriting even for a single query, which differ greatly in execution performance. We propose cost-based techniques for selecting an efficient variant from the many choices. Our first contribution is a practical scheme that modelsmagic sets rewriting as a special join method that can be added to any cost-based query optimizer. We derive cost formulas that allow an optimizer to choose the best variant of the rewriting and to decide whether it is beneficial. The order of complexity of the optimization process is preserved by limiting the search space in a reasonable manner. We have implemented this technique in IBM&#039;s DB2 C/S V2 database system. Our performance measurements demonstrate that the costbasedmagic optimization technique performs well, and that without it, several poor decisions could be made. Our second contribution is a formal algebraic model of ...
452|Efficient Dynamic Programming Algorithms for Ordering Expensive Joins and Selections|1 Introduction Traditional work on algebraic query optimization has mainly focused on the problem of ordering joins in a query. Restrictions like selections and projections are generally treated by &amp;quot;push-down rules&amp;quot;. According to these, selections and projections should be pushed down the query plan as far as possible. These heuristic rules worked quite well for traditional relational database systems where the evaluation of selection predicates is of neglectable cost and every selection reduces the cost of subsequent joins. As pointed out by Hellerstein, Stonebraker [5], this is no longer true for modern database systems like object-oriented DBMSs? Research supported by the German Research Association (DFG) under contract
453|Query Optimization over Crowdsourced Data ? |Deco is a comprehensive system for answering declarative queries posed over stored relational data together with data obtained ondemand from the crowd. In this paper we describe Deco’s costbased query optimizer, building on Deco’s data model, query language, and query execution engine presented earlier. Deco’s objective in query optimization is to find the best query plan to answer a query, in terms of estimated monetary cost. Deco’s query semantics and plan execution strategies require several fundamental changes to traditional query optimization. Novel techniques incorporated into Deco’s query optimizer include a cost model distinguishing between “free ” existing data versus paid new data, a cardinality estimation algorithm coping with changes to the database state during query execution, and a plan enumeration algorithm maximizing reuse of common subplans in a setting that makes reuse challenging. We experimentally evaluate Deco’s query optimizer, focusing on the accuracy of cost estimation and the efficiency of plan enumeration. 1.
454|Human-powered Sorts and Joins |Crowdsourcing marketplaces like Amazon’s Mechanical Turk (MTurk) make it possible to task people with small jobs, such as labeling images or looking up phone numbers, via a programmatic interface. MTurk tasks for processing datasets with humans are currently designed with significant reimplementation of common workflows and ad-hoc selection of parameters such as price to pay per task. We describe how we have integrated crowds into a declarative workflow engine called Qurk to reduce the burden on workflow designers. In this paper, we focus on how to use humans to compare items for sorting and joining data, two of the most common operations in DBMSs. We describe our basic query interface and the user interface of the tasks we post to MTurk. We also propose a number of optimizations, including task batching, replacing pairwise comparisons with numerical ratings, and pre-filtering tables before joining them, which dramatically reduce the overall cost of running sorts and joins on the crowd. In an experiment joining two sets of images, we reduce the overall cost from $67 in a naive implementation to about $3, without substantially affecting accuracy or latency. In an end-to-end experiment, we reduced cost by a factor of 14.5. 1.
455|On Saying &#034;Enough Already!&#034; in SQL|In this paper, we study a simple SQL extension that enables query writers to explicitly limit the cardinality of a query result. We examine its impact on the query optimization and run-time execution components of a relational DBMS, presenting two approaches---a Conservative approach and an Aggressive approach---to exploiting cardinality limits in relational query plans. Results obtained from an empirical study conducted using DB2 demonstrate the benefits of the SQL extension and illustrate the tradeoffs between our two approaches to implementing it.  1 Introduction  The SQL-92 query language includes support for a wide range of relational query operations, including selection, projection, many flavors of joins, unions, sorting, aggregation, grouping, and subqueries [MS93]. In addition, SQL continues to evolve, with extensions such as the object features of SQL3 and the control constructs of SQL/PSM. Surprisingly, despite its impressive query power, SQL provides no way to specify a lim...
456|Answering Search Queries with CrowdSearcher |Web users are increasingly relying on social interaction to complete and validate the results of their search activities. While search systems are superior machines to get world-wide information, the opinions collected within friends and expert/local communities can ultimately determine our decisions: human curiosity and creativity is often capable of going much beyond the capabilities of search systems in scouting “interesting ” results, or suggesting new, unexpected search directions. Such personalized interaction occurs in most times aside of the search systems and processes, possibly instrumented and mediated by a social network; when such interaction is completed and users resort to the use of search systems, they do it through new queries, loosely related to the previous search or to the social interaction. In this paper we propose CrowdSearcher, a novel search paradigm
457|Deco: Declarative crowdsourcing|Crowdsourcing enables programmers to incorporate “human com-putation ” as a building block in algorithms that cannot be fully automated, such as text analysis and image recognition. Simi-larly, humans can be used as a building block in data-intensive applications—providing, comparing, and verifying data used by applications. Building upon the decades-long success of declara-tive approaches to conventional data management, we use a similar approach for data-intensive applications that incorporate humans. Specifically, declarative queries are posed over stored relational data as well as data computed on-demand from the crowd, and the underlying system orchestrates the computation of query answers. We present Deco, a database system for declarative crowdsourc-ing. We describe Deco’s data model, query language, and our pro-totype. Deco’s data model was designed to be general (it can be instantiated to other proposed models), flexible (it allows methods for data cleansing and external access to be plugged in), and prin-cipled (it has a precisely-defined semantics). Syntactically, Deco’s query language is a simple extension to SQL. Based on Deco’s data model, we define a precise semantics for arbitrary queries involv-ing both stored data and data obtained from the crowd. We then describe the Deco query processor which uses a novel push-pull hybrid execution model to respect the Deco semantics while coping with the unique combination of latency, monetary cost, and uncer-tainty introduced in the crowdsourcing environment. Finally, we describe our current prototype, and we experimentally explore the query processing alternatives provided by Deco. 1.
458|Deco: A system for declarative crowdsourcing |Deco is a system that enables declarative crowdsourcing: answer-ing SQL queries posed over data gathered from the crowd as well as existing relational data. Deco implements a novel push-pull hybrid execution model in order to support a flexible data model and a precise query semantics, while coping with the combination of latency, monetary cost, and uncertainty of crowdsourcing. We demonstrate Deco using two crowdsourcing platforms: Amazon Mechanical Turk and an in-house platform, to show how Deco pro-vides a convenient means of collecting and querying crowdsourced data. 1.
459|Declarative Platform for Data Sourcing Games|Harnessing a crowd of users for the collection of mass data (data sourcing) has recently become a wide-spread practice. One effective technique is based on games as a tool that attracts the crowd to contribute useful facts. We focus here on the data management layer of such games, and observe that the development of this layer involves challenges such as dealing with probabilistic data, combined with recursive manipulation of this data. These challenges are difficult to address using current declarative data management frameworks, and we thus propose here a novel such framework, and demonstrate its usefulness in expressing different aspects in the data management of Trivia-like games. We have implemented a system prototype with our novel data management framework at its core, and we highlight key issues in the system design, as well as our experimentations that indicate the usefulness and scalability of the approach.
460|Query processing over crowdsourced data, http://ilpubs.stanford.edu:8090/1052|We are building Deco, a comprehensive system for answering declar-ative queries posed over stored relational data together with data gathered from the crowd. In this paper we present Deco’s query processor, building on Deco’s data model and query language pre-sented earlier. In general, it has been observed that query process-ing over crowdsourced data must contend with issues and tradeoffs involving cost, latency, and uncertainty that don’t arise in tradi-tional query processing. Deco’s overall objective in query execu-tion is to maximize parallelism while fetching data from the crowd (to keep latency low), but only when the parallelism will not issue too many tasks (which would increase cost). Meeting this objec-tive requires a number of changes from traditional query execution. First, Deco’s query processor uses a hybrid execution model, which respects Deco semantics while enabling our objective. Our objec-tive also requires prioritizing accesses to crowdsourced data, which turns out to be an interesting NP-hard problem. Finally, because Deco incorporates resolution functions to handle the uncertainty in crowdsourced data, query execution bears as much similarity to in-cremental view maintenance as to a traditional iterator model. The paper includes initial experimental results, focusing primarily on how our query execution model and access prioritization scheme maximize parallelism without increasing cost. 1.
461|Variance Aware Optimization of Parameterized Queries |Parameterized queries are commonly used in database applications. In a parameterized query, the same SQL statement is potentially executed multiple times with different parameter values. In today?s DBMSs the query optimizer typically chooses a single execution plan that is reused for multiple instances of the same query. A key problem is that even if a plan with low average cost across instances is chosen, its variance can be high, which is undesirable in many production settings. In this paper, we describe techniques for selecting a plan that can better address the trade-off between the average and variance of cost across instances of a parameterized query. We show how to efficiently compute the skyline in the average-variance cost space. We have implemented our techniques on top of a commercial DBMS. We present experimental results on benchmark and real-world decision support queries.
462|Join Query Optimization in Distributed Databases |Abstract- Query Optimization is to use the best plan for the query that improves the performance of the query. Query Optimization is difficult in distributed databases as compared to centralized databases. Queries in distributed databases are effected by factors such as insertion methods of the data into the remote server and transmission time between servers. Response time of the query depends upon the transmission time, local processing speed. A I.
463|From Structured Documents to Novel Query Facilities|Structured documents (e.g., SGML) can benefit a lot from database support and more specifically from object-oriented database (OODB) management systems. This paper describes a natural mapping from SGML documents into OODB&#039;s and a formal extension of two OODB query languages (one SQL-like and the other calculus) in order to deal with SGML document retrieval. Although motivated by structured documents, the extensions of query languages that we present are general and useful for a variety of other OODB applications. A key element is the introduction of paths as first class citizens. The new features allow to query data (and to some extent schema) without exact knowledge of the schema in a simple and homogeneous fashion. 1 Introduction  Structured documents are central to a wide class of applications such as software engineering, libraries, technical documentation, etc. They are often stored in file systems and document access tools are somewhat limited. We believe that (object-oriented) d...
464|Finding Regular Simple Paths In Graph Databases|We consider the following problem: given a labelled directed graph G and a regular expression R, find all pairs of nodes connected by a simple path such that the concatenation of the labels along the path satisfies R. The problem is motivated by the observation that many recursive queries in relational databases can be expressed in this form, and by the implementation of a query language, G+ , based on this observation. We show that the problem is in general intractable, but present an algorithm than runs in polynomial time in the size of the graph when the regular expression and the graph are free of conflicts. We also present a class of languages whose expressions can always be evaluated in time polynomial in the size of both the graph and the expression, and characterize syntactically the expressions for such languages. 
465|MedMaker: A Mediation System Based on Declarative Specifications|Mediators are used for integration of heterogeneous information sources. We present a system for declaratively specifying mediators. It is targeted for integration of sources with unstructured or semi-structured data and/or sources with changing schemas. We illustrate the main features of the Mediator Specification Language (MSL), show how they facilitate integration, and describe the implementation of the system that interprets the MSL specifications. 
466|Integrating a Structured-Text Retrieval System with an Object-Oriented Database System|We describe the integration of a structured-text retrieval system (TextMachine) into an  object-oriented database system (OpenODB). Our approach is a light-weight one, using the external  function capability of the database system to encapsulate the text retrieval system as an  external information source. Yet, we are able to provide a tight integration in the query language  and processing; the user can access the text retrieval system using a standard database query  language. The efficient and effective retrieval of structured text performed by the text retrieval  system is seamlessly combined with the rich modeling and general-purpose querying capabilities  of the database system, resulting in an integrated system with querying power beyond those of  the underlying systems. The integrated system also provides uniform access to textual data in  the text retrieval system and structured data in the database system, thereby achieving information  fusion. We discuss the design and imple...
467|Efficiency In The Columbia Database Query Optimizer|An abstract of the thesis of Yongwen Xu for the Master of Science in Computer Science presented February 12,1998. Title: Efficiency in the Columbia Database Query Optimizer Query optimization is an area where database systems can achieve significant performance gains. Modern database applications demand optimizers with high extensibility and efficiency. Although more than one decade&#039;s efforts have been contributed to these areas, the state of the art in optimizer research is still not adequate for the demands of business. The goal of our Columbia project is to provide efficient and extensible tools for query optimization, particularly for complex queries and new data models. Efficiency is the main focus of this thesis. This thesis describes the design and implementation of the Columbia Query Optimizer, which obtains significant performance improvement while extensibility is not sacrificed. Based on the top-down optimization algorithm of the Cascades Optimizer Framework, Columbia simpli...
468|Starburst Mid-Flight: As the Dust Clears|ter, is improving the design of relational database management sys-tems and enhancing their performance, while building an extensible system to better support nontraditional applications (such as engineer-ing, geographic, office, etc.), and to serve as a testbed for future im-provements in database technology. As of November 1989, we have an initial prototype of our system up and running. In this paper, we reflect on the design and implementation of the Starburst system to date. We examine some key design decisions, and how they affect the goal of improved structure and performance. We also examine how well we have met our goal of extensibility: what aspects of the system are ex-tensible, how extensions can be done, and how easy it is to add exten-sions. We discuss some actual extensions to the system, including the experiences of our first real customizers. Index Terms-Access methods, data structures, extensibility, plan optimization, query processing, relational database system, rule sys-tems.
469|Readings in object-oriented database systems|This paper summarizes the interface, implementation, and use of a server process that is used as a backend by an object-oriented database system. This server is responsible for managing objects on secondary storage, managing transactions, and implementing a simple form of trigger. We sketch the interface of this system and point out some of the more interesting implementation issues that were encountered in building it. Client processes communicate asynchronously with the server by message sending. The system is designed to be as efficient as possible since one of its clients is the GARDEN system, an object-oriented programming environment. GARDEN views both static and dynamic program pieces as objects. Our back-end server provides persistent and sharable storage for GARDEN. The paper includes an extended example of how GARDEN makes use of this resource. 1.
470|A Region Based Query Optimizer through Cascades Optimizer Framework|The Cascades Query Optimizer Framework is a tool to help the database implementor (DBI) in constructing a query optimizer for a DBMS. It is data model independent and allows to code a query optimizer by providing the implementations of the subclasses of prede ned interface classes. When the implementations of the required classes are provided properly, the generated optimizer produces the optimum execution plans for the queries. Although providing the complete set of rules and thus nding the optimum execution plans are bene cial for most of the queries, the query optimization time increases unacceptably for certain types of queries, e.g., for star queries. Hence it is important to be able to limit the number of alternative plans considered by the optimizer for speci c types of queries by using the proper heuristics for each type. This leads to the concept of region based query optimization, where di erent types of queries are optimized by using di erent search strategies in each region. This paper describes our experiences in developing a region based query optimizer through Cascades. Cascades&#039; guidance structures provide the facilities required for the design and implementation of a region based optimizer. The performance comparisons between a region based query optimizer and an exhaustive (which uses the complete rule set without heuristic guidance) query optimizer, both generated through Cascades, indicate that the region based optimizer has a superior performance. In the performance analysis, both the sum of optimization and execution times, namely the response time, and the quality of the plans generated are investigated. 1
471|Efficient and Effective Querying by Image Content|In the QBIC (Query By Image Content) project we are studying methods to query large  on-line image databases using the images&#039; content as the basis of the queries. Examples of  the content we use include color, texture, and shape of image objects and regions. Potential  applications include medical (&#034;Give me other images that contain a tumor with a texture like this  one&#034;), photo-journalism (&#034;Give me images that have blue at the top and red at the bottom&#034;),  and many others in art, fashion, cataloging, retailing, and industry.  We describe a set of novel features and similarity measures allowing query by color, texture,  and shape of image object. We demonstrate the effectiveness of the QBIC system with normalized  precision and recall experiments on test databases containing over 1000 images and 1000  objects populated from commercially available photo clip art images, and of images of airplane  silhouettes. We also consider the efficient indexing of these features, specifically addre...
472|Color indexing|Computer vision is embracing a new research focus in which the aim is to develop visual skills for robots that allow them to interact with a dynamic, realistic environment. To achieve this aim, new kinds of vision algorithms need to be developed which run in real time and subserve the robot&#039;s goals. Two fundamental goals are determin-ing the location of a known object. Color can be successfully used for both tasks. This article demonstrates that color histograms of multicolored objects provide a robust, efficient cue for index-ing into a large database of models. It shows that color histograms are stable object representations in the presence of occlusion and over change in view, and that they can differentiate among a large number of objects. For solving the identification problem, it introduces a technique called Histogram Intersection, which matches model and im-age histograms and a fast incremental version of Histogram Intersection, which allows real-time indexing into a large database of stored models. For solving the location problem it introduces an algorithm called Histogram Backprojection, which performs this task efficiently in crowded scenes. 1
473|Computer Vision|Driver inattention is one of the main causes of traffic accidents. Monitoring a driver to detect inattention is a complex problem that involves physiological and behavioral elements. Different approaches have been made, and among them Computer Vision has the potential of monitoring the person behind the wheel without interfering with his driving. In this paper I have developed a system that can monitor the alertness of drivers in order to prevent people from falling asleep at the wheel. The other main aim of this algorithm is to have efficient performance on low quality webcam and without the use of infrared light which is harmful for the human eye. Motor vehicle accidents cause injury and death, and this system will help to decrease the amount of crashes due to fatigued drivers. The proposed algorithm will work in three main stages. In first stage the face of the driver is detected and tracked. In the second stage the facial features are extracted for further processing. In last stage the most crucial parameter is monitored which is eye’s status. In the last stage it is determined that whether the eyes are closed or open. On the basis of this result the warning is issued to the driver to take a break.
474|Voronoi diagrams -- a survey of a fundamental geometric data structure|This paper presents a survey of the Voronoi diagram, one of the most fundamental data structures in computational geometry. It demonstrates the importance and usefulness of the Voronoi diagram in a wide variety of fields inside and outside computer science and surveys the history of its development. The paper puts particular emphasis on the unified exposition of its mathematical and algorithmic properties. Finally, the paper provides the first comprehensive bibliography on Voronoi diagrams and related structures.
475|The grid file: an adaptable, symmetric multikey file structure|Traditional file structures that provide multikey access to records, for example, inverted files, are extensions of file structures originally designed for single-key access. They manifest various deficiencies in particular for multikey access to highly dynamic files. We study the dynamic aspects of tile structures that treat all keys symmetrically, that is, file structures which avoid the distinction between primary and secondary keys. We start from a bitmap approach and treat the problem of file design as one of data compression of a large sparse matrix. This leads to the notions of a grid partition of the search space and of a grid directory, which are the keys to a dynamic file structure called the grid file. This tile system adapts gracefully to its contents under insertions and deletions, and thus achieves an upper hound of two disk accesses for single record retrieval; it also handles range queries and partially specified queries efficiently. We discuss in detail the design decisions that led to the grid file, present simulation results of its behavior, and compare it to other multikey access file structures.
476|New Techniques for Best-Match Retrieval|A scheme to answer best-match queries from a file containing a collection of objects is described. A best-match query is to find the objects in the file that are closest (according to some (dis)similarity measure) to a given target. Previous work [5, 331 suggests that one can reduce the number of comparisons required to achieve the desired results using the triangle inequality, starting with a data structure for the file that reflects some precomputed intrafile distances. We generalize the technique to allow the optimum use of any given set of precomputed intrafile distances. Some empirical results are presented which illustrate the effectiveness of our scheme, and its performance relative to previous algorithms.
477|Semantic Query Optimization by Subsumption in OODB|: The purpose of semantic query optimization is to use semantic knowledge (e.g. integrity constraints) for transforming a query into a form that may be answered more efficiently than the original version. This paper proposes a general method for semantic query optimization in the framework of Object Oriented Database Systems. The method is applicable to the class of conjunctive queries and is based on two ingredients: a formalism able to express both class descriptions and integrity constraints rules as types; subsumption computation between types to evaluate the logical implications expressed by integrity constraints rules. 1 Introduction  The purpose of semantic query optimization is to transform a query into an equivalent one that may be answered more efficiently than the original version. The basic idea is to use semantic knowledge about the database to transform the query. Informally, semantic equivalence means that the transformed query has the same answer as the original query o...
478|Classification in  the KL-ONE knowledge representation system|KL-ONE lets one define and use a class of descriptive terms called Concepts, where each Concept denotes a set of objects A subsumption relation between Concepts is defined which is related to set inclusion by way of a semantics for Concepts. This subsumption relation defines a partial order on Concepts, and KL-ONE organizes all Concepts into a taxonomy that reflects this partial order. Classification is a process that takes a new Concept and determines other Concepts that either subsume it or that it subsumes, thereby determining the location for the new Concept within a given taxonomy. We discuss these issues and demonstrate some uses of the classification algorithm.  
479|A semantics of multiple inheritance|There are two major ways of structuring data in programming languages. The first and common one, used for example in Pascal, can be said to derive from standard branches of mathematics. Data is organized as cartesian products (i.e. record types), disjoint sums (i.e. unions or variant types) and function spaces (i.e. functions and procedures).
480|Subsumption between Queries to Object-Oriented Databases|Most work on query optimization in relational and object-oriented databases has concentrated on tuning algebraic expressions and the physical access to the database contents. The attention to semantic query optimization, however, has been restricted due to its inherent complexity. We take a second look at the problem for queries in object-oriented databases and find that reasoning techniques for concept languages developed in Artificial Intelligence apply for the following reasons: concept languages have been tailored for efficiency and their semantics is compatible with class and query definitions in object-oriented databases. We propose a query optimizer which decides subset relationships between a query and a view (a simpler query whose answer is stored) in polynomial time.   This work was supported in part by the Commission of the European Communities under ESPRIT Basic Research Action 6810 (Compulog 2), by the German Ministry of Research and Technology under grant ITW 92-01 (TACOS...
481|Acquisition and Validation of Complex Object Database Schemata Supporting Multiple Inheritance|We present an intelligent tool for the acquisition of object oriented schemata supporting multiple inheritance, which preserves taxonomy coherence and performs taxonomic inferences. Its theoretical framework is based on terminological logics, which have been developed in the area of artificial intelligence. The framework includes a rigorous formalization of complex objects, which is able to express cyclic references on the schema and instance level; a subsumption algorithm, which computes all implied specialization relationships between types; and an algorithm to detect incoherent types, i.e., necessarily empty types. Using results from formal analyses of knowledge representation languages, we show that subsumption and incoherence detection are computationally intractable from a theoretical point of view. However, the problems appear to be feasible in almost all practical cases.
482|Consistency Checking in Complex Object Database Schemata with Integrity Constraints|Integrity constraints are rules which should guarantee the integrity of a database. Provided that an adequate mechanism to express them is available, the following question arises: is there any way to populate a database which satisfies the constraints supplied by a database designer? i.e., does the database schema, including constraints, admit at least a non--empty model? This work gives an answer to the above question in a complex object database environment, providing a theoretical framework including the following ingredients: two alternative formalisms, able to express a relevant set of state integrity constraints with a declarative style; two specialized reasoners, based on the tableaux calculus, able to check the consistency of complex objects database schemata expressed with the two formalisms. The proposed formalisms share a common kernel, which supports complex objects and object identifiers, and allow the expression of acyclic descriptions of: classes, nested relati...
483|Query Optimization in Deductive Object Bases|1  . Deductive object bases are extended database systems  which amalgamate structural object-orientation with logical specification.  Queries in such a system are regarded both as classes and as  deduction rules. Besides a general architecture for query processing in  deductive object bases, two specific query optimization techniques are  presented: semantic query optimization with structural axioms of the  object base, and view maintenance optimization. The approach has  been formalized in the language Telos and implemented in the system  ConceptBase.  1  This work has been supported in part by ESPRIT BRA 3012 Compulog. A version of this paper also appears in Freytag, Maier, Vossen (eds.): Query processing in objectoriented, complex-object, and nested relation databases, Morgan Kaufmann, 1992.  1  1. Introduction  Traditionally, databases are systems for storing and accessing large amounts of shared persistent data in a secure way. Database research has always been concerned with pro...
484|An efficient semantic query optimization algorithm|Semantic query optimization uses problem-specific knowledge, represented as semantic constraints, to answer queries efficiently. Although the potential cost savings from semantic query optimization have been amply demonstrated, so far few cheap and effective search strategy for an optimal set of query transformations have been reported. This paper is based on our experience in designing and implementing a prototype semantic query optimizer for an object-oriented database system. Our approach tentatively applies all the possible transformations and delays the choice of beneficial transformations till the end. This approach makes the order of transformations immaterial and makes it possible to have an algorithm for query transformations that is of polynomial complexity. Preliminary experiments on the prototype show that the optimizer performs well for large databases. 1.
485| Towards Empirically Driven Query Optimization |Query optimization is a hard problem with exponential complexity. In this paper, we analyze the properties of query plans generated by a DBMS. Using these as plan logs, we discover the least cost plan for a new query. We developed a framework which collects statistics from the plans returned by the query optimizers and uses a distance function to select the least cost plan for a new query. The experimental results validate our approach by selecting near-optimal plans for a given set of test queries.  
486|Data Mining: Concepts and Techniques|Our capabilities of both generating and collecting data have been increasing rapidly in the last several decades. Contributing factors include the widespread use of bar codes for most commercial products, the computerization of many business, scientific and government transactions and managements, and advances in data collection tools ranging from scanned texture and image platforms, to on-line instrumentation in manufacturing and shopping, and to satellite remote sensing systems. In addition, popular use of the World Wide Web as a global information system has flooded us with a tremendous amount of data and information. This explosive growth in stored data has generated an urgent need for new techniques and automated tools that can intelligently assist us in transforming the vast amounts of data into useful information and knowledge. This book explores the concepts and techniques of data mining, a promising and flourishing frontier in database systems and new database applications. Data mining, also popularly referred to as knowledge discovery in databases (KDD), is the automated or convenient extraction of patterns representing knowledge implicitly stored in large databases, data warehouses, and other massive information repositories. Data mining is a multidisciplinary field, drawing work from areas including database technology, artificial intelligence, machine learning, neural networks, statistics, pattern recognition, knowledge based systems, knowledge acquisition, information retrieval, high performance computing, and data visualization. We present the material in
487|On the Complexity of Database Query Optimization|In this thesis, we consider the complexity of computing the optimal join order sequences for star queries and general queries. We consider the following join methods in our thesis - indexed nested loop joins, sort-merge joins, hash joins and block nested loop joins. The use of cartesian products is avoided and only linear trees are considered for query execution.
488|Memory Sensitive Query Optimization: An Introductory Study|Query optimizers have traditionally been memory insensitive, optimizing queries under the assumption that each operator in the resulting execution plan will be allotted the complete system memory. However, with main memory becoming an increasingly inexpensive resource, systems with large amounts of memory are fairly common. This thesis explores the idea of making query optimizers memory sensitive, so that memory requirements of each operator may be considered while optimizing the query. The available memory may be divided among a number of operators of the query execution plan. These operators can then be executed in a pipeline without the need for writing to disk at an intermediate stage; unlike the traditional optimizer, which forces a disk write after each operator, to free memory.
489|A Multi-Query Optimizer for Monet|Database systems allow for concurrent use of several applications (and query interfaces). Each application  generates an \optimal&#034; plan|a sequence of low-level database operators|for accessing the database. The  queries posed by users through the same application can be optimized together using traditional multi-query  optimization techniques. However, the commonalities among queries of dierent applications are not exploited.  In this paper we present an ecient inter-application multi-query optimizer that re-uses previously computed  (intermediate) results and eliminates redundant work. Experimental results on a single CPU system and a  parallel system show that the inter-application multi-query optimizer improves the query evaluation performance  signicantly.  1991 ACM Computing Classication System: [H.2.4] Main-Memory Database Systems, Query Processing  Keywords and Phrases: inter-application multi-query optimization, main-memory databases, data mining  Note: Work carried out unde...
490|MIL Primitives For Querying A Fragmented World|In query-intensive database application areas, like decision support and data mining, systems that use vertical fragmentation have a significant performance advantage. In order to support relational or object oriented applications on top of such a fragmented data model, a flexible yet powerful intermediate language is needed. This problem has been successfully tackled in Monet, a modern extensible database kernel developed by our group. We focus on the design choices made in the Monet Interpreter Language (MIL), its algebraic query language, and outline how its concept of tactical optimization enhances and simplifies the optimization of complex queries. Finally, we summarize the experience gained in Monet by creating a highly efficient implementation of MIL. 
491|Monet. An Impressionist Sketch of an Advanced Database System|Monet is a customizable database system developed at CWI and University of Amsterdam, intended to be used as the database backend for widely varying application domains. It is designed to get maximum database performance out of today&#039;s workstations and multiprocessor systems. It has already achieved considerable success in supporting a Data Mining application [12, 13], and work is well under way in a project where it is used in a high-end GIS application. Monet is a type- and algebra-extensible database system and employs shared memory parallelism. In this paper, we give the goals and motivation of Monet, and outline its architectural features, including its use of the Decomposed Storage Model (DSM), emphasis on bulk operations, use of main virtual-memory and server customization. As a case example, we discuss some issues on how to build a GIS on top of Monet; amongst others how Monet can handle the very large data volumes involved.   Parts of this work are supported by SION grant no. ...
492|On the Complexity of Generating Optimal Plans with Cross Products (Extended Abstract)  (1997) |In modern advanced database systems the optimizer  is often faced with the problem of finding optimal  evaluation strategies for queries involving a large number  of joins. Examples are queries generated by deductive  database systems and path expressions in  object-oriented database systems. The best plan can  be found in the very large search space of bushy trees  where plans are allowed to contain cross products. A  general question arises: For which (sub-) problems  can we expect to find polynomial algorithms generating  the best plan? We attack this question from  both ends of the spectrum. First, we show that we  cannot expect to find any polynomial algorithm for  any subproblem as long as optimal bushy trees are  to be generated. More specifically, we show that the  problem is NP-hard independent of the query graph.
493|SQOPI: Semantic Query Optimization Framework |Semantic query optimization uses semantic knowledge in databases to rewrite queries and logic programs for the purpose of more efficient query evaluation. There has been a large body of work in the area of semantic query optimization. But, unfortunately, till now no commercial application of sematic query optimization techniques has received wide attention. In this paper, we address this problem by developing a unified framework (Application Programming Interface) called SQOPI that could be used by any application developer to semantically optimize queries executed against relational database regardless of DBMS type used. Our results show that SQOPI improves both time and I/O efficiency.
494|Querying with Intrinsic Preferences|The handling of user preferences is becoming an increasingly important issue in present-day information systems. Among others, preferences are used for information filtering and extraction to reduce the volume of data presented to the user. They are also used to keep track of user profiles and formulate policies to improve and automate decision making. We propose a logical framework for formulating preferences and its embedding into relational query languages. The framework is simple, and entirely neutral with respect to the properties of preferences. It makes it possible to formulate different kinds of preferences and to use preferences in querying databases. We demonstrate the usefulness of the framework through numerous examples.  
495|Conceptual queries using ConQuer-II|Formulating non-trivial queries in relational languages such as SQL and QBE can prove daunting to end users. ConQuer is a conceptual query language that allows users to formulate queries naturally in terms of elementary relationships, operators such as “and”, “or”, “not ” and “maybe”, contextual for-clauses and object-correlation, thus avoiding the need to deal explicitly with implementation details such as relational tables, null values, outer joins, group-by clauses and correlated subqueries. While most conceptual query languages are based on the Entity-Relationship approach, ConQuer is based on Object-Role Modeling (ORM), which exposes semantic domains as conceptual object types, allowing queries to be formulated via paths through the information space. As a result of experience with the first implementation of ConQuer, the language has been substantially revised and extended to become ConQuer–II, and a new tool, ActiveQuery, has been developed with an improved interface. ConQuer-II’s new features such as arbitrary correlation and subtyping enable it to be used for a wide range of advanced conceptual queries. Introduction and Related Work
496|Implication and Referential Constraints: A New Formal Reasoning|In this paper, we address the issue of reasoning with two classes of commonly used semantic integrity constraints in database and knowledge-base systems: implication constraints and referential constraints. We first consider a central problem in this respect, the IRC-refuting problem, which is to decide whether a conjunctive query always produces an empty relation on (finite) database instances satisfying a given set of implication and referential constraints. Since the general problem is undecidable, we only consider acyclic referential constraints. Under this assumption, we prove that the IRC--refuting problem is decidable, and give a novel necessary and sufficient condition for it. Under the same assumption, we also study several other problems encountered in semantic query optimization, such as the semantics-based query containment problem, redundant join problem, and redundant selection-condition problem, and show that they are polynomially equivalent or reducible to the IRC-refuting problem. Moreover, we give results on reducing the complexity for some special cases of the IRC-refuting problem. 
497|Exploiting Constraint-Like Data Characterizations in Query Optimization|Query optimizers nowadays draw upon many sources of information about the database to optimize queries. They employ runtime statistics in cost-based estimation of query plans. They employ integrity constraints in the query rewrite process. Primary and foreign key constraints have long played a role in the optimizer, both for rewrite opportunities and for providing more accurate cost predictions. More recently, other types of integrity constraints are being exploited by optimizers in commercial systems, for which certain semantic query optimization techniques have now been implemented.
498|Discovering Robust Knowledge from Databases that Change|Many applications of knowledge discovery and data mining such as rule discovery  for semantic query optimization, database integration and decision support, require  the knowledge to be consistent with data. However, databases usually change over  time and makemachine-discovered knowledge inconsistent. Useful knowledge should  be robust against database changessothatitisunlikely to become inconsistentafter  database changes. This paper defines this notion of robustness in the context  of relational databases that contain multiple relations and describes how robustness of  first-order Horn-clause rules can be estimated and applied in knowledge discovery.Our  experiments show that the estimation approach can accurately predict the robustness  of a rule.  
499|Using domain knowledge in knowledge discovery|With the explosive growth of the size of databases, many knowledge discovery applications deal with large quantities of data. There is an urgent need to develop methodologies which will allow the applications to fo-cus search to a potentially interesting and relevant portion of the data, which can reduce the computa-tional complexity of the knowledge discovery process and improve the int,erestingness of discovered knowl-edge. Previous work on semantic query optimization, which is an approach to take advantage of domain knowledge for query optimization, has demonstrated that significant cost reduction can be achieved by re-formulating a query into a less expensive yet equivalent query which produces the same answer as the original one. In this paper, we introduce a method to utilize three types of domain knowledge in reducing the cost of finding a potentially interesting and relevant portion of the data while improving the quality of discovered knowledge. In addition, we propose a method to se-lect relevant domain knowledge without an exhaustive search of all domain knowledge. The contribution of this paper is that’we lay out a general framework for using domain knowledge in the knowledge discovery process effectively by providing guidelines.
500|Open Issues in Semantic Query Optimization in Related DBMS”, IV. Working paper series |After two decades of research into Semantic Query Optimization (SQO) there is clear agreement as to the efficacy of SQO. However, although there are some experimental implementations there are still no commercial implementations. We first present a thorough analysis of research into SQO. We identify three problems which inhibit the effective use of SQO in Relational Database Management Sys-tems (RDBMS). We then propose solutions to these problems and describe first steps towards the implementation of an effective semantic query optimizer for re-
501|MULTIDATABASE GLOBAL QUERY OPTIMIZATION |Data requests ro mulriaiztabase (MDBSs) are posed through non-procedural languages such as SQL and for a global data request optimization must be performed to achieve good system performance. However, the issues are ofren complicated in multidatabases, due to addirional issues arising because of the autonomy and heterogeneity restrictions of the independent local DBMSs. The data translation problem between various local systems can be observed at the schema level and at the instance level. We have identified the need for a domain translation table in mulridatabase query processing and discuss methods of implementing it. Some observations about multidatabase inter-site joins are made and four
502|Bringing Order to Query Optimization|A variety of developments combine to highlight the need for respecting order when manipulating relations.
503|Sequence Query Processing|Many applications require the ability to manipulate sequences of data. We motivate the importance of sequence query processing, and present a framework for the optimization of sequence queries based on several novel techniques. These include query transformations, optimizations that utilize meta--data, and caching of intermediate results. We present a bottom-up algorithm that generates an efficient query evaluation plan based on cost estimates. This work also identifies a number of directions in which future research can be directed.  1 Introduction  Many real life applications manipulate data that is inherently sequential. Such data is logically viewed and queried in terms of a sequence abstraction and is often physically stored as a sequence. Databases should (a) allow sequences to be queried in a declarative manner, utilizing the ordered semantics of the data, and (b) take advantage of the opportunities available for query optimization. Relational databases are inadequate in this re...
504|Supporting lists in a data model (a timely approach  (1992) |This paper considers the problem of adding list as a type constructor to an object-oriented data model. In particular, we are concerned with how lists in a database can be constructed and how they can be queried. We propose that operators from a discrete, linear-time temporal logic provide a natural basis for making assertions about the ordering of elements in a list. We then show how such assertions may be incor-porated into a query algebra by extending the Melam-pus Data Model (MDM) with a list type constructor and by allowing temporal assertions as predicates on lists. The extended algebra allows the expression of a significantly larger class of queries than previously possible. Furthermore, temporal operators provide a basis for creating new lists that satisfy desired order-ing properties. For example, sorting is shown to fall out as a special case. This paper also describes a new framework based on Boolean circuits for evaluating the truth of an assertion on a given list. This framework provides many opportunities for optimization and par-allelism, and it lends insight to the meaning and com-plexity of a temporal formula.
505|Query Plans for Conventional and Temporal Queries Involving Duplicates and Ordering|Most real-world database applications contain a substantial portion of time-referenced, or temporal, data. Recent advances in temporal query languages show that such database applications could benefit substantially from builtin temporal support in the DBMS. To achieve this, temporal query representation, optimization, and processing mechanisms must be provided. This paper presents a general, algebraic foundation for query optimization that integrates  conventional and temporal query optimization and is suitable for providing temporal support both via a stand-alone temporal DBMS and via a layer on top of a conventional DBMS. By capturing duplicate removal and retention and order preservation for all queries, as well as coalescing for temporal queries, this foundation formalizes and generalizes existing approaches.
506|A Foundation for Conventional and Temporal Query Optimization Addressing Duplicates and Ordering|AbstractÐMost real-world databases contain substantial amounts of time-referenced, or temporal, data. Recent advances in temporal query languages show that such database applications may benefit substantially from built-in temporal support in the DBMS. To achieve this, temporal query representation, optimization, and processing mechanisms must be provided. This paper presents a foundation for query optimization that integrates conventional and temporal query optimization and is suitable for both conventional DBMS architectures and ones where the temporal support is obtained via a layer on top of a conventional DBMS. This foundation captures duplicates and ordering for all queries, as well as coalescing for temporal queries, thus generalizing all existing approaches known to the authors. It includes a temporally extended relational algebra to which SQL and temporal SQL queries may be mapped, six types of algebraic equivalences, concrete query transformation rules that obey different equivalences, a procedure for determining which types of transformation rules are applicable for optimizing a query, and a query plan enumeration algorithm. The presented approach partitions the work required by the database implementor to develop a provably correct query optimizer into four stages: The database implementor has to 1) specify operations formally, 2) design and prove correct appropriate transformation rules that satisfy any of the six equivalence types, 3) augment the mechanism that determines when the different types of rules are applicable to ensure that the enumeration algorithm applies the rules correctly, and 4) ensure that the mapping generates a correct initial query plan. Index TermsÐTemporal databases, query optimization, transformation rules, temporal algebra, duplicate elimination, coalescing. 1
507|Processing Top N and Bottom N Queries|this paper addresses the question of how top N and bottom N queries can be processed efficiently; moreover, we address the question of how such support can be provided as a natural extension of existing relational query processing architectures. In a nutshell, our goal is to evaluate such queries with as little wasted work
508|On understanding types, data abstraction, and polymorphism|Our objective is to understand the notion of type in programming languages, present a model of typed, polymorphic programming languages that reflects recent research in type theory, and examine the relevance of recent research to the design of practical programming languages. Object-oriented languages provide both a framework and a motivation for exploring the interaction among the concepts of type, data abstraction, and polymorphism, since they extend the notion of type to data abstraction and since type inheritance is an important form of polymorphism. We develop a ?-calculus-based model for type systems that allows us to explore these interactions in a simple setting, unencumbered by complexities of production programming languages. The evolution of languages from untyped universes to monomorphic and then polymorphic type systems is reviewed. Mechanisms for polymorphism such as overloading, coercion, subtyping, and parameterization are examined. A unifying framework for polymorphic type systems is developed in terms of the typed ?-calculus augmented to include binding of types by quantification as well as binding of values by abstraction. The typed ?-calculus is augmented by universal quantification to model generic functions with type parameters, existential quantification and packaging (information hiding) to model abstract data types, and
509|CLASSIC: A Structural Data Model for Objects|CLASSIC is a data model that encourages the description ofobjects not only in terms of their relations to other known objects, but in terms of a level of intensional structure as well. The CLASSIC language of structured descriptions permits i) partial descriptions of individuals, under an `open world&#039; assumption, ii) answers to queries either as extensional lists of valuesorasdescriptions that necessarily hold of all possible answers, and iii) an easily extensible schema, which can be accessed uniformly with the data. One of the strengths of the approach is that the same language plays multiple roles in the processes of defining and populating the DB, as well as querying and answering. classic (for which we have a prototype main-memory implementation) can actively discover new information about objects from several sources: it can recognize new classes under which an object falls based on a description of the object, it can propagate some deductive consequences of DB upda...
510|ILOG: Declarative Creation and Manipulation of Object Identifiers|yosikawaQkyoto-su.ac.jp Abstract: This paper introduces ILOG ( a declarative language in the style of (stratified) datalog ( which can be used for querying, schema translation, and schema augmentation in the context of object-based data models. The semantics of ILOG is based on the use of Skolem functors, and is closely related to semantics for object-based data manipulation languages which provide mechanisms for explicit creation of object identifiers (OIDs). A normal form is presented for ILOG ’ programs not involving recursion through OID creation, which identifies a precise correspondence between OIDs created in the target, and values and OIDs in the source. The expressive power of various sublanguages of ILOG ’ is shown to range from a natural generalization of the conjunctive queries to the object-based context, to a language which can specify all computable database translat.ions (up to duplicate copies). The issue of testing vuliilityof ILOG programs translat.ing one semantic schema to another is studied: cases are presented for which several-validity issues (e.g., functional and/or subset relationships in the
511|An essential hybrid reasoning system: knowledge and symbol level accounts of KRYPTON|Hybrid inference systems are an important way to address the fact that intelligent systems have muiltifaceted representational and reasoning competence. KRYPTON is an experimental prototype that competently handles both terminological and assertional knowledge; these two kinds of information are tightly linked by having sentences in an assertional component be formed using structured complex predicates denned in a complementary terminological component. KRYPTON is unique in that it combines in a completely integrated fashion a frame-based description language and a first-order resolution theorem-prover. We give here both a formal Knowledge Level view of the user interface to KRYPTON and the technical Symbol Level details of the integration of the two disparate components, thus providing an essential picture of the abstract function that KRYPTON computes and the implementation technology needed to make it work. We also illustrate the kind of complex question the system can answer. I
512|From Relational to Object-Oriented Integrity Simplification|1  Relational integrity checking technology can be transfered to deductive object bases  by utilizing a simple logical framework for objects. The principles of object identity,  aggregation and classification allow a more efficient constraint control by finer  granularity of updates, composite updates and semantic constraint simplification. In  many cases, meta-level constraints and deductive rules can be handled efficiently by a  stepwise compilation approach. An extended integrity subsystem with these features  has been implemented in the deductive object base ConceptBase.  1  This work was supported in part by the Commission of the European Community under ESPRIT Basic Research Action 3012 (CompuLog). A version of this paper will also appear in the Proc. Second Int. Conf. on Deductive and Object-Oriented Databases, Munich, Dec. 1991  1. Introduction  Comprehensive and efficient integrity maintenance has been quoted as one of the major problems in next-generation databases. Systems l...
513|External Semantic Query Simplification: A Graph-theoretic Approach and its Implementation in Prolog|Semantic query simplification utilizes integrity constraints enforced in a database system for reducing the number of tuple variables and terms in a relational calculus query. To a large degree, this can be done by a system that is external to the DBMS. The paper advocates the application of database theory in such a system and describes a working prototype of an external semantic query simplifier implemented in Prolog. The system employs a graph-theoretic approach to integrate tableau techniques and algorithms for the syntactic simplification of queries containing inequality conditions. The use of integrity constraints is shown not only to improve efficiency but also to permit more meaningful error messages to be generated, particularly in the case of an empty query result. The paper concludes with outlining an extension to the multi-user case.
514|Deductive Integrity Maintenance in an Object-Oriented Setting|The extension of integrity checking methods proposed for deductive relational databases to the case of object-oriented deductive databases offers new opportunities for more efficient consistency control: a reduction of the search space by finer granularity of updates, and a reduction of runtime integrity checking by incremental maintenance of the executable code generated for evaluating simplified rules and constraints within the database. Such an extended integrity system has been implemented in the KBMS ConceptBase. This work was supported in part by the Commission of the European Community under ESPRIT Basic Research Action 3012 (CompuLog). Thanks to Matthias Jarke, Hendrik Decker and Gerhard Steinke for help and discussions. A version of this paper also appeared as MIP-9013, Universiat Passau, Germany, 1990.  1. Introduction 2  1. Introduction  Today&#039;s database community is still searching for the next generation of database systems [ABD*89,ADV90]. The relational model is known fo...
515|On Implicate Discovery and Query Optimization |Boolean expression simplification is a well-known problem in the history of Computer Science. The problem of determining prime implicates from an arbitrary Boolean expression has been mostly studied in the contexts of hardware design and automated reasoning. While many of the same principles can be applied to the simplification of search conditions in ANSI SQL queries, the richness of its language and SQL’s three-valued logic present a number of challenges. We propose a modified version of a matrix-based normalization algorithm suitable for normalizing SQL search conditions in constrained-memory environments. In particular, we describe a set of tradeoffs that enable our algorithm to discover a useful set of implicates without requiring a complete conversion of the input condition to a normal form, preventing a combinatorial explosion in the number of terms. 1. Introduction and
516|Single table access using multiple indexes: Optimization, execution, and concurrency control techniques|Abstract Many data base management systems ’ query optimizers choose at most one index for accessing the records of a table in a given query, even though many indexes may exist on the table. In spite of the fact that there are some systems which use multiple indexes, very little has been published about the concurrency control or query optimization implications (e.g., deciding how many indexes to use) of using multiple indexes. This paper addresses these issues and presents solutions to the associated problems. Techniques are presented for the efficient handling of record ID lists, elimination of some locking, and determination of how many and which indexes to use. The techniques are adaptive in the sense that the execution strategies may be modified at run-time (e.g., not use some indexes which were to have been used), if the assumptions made at optimization-time (e.g., about selectivities) turn out to be wrong. Opportunities for exploiting parallelism are also identified. A subset of our ideas have been implemented in IBM’s DB2 V2R2 relational data base management system. 1.
517|CNF and DNF Considered Harmful for Computing Prime Implicants / Implicates|Several methods to compute the prime implicants and the prime implicates of a negation normal form (NNF) formula are developed and implemented. An algorithm PI is introduced that is an extension to negation normal form of an algorithm given by Jackson and Pais. A correctness proof of the PI algorithm is given. The PI algorithm alone is sufficient in a computational sense. However, it can be combined with path dissolution, and it is shown empirically that this is often an advantage.
518|Using Subsumption in Semantic Query Optimization|Semantic Query Optimization optimize query processing by transforming a query into a semantically equivalent one, i.e. a query whose result is the same for every instance of the database. Semantic query optimization can be obtained by a query rewriting that takes into account the integrity constraints the database must satisfy. In this paper we analyze the possibility of implementing semantic optimization in OODBMs by means of the subsumption relation computation over a database schema. Subsumption relations over a schema includes all the specialization relations between classes (types) both explicitly declared in the schema and entailed by the schema. Subsumption computation has been previously used in the database area to perform schemata acquisition and checking for coherence and query processing optimization. In this paper we extend complex object data models with integrity constraints expressed as rules and propose to apply subsumption computation to perform semantic query optimiz...
519|A Scheme for Integrating Concrete Domains into Concept Languages|A drawback which concept languages based on kl-one have is that all the terminological knowledge has to be defined on an abstract logical level. In many applications, one would like to be able to refer to concrete domains and predicates on these domains when defining concepts. Examples for such concrete domains are the integers, the real numbers, or also non-arithmetic domains, and predicates could be equality, inequality, or more complex predicates. In the present paper we shall propose a scheme for integrating such concrete domains into concept languages rather than describing a particular extension by some specific concrete domain. We shall define a terminological and an assertional language, and consider the important inference problems such as subsumption, instantiation, and consistency. The formal semantics as well as the reasoning algorithms are given on the scheme level. In contrast to existing kl-one based systems, these algorithms will be not only sound but also complete. The...
520|Terminological Cycles in KL-ONE-based Knowledge Representation Languages|Cyclic definitions are often prohibited in terminological knowledge representation languages because, from a theoretical point of view, their semantics is not clear and, from a practical point of view, existing inference algorithms may go astray in the presence of cycles. In this paper, we shall consider terminological cycles in a very small KL-ONE-based language. For this language, the effect of the three types of semantics introduced by (Nebel 1987,1989,1989a) can be completely described with the help of finite automata. These descriptions pro-vide a rather intuitive understanding of terminologies with cyclic definitions and give insight into the essen-tial features of the respective semantics. In addition, one obtains algorithms and complexity results for subsump-tion determination. As it stands, the greatest fixed-point semantics comes off best. The characterization of this semantics is easy and has an obvious intuitive interpretation. Furthermore, important constructs- such as value-restriction with respect to the transitive or reflexive-transitive closure of a role- can easily be expressed. 1.
521|Terminological Reasoning and Information Management|Reasoning with terminological logics is a subfield in the area of knowledge representation that evolved from the representation language kl-one.  Its main purpose is to automatically determine the location of a a new concept description (or object description) in a partially ordered set of given concepts. It seems to be a promising approach to apply the techniques developed in this area to the development of new object-based database models. The main advantages are a uniform query and database definition language and the utilization of an indexing technique, which we call semantic indexing. 
522|Cost-Based Query Optimization with Heuristics |Abstract — In today’s computational world,cost of computation is the most significant factor for any database management system.Searching a query from a database incurs various computational costs like processor time and communication time.Then, there are costs because of operations like projection, selection, join etc.DBMS strives to process the query in the most efficient way (in terms of ‘Time’) to produce the answer.In this paper we proposed a novel method for query optimization using heuristic based approach. In the proposed algorithm,a query is searched using the storage file which shows an improvement with respect to the earlier query optimization techniques. Also, the improvement increases once the query goes more complicated and for nesting query. Index Terms—Heuristic,query,optimization,usage factor,storage file,magic tree,cost,weighted.
523|Minimal Probing: Supporting Expensive Predicates for Top-k Queries|This paper addresses the problem of evaluating ranked top-    queries with expensive predicates. As major DBMSs now all support expensive user-defined predicates for Boolean queries, we believe such support for ranked queries will be even more important: First, ranked queries often need to model user-specific concepts of preference, relevance, or similarity, which call for dynamic user-defined functions. Second, middleware systems must incorporate external predicates for integrating autonomous sources typically accessible only by per-object queries. Third, fuzzy joins are inherently expensive, as they are essentially user-defined operations that dynamically associate multiple relations. These predicates, being dynamically defined or externally accessed, cannot rely on index mechanisms to provide zero-time sorted output, and must instead require per-object probe to evaluate. The current standard sort-merge framework for ranked queries cannot efficiently handle such predicates because it must completely probe all objects, before sorting and merging them to produce top-    answers. To minimize expensive probes, we thus develop the formal principle of &#034;necessary probes,&#034; which determines if a probe is absolutely required. We then propose Algorithm MPro which, by implementing the principle, is provably optimal with minimal probe cost. Further, we show that MPro can scale well and can be easily parallelized. Our experiments using both a real-estate benchmark database and synthetic datasets show that MPro enables significant probe reduction, which can be orders of magnitude faster than the standard scheme using complete probing.
524|A Study of Query Optimization for |In Korean, the space usage of compound nouns produces the term mismatch. Korean compound nouns may be a sequence of single nouns without blanks or may be delimeted by
525|Improving Query Optimization |In this paper we present a technique for the optimization of  (partially) bound queries over disjunctive deductive databases. In particular,  we extend the magic-set optimization technique (originally de  ned  for non-disjunctive deductive databases) to the disjunctive case. The  method presented in this paper improves a similar approach presented  in [7] by reducing the number of additionally introduced predicates and  rules.
526|Stable Semantics for Disjunctive Programs|We introduce the stable model semantics for disjunctive logic programs and deductive databases, which generalizes the stable model semantics, defined earlier for normal (i.e., non-disjunctive) programs. Depending on whether only total (2-valued) or all partial (3-valued) models are used we obtain the disjunctive stable semantics or the partial disjunctive stable semantics, respectively. The proposed semantics are shown to have the following properties: ffl For normal programs, the disjunctive (respectively, partial disjunctive) stable semantics coincides with the stable (respectively, partial stable) semantics. ffl For normal programs, the partial disjunctive stable semantics also coincides with the well-founded semantics. ffl For locally stratified disjunctive programs both (total and partial) disjunctive stable semantics coincide with the perfect model semantics. ffl The partial disjunctive stable semantics can be generalized to the class of all disjunctive logic programs. ffl B...
527|The KR System dlv: Progress Report, Comparisons and Benchmarks|dlv is a knowledge representation  system, based on disjunctive logic  programming, which offers frontends  to several advanced KR formalisms. The system has
528|A Deductive System for Nonmonotonic Reasoning|Abstract. Disjunctive Deductive Databases (DDDBs)-- function-free disjunctive logic programs with negation in rule bodies allowed-- have been recently recognized as a powerful tool for knowledge representation and commonsense reasoning. Much research as been spent on issues like semantics and complexity of DDDBs, but the important area of imple-menting DDDBs has been less addressed so far. However, a thorough investigation thereof is a basic requirement for building systems which render previous foundational work on DDDBs useful for practice. This paper presents the architecture ofa DDDB system currently devel-oped at TU Vienna in the FWF project P11580-MAT &#039;~A Query System for Disjunctive Deductive Databases&#034;. 1 In t roduct ion The study of integrating databases with logic programming opened in the past the field of deductive databases. Basically, a deductive database is a function-free logic program, i.e., a datalog program (possibly extended with negation). Several advanced eductive database systems utilize logic programming and extensions thereof or querying relational databases, e.g. [14, 21, 24]. The need for representing disjunctive (or incomplete) information led to Dis-junctive Deductive Databases (DDDBs) [18]. They can be seen as function-free disjunctive logic programs, i.e., disjunctive datalog programs [19, 12]. DDDBs are nowadays widely recognized as a valuable tool for knowledge representation a d reasoning [1, 17, 30, 13, 19]. The strong interest in enhancing deductive databases by disjunction is documented by a number of publications (cf. [17]) and special workshops dedicated to this subject (cf. [30]). An important merit of DDDBs over normal (i.e., disjunction-free) logic programming is its capability to model incomplete knowledge [1, 17].
529|Query Optimization for Dynamic Graphs |Given a query graph that represents a pattern of interest, the emerg-ing pattern detection problem can be viewed as a continuous query problem on a dynamic graph. We present an incremental algorithm for continuous query processing on dynamic graphs. The algorithm is based on the concept of query decomposition; we decompose a query graph into smaller subgraphs and assemble the result of sub-queries to find complete matches with the specified query. The nov-elty of our work lies in using the subgraph distributional statistics collected from the dynamic graph to generate the decomposition. We introduce a “Lazy Search &#034; algorithm where the search strategy is decided on a vertex-to-vertex basis depending on the likelihood of a match in the vertex neighborhood. We also propose a metric named “Relative Selectivity &#034; that is used to select between differ-ent query decomposition strategies. Our experiments performed on real online news, network traffic stream and a synthetic social net-work benchmark demonstrate 10-100x speedups over competing approaches. 1.
530|Models and issues in data stream systems|In this overview paper we motivate the need for and research issues arising from a new model of data processing. In this model, data does not take the form of persistent relations, but rather arrives in multiple, continuous, rapid, time-varying data streams. In addition to reviewing past work relevant to data stream systems and current projects in the area, the paper explores topics in stream query languages, new requirements and challenges in query processing, and algorithmic issues. 
531|An algorithm for subgraph isomorphism|Subgraph isomorphism can be determined by means of a brute-force tree-search enumeration procedure. In this paper a new algorithm is introduced that attains efficiency by inferentially eliminating successor nodes in the tree search. To assess the time actually taken by the new algorithm, subgraph isomorphism, clique detection, graph isomorphism, and directed graph isomorphism experiments have been carried out with random and with various nonrandom graphs. A parallel asynchronous logic-in-memory implementation of a vital part of the algorithm is also described, although this hardware has not actually been bmlt The hardware implementation would allow very rapid determination of isomorphism.
532|THIRTY YEARS OF GRAPH MATCHING IN PATTERN RECOGNITION|A recent paper posed the question: &#034;Graph Matching: What are we really talking about?&#034;. Far from providing a definite answer to that question, in this paper we will try to characterize the role that graphs play within the Pattern Recognition field. To this aim two taxonomies are presented and discussed. The first includes almost all the graph matching algorithms proposed from the late seventies, and describes the different classes of algorithms. The second taxonomy considers the types of common applications of graph-based techniques in the Pattern Recognition and Machine Vision field.
533|Closure-Tree: An Index Structure for Graph Queries|Graphs have become popular for modeling structured data. As a result, graph queries are becoming common and graph indexing has come to play an essential role in query processing. We introduce the concept of a graph closure, a generalized graph that represents a number of graphs. Our indexing technique, called Closure-tree, organizes graphs hierarchically where each node summarizes its descendants by a graph closure. Closure-tree can efficiently support both subgraph queries and similarity queries. Subgraph queries find graphs that contain a specific subgraph, whereas similarity queries find graphs that are similar to a query graph. For subgraph queries, we propose a technique called pseudo subgraph isomorphism which approximates subgraph isomorphism with high accuracy. For similarity queries, we measure graph similarity through edit distance using heuristic graph mapping methods. We implement two kinds of similarity queries: K-NN query and range query. Our experiments on chemical compounds and synthetic graphs show that for subgraph queries, Closure-tree outperforms existing techniques by up to two orders of magnitude in terms of candidate answer set size and index size. For similarity queries, our experiments validate the quality and efficiency of the presented algorithms. 
534|Structural Join Order Selection for XML Query Optimization. Tech Report. Available at http://www.eecs.umich.edu/ yuwu/SJOS.pdf |Structural join operations are central to evaluating queries against XML data, and are typically responsible for consuming a lion’s share of the query processing time. Thus, structural join order selection is at the heart of query optimization in an XML database, just as (value-based) join order selection is central to relational query optimization. In this paper, we introduce five algorithms for structural join order optimization for XML tree pattern matching and present an extensive experimental evaluation. Our experiments demonstrate that many relational rules of thumb are no longer appropriate: for instance, using dynamic programming style optimization is not efficient; limiting consideration to left-deep plans usually misses the best solution. Our experiments also show that a Dynamic Programming optimization with Pruning (DPP) algorithm can find the optimal solution, with low cost relative to the traditional Dynamic Programming (DP) algorithm; and an optimization technique that only considers Fully Pipelined (FP) plans can very quickly choose a plan that in most cases is close to optimal. Our recommendation is that DPP should be used in XML query optimizers where query execution time is expected to be significant, and that FP should be used where it is important to find a good (but not necessarily the best) plan quickly. 1
535|Fast best-effort pattern matching in large attributed graphs|We focus on large graphs where nodes have attributes, such as a social network where the nodes are labelled with each person’s job title. In such a setting, we want to find subgraphs that match a user query pattern. For example, a ‘star ’ query would be, “find a CEO who has strong interactions with a Manager, a Lawyer, and an Accountant, or another structure as close to that as possible”. Similarly, a ‘loop ’ query could help spot a money laundering ring. Traditional SQL-based methods, as well as more recent graph indexing methods, will return no answer when an exact match does not exist. Our method can find exact-, as well as near-matches, and it will present them to the user in our proposed ‘goodness ’ order. For example, our method tolerates indirect paths between, say, the ‘CEO ’ and the ‘Accountant ’ of the above sample query, when direct paths do not exist. Its second feature is scalability. In general, if the query has nq nodes and the data graph has n nodes, the problem needs polynomial time complexity O(n nq), which is prohibitive. Our G-Ray (“Graph X-Ray”) method finds high-quality subgraphs in time linear on the size of the data graph. Experimental results on the DLBP author-publication graph (with 356K nodes and 1.9M edges) illustrate both the effectiveness and scalability of our approach. The results agree with our intuition, and the speed is excellent. It takes 4 seconds on average for a 4node query on the DBLP graph.
536|On Graph Query Optimization in Large Networks |The dramatic proliferation of sophisticated networks has resulted in a growing need for supporting effective querying and mining methods over such large-scale graph-structured data. At the core of many advanced network operations lies a common and critical graph query primitive: how to search graph structures efficiently within a large network? Unfortunately, the graph query is hard due to the NP-complete nature of subgraph isomorphism. It becomes even challenging when the network examined is large and diverse. In this paper, we present a high performance graph indexing mechanism, SPath, to address the graph query problem on large networks. SPath leverages decomposed shortest paths around vertex neighborhood as basic indexing units, which prove to be both effective in graph search space pruning and highly scalable in index construction and deployment. Via SPath, a graph query is processed and optimized beyond the traditional vertex-at-a-time fashion to a more efficient path-at-a-time way: the query is first decomposed to a set of shortest paths, among which a subset of candidates with good selectivity is picked by a query plan optimizer; Candidate paths are further joined together to help recover the query graph to finalize the graph query processing. We evaluate SPath with the state-of-the-art GraphQL on both real and synthetic data sets. Our experimental studies demonstrate the effectiveness and scalability of SPath, which proves to be a more practical and efficient indexing method in addressing graph queries on large networks. 1.
537|Efficient subgraph matching on billion node graphs|The ability to handle large scale graph data is crucial to an increasing number of applications. Much work has been dedicated to supporting basic graph operations such as subgraph matching, reachability, regular expression matching, etc. In many cases, graph indices are employed to speed up query processing. Typically, most indices require either super-linear indexing time or super-linear indexing space. Unfortunately, for very large graphs, super-linear approaches are almost always infeasible. In this paper, we study the problem of subgraph matching on billion-node graphs. We present a novel algorithm that supports efficient subgraph matching for graphs deployed on a distributed memory store. Instead of relying on super-linear indices, we use efficient graph exploration and massive parallel computing for query processing. Our experimental results demonstrate the feasibility of performing subgraph matching on web-scale graph data. 1.
538|Neighborhood based fast graph search in large networks|Complex social and information network search becomes impor-tant with a variety of applications. In the core of these applications, lies a common and critical problem: Given a labeled network and a query graph, how to efficiently search the query graph in the tar-get network. The presence of noise and the incomplete knowledge about the structure and content of the target network make it unre-alistic to find an exact match. Rather, it is more appealing to find the top-k approximate matches. In this paper, we propose a neighborhood-based similarity mea-sure that could avoid costly graph isomorphism and edit distance computation. Under this new measure, we prove that subgraph sim-ilarity search is NP hard, while graph similarity match is polyno-mial. By studying the principles behind this measure, we found an information propagation model that is able to convert a large net-
540|A space efficient streaming algorithm for triangle counting using the birthday paradox |We design a space efficient algorithm that approximates the transitivity (global clustering coefficient) and total triangle count with only a single pass through a graph given as a stream of edges. Our procedure is based on the classic prob-abilistic result, the birthday paradox. When the transitivity is constant and there are more edges than wedges (common properties for social networks), we can prove that our algo-rithm requires O( n) space (n is the number of vertices) to provide accurate estimates. We run a detailed set of experi-ments on a variety of real graphs and demonstrate that the memory requirement of the algorithm is a tiny fraction of the graph. For example, even for a graph with 200 million edges, our algorithm stores just 60,000 edges to give accu-rate results. Being a single pass streaming algorithm, our procedure also maintains a real-time estimate of the transi-tivity/number of triangles of a graph, by storing a miniscule fraction of edges.
541|gSketch: On Query Estimation in Graph Streams |Many dynamic applications are built upon large network infrastructures, such as social networks, communication networks, biological networks and the Web. Such applications create data that can be naturally modeled as graph streams, in which edges of the underlying graph are received and updated sequentially in a form of a stream. It is often necessary and important to summarize the behavior of graph streams in order to enable effective query processing. However, the sheer size and dynamic nature of graph streams present an enormous challenge to existing graph management techniques. In this paper, we propose a new graph sketch method, gSketch, which combines well studied synopses for traditional data streams with a sketch partitioning technique, to estimate and optimize the responses to basic queries on graph streams. We consider two different scenarios for query estimation: (1) A graph stream sample is available; (2) Both a graph stream sample and a query workload sample are available. Algorithms for different scenarios are designed respectively by partitioning a global sketch to a group of localized sketches in order to optimize the query estimation accuracy. We perform extensive experimental studies on both real and synthetic data sets and demonstrate the power and robustness of gSketch in comparison with the state-of-the-art global sketch method. 1.
542|Relational Languages and Data Models for Continuous Queries on Sequences and Data Streams|Most data stream management systems are based on extensions of the relational data model and query languages, but rigorous analyses of the problems and limitations of this approach, and how to overcome them, are still wanting. In this article, we elucidate the interaction between stream-oriented extensions of the relational model and continuous query language constructs, and show that the resulting expressive power problems are even more serious for data streams than for databases. In particular, we study the loss of expressive power caused by the loss of blocking query operators, and characterize nonblocking queries as monotonic functions on the database. Thus we introduce the notion of N B-completeness to assure that a query language is as suitable for continuous queries as it is for traditional database queries. We show that neither RA nor SQL are N B-complete on unordered sets of tuples, and the problem is even more serious when the data model is extended to support order—a sine-qua-non in data stream applications. The new limitations of SQL, compounded with well-known problems in applications such as sequence queries and data mining, motivate our proposal of extending the language with user-defined aggregates (UDAs). These can be natively coded in SQL, according to simple syntactic rules that set nonblocking aggregates apart from blocking ones. We first prove that SQL with UDAs is Turing complete. We then prove that SQL with monotonic UDAs and union operators can express all monotonic set functions computable by a Turing machine (N B-completeness) and
543|Fast Triangle Counting through Wedge Sampling|Graphs and networks are used to model interactions in a variety of contexts, and there is a growing need to be able to quickly assess the qualities of a graph in order to understand its underlying structure. Some of the most useful metrics are triangle based and give a measure of the connectedness of “friends of friends. ” Counting the number of triangles in a graph has, therefore, received considerable attention in recent years. We propose new sampling-based methods for counting the number of triangles or the number of triangles with vertices of specified degree in an undirected graph and for counting the number of each type of directed triangle in a directed graph. The number of samples depends only on the desired relative accuracy and not on the size of the graph. We present extensive numerical results showing that our methods are often much better than the error bounds would suggest. In the undirected case, our method is generally superior to other approximation approaches; in the undirected case, ours is the first approximation method proposed.  
544|Massive Scale Cyber Traffic Analysis: A Driver for Graph Database Research |We consider cyber traffic analysis (TA) as a challenge problem for research in graph database systems. TA involves observing and analyzing connections between clients, servers, hosts, and actors within IP networks, over time, to detect suspicious patterns. Towards that end, NetFlow (or more generically, IPFLOW) data are available from routers and servers which summarize coherent groups of IP packets flowing through the network. The ability to cast IPFLOW data as a massive graph and query it interactively is potentially transformative for cybersecurity, but issues of scale and data complexity pose challenges for current technology. In this paper, we outline requirements and opportunities for graphstructured IPFLOW analytics based on our experience with real IPFLOW databases. We describe real use cases from the security domain, cast them as graph patterns, show how to express them in two graph-oriented query languages (SPARQL and Datalog), and use these examples to motivate a new class of “hybrid ” graph-relational systems.
545|EAGr: Supporting Continuous Ego-centric Aggregate Queries over Large Dynamic Graphs |In this paper, we present EAGr, a system for supporting large num-bers of continuous neighborhood-based (“ego-centric”) aggregate queries over large, highly dynamic, and rapidly evolving graphs. Examples of such queries include computation of personalized, tai-lored trends in social networks, anomaly or event detection in com-munication or financial transaction networks, local search and alerts in spatio-temporal networks, to name a few. Key challenges in sup-porting such continuous queries include very high update rates typ-ically seen in these situations, large numbers of queries that need to be executed simultaneously, and stringent low latency require-ments. In this paper, we propose a flexible, general, and extensible in-memory framework for executing different types of ego-centric aggregate queries over large dynamic graphs with low latencies. Our framework is built around the notion of an aggregation overlay
546|Streamworks: A system for dynamic graph search |Acting on time-critical events by processing ever growing social media, news or cyber data streams is a major technical challenge. Many of these data sources can be modeled as multi-relational graphs. Mining and searching for subgraph patterns in a continuous setting requires an efficient approach to incremental graph search. The goal of our work is to enable real-time search capabilities for graph databases. This demonstration will present a dynamic graph query system that leverages the structural and semantic characteristics of the underlying multi-relational graph.
547|On the Power of Magic|This paper considers the efficient evaluation of recursive queries expressed using Horn Clauses. We define sideways information passing formally and show how a query evaluation algorithm may be defined in terms of sideways information passing and control. We then consider a class of information passing strategies that suffices to describe most query evaluation algorithms in the database literature, and show that these strategies may always be implemented by rewriting a given program and evaluating the rewritten program bottom-up. We describe in detail several algorithms for rewriting a program. These algorithms generalize the Counting and Magic Sets algorithms to work with arbitrary programs. Safety and optimality of the algorithms are also considered. 1. Introduction  The evaluation of recursive queries expressed as sets of Horn Clauses over a database has recently received much attention. Consider the following program:  anc (X, Y) :- par (X, Y) anc (X, Y) :- par (X, Z), anc (Z, Y)  ...
548|Pushing Constraint Selections|this paper, we present a procedure that generates and propagates minimum QRP-constraints (if it terminates), based on the definition and uses of program predicates (Section 4). By propagating minimum QRP-constraints to the original program, we obtain a program that fully utilizes the constraint information present in the original program. This procedure is based on two sub-procedures: 1. Procedure Gen Prop predicate-constraints, which generates and propagates constraints that are satisfied by program predicates based on their definitions.
549|Foundations of Aggregation Constraints|We introduce a new constraint domain, aggregation constraints, that is useful in database query languages, and in constraint logic programming languages that incorporate aggregate functions. We formally study the fundamental problem of determining if a conjunction of aggregation constraints is satisfiable, and show that, for many classes of aggregation constraints, the problem is undecidable. We describe a complete and minimal axiomatization of aggregation constraints, for the SQL aggregate functions min, max, sum, count and average, over a non-empty, finite multiset on several domains. This axiomatization helps identify classes of aggregation constraints for which the satisfiability check is efficient. We present a polynomial-time algorithm that directly checks for satisfiability of a conjunction of aggregation range constraints over a single multiset; this is a practically useful class of aggregation constraints. We discuss the relationships between aggregation constraints over a non...
550|Constraints and Redundancy in Datalog|Two types of redundancies in datalog programs are considered. Redundancy based on reachability  eliminates rules and predicates that do not participate in any derivation tree of a fact for the query predicate. Redundancy based on irrelevance  is similar, but considers only minimal derivation trees, that is, derivation trees having no pair of identical atoms, such that one is an ancestor of the other. Algorithms for detecting these redundancies are given, including the case of programs with constraint literals. These algorithms not only detect redundancies in the presence of constraints, but also push constraints from the given query and rules to the EDB predicates. Under certain assumptions discussed in the paper, the constraints are pushed to the EDB as tightly as possible.  
551|Magic Conditions|Much recent work has focussed on the bottomup evaluation of Datalog programs. One approach, called magic-sets, is based on rewriting a logic program so that bottom-up fixpoint evaluation of the program avoids generation of irrelevant facts ([BMSU86, BR87, Ram88]). It is widely believed that the principal application of the magicsets technique is to restrict computation in recursive queries using equijoin predicates. We extend the magic-sets transformation to use predicates other than equality (X ? 10, for example). This Extended Magic-Set technique has practical utility in &#034;real&#034; relational databases, not only for recursive queries, but for non-recursive queries as well; in ([MFPR90]) we use the results in this paper and those in [MPR90] to define a magic-set transformation for relational databases supporting SQL and its extensions, going on to describe an implementation of magic in Starburst ([HFLP89]). We also give preliminary performance measurements. In extending magic-sets, we des...
552|Equivalence, Query-Reachability, and Satisfiability in Datalog Extensions (Extended Abstract)  (1993) |Alon Y. Levy    Stanford University levy@cs.stanford.edu  Inderpal Singh Mumick  AT&amp;T Bell Laboratories mumick@research.att.com  Yehoshua Sagiv  y  Hebrew University sagiv@cs.huji.ac.il  Oded Shmueli  z  Technion---Israel Institute of Technology oshmu@cs.technion.ac.il  Abstract  We consider the problems of equivalence, satisfiability and query-reachability for datalog programs with negation and dense-order constraints. These problems are important for optimizing datalog programs. We show that both queryreachability and satisfiability are decidable for programs with stratified negation provided that negation is applied only to EDB predicates or that all EDB predicates are unary. In the latter case, we show that equivalence is also decidable. The algorithms we present are also used to push constraints from a given query to the EDB predicates. Finally, we show that satisfiability is undecidable for datalog programs with unary IDB predicates, stratified negation and the interpreted predic...
553|Particle swarm optimization| A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.
554|Evolutionary Programming Made Faster|Evolutionary programming (EP) has been applied with success to many numerical and combinatorial optimization problems in recent years. EP has rather slow convergence rates, however, on some function optimization problems. In this paper, a &#034;fast EP&#034; (FEP) is proposed which uses a Cauchy instead of Gaussian mutation as the primary search operator. The relationship between FEP and classical EP (CEP) is similar to that between fast simulated annealing and the classical version. Both analytical and empirical studies have been carried out to evaluate the performance of FEP and CEP for different function optimization problems. This paper shows that FEP is very good at search in a large neighborhood while CEP is better at search in a small local neighborhood. For a suite of 23 benchmark problems, FEP performs much better than CEP for multimodal functions with many local minima while being comparable to CEP in performance for unimodal and multimodal functions with only a few local minima. This paper also shows the relationship between the search step size and the probability of finding a global optimum and thus explains why FEP performs better than CEP on some functions but not on others. In addition, the importance of the neighborhood size and its relationship to the probability of finding a near-optimum is investigated. Based on these analyses, an improved FEP (IFEP) is proposed and tested empirically. This technique mixes different search operators (mutations). The experimental results show that IFEP performs better than or as well as the better of FEP and CEP for most benchmark problems tested.
555|Using selection to improve particle swarm optimization|This paper describes a evolutionary optimization algorithm that is a hybrid based on particle swarm but with the addition of a standard selection mechanism from evolutionary computations. A comparison is per-formed between hybrid swarm and particle swarm that shows selection to provide an advantage for some but not all complex functions. 1
556|Nature&#039;s way of optimizing|We propose a general-purpose method for finding high-quality solutions to hard optimization problems, inspired by self-organizing processes often found in nature. The method, called Extremal Optimization, successively eliminates extremely undesirable components of sub-optimal solutions. Drawing upon models used to simulate far-from-equilibrium dynamics, it complements approximation methods inspired by equilibrium statistical physics, such as Simulated Annealing. With only one adjustable parameter, its performance proves competitive with, and often superior to, more elaborate stochastic optimization procedures. We demonstrate it here on two classic hard optimization problems: graph partitioning and the traveling salesman problem.
557|Hybrid Particle Swarm Optimiser with Breeding and Subpopulations|In this paper we present two hybrid Particle  Swarm Optimisers combining the idea of the particle  swarm with concepts from Evolutionary Algorithms.
558|Stretching technique for obtaining global minimizers through Particle Swarm Optimization|The Particle Swarm Optimizer, like many other evolutionary and classical minimization methods,  suffers the problem of occasional convergence to local minima, especially in multimodal and scattered  landscapes. In this work we propose a modification of the Particle Swarm Optimizer that makes use  of a new technique, named Function &#034;Stretching&#034;, to alleviate the local minima problem. Function  &#034;Stretching&#034; consists of a two--stage transformation of the objective function that eliminates local minima,  while preserving global ones. Experiments indicate that the Particle Swarm Optimizer equipped with  the &#034;Stretching&#034; technique exhibits good performance and results in finding global minima reliably and  predictably.  1 
559|Initializing the particle swarm optimization using nonlinear simplex method |Abstract:- Initialization of the population in Evolutionary Computation algorithms is an issue of ongoing research. Proper initialization may help the algorithm to explore the search space more efficiently and detect better solutions. In this paper, the Nonlinear Simplex Method is used to initialize the swarm of the Particle Swarm technique. Experiments for several well-known benchmark problems imply that better convergence rates and success rates can be achieved by initializing the swarm this way.
560|A Survey of Distributed Query Optimization|Abstract: The distributed query optimization is one of the hardest problems in the database area. The great commercial success of database systems is partly due to the development of sophisticated query optimization technology where users pose queries in a declarative way using SQL or OQL and the optimizer of the database system finds a good way (i. e. plan) to execute these queries. The optimizer, for example, determines which indices should be used to execute a query and in which order the operations of a query (e. g. joins, selects, and projects) should be executed. To this end, the optimizer enumerates alternative plans, estimates the cost of every plan using a cost model, and chooses the plan with lowest cost. There has been much research into this field. In this paper, we study the problem of distributed query optimization; we focus on the basic components of the distributed query optimizer, i. e. search space, search strategy, and cost model. A survey of the available work into this field is given. Finally, some future work is highlighted based on some recent work that uses mobile agent technologies.
561|Parallel database systems: the future of high performance database systems|Abstract: Parallel database machine architectures have evolved from the use of exotic hardware to a software parallel dataflow architecture based on conventional shared-nothing hardware. These new designs provide impressive speedup and scaleup when processing relational database queries. This paper reviews the techniques used by such systems, and surveys current commercial and research systems. 1.
562|Mobile Software Agents: An Overview|he term mobile agent contains two separate and distinct concepts: mobility and agency. 1 merce, and multimedia communication, but it will also create more challenges in organizing information and facilitating its efficient retrieval. From the network perspective, there will be additional challenges and problems in meeting bandwidth requirements and network management. Many researchers believed that the mobile agent paradigm (mobile object) could propose several attractive solutions to deal with such challenges and problems. A number of mobile agent systems have been designed and implemented in academic institutions and commercial firms. However, few applications were found to take advantage of the mobile agent. Among the hurdles facing this emerging paradigm are concerns about security requirements and efficient resource management. This article introduces the core concepts of this emerging paradigm, and attempts to present an account of current research efforts in the context of telecommunications. The goal is to provide the interested reader with a clear background of the opportunities and challenges this emerging paradigm brings about, and a descriptive look at some of the forerunners that are providing experimental technologies supporting this paradigm. •Exporting mobile agent states •Mobile agent data transfer •Transparent communication •Security •Secrecy and privacy
563|Mobile Objects and Mobile Agents: The Future of Distributed Computing?|This paper will lead you into the world of mobile agents, an  emerging technology that makes it very much easier to design, implement, and  maintain distributed systems. You will find that mobile agents reduce the  network traffic, provide an effective means of overcoming network latency, and  perhaps most importantly, through their ability to operate asynchronously and  autonomously of the process that created them, helps you to construct more  robust and fault-tolerant. Read on and let us introduce you to software agents -  the mobile as well as the stationary ones. We will explain all the benefits of  mobile agents and demonstrate the impact they have on the design of  distributed systems before concluding this paper with a brief overview of some  contemporary mobile agent systems.
564|Distributed and Parallel Database Systems|this paper, we present an overview of the distributed DBMS and parallel DBMS technologies, highlight the unique characteristics of each, and indicate the similarities between them. This discussion should help establish their unique and complementary roles in data management. Underlying Principles
565|Industrial-Strength Parallel Query Optimization: issues and lessons|In the industrial context of the EDS project, we have designed and implemented a query optimizer which we have integrated within a parallel database system. The optimizer takes as input a query expressed in ESQL, an extension of SQL with objects and rules, and produces a minimum cost parallel execution plan. Our research agenda has focused on several difficult problems: support of ESQL&#039;s advanced features such as path expressions and recursion, modelling of parallel execution spaces and extensibility of the search strategy. In this paper, we give a retrospective on the optimizer project with emphasis on our design goals, research contributions and implementation decisions. We also describe the current optimizer prototype and report on experiments performed with a pilot application. Finally, we present the lessons learned. 1 Introduction EDS is an ESPRIT project started in 1989 by Bull, ICL, Siemens, ECRC and INRIA with the major goal of producing a parallel database server [11] which e...
566|Optimization Of Parallel Execution For Multi-Join Queries|In this paper, we study the subject of exploiting inter-operator parallelism to optimize the execution of multi-join queries. Specifically, we focus on two major issues: (i) scheduling the execution sequence of multiple joins within a query, and (ii) determining the number of processors to be allocated for the execution of each join operation obtained in (i). For the first issue, we propose and evaluate by simulation several methods to determine the general join sequences, or bushy trees. Despite their simplicity, the heuristics proposed can lead to the general join sequences which significantly outperform the optimal sequential join sequence. The quality of the join sequences obtained by the proposed heuristics is shown to be fairly close to that of the optimal one. For the second issue, it is shown that the processor allocation for exploiting inter-operator parallelism is subject to more constraints, such as execution dependency and system fragmentation, than those in the study of in...
567|Parallel Query Processing with Zigzag Trees|In this paper, we describe our approach to the compile-time optimization and parallelization of  queries for execution in DBS3 or EDS. DBS3 is a shared-memory parallel database system,  while the EDS system has a distributed-memory architecture. Because DBS3 implements a  parallel dataflow execution model, this approach applies to both architectures. Using randomized  search strategies enables exploring a search space large enough to include zigzag trees, which  are intermediate between left-deep and right-deep trees. Zigzag trees are shown to provide better  response time than right-deep trees in case of limited memory. Performance measurements run  using the DBS3 prototype show the advantages of zigzag trees under various conditions.  
568|Fast, Randomized Join-Order Selection -- Why Use Transformations?|We study the effectiveness of probabilistic selection of join-query evaluation plans  without reliance on tree transformation rules. Instead, each candidate plan is chosen  uniformly at random from the space of valid evaluation orders. This leads to a  transformation-free strategy where a sequence of random plans is generated and the  plans are compared on their estimated costs. The success of this strategy depends on  the ratio of &#034;good&#034; evaluation plans in the space of alternatives, the efficient generation  of random candidates, and an accurate estimation of their cost.  To avoid a biased exploration of the space, we solved the open problem of efficiently  generating random, uniformly-distributed evaluation orders, for queries with acyclic  graphs. This benefits any optimization or sampling scheme in which a random choice  of (initial) query plans is required.  A direct comparison with iterative improvement and simulated annealing, using a  proven cost-evaluator, shows that our transformation-free strategy converges faster and  yields solutions of comparable cost.
569|A Tree-Decomposition Approach to Parallel Query Optimization|In this paper we present an approach for transforming a relational join tree into a detailed execution plan with resource allocation information, for execution on a parallel machine. Our approach starts by transforming a query tree, such as might be generated by a sequential optimizer, into an operator tree which is then partitioned into a forest of linear chains of pipelined operators. We present an algorithm for scheduling these chains in conformance to their precedence ordering on a parallel machine. The aim of the scheduling is to achieve some objective like minimum response time or maximum speedup, etc. We provide a set of experiments to demonstrate the quality of the allocation plans generated by the method. A major benefit of the method is that it uses inter-operator, intra-operator and pipelining methods of parallelizing relational query operators simultaneously. Our experiments show that the heuristic scheduling method creates schedules that are close to optimal while being ve...
570|Mobile Agents - The Right Vehicle for Distributed Sequential Computing|Distributed sequential computing uses the collective memory of a network of workstations to reduce paging overhead. In contrast to the &#034;page farm&#034; approach, in which a stationary program remotely accesses data, distributed sequential computing moves the code to the data. In this paper, we show that mobile agents are the most natural and effective way to implement this approach. This is because mobile agents preserve the algorithmic integrity of sequential programs, while a message passing implementation requires a complete restructuring of the code.
571|Algebraic identities and query optimization|in a parametric model for relational temporal databases
572|The Temporal Query Language TQuel|This paper defines aggregates in the temporal query language TQuel and provides their rormal semantics in the tuple relational calculus. A rormal semantics (or Que! aggregates is defined in the process. Multiple aggregates; aggregates appearing in the where, when, valid, and as-or clauses; nested aggregation; and instantaneous, cumulative, and unique variants are supported. These aggregates give the user a rich set or statistical functions that range over time, while requiring minimal additions to TQuel and its semantics..:&#039;l&#039;bi1 work wu nppolied bJ NSF (l&#039;&amp;lli DCR·8402330 and by a Junior Faculty Denlopmnt Awud from the UNC. CH FoUD.datioa. The &amp;nt aat.hor wu npport.ed ia pan by u IBM Faculty Developmnt. Award..,!;
573|Evaluation of Relational Algebras Incorporating the Time Dimension in Databases|The relational algebra is a procedural query language for relational databases. In this paper we survey extensions of the relational algebra that can query databases recording time-varying data. Such an algebra is a critical part of a temporal DBMS. We identify 26 criteria that provide an objective basis for evaluating temporal algebras, Seven of the criteria are shown to be mutually unsatisfiable, implying there can be no perfect temporal algebra, Choices made as to which of the incompatible criteria are satisfied characterize existing algebras Twelve time-oriented algebras are summarized and then evaluated against the criteria. We demonstrate that the design space has in some sense been explored in that all combinations of basic design decisions have at least one representative algebra. Coverage of the remaining criteria provides one measure of the quality of each algebra We argue that all of the criteria are independent and that the criteria identified as compatible are indeed so, Finally, we list plausible properties proposed by others that are either subsumed by other criteria, are not well defined, or have no objective basis for being evaluated. The algebras realize many different approaches to what appears initially to be a straightforward design task.
574|Efficient Indexing Methods for Temporal Relations|Abstract-The size of temporal databases and the semantics cases are subsequently investigated: 1) Dynamic structures for of temporal queries pose challenges for the design of efhcient indexing methods. The primary issues that affect the design of indexing methods are examined, and propose several structures and algorithms for specific cases. Indexing methods for timebased queries are developed, queries on the surrogate or timesurrogate and time indexing (ST); 2) Static and dynamic partitioning algorithms for the time-line in the context of temporal attribute and time indexing; and 3) Time-indexing for append-only database. In all the designs, the focus is on invariant key and time, and temporal attribute and time. In the the role of the time attribute. latter case, several methods are presented that partition the timeline, in order to balance the distribution of tuple-pointers within the index. The methods are analyzed against alternatives, and present appropriate empirical results. Index Terms-Indexing, physical organization, query processing, searching, temporal databases. The paper is organized as follows. In Section II, we discuss the relational representation of data in the temporal context, followed by a framework for analyzing the physical design of a temporal database. In Section III, we introduce the APtree, which is designed for time-based query operations on an append-only database, and is subsequently incorporated into our surrogate-time index of Section IV. In Section V, we I.
575|Experiments in Query Optimization|this report, one CLARITECH linguist, and two non-technical volunteers. The 50 ad-hoc topics were divided up Non-Relevant
576|A comparison of classifiers and document representations for the routing problem|In this paper, we compare learning techniques based on statistical classification to traditional methods of relevance feedback for the document routing problem. We consider three classification techniques which have decision rules that are derived via explicit error minimization: linear discriminant analysis, logistic regression, and neural networks. We demonstrate that the classifiers perform 1015 % better than relevance feedback via Rocchio expansion for the TREC-2 and TREC-3 routing tasks. 

Error minimization is difficult in high-dimensional feature spaces because the convergence process is slow and the models are prone to overfitting. We use two different strategies, latent semantic indexing and optimal term selection, to reduce the number of features. Our results indicate that features based on latent semantic indexing are more effective for techniques such as linear discriminant analysis and logistic regression, which have no way to protect against overfitting. Neural networks perform equally well with either set of features and can take advantage of the additional information available when both feature sets are used as input.
577|Noun-Phrase Analysis in Unrestricted Text for Information Retrieval|Information retrieval is an important application area of natural-language processing where one encounters the genuine challenge of processing large quantities of unrestricted natural-language text. This paper reports on the application of a few simple, yet robust and efficient nounphrase analysis techniques to create bet- ter indexing phrases for information retrieval. In particular, we describe a hybrid approach to the extraction of meaningful (continuous or discontinuous) subcompounds from complex noun phrases using both corpus statistics and linguistic heuristics. Results of experiments show that indexing based on such extracted sub- compounds improves both recall and precision in an information retrieval system. The noun-phrase analysis techniques are also potentially useful for book indexing and automatic thesaurus extraction.
579|A Statistical Approach to Automatic OCR Error Correction In Context|This paper describes an automatic, context-sensitive, word-error correction system based  on statistical language modeling (SLM) as applied to optical character recognition (OCR) postprocessing.
580|Okapi at TREC-6 - Automatic ad hoc, VLC, routing, filtering and QSDR|this paper; comparisons between passage and non-passage runs can be seen in a few of the tables.
581|ATT at TREC-6|TREC-6 is AT&amp;T&#039;s first independent TREC participation. We are participating in the main tasks (adhoc, routing), the filtering track, the VLC track, and the SDR track  1  This year, in the main tasks, we experimented with multi-pass query expansion using Rocchio&#039;s formulation. We concentrated a reasonable amount of our effort on our VLC track system, which is based on locally distributed, disjoint, and smaller sub-collections of the large collection. Our filtering track runs are based on our routing runs, followed by similarity thresholding to make a binary decision of the relevance prediction for a document. 1 Introduction  TREC-6 is the first TREC in which AT&amp;T is participating as an independent group. Much of our work is largely inspired by Smart&#039;s philosophy of fully automatic processing of large text collections. Our participation is based on an internally modified version of Cornell&#039;s SMART system. We submitted runs for the adhoc task, the routing task, the filtering track, the VL...
582|Passage-Based Refinement (MultiText Experiments for TREC-6)  (1998) |The MultiText system retrieves passages, rather than entire documents, that are likely to be relevant to a particular topic. For all runs, we used the reciprocal of the length of each passage as an estimate of its likely relevance and ranked accordingly. For the manual adhoc task we explored the limits of user interaction by judging some 13,000 documents based on retrieved passages. For the automatic adhoc task we used retrieved passages as a feedback source for new query terms. For the routing task we estimated probability of relevance from passage length and used this estimate to construct a compound (tiered) query which was used to rank the new data using passage length. For the Chinese track we indexed individual characters rather than segmented words or bigrams and used manually constructed queries and passage-length ranking. For the high precision track we performed judgements on passages using an interface similar to that used for the manual adhoc task. The Very Large Collection...
583|Experiments in Spoken Document Retrieval at CMU|We describe our submission to the TREC-7 Spoken Document Retrieval (SDR) track and the speech recognition and information retrieval engines. We present SDR evaluation results and a brief analysis. A few developments are also described in greater detail including:  . A new, probabilistic retrieval engine based on language models.  . A new, TFIDF-based weighting function that incorporates word error probability.  . The use of a simple confidence estimate for word probability based on speech recognition lattices. Although improvements over a development test set were promising, the new techniques failed to yield significant gains in the evaluation test set. 1. The SDR Data and Task  The entire set of speech data for the 1998 TREC-7 spoken document retrieval track consisted of 153 hours of broadcast news, approximately 80 for training and 73 for testing. The data had been segmented into stories and manually transcribed. In the test set, there were three &#034;versions&#034; of the data available: A manually generated transcript, speech recognition transcripts based on IBM and CMU recognizers, and the raw audio data, to be transcribed by our own recognizer. The entire training set was used to train acoustic models for the speech recognition system. The remainder was held out as unseen test data. There were about 3245 stories in the training data set and 2866 in the test set. To develop and debug the system, the TREC-6 evaluation set was used in a Known-Item Retrieval system -- where every query has only one document assigned as relevant. In our experiments on the evaluation test set, the average precision of the retrieval for each of the relevant documents was used to judge the quality of the retrieval. However, since relevance judgements were not available for the development test se...
584|OCR Correction and Query Expansion for Retrieval on OCR Data - CLARIT TREC-5 Confusion Track Report|this report we first give a brief description of the OCR correction and query expansion techniques, and then discuss the results of our experiments. 2 The Automatic OCR Correction System
585|Experiments on Chinese Text Indexing ---CLARIT TREC-5 Chinese Track Report |Introduction  The focus of the CLARIT  TM1  Chinese Track Experiments is on investigating the effectiveness of different automatic indexing methods for retrieval over Chinese texts. In particular, we explored indexing using linguistic units (words, compound words, and phrases), single Chinese characters, and overlapping character bigrams. In addition to fully automatic processing of queries, we ran experiments with manually constructed term vector queries supplemented by Boolean type constraints. The constraints were used for selecting documents for CLARIT automatic feedback or as a mean of refining the final set of retrieved documents [Mili&#039;c-Frayling et al. 1997]. All the experiments were conducted using the CLARIT retrieval system [Evans &amp; Lefferts 1995]. Since its current NLP component does not support the parsing of Chinese texts, we designed an appropriate parsing module and pre-processed the documents before submitting them for indexing and retrieval
586|Conventional Query Optimization Research:|Sprinkled with case studies here and there,
587|Buffer Pool Aware Query Optimization |With the advent of 64-bit processors, large main memories are set to become very common. This in turn translates to larger buffer pool configurations in database servers. Query optimizers however, currently assume all data is disk resident while optimizing queries. This assumption will no longer be valid when buffer pools become 100’s of gigabytes in size. In this paper we examine how data presence in the buffer pool can affect the choice of query plans in an optimizer. We examine the possible benefits of buffer-pool aware query optimization and propose a generic architecture for implementing such an optimizer. 1.
588|Main memory database systems: An overview|Abstract-Memory resident database systems (MMDB’s) store their data in main physical memory and provide very high-speed access. Conventional database systems are optimized for the particular characteristics of disk storage mechanisms. Memory resident systems, on the other hand, use different optimizations to structure and organize data, as well as to make it reliable. This paper surveys the major memory residence optimizations and briefly discusses some of the memory resident systems that have been designed or implemented. Index Terms- Access methods, application programming in-terface, commit processing, concurrency control, data clustering, data representation, main memory database system (MMDB), query processing, recovery. Invited Paper I.
589|Join synopses for approximate query answering|In large data warehousing environments, it is often advantageous to provide fast, approximate answers to complex aggregate queries based on statistical summaries of the full data. In this paper, we demonstrate the difficulty of providing good approximate answers for join-queries using only statistics (in particular, samples) from the base relations. We propose join synopses (join samples) as an effective solution for this problem and show how precomputing just one join synopsis for each relation suffices to significantly improve the quality of approximate answers for arbitrary queries with foreign key joins. We present optimal strategies for allocating the available space among the various join synopses when the query work load is known and identify heuristics for the common case when the work load is not known. We also present efficient algorithms for incrementally maintaining join synopses in the presence of updates to the base relations. One of our key contributions is a detailed analysis of the error bounds obtained for approximate answers that demonstrates the trade-offs in various methods, as well as the advantages in certain scenarios of a new subsampling method we propose. Our extensive set of experiments on the TPC-D benchmark database show the effectiveness of join synopses and various other techniques proposed in this paper. 1
590|MultiObjective Parametric Query Optimization |Classical query optimization compares query plans accord-ing to one cost metric and associates each plan with a con-stant cost value. In this paper, we introduce the Multi-Objective Parametric Query Optimization (MPQ) problem where query plans are compared according to multiple cost metrics and the cost of a given plan according to a given metric is modeled as a function that depends on multiple parameters. The cost metrics may for instance include ex-ecution time or monetary fees; a parameter may represent the selectivity of a query predicate that is unspecied at optimization time. MPQ generalizes parametric query optimization (which allows multiple parameters but only one cost metric) and multi-objective query optimization (which allows multiple cost metrics but no parameters). We formally analyze the novel MPQ problem and show why existing algorithms are inapplicable. We present a generic algorithm for MPQ and a specialized version for MPQ with piecewise-linear plan cost functions. We prove that both algorithms nd all relevant query plans and experimentally evaluate the performance of our second algorithm in a Cloud computing scenario. 1.
591|Multi-dimensional Resource Scheduling for Parallel Queries|Scheduling query execution plans is an important component of query optimization in parallel database systems. The problem is particularly complex in a shared-nothing execution environment, where each system node represents a collection of time-shareable resources (e.g., CPU(s), disk(s), etc.) and communicates with other nodes only by message-passing. Significant research effort has concentrated on only a subset of the various forms of intra-query parallelism so that scheduling and synchronization is simplified. In addition, most previous work has focused its attention on one-dimensional models of parallel query scheduling, effectively ignoring the potential benefits of resource sharing. In this paper, we develop an approach that is more general in both directions, capturing all forms of intra-query parallelism and exploiting sharing of multi-dimensional resource nodes among concurrent plan operators. This allows scheduling a set of independent query tasks (i.e., operator pipelines) to be seen as an instance of the multidimensional bin-design problem. Using a novel quantification of coarse grain parallelism, we present a list scheduling heuristic algorithm that is provably near-optimal in the class of coarse grain parallel executions (with a worst-case performance ratio that depends on the number of resources per node and the granularity parameter). We then extend this algorithm to handle the operator precedence constraints in a bushy query plan by splitting the execution of the plan into synchronized phases. Preliminary performance results confirm the effectiveness of our scheduling algorithm compared both to previous approaches and the optimal solution. Finally, we present a technique that allows us to relax the coarse granularity restriction and obtain a list scheduling method that is provably near-optimal in the space of all possible parallel schedules.
592|Havasu: A Multi-Objective, Adaptive Query Processing Framework for Web Data Integration|Mediators for web-based data integration need the ability to handle multiple, often conflicting  objectives, including cost, coverage and execution flexibility. This requires the development of query  planning algorithms that are capable of multi-objective query optimization, as well as techniques for  automatically gathering the requisite cost/coverage statistics from the autonomous data sources. We  are designing a query processing framework called Havasu to handle these challenges. We will present  the architecture of Havasu and describe the implementation and evaluation of its query planning and  statistics gathering modules.
593|PET: Reducing Database Energy Cost via Query Optimization |Energy conservation is a growing important issue in designing modern database management system (DBMS). This requires a deep thinking about the tradeoffs between energy and performance. Despite the significant amount of efforts at the hardware level to make the major components consume less energy, we argue for a revisit of the DBMS query processing mechanism to identify and harvest the potential of energy saving. However, the state-of-art architecture of DBMS does not take energy usage into consideration in its design. A major challenge in developing an energy-aware DBMS is to design and implement a cost-based query optimizer that evaluates query plans by both performance and energy costs. By following such a strategy, our previous work revealed the fact that energy-efficient query plans do not necessarily have the shortest processing time. This demo proposal introduces PET – an energy-aware query optimization framework that is built as a part of the PostgreSQL kernel. PET, via its power cost estimation module and plan evaluation model, enables the database system to run under a DBA-specified energy/performance tradeoff level. PET contains a power cost estimator that can accurately estimate the power cost of query plans at compile time, and a query evaluation engine that the DBA could configure key PET parameters towards the desired tradeoff. The software to be demonstrated will also include workload engine for producing large quantities of queries and data sets. Our demonstration will show how PET functions via a comprehensive set of views from its graphical user interface named PET Viewer. Through such interfaces, a user can achieve a good understanding of the energyrelated query optimization and cost-based plan generation. Users are also allowed to interact with PET to experience the different energy/performance tradeoffs by changing PET and workload parameters at query runtime. 1.
594|Approximation Schemes for Many-Objective Query Optimization|The goal of multi-objective query optimization (MOQO) is to find query plans that realize a good compromise between conflicting objectives such as minimizing execution time and minimizing monetary fees in a Cloud scenario. A previously proposed exhaustive MOQO algorithm needs hours to op-timize even simple TPC-H queries. This is why we pro-pose several approximation schemes for MOQO that gener-ate guaranteed near-optimal plans in seconds where exhaus-tive optimization takes hours. We integrated all MOQO algorithms into the Postgres op-timizer and present experimental results for TPC-H queries; we extended the Postgres cost model and optimize for up to nine conflicting objectives in our experiments. The pro-posed algorithms are based on a formal analysis of typical cost functions that occur in the context of MOQO. We iden-tify properties that hold for a broad range of objectives and can be exploited for the design of future MOQO algorithms.
595|On the Stability of Plan Costs and the Costs of Plan Stability |Predicate selectivity estimates are subject to considerable run-time variation relative to their compile-time estimates, often leading to poor plan choices that cause inflated response times. We present here a parametrized family of plan generation and selection algorithms that replace, whenever feasible, the optimizer’s solely costconscious choice with an alternative plan that is (a) guaranteed to be near-optimal in the absence of selectivity estimation errors, and (b) likely to deliver comparatively stable performance in the presence of arbitrary errors. These algorithms have been implemented within the PostgreSQL optimizer, and their performance evaluated on a rich spectrum of TPC-H and TPC-DS-based query templates in a variety of database environments. Our experimental results indicate that it is indeed possible to identify robust plan choices that substantially curtail the adverse effects of erroneous selectivity estimates. In fact, the plan selection quality provided by our algorithms is often competitive with those obtained through apriori knowledge of the plan search and optimality spaces. The additional computational overheads incurred by the replacement approach are miniscule in comparison to the expected savings in query execution times. We also demonstrate that with appropriate parameter choices, it is feasible to directly produce anorexic plan diagrams, a potent objective in query optimizer design. 1.
596|Rule Induction for Semantic Query Optimization|Semantic query optimization can dramatically speed up database query answering by knowledge intensive reformulation. But the problem of how to learn required semantic rules has not previously been solved. This paper describes an approach using an inductive learning algorithm to solve the problem. In our approach, learning is triggered by user queries and then the system induces semantic rules from the information in databases. The inductive learning algorithm used in this approach can select an appropriate set of relevant attributes from a potentially huge number of attributes in real-world databases. Experimental results demonstrate that this approach can learn sufficient background knowledge to reformulate queries and provide a 57 percent average performance improvement.  1 INTRODUCTION  Speeding up a system&#039;s performance is one of the major goals of machine learning. Explanation-based learning is typically used for speedup learning, while applications of inductive learning are usual...
597|Retrieving And Integrating Datafrom Multiple Information Sources|With the current explosion of data, retrieving and integrating information  from various sources is a critical problem. Work in multidatabase systems  has begun to address this problem, but it has primarily focused on methods  for communicating between databases and requires significant effort for each  new database added to the system. This paper describes a more general  approach that exploits a semantic model of a problem domain to integrate  the information from various information sources. The information sources  handled include both databases and knowledge bases, and other information  sources (e.g., programs) could potentially be incorporated into the system.  This paper describes how both the domain and the information sources are  modeled, shows how a query at the domain level is mapped into a set of  queries to individual information sources, and presents algorithms for automatically  improving the efficiency of queries using knowledge about both the  domain and the informat...
598|Learning With Many Irrelevant Features|In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires \Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  +  p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that--- contrary to expectations---these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURE...
599|The Utility of Knowledge in Inductive Learning|In this paper, we demonstrate how different forms of background knowledge can be integrated with an inductive method for generating constant-free Horn clause rules. Furthermore, we evaluate, both theoretically and empirically, the effect that these types of knowledge have on the cost of learning a rule and on the accuracy of a learned rule. Moreover, we demonstrate that a hybrid explanation-based and inductive learning method can advantageously use an approximate domain theory, even when this theory is incorrect and incomplete.  1 Introduction  Most existing systems that combine empirical and explanation-based learning severely restrict the complexity of the language for expressing the concept definition. For example, some systems require that the concept definition be expressed in terms of attribute-value pairs (Lebowitz, 1986; Danyluk, 1989). Others effectively restrict the concept definition language to that of propositional calculus, by only allowing unary predicates (Hirsh, 1989;...
600|Learning Transformation Rules for Semantic Query Optimization: A Data-Driven Approach|Learning query transformation rules is vital for the success of semantic  query optimization in domains where the user cannot provide a comprehensive  set of integrity constraints. Finding these rules is a discovery task because of  the lack of target. Previous approaches to learning query transformation rules  have been based on analyzing past queries. We propose a new approach to  learning query transformation rules based on analyzing the existing data in the  database. This paper describes a framework and a closure algorithm to learning  rules from a given data-distribution. We characterize the correctness, completeness  and complexity of the proposed algorithm and provide a detailed example  to illustrate the framework.  Keywords: Rule discovery, semantic query optimization, discovery in data.  Areas Addressed: Learning and Discovery in Database, Data Engineering Tools, Highlevel Query Answering, Applications in Query Optimization.  Postal Address 4-192 EE/CS Bldg., 200 Union Stree...
601|Reformulating Query Plans For Multidatabase Systems|A practical heterogeneous, distributed multidatabase system must answer queries efficiently. Conventional query optimization techniques are not adequate here because these techniques are dependent on the database structure, and rely on limited information which is not  sufficient in complicated multidatabase queries. This paper presents an automated approach to reformulating query plans to improve the efficiency of multidatabase queries. This approach uses database abstractions, the knowledge about the contents of databases, to reformulate a query plan into less expensive but semantically equivalent one. We present two algorithms. The first algorithm reformulates subqueries to individual databases, the second algorithm extends the first one and reformulates the entire query plan. Empirical results show that the reformulations can provide significant savings with minimal overhead. The reformulation approach provides a global reduction in the amount of  the intermediate data as well as local opt...
602|Discovering Regularities from Knowledge Bases|Knowledge bases open new horizons for machine learning research. One challenge is to design learning programs to expand the knowledge base using the knowledge that is currently available. This paper addresses the problem of discovering regularities in large knowledge bases that contain many assertions in different domains. The paper begins with a definition of regularities and gives the motivation for such a definition. It then outlines a framework that attempts to integrate induction with knowledge. Although the implementation of the framework currently uses only a statistical method for confirming hypotheses, its application to some real knowledge base has shown some encouraging and interesting results. 1 Introduction  Discovering regularities from knowledge bases is an important problem for both Knowledge Engineering and Machine Learning. Since the task is aimed at generating new knowledge, its success will directly contribute to knowledge acquisition and the construction of large k...
603|Semantic Query Optimization in the Presence of Types |Both semantic and type-based query optimization rely on the idea that queries often exhibit non-trivial rewritings if the state space of the database is restricted. Despite their close connection, these two problems to date have always been studied separately. We present a unifying, logic-based framework for query optimization in the presence of data dependencies and type information. It builds upon the classical chase algorithm and extends existing query minimization techniques to considerably larger classes of queries and dependencies. In particular, our setting requires chasing conjunctive queries (possibly with union and negation) in the presence of dependencies containing negation and disjunction. We study the applicability of the chase in this setting, develop novel conditions that guarantee its termination, identify fragments for which minimal query computation is always possible (w.r.t. a generic cost function), and investigate the complexity of related decision problems.
604|Data Integration: A Theoretical Perspective|Data integration is the problem of combining data residing at different sources, and providing the user with a unified view of these data. The problem of designing data integration systems is important in current real world applications, and is characterized by a number of issues that are interesting from a theoretical point of view. This document presents on overview of the material to be presented in a tutorial on data integration. The tutorial is focused on some of the theoretical issues that are relevant for data integration. Special attention will be devoted to the following aspects: modeling a data integration application, processing queries in data integration, dealing with inconsistent data sources, and reasoning on queries.
605|Tractable reasoning and efficient query answering in description logics: The DL-Lite family| We propose a new family of Description Logics (DLs), called DL-Lite, specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. Reasoning here means not only computing subsumption between concepts, and checking satisfiability of the whole knowledge base, but also answering complex queries (in particular, unions of conjunctive queries) over the instance level (ABox) of the DL knowledge base. We show that, for the DLs of the DL-Lite family, the usual DL reasoning tasks are polynomial in the size of the TBox, and query answering is LogSpace in the size of the ABox (i.e., in data complexity). To the best of our knowledge, this is the first result of polynomial time data complexity for query answering over DL knowledge bases. Notably our logics allow for a separation between TBox and ABox reasoning during query evaluation: the part of the process requiring TBox reasoning is independent of the ABox, and the part of the process requiring access to the ABox can be carried out by an SQL engine, thus taking advantage of the query optimization strategies provided by current Data Base Management Systems. Since it can be shown that even slight extensions to the logics of the DL-Lite family make query answering at least NLogSpace in data complexity, thus ruling out the possibility of using on-the-shelf relational technology for query processing, we can conclude that the logics of the DL-Lite family are the maximal DLs supporting efficient query answering over large amounts of instances. 
606|Testing implications of data dependencies|Presented is a computation method-the chase-for testing implication of data dependencies by a set of data dependencies. The chase operates on tableaux similar to those of Aho, Sagiv, and Ullman. The chase includes previous tableau computation methods as special cases. By interpreting tableaux alternately as mappings or as templates for relations, it is possible to test implication of join dependencies (including multivalued dependencies) and functional dependencies by a set of depen-dencies.
608|A general Datalog-based framework for tractable query answering over ontologies|Ontologies play a key role in the Semantic Web [4], data modeling, and information integration [16]. Recent trends in ontological reasoning have shifted from decidability issues to tractability ones, as e.g. reflected by the work on the DL-Lite family of tractable description logics (DLs) [11, 19]. An important result of these works is that the main
609|Horn clauses and database dependencies|Abstract. Certain first-order sentences, called &amp;quot;dependencies, &amp;quot; about relations in a database are defined and studied. These dependencies seem to include all prewously defined dependencies as special cases A new concept is mtroduced, called &amp;quot;faithfulness (with respect to direct product), &amp;quot; which enables powerful results to be proved about the existence of &amp;quot;Armstrong relations &amp;quot; in the presence of these new dependencies. (An Armstrong relaUon is a relation that obeys precisely those dependencies that are the logical consequences of a given set of dependencies.) Results are also obtained about characterizing the class of projections of those relations that obey a given set of dependencies.
610|J.B.: The chase revisited |We revisit the classical chase procedure, studying its properties as well as its applicability to standard database problems. We settle (in the negative) the open problem of decidability of termination of the standard chase, and we provide sufficient termination conditions which are strictly less over-conservative than the best previously known. We investigate the adequacy of the standard chase for checking query containment under constraints, constraint implication and computing certain answers in data exchange. We find room for improvement after gaining a deeper understanding of the chase by separating the algorithm from its result. We identify the properties of the chase result that are essential to the above applications, and we introduce the more general notion of an F-universal model set, which supports query and constraint languages that are closed under a class F of mappings. By choosing F appropriately, we extend prior results all the way to existential first-order queries and ??-first-order constraints (and various standard sublanguages). We show that the standard chase is incomplete for finding universal model sets, and we introduce the extended core chase which is complete, i.e. finds an F-universal model set when it exists. A key advantage of the new chase is that the same algorithm can be applied for the mapping classes F of interest, by simply modifying appropriately the set of constraints given as input. Even when restricted to the typical input in prior work (unions of conjunctive queries and embedded dependencies), the new chase supports certain answer computation and containment/implication tests in strictly more cases than the incomplete standard chase.
611|Main Memory Oriented Optimization of OO Queries using Typed Datalog with Foreign Predicates|Object-oriented DBMSs (OODBs) have created a demand for relationally  complete, extensible, and declarative object-oriented (OO)  query languages. Until now, run time performance of such languages  was far behind that of procedural OO interfaces. One reason is the internal  use of a relational engine with magnetic disk resident databases.  We address the processing of the declarative OO language WS-OSQL,  provided by the fully operational prototype OODB called WS-IRIS.  A WS-IRIS database is MM resident. The system architecture, data  structures, and optimization techniques are designed accordingly. WSOSQL  queries are compiled into an OO extension of Datalog called  ObjectLog, providing for objects, typing, overloading, and foreign  predicates for extensibility. We present cost based optimizations in  WS-IRIS using ObjectLog. Performance tests show that WS-IRIS  is about as fast as current OODBs with procedural interfaces only  and is much faster than known relationally complete syste...
612|SPROUT: Lazy vs. eager query plans for tuple-independent probabilistic databases|Abstract—A paramount challenge in probabilistic databases is the scalable computation of confidences of tuples in query results. This paper introduces an efficient secondary-storage operator for exact computation of queries on tuple-independent probabilistic databases. We consider the conjunctive queries without self-joins that are known to be tractable on any tupleindependent database, and queries that are not tractable in generalbutbecometractableonprobabilisticdatabasesrestricted by functional dependencies. Our operator is semantically equivalent to a sequence of aggregations and can be naturally integrated into existing relational query plans. As a proof of concept, we developed an extension of the PostgreSQL 8.3.3 query engine called SPROUT. We study optimizations that push or pull our operator or parts thereof past joins. The operator employs static information, such
613|Deciding Containment for Queries with Complex Objects and Aggregations|We address the problem of query containment and query equivalence for complex objects. We show that for a certain conjunctive query language for complex objects, query containment and weak query equivalence are decidable. Our results have two consequences. First, when the answers of the two queries are guaranteed not to contain empty sets, then weak equivalence coincides with equivalence, and our result answers partially an open problem about the equivalence of nest; unnest queries for complex objects [GPG90]. Second, we derive an NP-complete algorithm for checking the equivalence of certain conjunctive queries with grouping and aggregates. Our results rely on a translation of the containment and equivalence conditions for complex objects into novel conditions on conjunctive queries, which we call simulation and strong simulation. These conditions are more complex than containment of conjunctive queries, because they involve arbitrary numbers of quantifier alternations. We prove that c...
614|Conjunctive query containment and answering under description logics constraints|Query containment and query answering are two important computational tasks in databases. While query answering amounts to compute the result of a query over a database, query containment is the problem of checking whether for every database, the result of one query is a subset of the result of another query. In this paper, we deal with unions of conjunctive queries, and we address query containment and query answering under Description Logic constraints. Every such constraint is essentially an inclusion dependency between concepts and relations, and their expressive power is due to the possibility of using complex expressions in the specification of the dependencies, e.g., intersection and difference of relations, special forms of quantification, regular expressions over binary relations. These types of constraints capture a great variety of data models, including the relational, the entity-relationship, and the object-oriented model, all extended with various forms of constraints. They also capture the basic features of the ontology languages used in the context of the Semantic Web. We present the following results on both query containment and query answering. We provide a method for query containment under Description Logic constraints, thus showing that the problem
615|Interaction between Path and Type Constraints|This paper investigates that interaction. In particular it studies constraint implication problems, which are important both in understanding the semantics of type/constraint systems and in query optimization. It shows that path constraints interact with types in a highly intricate way. For that purpose a number of results on path constraint implication are established in the presence and absence of type systems. These results demonstrate that adding a type system may in some cases simplify reasoning about path constraints and in other cases make it harder. For example, it is shown that there is a path constraint implication problem that is decidable in PTIME in the untyped context, but that becomes undecidable when a type system is added. On the other hand, there is an implication problem that is undecidable in the untyped context, but becomes not only decidable in cubic time but also finitely axiomatizable when a type system is imposed
616|Query reformulation with constraints |Let S1, S2 be two schemas, which may overlap, C be a set of constraints on the joint schema S1 ? S2, and q1 be a S1-query. An (equivalent) reformulation of q1 in the presence of C is a S2-query, q2, such that q2 gives the same answers as q1 on
617|On Chase Termination Beyond Stratification |We study the termination problem of the chase algorithm, a central tool in various database problems such as the constraint implication problem, Conjunctive Query optimization, rewriting queries using views, data exchange, and data integration. The basic idea of the chase is, given a database instance and a set of constraints as input, to fix constraint violations in the database instance. It is wellknown that, for an arbitrary set of constraints, the chase does not necessarily terminate (in general, it is even undecidable if it does or not). Addressing this issue, we review the limitations of existing sufficient termination conditions for the chase and develop new techniques that allow us to establish weaker sufficient conditions. In particular, we introduce two novel termination conditions called safety and inductive restriction, and use them to define the so-called T-hierarchy of termination conditions. We then study the interrelations of our termination conditions with previous conditions and the complexity of checking our conditions. This analysis leads to an algorithm that checks membership in a level of the T-hierarchy and accounts for the complexity of termination conditions. As another contribution, we study the problem of data-dependent chase termination and present sufficient termination conditions w.r.t. fixed instances. They might guarantee termination although the chase does not terminate in the general case. As an application of our techniques beyond those already mentioned, we transfer our results into the field of query answering over knowledge bases where the chase on the underlying database may not terminate, making existing algorithms applicable to broader classes of constraints. 1.
618|Inference of Well-typings for Logic Programs with Application to Termination Analysis|This paper develops a method to infer a polymorphic well-typing for a logic program. One of the main motivations is to contribute to a better automation of termination analysis in logic programs, by deriving types from which norms can automatically be constructed. Previous work on type-based termination analysis used either types declared by the user, or automatically generated monomorphic types describing the success set of predicates. Declared types are typically more precise and result in stronger termination conditions than those obtained with inferred types. Our type inference procedure involves solving set constraints generated from the program and derives a well-typing in contrast to a success-set approximation. Experiments show that our automatically inferred well-typings are close to the declared types and thus result in termination conditions that are as good as those obtained with declared types for all our experiments to date. We describe the method, its implementation and experiments with termination analysis based on the inferred types.
619|On Path-functional Dependencies as Firstclass Citizens in Description Logics|We investigate whether path-functional dependencies can be granted full status as a concept constructor in a Boolean-complete description logic. In particular, we show that this leads to undecidability of the associated logical implication problem if such dependencies are allowed within the scope of a negation or on the left-hand-side of inclusion dependencies. We then show that allowing such dependencies to occur in the scope of monotone concept constructors on the right-hand-side of inclusion dependencies will still lead to decidable implication problems. 1
620|Scheduling and Caching in Multi-Query Optimization |Database systems frequently have to execute a batch of related queries. Multi-query optimization exploits evaluation plans that share common results. Current approaches to multi-query optimization assume there is infinite disk space, and very limited memory space. Pipelining was the only option considered for avoiding expensive disk writes. The availability of fairly large and inexpensive main memory motivates the need to make best use of available main memory for caching shared results, and scheduling queries in a manner that facilitates caching. Pipelining needs to be exploited at the same time. We look at the problem of multi-query optimization taking into account query scheduling, caching and pipelining. We first prove that MQO with either just query scheduling or just caching is NP-complete. We then provide the first known algorithms for the most general MQO problem with scheduling, caching and pipelining. After showing the connections of this problem with other traditional scheduling problems and graph theoretic problems we outline heuristics for MQO with scheduling, caching and pipelining. 1
621|Typesetting Concrete Mathematics|... tried my best to make the book mathematically interesting, but I also knew that it would be typographically interesting-because it would be the first major use of a new typeface by Hermann Zapf, commissioned by the American Mathematical Society. This typeface, called AMS Euler, had been carefully digitized and put into METAFONT form by Stanford&#039;s digital typography students [a]; but it had not yet been &#034;tuned up &#034; for real applications. My new book served as an ideal test case, because (1) it involved a great variety of mathematical formulas; (2) I was highly motivated to make the book readable and attractive; (3) my experiences with tuning up Computer Modern gave me insights into how to set the mysterious font parameters used by TEX in math mode; and (4) the book was in fact being dedicated to Leonhard Euler, the great mathematician after whom the typeface was named. The underlying philosophy of Zapf&#039;s Euler design was to capture the flavor of mathematics as it might be written by a mathematician with excellent handwriting. For example, one of the earmarks of AMS Euler is its zero, &#039;O&#039;, which is slightly pointed at the top because a handwritten zero rarely closes together smoothly when the curve returns to its starting point. A handwritten rather than mechanical style is appropriate for mathematics because people generally create math with pen, pencil, or chalk. The Euler letters are upright, not italic, so that there is a general consistency with mathematical symbols like plus signs and parentheses, and so that built-up formulas fit together comfortably. AMS Euler includes seven alphabets: Uppercase Roman (ABC through XYZ), lowercase Roman (abc through xyz), uppercase
622|Materialized View Selection and Maintenance Using Multi-Query Optimization|Materialized views have been found to be very effective at speeding up queries, and are increasingly being supported by commercial databases and data warehouse systems. However, whereas the amount of data entering a warehouse and the number of materialized views are rapidly increasing, the time window available for maintaining materialized views is shrinking. These trends necessitate efficient techniques for the maintenance of materialized views.  In this paper, we show how to find an efficient plan for the maintenance of a set of materialized views, by exploiting common subexpressions between different view maintenance expressions. In particular, we show how to efficiently select (a) expressions and indices that can be effectively shared, by transient materialization; (b) additional expressions and indices for permanent materialization; and (c) the best maintenance plan -- incremental or recomputation -- for each view. These three decisions are highly interdependent, and the choice of...
623|Design and Evaluation of Alternative Selection Placement Strategies in Optimizing Continuous Queries|selection placement strategies for optimizing a very large number of continuous queries in an Internet environment. Two grouping strategies, PushDown and PullUp, in which selections are either pushed below, or pulled above, joins are proposed and investigated. While our earlier research has demonstrated that the incremental group optimization can significantly outperform an ungrouped approach, the results from this paper show that different incremental group optimization strategies can have significantly different performance characteristics. Surprisingly, in our studies, PullUp, in which selections are pulled above joins, is often better and achieves an average 10-fold performance improvement over PushDown (occasionally 100 times faster). Furthermore, a revised algorithm of PullUp, termed filtered PullUp is proposed that is able to further reduce the cost of PullUp by 75% when the union of the selection predicates is selective. Detailed cost models, which consider several special parameters, including (1) characteristics of queries to be grouped, and (2) characteristics of data changes, are presented in this paper. Preliminary experiments using an implementation of both strategies show that our models are fairly accurate in predicting the results obtained from the implementation of these techniques in the Niagara system. This work can serve as the basis for building a cost-based incremental group query optimizer to choose a better grouping strategy.
624|Minimizing Space Usage in Evaluation of Expression Trees |. An important issue in the evaluation of expression trees (and more generally, directed acyclic graphs (dags)) is the order in which the nodes of the graph are evaluated. In the register sufficiency problem, the evaluation begins with none of the nodes of the dag being placed in registers. On the other hand, if an expression dag, containing variables, is being evaluated in an environment then the nodes of the graph, with out-degree zero, are variables, whose values are contained in the environment. Hence space occupied before the evaluation begins equals the size of the environment. Thus, the strategy to minimize space usage during the evaluation of the dag has to take into account the initial space occupied by the nodes of out-degree zero. By a simple reduction from the register sufficiency problem it is shown that, for an arbitrary dag, the problem of selecting an optimal evaluation strategy which minimizes space is NP-Complete. While the register sufficiency problem for a tree has ...
625|Query Optimization For Repository-Based Applications |Query optimization strategies for repository systems must take into account the rich and often unpredictable structure of metadata, as well as supporting complex analysis of relationships between those structures. This paper describes rationale and design of a cost-based query optimizer offered in ConceptBase, a metadata manager that supports these capabilities by a deductive object-oriented data model. In contrast to most implemented DBMS, the optimizer is not based on the concept of join selectivity, but on detailed distribution information about object and literal fan-out, with heavy exploitation of materialized views and end-biased histograms. Experiences from two real-world applications in software configuration management and cooperative design demonstrate the practical advantages but also point to some lessons for further improvement.
626|Telos: Representing Knowledge About Information Systems|This paper describes a language that is intended to support software engineers in the development of information systems throughout the software lifecycle. This language is not a programming language. Following the example of a number of other software engineering projects, our work is based on the premise that information system development is knowledge-intensive and that the primary responsibility of any language intended to support this task is to be able to formally represent the relevant knowledge.
627|On the Estimation of Join Result Sizes| Good estimates of join result sizes are critical for query op-timization in relational database management systems. We address the problem of incrementally obtaining accurate and consistent estimates of join result sizes. We have invented a new rule for choosing join selectiv-ities for estimating join result sizes. The rule is part of a new unified algorithm called Algorithm ELS (Equivalence and Largest Selectivity). Prior to computing any result sizes, equivalence classes are determined for the join columns. The algorithm also takes into account the effect of local predicates on table and column cardinalities. These computa-tions allow the correct selectivity values for each eligible join predicate to be computed. We show that the algorithm is correct and gives better estimates than current estimation algorithms. 
628|Subsumption for Semantic Query Optimization in OODB|The purpose of semantic query optimization is to use semantic knowledge (e.g. integrity constraints) for transforming a query into an equivalent  one that may be answered more efficiently than the original version. This paper proposes a general method for semantic query optimization in the framework of OODBs (Object Oriented Database Systems). The method is applicable to the class of conjunctive queries and is based on two ingredients: a description logic able to express both class descriptions and integrity constraints rules (IC rules) as types; subsumption  computation between types to evaluate the logical implications expressed by IC rules. 1 Introduction  In database environment, semantic knowledge is usually expressed in terms of IC rules, that is if then rules on the attributes of a database schema (i.e., roughly a Tbox of a Terminological Knowledge Representation System (TKRS)). Informally, semantic equivalence means that the transformed query has the same answer as the original...
629|Decidable reasoning in terminological knowledge representation systems|Terminological Knowledge Representation Systems (TKRSs) are tools for designing and using knowledge bases that make use of terminological languages (or concept languages). The TKRS we consider in this paper is of practical interest since it goes beyond the capabilities of presently available TKRSs. First, our TKRS is equipped with a highly expressive concept, language, called ALCNR, including general complements of concepts, number restrictions and role conjunction. Second, it allows one to express inclusion statements between general concepts, in particular to express terminological cycles. We provide a sound, complete and terminating calculus for reasoning in ALCNR-knowledge bases based on the general technique of constraint systems.
630|Rule-Based Multi-Query Optimization|Data stream management systems usually have to process many long-running queries that are active at the same time. Multiple queries can be evaluated more efficiently together than independently, because it is often possible to share state and computation. Motivated by this observation, various Multi-Query Optimization (MQO) techniques have been proposed. However, these approaches suffer from two limitations. First, they focus on very specialized workloads. Second, integrating MQO techniques for CQL-style stream engines and those for event pattern detection engines is even harder, as the processing models of these two types of stream engines are radically different. In this paper, we propose a rule-based MQO framework. This framework incorporates a set of new abstractions, extending their counterparts, physical operators, transformation rules, and streams, in a traditional RDBMS or stream processing system. Within this framework, we can integrate new and existing MQO techniques through the use of transformation rules. This allows us to build an expressive and scalable stream system. Just as relational optimizers are crucial for the success of RDBMSes, a powerful multi-query optimizer is needed for data stream processing. This work lays the foundation for such a multi-query optimizer, creating opportunities for future research. We experimentally demonstrate the efficacy of our approach. 
631|The CQL Continuous Query Language: Semantic Foundations and Query Execution|CQL, a Continuous Query Language, is supported by the STREAM prototype Data Stream  Management System at Stanford. CQL is an expressive SQL-based declarative language for  registering continuous queries against streams and updatable relations. We begin by presenting  an abstract semantics that relies only on &#034;black box&#034; mappings among streams and relations.
632|Composite event specification in active databases: Model and implementation|Active database systems require facilities to specify triggers that fire when specified events occur. We propose a language for specifying composite events as eveti expressions, formed using event operators and events (primitive or composite). An event expression maps an event history to anothe-r event history that contains only the events at which the event expression is “satisfied ” and at which the trigger should 6re. We present several examples illustrating how quite complex event specifications are possible using event expressions. In addition to the basic event operators, we also provide facilities that make it easier to specify composite events. “Pipes ” allow users to isolate sub-histories of interest. “Correlation variables ” allow users to ensure that different parts of an event expression are satisfied by the same event,
633|High-performance complex event processing over streams|In this paper, we present the design, implementation, and evaluation of a system that executes complex event queries over real-time streams of RFID readings encoded as events. These complex event queries filter and correlate events to match specific patterns, and transform the relevant events into new composite events for the use of external monitoring applications. Stream-based execution of these queries enables time-critical actions to be taken in environments such as supply chain management, surveillance and facility management, healthcare, etc. We first propose a complex event language that significantly extends existing event languages to meet the needs of a range of RFID-enabled monitoring applications. We then describe a query plan-based approach to efficiently implementing this language. Our approach uses native operators to efficiently handle query-defined sequences, which are a key component of complex event processing, and pipelines such sequences to subsequent operators that are built by leveraging relational techniques. We also develop a large suite of optimization techniques to address challenges such as large sliding windows and intermediate result sizes. We demonstrate the effectiveness of our approach through a detailed performance analysis of our prototype implementation as well as through a comparison to a state-of-the-art stream processor. 1
634|Towards Expressive Publish/Subscribe Systems|Abstract. Traditional content based publish/subscribe (pub/sub) systems allow users to express stateless subscriptions evaluated on individual events. However, many applications such as monitoring RSS streams, stock tickers, or management of RFID data streams require the ability to handle stateful subscriptions. In this paper, we introduce Cayuga, a stateful pub/sub system based on nondeterministic finite state automata (NFA). Cayuga allows users to express subscriptions that span multiple events, and it supports powerful language features such as parameterization and aggregation, which significantly extend the expressive power of standard pub/sub systems. Based on a set of formally defined language operators, the subscription language of Cayuga provides non-ambiguous subscription semantics as well as unique opportunities for optimizations. We experimentally demonstrate that common optimization techniques used in NFA-based systems such as state merging have only limited effectiveness, and we propose novel efficient indexing methods to speed up subscription processing. In a thorough experimental evaluation we show the efficacy of our approach. 1
635|Efficient pattern matching over event streams|Pattern matching over event streams is increasingly being employed in many areas including financial services, RFID-based inventory management, click stream analysis, and elec-tronic health systems. While regular expression matching is well studied, pattern matching over streams presents two new challenges: Languages for pattern matching over streams are significantly richer than languages for regular expression matching. Furthermore, efficient evaluation of these pattern queries over streams requires new algorithms and optimiza-tions: the conventional wisdom for stream query processing (i.e., using selection-join-aggregation) is inadequate. In this paper, we present a formal evaluation model that offers precise semantics for this new class of queries and a query evaluation framework permitting optimizations in a principled way. We further analyze the runtime complex-ity of query evaluation using this model and develop a suite of techniques that improve runtime efficiency by exploiting sharing in storage and processing. Our experimental results provide insights into the various factors on runtime perfor-mance and demonstrate the significant performance gains of our sharing techniques.
636|Scheduling for Shared Window Joins Over Data Streams|Continuous Ouery (CO) systems typically exploit  commonality among query expressions to achieve  improved efficiency through shared processing. Recently  proposed CO systems have introduced window  specifications in order to support unbounded  data streams. There has been, however, little investigation  of sharing for windowed query operators.
637|Cayuga: A general purpose event monitoring system|System for scalable event processing. We present a query language based on Cayuga Algebra for naturally expressing complex event patterns. We also describe several novel system design and implementation issues, focusing on Cayuga’s query processor, its indexing approach, how Cayuga handles simultaneous events, and its specialized garbage collector. 1.
638|On-the-fly sharing for streamed aggregation|Data streaming systems are becoming essential for monitoring applications such as financial analysis and network intrusion detection. These systems often have to process many similar but different queries over common data. Since executing each query separately can lead to significant scalability and performance problems, it is vital to share resources by exploiting similarities in the queries. In this paper we present ways to efficiently share streaming aggregate queries with differing periodic windows and arbitrary selection predicates. A major contribution is our sharing technique that does not require any up-front multiple query optimization. This is a significant departure from existing techniques that rely on complex static analyses of fixed query workloads. Our approach is particularly vital in streaming systems where queries can join and leave the system at any point. We present a detailed performance study that evaluates our strategies with an implementation and real data. In these experiments, our approach gives us as much as an order of magnitude performance improvement over the state of the art. 1.
639|Multiple aggregations over data streams|Monitoring aggregates on IP traffic data streams is a compelling application for data stream management systems. The need for exploratory IP traffic data analysis naturally leads to posing related aggregation queries on data streams, that differ only in the choice of grouping attributes. In this paper, we address this problem of efficiently computing multiple aggregations over high speed data streams, based on a two-level LFTA/HFTA DSMS architecture, inspired by Gigascope. Our first contribution is the insight that in such a scenario, additionally computing and maintaining fine-granularity aggregation queries (phantoms) at the LFTA has the benefit of supporting shared computation. Our second contribution is an investigation into the problem of identifying beneficial LFTA configurations of phantoms and user-queries. We formulate this problem as a cost optimization problem, which consists of two sub-optimization problems: how to choose phantoms and how to allocate space for them in the LFTA. We formally show the hardness of determining the optimal configuration, and propose cost greedy heuristics for these independent sub-problems based on detailed analyses. Our final contribution is a thorough experimental study, based on real IP traffic data, as well as synthetic data, to demonstrate the effectiveness of our techniques for identifying beneficial configurations. 1.
640|The Case for Precision Sharing|Sharing has emerged as a key idea of static and  adaptive stream query processing systems. Inherent  in these systems is a tension between sharing  common work and avoiding unnecessary work.
641|Estreams: Towards an Integrated Model for Event and Stream Processing|In this paper, we analyze the similarities and differences between the event and stream processing models. Although research seems to address different types of applications, there are a number of similarities and differences between the two models. We argue that for most of the applications considered for stream processing, event component is needed and is not currently supported. By synthesizing these two and combining their strengths, we argue that the integrated model will be better than the sum of its parts. We then propose our integrated model and its computation to combine the capabilities of both models for applications that require not only to monitor changes through continuous queries, but also to express and process complex events generated by continuous queries. We introduce the notion of a semantic window, which significantly extends the current window concept for continuous queries, and stream modifiers in order to extend current stream computation model for complicated change detection. We further discuss the extension of event specification to include continuous queries. Finally, we discuss the implementation of our integrated model using the MavStream system and the Local Event Detector of Sentinel.
642|Testing the Quality of a Query Optimizer|Today, database technology is used in many different application areas. Therefore, the need to understand how well a particular database management system (DBMS) suits the requirements of a given application has become an important task. One way to address this need is to provide means to measure and to verify the quality of a database management system and its query optimizer. Additionally, since database implementors continue to improve the query optimizer of their specific systems, it becomes especially important for them and the users of those systems to evaluate those changes on a large scale. In particular, changes of the optimizer must be understood by both groups of people. Individual test environments or standardized benchmarks are commonly used to evaluate the quality of an optimizer. Almost all of them are only suited for a particular, artificial database schema and lack the flexibility of determining the size and the shape of queries to be tested and the database to be used...
643|Query optimization for GIS using filters|When viewing present-day technical applications that rely on the use of database systems, one notices that new techniques must be integrated in database management systems to be able to support these applications efficiently. This paper views one of these techniques in the context of supporting a Geographic Information System. For efficient retrieval of geometric data, we show that queries can be optimized by filtering data not with just one but with several simple filters. A prototype of a query optimizer/evaluator using this new technique is described together with preliminary test results. 1 Introduction In the past, much research has been done on query optimization techniques for (relational) databases [1]. Optimization efforts mainly concentrated on queries stemming from administrative applications. Recently, databases are increasingly used not only for administrative, but also for technical applications. Technical applications, e.g. CAD/CAM systems, GIS, and multimedia systems, p...
644|Efficient Processing of Spatial Joins Using R-Trees|Abstract: In this paper, we show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execu-tion. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that pre-serves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance compar-ison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. Topics: spatial database systems, parallel database systems 1
645|Redundancy in spatial databases|Spatial objects other than points and boxes can be stored in spatial indexes, but the techniques usually require the use of approximations that can be arbitrarily bad. This leads to poor performance and highly inaccurate responses to spatial queries. The situation can be improved by storing some objects in the index redundantly. Most spatial indexes permit no flexibility in adjusting the amount of redundancy. Spatial indexes based on z-order permit this flexibility. Accuracy of the query response increases with redundancy, (there is a “diminishing return ” effect). Search time, as measured by disk accesses first decreases and then increases with redundancy. There is, therefore, an optimal amount of redundancy (for a given data set). The optimal use of redundancy for z-order is explored through analysis of the z-order starch algorithm and through experiments. 1
646|Comparison of approximations of complex objects used for approximation-based query processing in spatial database systems|The management of geometric objects is a prime example of an application where efficiency is the bottleneck; this bottleneck cannot be eliminated without using suitable access structures. The most popular approach for handling complex spatial objects in spatial access methods is to use their minimum bounding boxes as a geometric key. Obviously, the rough approximation by bounding boxes provides a fast but inaccurate filter for the set of answers to a query. In order to speed up the query processing by a better approximation quality, we investigate six different types of approximations. Depending on the complexity of the objects and the type of queries, the approximations 5-corner, ellipse and rotated bounding box clearly outperform the bounding box. An important ingredient of our approach is to organize these approximations in efficient spatial access
647|Query Optimization in the Presence of Foreign Functions|The declarativeness of relational query languages is very attractive for developing applications. However, many applications also need to invoke external functions or to access data that is not stored in the database. It is not hard to express references to such foreign functions in the query language. However, the issue of cost-based optimization of relational queries in the presence of such foreign functions has not previously been addressed satisfactorily. In this paper, we describe a comprehensive approach to this problem. Our key observation is that the optimization must take into account semantic information about foreign functions. Therefore, we provide a simple declarative rule language to express such semantics. We present algorithms necessary for applying the rules and for generating the space of equivalent queries. The equivalent queries provide the optimizer with an enriched execution space. We show how we can modify the traditional join reordering algorithm based on dynami...
648|The Entity-Relationship Model: Toward a Unified View of Data|A data model, called the entity-relationship model, is proposed. This model incorporates some of the important semantic information about the real world. A special diagrammatic technique is introduced as a tool for database design. An example of database design and description using the model and the diagrammatic technique is given. Some implications for data integrity, infor-mation retrieval, and data manipulation are discussed. The entity-relationship model can be used as a basis for unification of different views of data: t,he network model, the relational model, and the entity set model. Semantic ambiguities in these models are analyzed. Possible ways to derive their views of data from the entity-relationship model are presented. Key Words and Phrases: database design, logical view of data, semantics of data, data models, entity-relationship model, relational model, Data Base Task Group, network model, entity set
649|Description Logic Programs: Combining Logic Programs with Description Logic|We show how to interoperate, semantically and inferentially, between the leading Semantic Web approaches to rules (RuleML Logic Programs) and ontologies (OWL/DAML+OIL Description Logic) via analyzing their expressive intersection. To do so, we define a new intermediate knowledge representation (KR) contained within this intersection: Description Logic Programs (DLP), and the closely related Description Horn Logic (DHL) which is an expressive fragment of first-order logic (FOL). DLP provides a significant degree of expressiveness, substantially greater than the RDFSchema fragment of Description Logic.
650|Pushing the EL envelope|Recently, it has been shown that the small description logic (DL) EL, which allows for conjunction and existential restrictions, has better algorithmic properties than its counterpart FL0, which allows for conjunction and value restrictions. Whereas the subsumption problem in FL0 becomes already intractable in the presence of acyclic TBoxes, it remains tractable in EL even with general concept inclusion axioms (GCIs). On the one hand, we extend the positive result for EL by identifying a set of expressive means that can be added to EL without sacrificing tractability. On the other hand, we show that basically all other additions of typical DL constructors to EL with GCIs make subsumption intractable, and in most cases even EXPTIMEcomplete. In addition, we show that subsumption in FL0 with GCIs is EXPTIME-complete. 
651|Query Answering in Inconsistent Databases|In this chapter, we summarize the research on querying inconsistent databases we have been conducting over the last five years. The formal framework we have used is based on two concepts: repair and consistent query answer. We describe different approaches to the issue of computing consistent query answers: query transformation, logic programming, inference in annotated logics, and specialized algorithms. We also characterize the computational complexity of this problem. Finally, we discuss related research in artificial intelligence, databases, and logic programming.
652|Data complexity of query answering in description logics|In this paper we study data complexity of answering conjunctive queries over Description Logic knowledge bases constituted by an ABox and a TBox. In particular, we are interested in characterizing the FOL-reducibility and the polynomial tractability boundaries of conjunctive query answering, depending on the expressive power of the Description Logic used to specify the knowledge base. FOL-reducibility means that query answering can be reduced to evaluating queries over the database corresponding to the ABox. Since firstorder queries can be expressed in SQL, the importance of FOL-reducibility is that, when query answering enjoys this property, we can take advantage of Data Base Management System (DBMS) techniques for both representing data, i.e., ABox assertions, and answering queries via reformulation into SQL. What emerges from our complexity analysis is that the Description Logics of the DL-Lite family are the maximal logics allowing conjunctive query answering through standard database technology. In this sense, they are the first Description Logics specifically tailored for effective query answering over very large ABoxes.
653|DL-Lite: Tractable description logics for ontologies|We propose a new Description Logic, called DL-Lite, specifically tailored to capture basic ontology languages, while keeping low complexity of reasoning. Reasoning here means not only computing subsumption between concepts, and checking satisfiability of the whole knowledge base, but also answering complex queries (in particular, conjunctive queries) over the set of instances maintained in secondary storage. We show that in DL-Lite the usual DL reasoning tasks are polynomial in the size of the TBox, and query answering is polynomial in the size of the ABox (i.e., in data complexity). To the best of our knowledge, this is the first result of polynomial data complexity for query answering over DL knowledge bases. A notable feature of our logic is to allow for a separation between TBox and ABox reasoning during query evaluation: the part of the process requiring TBox reasoning is independent of the ABox, and the part of the process requiring access to the ABox can be carried out by an SQL engine, thus taking advantage of the query optimization strategies provided by current DBMSs.
654|AL-log: Integrating Datalog and Description Logics|We presenan integrated system for knowledge representation, called AL-log, based on description logics and the deductive database language Datalog. AL-log embodies two subsystems, called structural and relational. The former allows for the definition of structural knowledge about classes of interest #concepts# and membership relation between objects and classes. The latter allows for the definition of relational knowledge about objects described in the structural component. The interaction between the two components is obtained by allowing constraints within Datalog clauses, thus requiring the variables in the clauses to range over the set of instances of a specified concept. We propose a method for query answering in AL-log based on constrained resolution, where the usual deduction procedure defined for Datalog is integrated with a method for reasoning on the structural knowledge. 
655|On the decidability and complexity of query answering over inconsistent and incomplete databases|In databases with integrity constraints, data may not satisfy the constraints. In this paper, we address the problem of obtaining consistent answers in such a setting, when key and inclusion dependencies are expressed on the database schema. We establish decidability and complexity results for query answering under different assumptions on data (soundness and/or completeness). In particular, after showing that the problem is in general undecidable, we identify the maximal class of inclusion dependencies under which query answering is decidable in the presence of key dependencies. Although obtained in a single database context, such results are directly applicable to data integration, where multiple information sources may provide data that are inconsistent with respect to the global view of the sources. 1.
656|Data Complexity of Reasoning in Very Expressive Description Logics|Data complexity of reasoning in description logics (DLs) estimates the performance of reasoning algorithms measured in the size of the ABox only. We show that, even for the very expressive DL SHIQ, satisfiability checking is data complete for NP. For applications with large ABoxes, this can be a more accurate estimate than the usually considered combined complexity, which is EXPTIMEcomplete. Furthermore, we identify an expressive fragment, Horn-SHIQ, which is data complete for P, thus being very appealing for practical usage.
657|Integrity Constraints for XML|this paper, we extend XML DTDs with several  classes of integrity constraints and investigate the complexity of reasoning about  these constraints. The constraints range over keys, foreign keys, inverse constraints  as well as ID constraints for capturing the semantics of object identities. They improve  semantic specifications and provide a better reference mechanism for native  XML applications. They are also useful in information exchange and data integration  for preserving the semantics of data originating in relational and object-oriented  databases. We establish complexity and axiomatization results for the (finite) implication  problems associated with these constraints. In addition, we study implication  of more general constraints, such as functional, inclusion and inverse constraints  defined in terms of navigation paths
658|QUONTO: QUerying ONTOlogies|One of the most important lines of research in Description Logics (DLs) is concerned with the trade-off between expressive power and computational complexity of sound and complete reasoning. Research carried out in the past on this
659|Query rewriting and answering under constraints in data integration systems|In this paper we address the problem of query answering and rewriting in global-as-view data integration systems, when key and inclusion dependencies are expressed on the global integration schema. In the case of sound views, we provide sound and complete rewriting techniques for a maximal class of constraints for which decidability holds. Then, we introduce a semantics which is able to cope with violations of constraints, and present a sound and complete rewriting technique for the same decidable class of constraints. Finally, we consider the decision problem of query answering and give decidability and complexity results. 1
660|Logic Programs for Consistently Querying Data Integration Systems|We solve the problem of obtaining answers to queries posed to a mediated integration system under the local-as-view paradigm that are consistent wrt to certain global integrity constraints. For this, the query program is combined with logic programming specifications under the stable model semantics of the class of minimal global instances, and of
661|Answering queries using views over description logics knowledge bases|Answering queries using views amounts to computing the answer to a query having information only on the extension of a set of precomputed queries (views). This problem is relevant in several fields, such as information integration, query optimization, and data warehousing, and has been studied recently in different settings. In this paper we address answering queries using views in a setting where intensional knowledge about the domain is represented using a very expressive Description Logic equipped with n-ary relations, and queries are nonrecursive datalog queries whose predicates are the concepts and relations that appear in the Description Logic knowledge base. We study the problem under different assumptions, namely, closed and open domain, and sound, complete, and exact information on view extensions. We show that under the closed domain assumption, in which the set of all objects in the knowledge base coincides with the set of objects stored in the views, answering queries using views is already intractable. We show also that under the open domain assumption the problem is decidable in double exponential time.
662|Reasoning with inclusion axioms in description logics: Algorithms and complexity|Abstract. The computational complexity of reasoning on pure concept expressions has been characterized completely for all relevant description logics. On the contrary, reasoning in the presence of schema axioms is not so well understood and far from being settled completely. An important class of schemata is that of primitive schemata (in which the schema axioms express only necessary conditions) possibly containing cycles. In this paper we provide, for a relevant class of description logics, a complete characterization of computational complexity of reasoning in these types of schemata, both in the presence and in the absence of cycles. The results are obtained by devising reasoning procedures, establishing direct reductions to show lower bounds, and introducing a general technique by which the constructor for existential quantification can be removed without influencing the result of reasoning. 1
663|On the update of description logic ontologies at the instance level|We study the notion of update of an ontology expressed as a Description Logic knowledge base. Such a knowledge base is constituted by two components, called TBox and ABox. The former expresses general knowledge about the concepts and their relationships, whereas the latter describes the state of affairs regarding the instances of concepts. We investigate the case where the update affects only the instance level of the ontology, i.e., the ABox. Building on classical approaches on knowledge base update, our first contribution is to provide a general semantics for instance level update in Description Logics. We then focus on DL-Lite, a specific Description Logic where the basic reasoning tasks are computationally tractable. We show that DL-Lite is closed with respect to instance level update, in the sense that the result of an update is always expressible as a new DL-Lite ABox. Finally we provide an algorithm that computes the result of an update in DL-Lite, and we show that it runs in polynomial time with respect to the size of both the original knowledge base and the update formula.
664|Linking data to ontologies: The description logic DL-LiteA|Abstract. One of the most interesting usages of shared conceptualizations is ontology-based data access. That is, to the usual data layer of an information system we superimpose a conceptual layer to be exported to the client. Such a layer allows the client to have a conceptual view of the information in the system, which abstracts away from how such information is maintained in the data layer of the system itself. While ontologies are the best candidates for realizing the conceptual layer, relational DBMSs are natural candidates for the management of the data layer. The need of efficiently processing large amounts of data requires ontologies to be expressed in a suitable fragment of OWL: the fragment should allow, on the one hand, for modeling the kind of intensional knowledge needed in real-world applications, and, on the other hand, for delegating to a relational DBMS the part of reasoning (in particular query answering) that deals with the data. In this paper, we propose one such a fragment, in fact the largest In several areas (e.g., Enterprise Application Integration, Data Integration [10], and the
665|On the Decidability and Finite Controllability of Query Processing in Databases with Incomplete Information|In this paper we study queries over relational databases with integrity constraints (ICs). The main problem we analyze is OWA query answering, i.e., query answering over a database with ICs under open-world assumption. The kinds of ICs that we consider are functional dependencies (in particular key dependencies) and inclusion dependencies; the query languages we consider are conjunctive queries (CQs), union of conjunctive queries (UCQs), CQs and UCQs with negation and/or inequality. We present a set of results about the decidability and finite controllability of OWA query answering under ICs. In particular: (i) we identify the decidability/undecidability frontier for OWA query answering under different combinations of the ICs allowed and the query language allowed; (ii) we study OWA query answering both over finite databases and over unrestricted databases, and identify the cases in which such a problem is finitely controllable, i.e., when OWA query answering over finite databases coincides with OWA query answering over unrestricted databases. Moreover, we are able to easily turn the above results into new results about implication of ICs and query containment under ICs, due to the deep relationship between OWA query answering and these two classical problems in database theory. In particular, we close two long-standing open problems in query containment, since we prove finite controllability of containment of conjunctive queries both under arbitrary inclusion dependencies and under key and foreign key dependencies. Besides their theoretical interest, we believe that the results of our investigation are very relevant in many research areas which have recently dealt with databases under an incomplete information assumption: e.g., view-based information access, ontologybased information systems, data integration, data exchange, and peer-to-peer information systems. 1.
666|Query Optimization for Distributed Database Systems |I would like to thank my supervisor Dr Dan Olteanu for his incredible level of enthusiasm and encouragement throughout the project. I am also very grateful for the continuous level of feedback and organisation as well as the amount of time he has devoted to answering my queries. I feel that I now approach complex and unknown problems with enthusiasm instead of apprehension as I used to. I couldn’t The query optimizer is widely considered to be the most important component of a database management system. It is responsible for taking a user query and searching through the entire space of equivalent execution plans for a given user query and returning the execution plan with the lowest cost. This plan can then be passed to the executer, which can carry out the query. Plans can vary significantly in cost therefore it is important for the optimizer to avoid very bad plans. In this thesis we consider queries in positive relational algrebra form involving the conjunction of projections, selections and joins. The query optimization problem faced by everyday query optimizers gets more and
667|The state of the art in distributed query processing|Distributed data processing is fast becoming a reality. Businesses want to have it for many reasons, and they often must have it in order to stay competitive. While much of the infrastructure for distributed data processing is already in place (e.g., modern network technology), there are a number of issues which still make distributed data processing a complex undertaking: (1) distributed systems can become very large involving thousands of heterogeneous sites including PCs and mainframe server machines ? (2) the state of a distributed system changes rapidly because the load of sites varies over time and new sites are added to the system? (3) legacy systems need to be integrated|such legacy systems usually have not been designed for distributed data processing and now need to interact with other (modern) systems in a distributed environment. This paper presents the state of the art of query processing for distributed database and information systems. The paper presents the \textbook &amp;quot; architecture for distributed query processing and a series of techniques that are particularly useful for distributed database systems. These techniques include special join techniques, techniques to exploit intra-query parallelism, techniques to reduce communication costs, and techniques to exploit caching and replication of data. Furthermore, the paper discusses di erent kinds of distributed systems such as client-server, middleware (multi-tier), and heterogeneous database systems and shows how query processing works in these systems. Categories and subject descriptors: E.5 [Data]:Files ? H.2.4 [Database Management Systems]: distributed databases, query processing ? H.2.5 [Heterogeneous Databases]: data translation General terms: algorithms ? performance Additional key words and phrases: query optimization ? query execution ? client-server databases ? middleware ? multi-tier architectures ? database application systems ? wrappers? replication ? caching ? economic models for query processing ? dissemination-based information systems 1
668|Constructing Optimal Bushy Processing Trees for Join Queries is NP-hard (Extended Abstract)  (1996) |We show that constructing optimal bushy processing trees for join queries is NP-hard. More specifically, we show that even the construction of optimal bushy trees for computing the cross product of a set of relations is NP-hard.
669|Multiple Query Optimization in Mediator Systems |With the increase in mediated applications, there are now mediated application servers that support concurrent client accesses. Multiple query optimization (MQO) is essential for efficient processing of concurrent read only queries. The MQO problem in mediated environments differs from that of ordinary relational MQO systems in two ways: first, in mediated environments, the MQO system must be capable of interacting with the &#034;local&#034; query optimizers of the various sources being accessed. Second, in mediated environments, the notionof what constitutes a &#034;common&#034; subexpression varies dramatically from one source to another, and the MQO system must be capable of working with these diverse notions of &#034;common&#034; subexpression. In this paper, we extend classical relational DBMS MQO techniques to the case of mediated applications. Our architecture has a compile-time and a runtime component. In the compile-time component, the developer describes common subexpressions via a set of structures calle...
670|Mediators in the architecture of future information systems|The installation of high-speed networks using optical fiber and high bandwidth messsage forwarding gateways is changing the physical capabilities of information systems. These capabilities must be complemented with corresponding software systems advances to obtain a real benefit. Without smart software we will gain access to more data, but not improve access to the type and quality of information needed for decision making. To develop the concepts needed for future information systems we model information processing as an interaction of data and knowledge. This model provides criteria for a high-level functional partitioning. These partitions are mapped into information process-ing modules. The modules are assigned to nodes of the distributed information systems. A central role is assigned to modules that mediate between the users &#039; workstations and data re-sources. Mediators contain the administrative and technical knowledge to create information needed for decision-making. Software which mediates is common today, but the structure, the interfaces, and implementations vary greatly, so that automation of integration is awkward. By formalizing and implementing mediation we establish a partitioned information sys-
671|Scaling Heterogeneous Databases and the Design of Disco|Access to large numbers of data sources introduces new problems for users of heterogeneous distributed databases. End users and application programmers must deal with unavailable data sources. Database administrators must deal with incorporating each new data source into the system. Database implementors must deal with the transformation of queries between query languages and schemas. The Distributed Information Search COmponent (DISCO) addresses these problems. Query processing semantics give meaning to queries that reference unavailable data sources. Data modeling techniques manage connections to data sources. The component interface to data sources flexibly handles different query languages and different interface functionalities. This paper describes in detail (a) the distributed mediator architecture of DISCO, (b) its query processing semantics, (c) the data model and its modeling of data source connections, and (d) the interface to underlying data sources. We describe several advantages of our system and describe the internal architecture of our planned prototype.
672|Interactive Plan Hints for Query Optimization |Commercial database systems expose query hints to fix poor plans produced by the query optimizer. However, current query hints are not flexible enough to deal with a variety of non-trivial scenarios, and can be at times cumbersome for DBAs to interact with. In this demonstration we present a framework that enables visual specification of hints to influence the optimizer to pick better plans. Our framework goes considerably beyond existing hinting mechanisms and significantly improves the usability of such functionality. Categories and Subject Descriptors
673|Query Optimization Techniques for Partitioned Tables |Table partitioning splits a table into smaller parts that can be accessed, stored, and maintained independent of one another. From their traditional use in improving query performance, partitioning strategies have evolved into a powerful mechanism to improve the overall manageability of database systems. Table partitioning simplifies administrative tasks like data loading, removal, backup, statistics maintenance, and storage provisioning. Query language extensions now enable applications and user queries to specify how their results should be partitioned for further use. However, query optimization techniques have not kept pace with the rapid advances in usage and user control of table partitioning. We address this gap by developing new techniques to generate efficient plans for SQL queries involving multiway joins over partitioned tables. Our techniques are designed for easy incorporation into bottom-up query optimizers that are in wide use today. We have prototyped these techniques in the PostgreSQL optimizer. An extensive evaluation shows that our partition-aware optimization techniques, with low optimization overhead, generate plans that can be an order of magnitude better than plans produced by current optimizers.
674|HadoopDB: An Architectural Hybrid of MapReduce and DBMS Technologies for Analytical Workloads |The production environment for analytical data management applications is rapidly changing. Many enterprises are shifting away from deploying their analytical databases on high-end proprietary machines, and moving towards cheaper, lower-end, commodity hardware, typically arranged in a shared-nothing MPP architecture, often in a virtualized environment inside public or private “clouds”. At the same time, the amount of data that needs to be analyzed is exploding, requiring hundreds to thousands of machines to work in parallel to perform the analysis. There tend to be two schools of thought regarding what technology to use for data analysis in such an environment. Proponents of parallel databases argue that the strong emphasis on performance and efficiency of parallel databases makes them wellsuited to perform such analysis. On the other hand, others argue that MapReduce-based systems are better suited due to their superior scalability, fault tolerance, and flexibility to handle unstructured data. In this paper, we explore the feasibility of building a hybrid system that takes the best features from both technologies; the prototype we built approaches parallel databases in performance and efficiency, yet still yields the scalability, fault tolerance, and flexibility of MapReduce-based systems. 1.
675|Integrating vertical and horizontal partitioning into automated physical database design|In addition to indexes and materialized views, horizontal and vertical partitioning are important aspects of physical design in a relational database system that significantly impact performance. Horizontal partitioning also provides manageability; database administrators often require indexes and their underlying tables partitioned identically so as to make common operations such as backup/restore easier. While partitioning is important, incorporating partitioning makes the problem of automating physical design much harder since: (a) The choices of partitioning can strongly interact with choices of indexes and materialized views. (b) A large new space of physical design alternatives must be considered. (c) Manageability requirements impose a new constraint on the problem. In this paper, we present novel techniques for designing a scalable solution to this integrated physical design problem that takes both performance and manageability into account. We have implemented our techniques and evaluated it on Microsoft SQL Server. Our experiments highlight: (a) the importance of taking an integrated approach to automated physical design and (b) the scalability of our techniques. 1.
676|Automating physical database design in a parallel database|LIMITED DISTRIBUTION NOTICE: This report has been submitted for publication outside of IBM and will probably be copyrighted if accepted for publication. Ithas
678|DB2 Parallel Edition|The rate of increase in database size and response time requirements has outpaced  advancements in processor and mass storage technology. One way to satisfy the increasing  demand for processing power and I/O bandwidth in database applications is to have a  number of processors, loosely or tightly coupled, serving database requests concurrently.  Technologies developed during the last decade have made commercial parallel database  systems a reality and these systems have made an inroad into the stronghold of traditionally  mainframe based large database applications. This paper describes the parallel database  project initiated at IBM Research at Hawthorne and the DB2/AIX-PE product based on  it.  1 Introduction  Large scale parallel processing technology has made giant strides in the past decade and there is no doubt that it has established a place for itself. However, almost all of the applications harnessing this technology are scientific or engineering applications. The lack of com...
679|Content-based routing: Different plans for different data|Query optimizers in current database systems are designed to pick a single efficient plan for a given query based on current statistical properties of the data. However, different subsets of the data can sometimes have very different statistical properties. In such scenarios it can be more efficient to process different subsets of the data for a query using different plans. We propose a new query processing technique called content-based routing (CBR) that eliminates the single-plan restriction in current systems. We present low-overhead adaptive algorithms that partition input data based on statistical properties relevant to query execution strategies, and efficiently route individual tuples through customized plans based on their partition. We have implemented CBR as an extension to the Eddies query processor in the TelegraphCQ system, and we present an extensive experimental evaluation showing the significant performance benefits of CBR. 1.
680|Incorporating Partitioning and Parallel Plans into the SCOPE Optimizer |Abstract — Massive data analysis on large clusters presents new opportunities and challenges for query optimization. Data partitioning is crucial to performance in this environment. However, data repartitioning is a very expensive operation so minimizing the number of such operations can yield very significant performance improvements. A query optimizer for this environment must therefore be able to reason about data partitioning including its interaction with sorting and grouping. SCOPE is a SQL-like scripting language used at Microsoft for massive data analysis. A transformation-based optimizer is responsible for converting scripts into efficient execution plans for the Cosmos distributed computing platform. In this paper, we describe how reasoning about data partitioning is incorporated into the SCOPE optimizer. We show how relational operators affect partitioning, sorting and grouping properties and describe
681|Partitioning Key Selection for a Shared-Nothing Parallel Database System|A shared nothing database system which tries to leverage the knowledge of partitioning attributes of relations can outperform a system where such knowledge is either not available or not used. The performance improvements are typically obtained by function shipping more database operations (joins, aggregates etc.), thus minimizing the communication overhead. In such a system, it is critical that the correct partitioning keys are selected so that the query workload is optimized. Previous research has ignored the importance of selecting the partitioning keys and have mostly focused on the degree of declustering. In this study we show that by following a systematic methodology, especially for the partitioning key selection and associated relation grouping issues, the entire data placement strategy for a given database schema and workload can be determined in a very efficient manner. We describe different flavors of this methodology and demonstrate the performance improvements resulting fr...
682|The evolution of a hierarchical partitioning algorithm for large-scale scientific data: three steps of increasing complexity|As scientific data sets grow exponentially in size, the need for scalable algorithms that heuristically partition the data increases. In this paper, we describe the threestep evolution of a hierarchical partitioning algorithm for large-scale spatio-temporal scientific data sets generated by massive simulations. The first version of our algorithm uses a simple top-down partitioning technique, which divides the data by using a four-way bisection of the spatio-temporal space. The shortcomings of this algorithm lead to the second version of our partitioning algorithm, which uses a bottom-up approach. In this version, a partition hierarchy is constructed by systematically agglomerating the underlying Cartesian grid that is placed on the data. Finally, the third version of our algorithm utilizes the intrinsic topology of the data given in the original scientific problem to build the partition hierarchy in a bottom-up fashion. Specifically, the topology is used to heuristically agglomerate the data at each level of the partition hierarchy. Despite the growing complexity in our algorithms, the third version of our algorithm builds partition hierarchies in less time and is able to build trees for larger size data sets as compared to the previous two versions. 1. Three hierarchical partitioning algorithms Scalable algorithms are needed to partition tera-scale data sets [1, 5]. This is especially true in scientific domains, where sizes of the data sets have grown exponentially in recent years. We describe the evolution of a hierarchical partitioning algorithm for large-scale scientific data sets. Specifically, large-scale simulation programs produce our data sets in mesh format. A data set in mesh format consists of interconnected grids of small zones, in which data points are stored. Figure 1 depicts the mesh produced from an astrophysics simulation of a star in its mid-life. Mesh data usually varies with time, consists of multiple dimensions (i.e., variables), and can contain irregular grids. Musick and Critchlow provide a nice introduction to scientific mesh data [4].
683|Selectivity-Based Partitioning: A Divide-and-Union Paradigm For Effective Query Optimization|Modern query optimizers select an efficient join ordering for a physical execution plan based essentially on the average join selectivity factors among the referenced tables. In this paper, we argue that this “monolithic ” approach can miss important opportunities for the effective optimization of relational queries. We propose selectivitybased partitioning, a novel optimization paradigm that takes into account the join correlations among relation fragments in order to essentially enable multiple (and more effective) join orders for the evaluation of a single query. In a nutshell, the basic idea is to carefully partition a relation according to the selectivities of the join operations, and subsequently rewrite the query as a union of constituent queries over the computed partitions. We provide a formal definition of the related optimization problem and derive properties that characterize the set of optimal solutions. Based on our analysis, we develop a heuristic algorithm for computing efficiently an effective partitioning of the input query. Results from a preliminary experimental study verify the effectiveness of the proposed approach and demonstrate its potential as an effective optimization technique.
684|Testing SQL Server’s Query Optimizer: Challenges, Techniques and Experiences |Query optimization is an inherently complex problem, and validating the correctness and effectiveness of a query optimizer can be a task of comparable complexity. The overall process of measuring query optimization quality becomes increasingly challenging as modern query optimizers provide more advanced optimization strategies and adaptive techniques. In this paper we present a practitioner’s account of query optimization testing. We discuss some of the unique issues in testing a query optimizer, and we provide a high-level overview of the testing techniques used to validate the query optimizer of Microsoft’s SQL Server. We offer our experiences and discuss a few ongoing challenges, which we hope can inspire additional research in the area of query optimization and DBMS testing. 1
685|Query Optimization of Distributed Pattern Matching |Abstract—Greedy algorithms for subgraph pattern matching operations are often sufficient when the graph data set can be held in memory on a single machine. However, as graph data sets increasingly expand and require external storage and partitioning across a cluster of machines, more sophisticated query optimization techniques become critical to avoid explosions in query latency. In this paper, we introduce several query optimization techniques for distributed graph pattern match-ing. These techniques include (1) a System-R style dynamic programming-based optimization algorithm that considers both linear and bushy plans, (2) a cycle detection-based algorithm that leverages cycles to reduce intermediate result set sizes, and (3) a computation reusing technique that eliminates redundant query execution and data transfer over the network. Experimental results show that these algorithms can lead to an order of magnitude improvement in query performance. I.
686|The Anatomy of a Large-Scale Hypertextual Web Search Engine|In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/

To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.

Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
687|Pregel: A system for large-scale graph processing|Many practical computing problems concern large graphs. Standard examples include the Web graph and various social networks. The scale of these graphs—in some cases billions of vertices, trillions of edges—poses challenges to their efficient processing. In this paper we present a computational model suitable for this task. Programs are expressed as a sequence of iterations, in each of which a vertex can receive messages sent in the previous iteration, send messages to other vertices, and modify its own state and that of its outgoing edges or mutate graph topology. This vertex-centric approach is flexible enough to express a broad set of algorithms. The model has been designed for efficient, scalable and fault-tolerant implementation on clusters of thousands of commodity computers, and its implied synchronicity makes reasoning about programs easier. Distributionrelated details are hidden behind an abstract API. The result is a framework for processing large graphs that is expressive and easy to program.
688|A Performance Evaluation of Four Parallel Join Algorithms in a Shared-Nothing Multiprocessor Environment|The join operator has been a cornerstone of relational database systems since their inception. As such, much time and effort has gone into making joins efficient. With the obvious trend towards multiprocessors, attention has focused on efficiently parallelizing the join operation. In this paper we analyze and compare four parallel join algorithms. Grace and Hybrid hash represent the class of hash-based join methods, Simple hash represents a looping algorithm with hashing, and our last algorithm is the more traditional sort-merge. The Gamma database machine serves as the host for the performance comparison. Gamma’s shared-nothing architecture with commercially available components is becoming increasingly common, both in research and in industry. 1.
689|Algorithmics and Applications of Tree and Graph Searching|Modern search engines answer keyword-based queries extremely efficiently. The impressive speed is due to clever inverted index structures, caching, a domain-independent knowledge of strings, and thousands of machines. Several research efforts have attempted to generalize keyword search to keytree and keygraph searching, because trees and graphs have many applications in next-generation database systems. This paper surveys both algorithms and applications, giving some emphasis to our own work.
690|Counting triangles and the curse of the last reducer|The clustering coefficient of a node in a social network is a fundamental measure that quantifies how tightly-knit the community is around the node. Its computation can be reduced to counting the number of triangles incident on the particular node in the network. In case the graph is too big to fit into memory, this is a non-trivial task, and previous researchers showed how to estimate the clustering coefficient in this scenario. A different avenue of research is to to perform the computation in parallel, spreading it across many machines. In recent years MapReduce has emerged as a de facto programming paradigm for parallel computation on massive data sets. The main focus of this work is to give MapReduce algorithms for counting triangles which we use to compute clustering coefficients. Our contributions are twofold. First, we describe a sequential triangle counting algorithm and show how to adapt it to the MapReduce setting. This algorithm achieves a factor of 10-100 speed up over the naive approach. Second, we present a new algorithm designed specifically for the MapReduce framework. A key feature of this approach is that it allows for a smooth tradeoff between the memory available on each individual machine and the total memory available to the algorithm, while keeping the total work done constant. Moreover, this algorithm can use any triangle counting algorithm as a black box and distribute the computation across many machines. We validate our algorithms on real world datasets comprising of millions of nodes and over a billion edges. Our results show both algorithms effectively deal with skew in the degree distribution and lead to dramatic speed ups over the naive implementation.
691|Graphs-at-a-time: Query Language and Access Methods for Graph Databases|With the prevalence of graph data in a variety of domains, there is an increasing need for a language to query and manipulate graphs with heterogeneous attributes and structures. We propose a query language for graph databases that supports arbitrary attributes on nodes, edges, and graphs. In this language, graphs are the basic unit of information and each query manipulates one or more collections of graphs. To allow for flexible compositions of graph structures, we extend the notion of formal languages from strings to the graph domain. We present a graph algebra extended from the relational algebra in which the selection operator is generalized to graph pattern matching and a composition operator is introduced for rewriting matched graphs. Then, we investigate access methods of the selection operator. Pattern matching over large graphs is challenging due to the NP-completeness of subgraph isomorphism. We address this by a combination of techniques: use of neighborhood subgraphs and profiles, joint reduction of the search space, and optimization of the search order. Experimental results on real and synthetic large graphs demonstrate that our graph specific optimizations outperform an SQL-based implementation by orders of magnitude.
692|Scalable SPARQL Querying of Large RDF Graphs |The generation of RDF data has accelerated to the point where many data sets need to be partitioned across multiple machines in order to achieve reasonable performance when querying the data. Although tremendous progress has been made in the Semantic Web community for achieving high performance data management on a single node, current solutions that allow the data to be partitioned across multiple machines are highly inefficient. In this paper, we introduce a scalable RDF data management system that is up to three orders of magnitude more efficient than popular multi-node RDF data management systems. In so doing, we introduce techniques for (1) leveraging state-of-the-art single node RDF-store technology (2) partitioning the data across nodes in a manner that helps accelerate query processing through locality optimizations and (3) decomposing SPARQL queries into high performance fragments that take advantage of how data is partitioned in a cluster.
694|A Distributed Graph Engine for Web Scale RDF Data |Much work has been devoted to supporting RDF data. But state-of-the-art systems and methods still cannot handle web scale RDF data effectively. Furthermore, many useful and general purpose graph-based operations (e.g., random walk, reachability, community discovery) on RDF data are not supported, as most existing systems store and index data in particular ways (e.g., as relational tables or as a bitmap matrix) to maximize one particular operation on RDF data: SPARQL query processing. In this paper, we introduce Trinity.RDF, a distributed, memory-based graph engine for web scale RDF data. Instead of managing the RDF data in triple stores or as bitmap matrices, we store RDF data in its native graph form. It achieves much better (sometimes orders of magnitude better) performance for SPARQL queries than the state-of-the-art approaches. Furthermore, since the data is stored in its native graph form, the system can support other operations (e.g., random walks, reachability) on RDF graphs as well. We conduct comprehensive experimental studies on real life, web scale RDF data to demonstrate the effectiveness of our approach. 1
695|Distance-Join: Pattern Match Query In a Large Graph Database * |The growing popularity of graph databases has generated interesting data management problems, such as subgraph search, shortest-path query, reachability verification, and pattern match. Among these, a pattern match query is more flexible compared to a subgraph search and more informative compared to a shortest-path or reachability query. In this paper, we address pattern match problems over a large data graph G. Specifically, given a pattern graph (i.e., query Q), we want to find all matches (in G) that have the similar connections as those in Q. In order to reduce the search space significantly, we first transform the vertices into points in a vector space via graph embedding techniques, coverting a pattern match query into a distance-based multi-way join problem over the converted vector space. We also propose several pruning strategies and a join order selection method to process join processing efficiently. Extensive experiments on both real and synthetic datasets show that our method outperforms existing ones by orders of magnitude. 1.
696|Counting Beyond a Yottabyte, or how SPARQL 1.1 Property Paths will Prevent Adoption of the Standard |SPARQL –the standard query language for querying RDF – provides only limited navigational functionalities, although these features are of fundamental importance for graph data formats such as RDF. This has led the W3C to include the property path feature in the upcoming version of the standard, SPARQL 1.1. We tested several implementations of SPARQL 1.1 handling property path queries, and we observed that their evaluation methods for this class of queries have a poor performance even in some very simple scenarios. To formally explain this fact, we conduct a theoretical study of the computational complexity of property paths evaluation. Our results imply that the poor performance of the tested implementations is not a problem of these particular systems, but of the specification itself. In fact, we show that any implementation that adheres to the SPARQL 1.1 specification (as of November 2011) is doomed to show the same behavior, the key issue being the need for counting solutions imposed by the current specification. We provide several intractability results, that together with our empirical results, provide strong evidence against the current semantics of SPARQL 1.1 property paths. Finally, we put our results in perspective, and propose a natural alternative semantics with tractable evaluation, that we think may lead to a wide adoption of the language by practitioners, developers and theoreticians.
697|GBASE: A Scalable and General Graph Management System |Graphs appear in numerous applications including cyber-security, the Internet, social networks, protein networks, recommendation systems, and many more. Graphs with millions or even billions of nodes and edges are common-place. How to store such large graphs efficiently? What are the core operations/queries on those graph? How to answer the graph queries quickly? We propose GBASE, a scalable and general graph management and mining system. The key novelties lie in 1) our storage and compression scheme for a parallel setting and 2) the carefully chosen graph operations and their efficient implementation. We designed and implemented an instance of GBASE using MAPREDUCE/HADOOP. GBASE provides a parallel indexing mechanism for graph mining operations that both saves storage space, as well as accelerates queries. We ran numerous experiments on real graphs, spanning billions of nodes and edges, and we show that our proposed GBASE is indeed fast, scalable and nimble, with significant savings in space and time.
698|Managing large dynamic graphs efficiently|There is an increasing need to ingest, manage, and query large volumes of graph-structured data arising in applications like social networks, communication networks, biological networks, and so on. Graph databases that can explicitly reason about the graphical nature of the data, that can support flexible schemas and nodecentric or edge-centric analysis and querying, are ideal for storing such data. However, although there is much work on singlesite graph databases and on efficiently executing different types of queries over large graphs, to date there is little work on understanding the challenges in distributed graph databases, needed to handle the large scale of such data. In this paper, we propose the design of an in-memory, distributed graph data management system aimed at managing a large-scale dynamically changing graph, and supporting low-latency query processing over it. The key challenge in
699|Capturing Topology in Graph Pattern Matching |Graph pattern matching is often defined in terms of subgraph isomorphism, an np-complete problem. To lower its complexity, various extensions of graph simulation have been considered instead. These extensions allow pattern matching to be conducted in cubic-time. However, they fall short of capturing the topology of data graphs, i.e., graphs may have a structure drastically different from pattern graphs they match, and the matches found are often too large to understand and analyze. To rectify these problems, this paper proposes a notion of strong simulation, a revision of graph simulation, for graph pattern matching. (1) We identify a set of criteria for preserving the topology of graphs matched. We show that strong simulation preserves the topology of data graphs and finds a bounded number of matches. (2) We show that strong simulation retains the same complexity as earlier extensions of simulation, by providing a cubic-time algorithm for computing strong simulation. (3) We present the locality property of strong simulation, which allows us to effectively conduct pattern matching on distributed graphs. (4) We experimentally verify the effectiveness and efficiency of these algorithms, using real-life data and synthetic data. 1.
700|Distributed Graph Pattern Matching |Graph simulation has been adopted for pattern matching to reduce the complexity and capture the need of novel applications. With the rapid development of the Web and social networks, data is typically distributed over multiple machines. Hence a natural question raised is how to evaluate graph simulation on distributed data. To our knowledge, no such distributed algorithms are in place yet. This paper settles this question by providing evaluation algorithms and optimizations for graph simulation in a distributed setting. (1) We study the impacts of components and data locality on the evaluation of graph simulation. (2) We give an analysis of a large class of distributed algorithms, captured by a message-passing model, for graph simulation. We also identify three complexity measures: visit times, makespan and data shipment, for analyzing the distributed algorithms, and show that these measures are essentially controversial with each other. (3) We propose distributed algorithms and optimization techniques that exploit the properties of graph simulation and the analyses of distributed algorithms. (4) We experimentally verify the effectiveness and efficiency of these algorithms, using both real-life and synthetic data. Categories and Subject Descriptors H.2.8 [Database Management]: Database applications— graph data, data mining
701|Sleepers and Workaholics: Caching Strategies in Mobile Environments (Extended Version)  (1994) |In the mobile wireless computing environment of the future a large number of users equipped with low powered palmtop machines will query databases over the wireless communication channels. Palmtop based units will often be disconnected for prolonged periods of time due to the battery power saving measures; palmtops will also frequently relocate between different cells and connect to different data servers at different times. Caching of frequently accessed data items will be an important technique that will reduce contention on the narrow bandwidth wireless channel. However, cache invalidation strategies will be severely affected by the disconnection and mobility of the clients. The server may no longer know which clients are currently residing under its cell and which of them are currently on. We propose a taxonomy of different cache invalidation strategies and study the impact of client&#039;s disconnection times on their performance. We study ways to further improve the efficiency of the ...
702|Data Model and Query Evaluation in Global Information Systems|. Global information systems involve a large number of information sources distributed over computer networks. The variety of information sources and disparity of interfaces makes the task of easily locating and efficiently accessing information over the network very cumbersome. We describe an architecture for global information systems that is especially tailored to address the challenges raised in such an environment, and distinguish our architecture from architectures of multidatabase and distributed database systems. Our architecture is based on presenting a conceptually unified view of the information space to a user, specifying rich descriptions of the contents of the information sources, and using these descriptions for optimizing queries posed in the unified view. The contributions of this paper include: (1) we identify aspects of site descriptions that are useful in query optimization; (2) we describe query optimization techniques that minimize the number of information source...
703|Equivalences among relational expressions with the union and difference operators|ABSTRACT Queries in relational databases can be formulated in terms of relational expressions using the relational operations elect, project, join, union, and difference The equivalence problem for these queries is studied with query optimization m mind It ts shown that testmg eqmvalence of relational expressions with the operators elect, project, join, and union is complete m the class FIt of the polynomial-time hierarchy A nonprocedural representation for queries formulated by these expressions i proposed This method of query representation can be viewed as a generahzatlon f tableaux or conjunctive queries (which are used to represent expressions with only select, project, and join) Furthermore, this method is extended to queries formulated by relatmnal expressions that also contain the difference operator, provided that the project operator isnot applied to subexpresstons with the difference operator A procedure for testing eqmvalence of these queries is given It ts shown that testmg containment of tableaux is a necessary step in testing equivalence of queries with union and difference Three important cases m which containment of tableaux can be tested m polynomial time are described, although the containment problem is shown to be NP-complete ven for tableaux that correspond to expressions with only one project and several join operators KEY WORDS AND PHRASES relatmnal database, relational algebra, query optimization, equivalence of queries, conjunctive query, tableau, NP-complete, polynomial-time hierarchy, H 2P-complete CR CATEGORIES 4 33, 5 25
704|Data Replication for Mobile Computers|Users of mobile computers will soon have online access to a large number of databases via wireless networks. Because of limited bandwidth, wireless communication is more expensive than wire communication. In this paper we present and analyze various static and dynamic data allocation methods. The objective is to optimize the communication cost between a mobile computer and the stationary computer that stores the online database. Analysis is performed in two cost models. One is connection (or time) based, as in cellular telephones, where the user is charged per minute of connection. The other is message based, as in packet radio networks, where the user is charged per message. Our analysis addresses both, the average case and the worst case for determining the best allocation method.  0 1 Introduction  Users of mobile computers, such as palmtops, notebook computers and personal communication systems, will soon have online access to a large number of databases via wireless networks. The ...
705|Adapting materialized views after redefinitions|We consider a variant of the view maintenance problem: How does one keep a materialized view up-to-date when the view definition itself changes? Can one do better than recomputing the view from the base relations? Traditional view maintenance tries to maintain the materialized view in response to modifications to the base relations; we try to “adapt ” the view in response to changes in the view definition. Such techniques are needed for applications where the user can change queries dynamically and see the changes in the results fast. Data archaeology, data visualization, and dynamic queries are examples of such applications. We consider all possible redefinitions of SQL SELECT-FROM-UHERE-GROUPBY, UNION, and EXCEPT views, and show how these views can be adapted using the old materialization for the cases where it is possible to do so. We identify extra information that can be kept with a materialization to facilitate redefinition. Multiple simultaneous changes to a view can be handled without necessarily materializing intermediate results. We iden-tify guidelines for users and database administrators that can be used to facilitate efficient view adaptation. 1
706|The Complexity of Querying Indefinite Data about Linearly Ordered Domains|In applications dealing with ordered domains, the available data is frequently indefinite. While the domain is actually linearly ordered, only some of the order relations holding between points in the data are known. Thus, the data provides only a partial order, and query answering involves determining what holds under all the compatible linear orders. In this paper we study the complexity of evaluating queries in logical databases containing such indefinite information. We show that in this context queries are intractable even under the data complexity measure, but identify a number of PTIME sub-problems. Data complexity in the case of monadic predicates is one of these PTIME cases, but for disjunctive queries the proof is non-constructive, using well-quasi-order techniques. We also show that the query problem we study is equivalent to the problem of containment of conjunctive relational database queries containing inequalities. One of our results implies that the latter is \Pi  p  2 ...
707|Multivalued Dependencies and a New Normal Form for Relational Databases|A new type of dependency, which includes the well-known functional dependencies as a special case, is defined for relational databases. By using this concept, a new (“fourth”) normal form for relation schemata is defined. This fourth normal form is strictly stronger than Codd’s “im-proved third normal form ” (or “Boyce-Codd normal form”). It is shown that, every relation schema can be decomposed into a family of relation schemata in fourth normal form without loss of information (that is, the original relation can be obtained from the new relations by taking joins). Key words and phrases: database design, multivalued dependency, functional dependency, fourth normal form, 4NF, third normal form, 3NF, Boyce-Codd normal form, normalization, decomposition, relational database
708|Flow algorithms for parallel query optimization|In this paper we address the problem of minimizing the response time of a multi-way join query using pipelined (inter-operator) parallelism, in a parallel or a distributed environment. We observe that in order to fully exploit the parallelism in the system, we must consider a new class of “interleaving” plans, where multiple query plans are used simultaneously to minimize the response time of a query (or maximize the tuple-throughput of the system). We cast the query planning problem in this environment as a “flow maximization problem”, and present polynomial-time algorithms that (statically) find the optimal set of plans to use for a large class of multi-way join queries. Our proposed algorithms also naturally extend to query optimization over web services. Finally we present an extensive experimental evaluation that demonstrates both the need to consider such plans in parallel query processing and the effectiveness of our proposed algorithms. 1
709|Practical Skew Handling in Parallel Joins|We present an approach to dealing with skew in parallel joins in database systems. Our approach is easily implementable within current parallel DBMS, and performs well on skewed data without degrading the performance of the system on non-skewed data. The main idea is to use multiple algorithms, each specialized for a di erent degree of skew, and to use a small sample of the relations being joined to determine which algorithm is appropriate. We developed, implemented, and experimented with four new skew-handling parallel join algorithms; one, which wecall virtual processor range partitioning, was the clear winner in high skew cases, while traditional hybrid hash join was the clear winner in lower skew or no skew cases. We present experimental results from an implementation of all four algorithms on the Gamma parallel database machine. To our knowledge, these are the rst reported skew-handling numbers from an actual implementation.  
710|Maximizing the output rate of multi-way join queries over streaming information sources|Recently there has been a growing interest in join query evaluation for scenarios in which inputs arrive at highly variable and unpredictable rates. In such scenarios, the focus shifts from completing the computation as soon as possible to producing a prefix of the output as soon as possible. To handle this shift in focus, most solutions to date rely upon some combination of streaming binary operators and “on-the-fly” execution plan reorganization. In contrast, we consider the alternative of extending existing symmetric binary join operators to handle more than two inputs. Toward this end, we have completed a prototype implementation of a multi-way join operator, which we term the “MJoin ” operator, and explored its performance. Our results show that in many instances the MJoin produces outputs sooner than any tree of binary operators. Additionally, since MJoins are completely symmetric with respect to their inputs, they can reduce the need for expensive runtime plan reorganization. This suggests that supporting multi-way joins in a single, symmetric, streaming operator may be a useful addition to systems that support queries over input streams from remote sites. 1
711|Adaptive Ordering of Pipelined Stream Filters|We consider the problem of pipelined filters, where a continuous stream of tuples is processed by a set of commutative filters. Pipelined filters are common in stream applications and capture a large class of multiway stream joins. We focus on the problem of ordering the filters adaptively to minimize processing cost in an environment where stream and filter characteristics vary unpredictably over time. Our core algorithm, A-Greedy (for Adaptive Greedy), has strong theoretical guarantees: If stream and filter characteristics were to stabilize, A-Greedy would converge to an ordering within a small constant factor of optimal. (In experiments A-Greedy usually converges to the optimal ordering.) One very important feature of A-Greedy is that it monitors and responds to selectivities that are correlated across filters (i.e., that are nonindependent), which provides the strong quality guarantee but incurs run-time overhead. We identify a three-way tradeoff among provable convergence to good orderings, run-time overhead, and speed of adaptivity. We develop a suite of variants of A-Greedy that lie at different points on this tradeoff spectrum. We have implemented all our algorithms in the STREAM prototype Data Stream Management System and a thorough performance evaluation is presented. 
712|Using State Modules for Adaptive Query Processing|We present a query architecture in which join operators are decomposed into their constituent data structures (State Modules, or SteMs), and dataflow among these SteMs is managed adaptively by an Eddy routing operator. Breaking the encapsulation of joins serves two purposes. First, it allows the Eddy to observe multiple physical operations embedded in a join algorithm, allowing for better calibration and control of these operations. Second, the SteM on a relation serves as a shared materialization point, enabling multiple competing access methods to share results, which can be leveraged by multiple competing join algorithms. Our architecture extends prior work significantly, allowing continuously adaptive decisions for most major aspects of traditional query optimization: choice of access methods and join algorithms, ordering of operators, and choice of a query spanning tree. SteMs introduce...
713|Exploiting correlated attributes in acquisitional query processing|Sensor networks and other distributed information systems (such as the Web) must frequently access data that has a high per-attribute acquisition cost, in terms of energy, latency, or computational resources. When executing queries that contain several predicates over such expensive attributes, we observe that it can be beneficial to use correlations to automatically introduce low-cost attributes whose observation will allow the query processor to better estimate the selectivity of these expensive predicates. In particular, we show how to build conditional plans that branch into one or more sub-plans, each with a different ordering for the expensive query predicates, based on the runtime observation of low-cost attributes. We frame the problem of constructing the optimal conditional plan for a given user query and set of candidate low-cost attributes as an optimization problem. We describe an exponential time algorithm for finding such optimal plans, and describe a polynomial-time heuristic for identifying conditional plans that perform well in practice. We also show how to compactly model conditional probability distributions needed to identify correlations and build these plans. We evaluate our algorithms against several real-world sensor-network data sets, showing several-times performance increases for a variety of queries versus traditional optimization techniques. 1.
714|Open Issues in Parallel Query Optimization|We provide an overview of query processing in parallel database systems and discuss several open issues in the optimization of queries for parallel machines. 
715|Coloring Away Communication in Parallel Query Optimization|We address the problem of finding parallel plans for SQL queries using the two-phase approach of join ordering and query rewrite (JOQR) followed by parallelization. We focus on the JOQR phase and develop optimization algorithms that account for communication as well as computation costs. Using a model based on representing the partitioning of data as a color, we devise an efficient algorithm for the problem of choosing the partitioning attributes in a query tree so as to minimize total cost. We extend our model and algorithm to incorporate the interaction of data partitioning with conventional optimization choices such as access methods and strategies for computing operators. Our algorithms apply to queries that include operators such as grouping, aggregation, intersection and set difference in addition to joins. 
716|Fast and simple approximation schemes for generalized flow| We present fast and simple fully...
717|Revisiting pipelined parallelism in multi-join query processing|Multi-join queries are the core of any integration service that integrates data from multiple distributed data sources. Due to the large number of data sources and possibly high volumes of data, the evaluation of multi-join queries faces increasing scalability concerns. State-of-the-art parallel multi-join query processing commonly assume that the application of maximal pipelined parallelism leads to superior performance. In this paper, we instead illustrate that this assumption does not generally hold. We investigate how best to combine pipelined parallelism with alternate forms of parallelism to achieve an overall effective processing strategy. A segmented bushy processing strategy is proposed. Experimental studies are conducted on an actual software system over a cluster of high-performance PCs. The experimental results confirm that the proposed solution leads to about 50 % improvement in terms of total processing time in comparison to existing state-of-the-art solutions. 1
718|Algorithms for distributional and adversarial pipelined filter ordering problems|Pipelined filter ordering is a central problem in database query optimization. The problem is to determine the optimal order in which to apply a given set of commutative filters (predicates) to a set of elements (the tuples of a relation), so as to find, as efficiently as possible, the tuples that satisfy all of the filters. Optimization of pipelined filter ordering has recently received renewed attention in the context of environments such as the web, continuous high-speed data streams, and sensor networks. Pipelined filter ordering problems are also studied in areas such as fault detection and machine learning under names such as learning with attribute costs, minimumsum set cover, and satisficing search. We present algorithms for two natural extensions of the classical pipelined filter ordering problem: (1) a distributional type problem where the filters run in parallel and the goal is to maximize throughput, and (2) an adversarial type problem where the goal is to minimize the expected value of multiplicative regret. We present two related algorithms for solving (1), both running in time O(n²), which improve on the O(n³ log n) algorithm of Kodialam. We use techniques from our algorithms for (1) to obtain an algorithm for (2).  
719|Adaptive and Robust Query Processing with SHARP|Database catalogs often do not contain enough statistical information to correctly cost all possible physical plans. In their absence, the optimizer can produce incorrect estimates and select sub-optimal plans for execution. To address this problem for a sub-class of queries, we propose SHARP, a new multi-join, adaptive, relational operator that joins three or more relations of a star-join. SHARP reduces the possible impact of optimizer mistakes by determining which plan to execute independently of optimization estimates. During normal query processing, SHARP collects statistics, and by using a combination of latebinding plan decisions and tuple routing strategies, it is able to change join order and table access methods. However, unlike previous tuple routing operators used for in-memory stream processing, SHARP was designed to process local relations with sizes much larger than available memory. We have implemented SHARP in the open-source DBMS Predator, and we present an extensive experimental evaluation showing the significant performance benefits of SHARP. 
720|Continual Queries for Internet Scale Event-Driven Information Delivery|In this paper we introduce the concept of continual queries, describe the design of a distributed  event-driven continual query system -- OpenCQ, and outline the initial implementation of OpenCQ  on top of the distributed interoperable information mediation system DIOM [21, 19]. Continual  queries are standing queries that monitor update of interest and return results whenever the update  reaches specified thresholds. In OpenCQ, users may specify to the system the information they would  like to monitor (such as the events or the update thresholds they are interested in). Whenever the  information of interest becomes available, the system immediately delivers it to the relevant users;  otherwise, the system continually monitors the arrival of the desired information and pushes it to the  relevant users as it meets the specified update thresholds. In contrast to conventional pull-based data  management systems such as DBMSs and Web search engines, OpenCQ exhibits two important featu...
721|Selection Predicate Indexing for Active Databases Using Interval Skip Lists|A new, efficient selection predicate indexing scheme for active database systems is introduced.  The selection predicate index proposed uses an interval index on an attribute of a relation or  object collection when one or more rule condition clauses are defined on that attribute. The  selection predicate index uses a new type of interval index called the interval skip list (IS-list).  The IS-list is designed to allow efficient retrieval of all intervals that overlap a point, while allowing  dynamic insertion and deletion of intervals. IS-list algorithms are described in detail. The IS-list  allows efficient on-line searches, insertions, and deletions, yet is much simpler to implement than  other comparable interval index data structures such as the priority search tree and balanced  interval binary search tree (IBS-tree). IS-lists require only one third as much code to implement  as balanced IBS-trees. The combination of simplicity, performance, and dynamic updateability  of the IS-li...
722|Scalable Multi-Query Optimization for SPARQL |Abstract—This paper revisits the classical problem of multiquery optimization in the context of RDF/SPARQL. We show that the techniques developed for relational and semi-structured data/query languages are hard, if not impossible, to be extended to account for RDF data model and graph query patterns expressed in SPARQL. In light of the NP-hardness of the multi-query optimization for SPARQL, we propose heuristic algorithms that partition the input batch of queries into groups such that each group of queries can be optimized together. An essential component of the optimization incorporates an efficient algorithm to discover the common sub-structures of multiple SPARQL queries and an effective cost model to compare candidate execution plans. Since our optimization techniques do not make any assumption about the underlying SPARQL query engine, they have the advantage of being portable across different RDF stores. The extensive experimental studies, performed on three popular RDF stores, show that the proposed techniques are effective, efficient and scalable. I.
724|Scalable semantic web data management using vertical partitioning|The dataset used for this benchmark is taken from the publicly available Barton Libraries dataset [1]. This data is provided by the Simile Project [3], which develops tools for library data management and interoperability. The data contains records that compose an RDF-formatted dump of the MIT Libraries Barton catalog, converted from raw data stored in an old library format standard called MARC (Machine Readable Catalog). Because of the multiple sources the data was derived from and the diverse nature of the data that is cataloged, the structure of the data is quite irregular. At the time of publication of this report, there are slightly more than 50 million triples in the dataset, with a total of 221 unique properties, of which the vast majority appear infrequently. Of these properties, 82 (37%) are multi-valued, meaning that they appear more than once for a given subject; however, these properties appear more often (77 % of the triples have a multi-valued property). The dataset provides a good demonstration of the relatively unstructured nature of Semantic Web data. 2. LONGWELL OVERVIEW Longwell [2] is a tool developed by the Simile Project, which provides a graphical user interface for generic RDF data exploration in a web browser. It begins by presenting the user with a list of the values the type property can take (such as Text or Notated Music in the library dataset). The user can click on the types of data he desires to further explore. Longwell shows the list of currently filtered resources (RDF subjects) in the main portion of the screen, and a list of filters in panels along the side. Each panel represents a property that is defined on the current filter, with popular object values for that property and their frequency also presented in this box. If the user selects an object value, this filters the working set of resources to those that have that property-object value defined,
725|The berlin sparql benchmark|for RDF are implemented by a growing number of storage systems and are used within enterprise and open Web settings. As SPARQL is taken up by the community, there is a growing need for benchmarks to compare the performance of storage systems that expose SPARQL endpoints via the SPARQL protocol. Such systems include native RDF stores as well as systems that rewrite SPARQL queries to SQL queries against non-RDF relational databases. This article introduces the Berlin SPARQL Benchmark (BSBM) for comparing the performance of native RDF stores with the performance of SPARQL-to-SQL rewriters across architectures. The benchmark is built around an e-commerce use case in which a set of products is offered by different vendors and consumers have posted reviews about products. The benchmark query mix emulates the search and navigation pattern of a consumer looking for a product. The article discusses the design of the BSBM benchmark and presents the results of a benchmark experiment comparing the performance of four popular RDF stores (Sesame, Virtuoso, Jena TDB, and Jena SDB) with the performance of two SPARQL-to-SQL rewriters (D2R Server and Virtuoso RDF Views) as well as the performance of two relational database management systems (MySQL and Virtuoso RDBMS).
726|A Fast Algorithm for the Maximum Clique Problem |Given a graph, in the maximum clique problem one wants to find
727|Scalable join processing on very large rdf graphs|With the proliferation of the RDF data format, engines for RDF query processing are faced with very large graphs that contain hundreds of millions of RDF triples. This paper addresses the resulting scalability problems. Recent prior work along these lines has focused on indexing and other physical-design issues. The current paper focuses on join processing, as the fine-grained and schema-relaxed use of RDF often entails star- and chain-shaped join queries with many input streams from index scans. We present two contributions for scalable join processing. First, we develop very light-weight methods for sideways in-formation passing between separate joins at query run-time, to provide highly effective filters on the input streams of joins. Second, we improve previously proposed algorithms for join-order optimization by more accurate selectivity esti-mations for very large RDF graphs. Experimental studies with several RDF datasets, including the UniProt collection, demonstrate the performance gains of our approach, outper-forming the previously fastest systems by more than an order of magnitude.
728|Maximum common subgraph isomorphism algorithms for the matching of chemical structures|Received 03.03.2002; accepted in final form 16.08.2002
729|The Expressive Power of SPARQL |Abstract. This paper studies the expressive power of SPARQL. The main result is that SPARQL and non-recursive safe Datalog with negation have equivalent expressive power, and hence, by classical results, SPARQL is equivalent from an expressive point of view to Relational Algebra. We present explicit generic rules of the transformations in both directions. Among other findings of the paper are the proof that negation can be simulated in SPARQL, that non-safe filters are superfluous, and that current SPARQL W3C semantics can be simplified to a standard compositional one. 1
730|Matrix &#034;Bit&#034;loaded: A Scalable Lightweight Join Query Processor for RDF Data |The Semantic Web community, until now, has used traditional database systems for the storage and querying of RDF data. The SPARQL query language also closely follows SQL syntax. As a natural consequence, most of the SPARQL query processing techniques are based on database query processing and optimization techniques. For SPARQL join query optimization, previous works like RDF-3X and Hexastore have proposed to use 6-way indexes on the RDF data. Although these indexes speed up merge-joins by orders of magnitude, for complex join queries generating large intermediate join results, the scalability of the query processor still remains a challenge. In this paper, we introduce (i) BitMat – a compressed bit-matrix structure for storing huge RDF graphs, and (ii) a novel, light-weight SPARQL join query processing method that employs an initial pruning technique, followed by a variable-binding-matching algorithm on BitMats to produce the final results. Our query processing method does not build intermediate join tables and works directly on the compressed data. We have demonstrated our method against RDF graphs of upto 1.33 billion triples – the largest among results published until now (single-node, non-parallel systems), and have compared our method with the state-ofthe-art RDF stores – RDF-3X and MonetDB. Our results show that the competing methods are most effective with highly selective queries. On the other hand, BitMat can deliver 2-3 orders of magnitude better performance on complex, low-selectivity queries over massive data.
731|Navigation- vs. Index-Based XML Multi-Query Processing|XML path queries form the basis of complex filtering of XML data. Most current XML path query processing techniques can be divided in two groups. Navigation-based algorithms compute results by analyzing an input document one tag at a time. In contrast, index-based algorithms take advantage of precomputed numbering schemes over the input XML document. In this paper we introduce a new indexbased technique, Index-Filter, to answer multiple XML path queries. Index-Filter uses indexes built over the document tags to avoid processing large portions of the input document that are guaranteed not to be part of any match. We analyze Index-Filter and compare it against Y-Filter, a stateof -the-art navigation-based technique. We show that both techniques have their advantages, and we discuss the scenarios under which each technique is superior to the other one. In particular, we show that while most XML path query processing techniques work off SAX events, in some cases it pays off to preprocess the input document, augmenting it with auxiliary information that can be used to evaluate the queries faster. We present experimental results over real and synthetic XML documents that validate our claims.
732|Massively multi-query join processing in publish/subscribe systems|There has been much recent interest in XML publish/subscribe systems. Some systems scale to thousands of concurrent queries, but support a limited query language (usually a fragment of XPath 1.0). Other systems support more expressive languages, but do not scale well with the number of concurrent queries. In this paper, we propose a set of novel query processing techniques, referred to as Massively Multi-Query Join Processing techniques, for processing a large number of XML stream queries involving value joins over multiple XML streams and documents. These techniques enable the sharing of representations of inputs to multiple joins, and the sharing of join computation. Our techniques are also applicable to relational event processing systems and publish/subscribe systems that support join queries. We present experimental results to demonstrate the effectiveness of our techniques. We are able to process thousands of XML messages with hundreds of thousands of join queries on real RSS feed streams. Our techniques gain more than two orders of magnitude speedup compared to the naive approach of evaluating such join queries.
733|Active knowledge: dynamically enriching RDF knowledge bases by Web services|The proliferation of knowledge-sharing communities and the ad-vances in information extraction have enabled the construction of large knowledge bases using the RDF data model to represent en-tities and relationships. However, as the Web and its latently em-bedded facts evolve, a knowledge base can never be complete and up-to-date. On the other hand, a rapidly increasing suite of Web services provide access to timely and high-quality information, but this is encapsulated by the service interface. We propose to lever-age the information that could be dynamically obtained from Web services in order to enrich RDF knowledge bases on the fly when-ever the knowledge base does not suffice to answer a user query. To this end, we develop a sound framework for appropriately generating queries to encapsulated Web services and efficient algo-rithms for query execution and result integration. The query gen-erator composes sequences of function calls based on the available service interfaces. As Web service calls are expensive, our method aims to reduce the number of calls in order to retrieve results with sufficient recall. Our approach is fully implemented in a complete prototype system named ANGIE1. The user can query and browse the RDF knowledge base as if it already contained all the facts from the Web services. This data, however, is gathered and integrated on the fly, transparently to the user. We demonstrate the viability and efficiency of our approach in experiments based on real-life data provided by popular Web services.
734|Rewriting Queries on SPARQL Views|The problem of answering SPARQL queries over virtual SPARQL views is commonly encountered in a number of settings, including while enforcing security policies to access RDF data, or when integrating RDF data from disparate sources. We approach this problem by rewriting SPARQL queries over the views to equivalent queries over the underlying RDF data, thus avoiding the costs entailed by view materialization and maintenance. We show that SPARQL query rewriting combines the most challenging aspects of rewriting for the relational and XML cases: like the relational case, SPARQL query rewriting requires synthesizing multiple views; like the XML case, the size of the rewritten query is exponential to the size of the query and the views. In this paper, we present the first native query rewriting algorithm for SPARQL. For an input SPARQL query over a set of virtual SPARQL views, the rewritten query resembles a union of conjunctive queries and can be of exponential size. We propose optimizations over the basic rewriting algorithm to (i) minimize each conjunctive query in the union; (ii) eliminate conjunctive queries with empty results from evaluation; and (iii) efficiently prune out big portions of the search space of empty rewritings. The experiments, performed on two RDF stores, show that our algorithms are scalable and independent of the underlying RDF stores. Furthermore, our optimizations have order of magnitude improvements over the basic rewriting algorithm in both the rewriting size and evaluation time.
735|Scalable Multi-Query Optimization for Exploratory Queries over Federated Scientific Databases |The diversity and large volumes of data processed in the Natural Sciences today has led to a proliferation of highlyspecialized and autonomous scientific databases with inherent and often intricate relationships. As a user-friendly method for querying this complex, ever-expanding network of sources for correlations, we propose exploratory queries. Exploratory queries are loosely-structured, hence requiring only minimal user knowledge of the source network. Evaluating an exploratory query usually involves the evaluation of many distributed queries. As the number of such distributed queries can quickly become large, we attack the optimization problem for exploratory queries by proposing several multi-query optimization algorithms that compute a global evaluation plan while minimizing the total communication cost, a key bottleneck in distributed settings. The proposed algorithms are necessarily heuristics, as computing an optimal global evaluation plan is shown to be np-hard. Finally, we present an implementation of our algorithms, along with experiments that illustrate their potential not only for the optimization of exploratory queries, but also for the multiquery optimization of large batches of standard queries. 1.
736|Dynamic Querying of Mass-Storage RDF Data with Rule-Based Entailment Regimes ? |Abstract. RDF Schema (RDFS) as a lightweight ontology language is gaining popularity and, consequently, tools for scalable RDFS inference and querying are needed. SPARQL has become recently a W3C standard for querying RDF data, but it mostly provides means for querying simple RDF graphs only, whereas querying with respect to RDFS or other entailment regimes is left outside the current specification. In this paper, we show that SPARQL faces certain unwanted ramifications when querying ontologies in conjunction with RDF datasets that comprise multiple named graphs, and we provide an extension for SPARQL that remedies these effects. Moreover, since RDFS inference has a close relationship with logic rules, we generalize our approach to select a custom ruleset for specifying inferences to be taken into account in a SPARQL query. We show that our extensions are technically feasible by providing benchmark results for RDFS querying in our prototype system GiaBATA, which uses Datalog coupled with a persistent Relational Database as a back-end for implementing SPARQL with dynamic rule-based inference. By employing different optimization techniques like magic set rewriting our system remains competitive with state-of-the-art RDFS querying systems. 1
737|Multiple Query Optimization by Cache-Aware Middleware using Query Teamwork|The multiple-query optimization (MQO) problem has been well-studied in the re-  search literature, usually by means of identifying and exploiting the occurence of common  subexpressions, and has required implementation in the database engine. Observing  that common subexpressions derive from common data, and that the amount of data  is usually greatest at the source, we propose an optimization technique that exploits  the presence of sharable access patterns to underlying data, especially scans of large  portions of tables or indexes, in environments where query queueing or hatching is an  acceptable approach. We show that simultaneous queries with such sharable accesses  have a tendency to form synchronous groups (teams) which benefit each other through  the operation of the disk cache, in effect using it as an implicit pipeline. We propose  a novel method of optimization, exploiting this tendency by scheduling the queries  to enhance this tendency, and show that this can be accomplished even from outside  the database engine with application server middleware. We present an algorithm for  scheduling from a queue of similar queries, designed to promote such teamwork. This  is implemented as middleware for use with a commercial database engine. Finally, we  present tests using the query mix from the TPC-R benchmark, achieving a speedup of  2.34 over the default scheduling provided by the database.
738|Query Optimization via Empty Joins|Abstract. A join of two relations in real databases is usually much smaller than their Cartesian product. This means that most of the com-binations of tuples in the crossproduct of the respective relations do not appear together in the join result. We characterize these missing combi-nations as ranges of attributes that do not appear together and present experimental results on their discovery from real data sets. We then ex-plore potential applications of this knowledge to query optimization. By modeling empty joins as materialized views, we show how knowledge of these regions can be used to improve query performance. 1
739|Knowledge Discovery in Databases: An Attribute-Oriented Approach|Knowledge discovery in databases, or data mining, is an important issue in the development of data- and knowledge-base systems. An attribute-oriented induction method has been developed for knowledge discovery in databases. The method integrates a machine learning paradigm, especially learning-from-examples techniques, with set-oriented database operations and extracts generalized data from actual data in databases. An attribute-oriented concept tree ascension technique is applied in generalization, which substantially reduces the computational complexity of database learning processes. Different kinds of knowledge rules, including characteristic rules, discrimination rules, quantitative rules, and data evolution regularities can be discovered efficiently using the attribute-oriented approach. In addition to learning in relational databases, the approach can be applied to knowledge discovery in nested relational and deductive databases. Learning can also be performed with databases containing noisy data and exceptional cases using database statistics. Furthermore, the rules discovered can be used to query database knowledge, answer cooperative queries and facilitate semantic query optimization. Based upon these principles, a prototyped database learning system, DBLEARN,  has been constructed for experimentation.
740|Using Inductive Learning To Generate Rules for Semantic Query Optimization|Semantic query optimization can dramatically speed up database query answering byknowledge intensive reformulation. But the problem of how to learn the required semantic rules has not been previously solved. This chapter presents a learning approach to solving this problem. In our approach, the learning is triggered by user queries. Then the system uses an inductive learning algorithm to generate semantic rules. This inductive learning algorithm can automatically select useful join paths and attributes to construct rules from a database with many relations. The learned semantic rules are effective for optimization because they will match query patterns and reflect data regularities. Experimental results show that this approach learns sufficient rules for optimization that produces a substantial cost reduction.   
742|Discovery and Application of Check Constraints in DB2|The traditional role of integrity constraints is to protect the integrity of data. But integrity constraints can and do play other roles in databases; for example, they can be used for query optimization. In this role, they do not need to model the domain; it is sufficient that they describe regularities that are true about the data currently stored in a database. In this paper we describe two algorithms for finding such regularities (in the syntactic form of check constraints) and discuss some of their applications in DB2.
743|Outerjoin Simplification and Reordering for Query Optimization |Conventional database optimizers take full advantage of associativity and commutativity properties of join to implement efficient and powerful optimizations on select/project/join queries. However, only limited optimization is performed on other binary operators. In this paper, we present the theory and algorithms needed to generate alternative evaluation orders for the optimization of queries containing outerjoins. Our results include both a complete set of transformation rules, suitable for new-generation, transformation-based optimizers, and a bottom-up join enumeration algorithm compatible with those used by traditional optimizers.
744|A polygen model for Heterogeneous Database Systems: The Source Tagging Perspective|This paper studies heterogeneous database systems from the multiple (poly) source @rrt) perspective. It aims at addressing issues such as “where is the data from ” and “which intermediate data sources were used to arrive at that data ”- issues which are critical to many users in utilizing information composed from multiple sources. Specifically, it presents a polygen model for resolving the Data Source Tagging and Intermediate Source Tagging problems. Secondly, it presents a data-driven query translation mechanism for mapping a polygen query into a set of local queries dynamically. A concrete example is also provided to exemplify polygen query processing. The significance of this paper lies not only in a precise characterization of a practical problem and a solution per se, but also in the establishment of a foundation for resolving many other critical research issues such as domain mismatch, semantic reconciliation, and data conflict amongst data retrieved from different sources. In a federated database environment with hundreds of databases, all of these issues are critical to their effective USt!. I.
745|Outerjoins as Disjunctions |The outerjoin operator is currently available in the query language of several major  DBMSs, and it is included in the proposed SQL2 standard draft. However, &#034;associativity  problems&#034; of the operator have been pointed out since its introduction. In this paper  we propose a shift in the intuition behind outerjoin: Instead of computing the join while  also preserving its arguments, outerjoin delivers tuples that come either from the join  or from the arguments. Queries with joins and outerjoins deliver tuples that come from  one out of several joins, where a single relation is a trivial join. An advantage of this  view is that, in contrast to preservation, disjunction is commutative and associative,  which is a significant property for intuition, formalisms, and generation of execution  plans.  Based on a disjunctive normal form, we show that some data merging queries cannot  be evaluated by means of binary outerjoins, and give alternative procedures to evaluate  those queries. We also explore several evaluation strategies for outerjoin queries,  including the use of semijoin programs to reduce base relations.  CR Subject Classification (1991): [H.2.4] Database Systems, Query Processing; [H.3.3]  Information Search and Retrieval, Query Formulation; [G.2.2] Graph Theory; [F.2.2]  Nonnumerical algorithms and problems.  Keywords and Phrases: Outerjoin, Query Graph, Query Formulation and Processing.  Note: The author had an ERCIM postdoctoral fellowship while conducting this work.  1 
746|Chapter 5 Query Optimization in Hera |While RDF and RDFS are widely acknowledged as a standard means for describing Web metadata, a standardized language for querying RDF metadata is still an open issue. Research groups coming both from industry and academia are presently involved in proposing several RDF query languages. Due to the lack of an RDF algebra such query languages use APIs to describe their semantics and optimization issues are mostly neglected. This chapter proposes RAL (an RDF algebra) as a reference mathematical study for RDF query languages and for performing RDF query optimization. We define the data model, we present the operators to manipulate the data, and we address the application of RAL for query optimization. RAL includes: extraction operators to retrieve the needed resources from the input RDF model, loop operators to support repetition, and construction operators to build the resulting RDF model. The Resource Description Framework (RDF) [Lassila and Swick, 1999; Brickley and Guha, 2004] is intended to serve as a metadata language for the Web and together with its
747|Resource Description Framework (RDF) Model and Syntax Specification  (1998) |This document is a revision of the public working draft dated 1998-08-19 incorporating suggestions received in review comments and further deliberations of the W3C RDF Model and Syntax Working Group. With the publication of this draft, the RDF Model and Syntax Specification enters &#034;last call.&#034; The last call period will end on October 23, 1998. Comments on this specification may be sent to www-rdf-comments@w3.org. The archive of public comments is available at http://www.w3.org/Archives/Public/www-rdf-comments. Significant changes from the previous draft are highlighted in Appendix E. While we do not anticipate substantial changes, we still caution that further changes are possible. Therefore while we encourage active implementation to test this specification we also recommend that only software that can be easily field-upgraded be implemented to this specification at this time. This is a W3C Working Draft for review by W3C members and other interested parties. Publication as a working draft does not imply endorsement by the W3C membership. The RDF Model and Syntax  Working Group will not allow early implementation to constrain their ability to make changes to this specification prior to final release. This is a draft document and may be updated, replaced or obsoleted by other documents at any time. It is inappropriate to cite W3C Working Drafts as other than &#034;work in progress&#034;. This work is part of the W3C Metadata Activity.
748|RQL: A Declarative Query Language for RDF|Real-scale Semantic Web applications, such as Web Portals and E-Marketplaces, require the management of voluminous metadata repositories containing descriptive information (i.e., metadata) about the available Web resources and services. Better knowledge about the meaning, usage, accessibility or quality of these resources and services will considerably facilitate the automated processing of both Web content and services. In this context, the Resource Description Framework (RDF) enables the creation and exchange of metadata as any other Web data. Although large volumes of RDF descriptions are already appearing (e.g., as exported Portal catalogs or service descriptions), sufficiently expressive declarative languages for querying both RDF descriptions and schemas are still missing. In this paper, we propose RQL, a new RDF query language, relying on a formal graph model that permits the interpretation of superimposed resource descriptions. RQL is an OQL-inspired adaptation of XML query languages to the peculiarities of RDF but, foremost, is an extension of this functionality for uniformly querying both descriptions and schemas. We illustrate the syntax, semantics and core functionality of RQL bymeans of a set of benchmark queries and report on the performance of RSSDB, our persistent RDF Store, for storing and querying voluminous RDF descriptions.
749|The semantic web: the roles of XML and RDF|The World Wide Web is possible because a set of widely established standards guarantees interoperability at various levels. Until now, the Web has been designed for direct human processing, but the next-generation Web, which Tim Berners-Lee and others call the “Semantic Web, ” aims at machine-processible information. 1 The Semantic Web will enable intelligent services—such as information brokers, search agents, and information filters—which offer greater functionality and interoperability than current stand-alone services. The Semantic Web will only be possible once further levels of interoperability have been established. Standards must be defined not only for the syntactic form of documents, but also for their semantic content. Notable among recent W3C standardization efforts are XML/XML schema and RDF/RDF schema, which facilitate semantic interoperability. In this article, we explain the role of ontologies in the architecture of the Semantic Web. We then briefly summarize key elements of XML and
750|Enabling Semantic Web Programming by Integrating RDF and Common Lisp|: This paper introduces &#034;Wilbur&#034;, an RDF and DAML toolkit implemented in  Common Lisp. Wilbur exposes the RDF data model as a frame-based representation system;  an object-oriented view of frames is adopted, and RDF data is integrated with the host language  by addressing issues of input/output, data structure compatibility, and error signaling.  Through seamless integration we have achieved a programming system well suited for  building &#034;Semantic Web&#034; applications.  1. 
751|Cost-based Query Optimization for XPath |Abstract: In this article, we address the issues which are related to the cost-based XML query optimization for XPath. Specially, we focus on the issue of how to determine the execution order for a given XPath expression according to the cost models. The main impact factor that dominates the execution order is the size of the intermediate results during the evaluation of queries. The hierarchy encoding scheme and the value-encoding histogram are introduced to support the size estimation of the path expressions. Two cost models are proposed to describe the costs of different types of join operations in the path expressions. A heuristic-based dynamic programming approach is proposed to determine the optimal execution tree. The primary experimental results demonstrate the validity of our approaches.
752|XMark: A Benchmark for XML Data Management|While standardization efforts for XML query  languages have been progressing, researchers  and users increasingly focus on the database  technology that has to deliver on the new  challenges that the abundance of XML documents  poses to data management: validation,  performance evaluation and optimization  of XML query processors are the upcoming  issues. Following a long tradition in  database research, we provide a framework  to assess the abilities of an XML database to  cope with a broad range of different query  types typically encountered in real-world scenarios.
753|Holistic twig joins: optimal XML pattern matching |XML employs a tree-structured data model, and, naturally, XML queries specify patterns of selection predicates on multiple elements related by a tree structure. Finding all occurrences of such a twig pattern in an XML database is a core operation for XML query processing. Prior work has typically decomposed the twig pattern into binary structural (parent-child and ancestor-descendant) relationships, and twig matching is achieved by: (i) using structural join algorithms to match the binary relationships against the XML database, and (ii) stitching together these basic matches. A limitation of this approach for matching twig patterns is that intermediate result sizes can get large, even when the input and output sizes are more manageable. In this paper, we propose a novel holistic twig join algorithm, TwigStack, for matching an XML query twig pattern. Our technique uses a chain of linked stacks to compactly represent partial results to root-to-leaf query paths, which are then composed to obtain matches for the twig pattern. When the twig pattern uses only ancestor-descendant relationships between elements, TwigStack is I/O and CPU optimal among all sequential algorithms that read the entire input: it is linear in the sum of sizes of the input lists and the nal result list, but independent of the sizes of intermediate results. We then show how to use (a modication of) B-trees, along with TwigStack, to match query twig patterns in sub-linear time. Finally, we complement our analysis with experimental results on a range of real and synthetic data, and query twig patterns. 1
754|Storing and querying ordered xml using a relational database system|XML is quickly becoming the de facto standard for data exchange over the Intemet. This is creating a new set of data management requirements involving XML, such as the need to store and query XML documents. Researchers have proposed using relational database systems to satisfy these requirements by devising ways to &amp;quot;shred &amp;quot; XML documents into relations, and translate XML queries into SQL queries over these relations. However, a key issue with such an approach, which has largely been ignored in the research literature, is how (and whether) the ordered XML data model can be efficiently supported by the unordered relational data model. This paper shows that XML&#039;s ordered data model can indeed be efficiently supported by a relational database system. This is accomplished by encoding order as a data value. We propose three order encoding methods that can be used to represent XML order in the relational data model, and also propose algorithms for translating ordered XPath expressions into SQL using these encoding methods. Finally, we report the results of an experimental study that investigates the performance of the proposed order encoding methods on a workload of ordered XML queries and updates. 1.
755|Minimization of Tree Pattern Queries|Tree patterns form a natural basis to query tree-structured data such as XML and LDAP. Since the efficiency of tree pattern matching against a tree-structured database depends on the size of the pattern, it is essential to identify and eliminate redundant nodes in the pattern and do so as quickly as possible. In this paper, we study tree pattern minimization both in the absence and in the presence of integrity constraints (ICs) on the underlying tree-structured database.  When no ICs are considered, we call the process of minimizing a tree pattern, constraint-independent minimization. We develop a polynomial time algorithm called CIM for this purpose. CIM&#039;s efficiency stems from two key properties: (i) a node cannot be redundant unless its children are, and (ii) the order of elimination of redundant nodes is immaterial. When ICs are considered for minimization, we refer to it as constraint-dependent minimization. For treestructured databases, required child/descendant and type co-occurrence ICs are very natural. Under such ICs, we show that the minimal equivalent query is unique. We show the surprising result that the algorithm obtained by first augmenting the tree pattern using ICs, and then applying CIM, always finds the unique minimal equivalent query; we refer to this algorithm as ACIM. While ACIM is also polynomial time, it can be expensive in practice because of its inherent non-locality. We then present a fast algorithm, CDM, that identifies and eliminates local redundancies due to ICs, based on propagating &#034;information labels&#034; up the tree pattern. CDM can be applied prior to ACIM for improving the minimization efficiency. We complement our analytical results with an experimental study that shows the effectiveness of our tree pattern minimization techniques.  1. 
756|From region encoding to extended dewey: On efficient processing of xml twig pattern matching|Finding all the occurrences of a twig pattern in an XML database is a core operation for efficient evaluation of XML queries. A number of algorithms have been proposed to process a twig query based on region encoding labeling scheme. While region encoding supports efficient determination of ancestor-descendant (or parent-child) relationship between two elements, we observe that the information within a single label is very limited. In this paper, we propose a new labeling scheme, called extended Dewey. This is a powerful labeling scheme, since from the label of an element alone, we can derive all the elements names along the path from the root to the element. Based on extended Dewey, we design a novel holistic twig join algorithm, called TJ-Fast. Unlike all previous algorithms based on region encoding, to answer a twig query, TJ-Fast only needs to access the labels of the leaf query nodes. Through this, not only do we reduce disk access, but we also support the efficient evaluation of queries with wildcards in branching nodes, which is very difficult to be answered by algorithms based on region encoding. Finally, we report our experimental results to show that our algorithms are superior to previous approaches in terms of the number of elements scanned, the size of intermediate results and query performance.
757|Counting Twig Matches in a Tree|We describe efficient algorithms for accurately estimating the number of matches of a small node-labeled tree, i.e., a twig, in a large node-labeled tree, using a summary data structure. This problem is of interest for queries on XML and other hierarchical data, to provide query feedback and for costbased query optimization. Our summary data structure scalably representsapproximate frequencyinformation about twiglets (i.e., small twigs) in the data tree. Given a twig query, the number of matches is estimated by creating a set of query twiglets, and combining two complementary approaches: Set Hashing, used to estimate the number of matches of each query twiglet, and Maximal Overlap, used to combine the query twiglet estimates into an estimate for the twig query. We propose several estimation algorithms that apply these approaches on query twiglets formed using variations on different twiglet decomposition techniques. We present an extensive experimental evaluation using several real XML...
758|Statistical Synopses for Graph-Structured XML Databases|Effective support for XML query languages is becoming increasingly important with the emergence of new applications that access large volumes of XML data. All existing proposals for querying XML (e.g., XQuery) rely on a pattern-specification language that allows path navigation and branching through the XML data graph in order to reach the desired data elements. Optimizing such queries depends crucially on the existence of concise synopsis structures that enable accurate compile-time selectivity estimates for complex path expressions over graph-structured XML data. In this paper, we introduce a novel approach to building and using statistical summaries of large XML data graphs for effective path-expression selectivity estimation. Our proposed graph-synopsis model (termed XSKETCH) exploits localized graph stability to accurately approximate (in limited space) the path and branching distribution in the data graph. To estimate the selectivities of complex path expressions over concise XSKETCH synopses, we develop an estimation framework that relies on appropriate statistical (uniformity and independence) assumptions to compensate for the lack of detailed distribution information. Given our estimation framework, we demonstrate that the problem of building an accuracy-optimal XSKETCH for a given amount of space is ### -hard, and propose an efficient heuristic algorithm based on greedy forward selection. Briefly, our algorithm constructs an XSKETCH synopsis by successive refinements of the label-split graph, the coarsest summary of the XML data graph. Our refinement operations act locally and attempt to capture important statistical correlations between data paths. Extensive experimental results with synthetic as well as real-life data sets verify the effectiveness of our app...
759|Bloom histogram: Path selectivity estimation for xml data with updates|Cost-based XML query optimization calls for accurate estimation of the selectivity of path expressions. Some other interactive and internet applications can also benefit from such estimations. While there are a number of estimation techniques proposed in the literature, almost none of them has any guarantee on the estimation accuracy within a given space limit. In addition, most of them assume that the XML data are more or less static, i.e., with few updates. In this paper, we present a framework for XML path selectivity estimation in a dynamic context. Specifically, we propose a novel data structure, bloom histogram, to approximate XML path frequency distribution within a small space budget and to estimate the path selectivity accurately with the bloom histogram. We obtain the upper bound of its estimation error and discuss the trade-offs between the accuracy and the space limit. To support updates of bloom histograms efficiently when underlying XML data change, a dynamic summary layer is used to keep exact or more detailed XML path information. We demonstrate through our extensive experiments that the new solution can
760|Minimising Simple XPath Expressions|We consider a subset of XPath expressions, called simple XPath expressions, which correspond to a class of conjunctive queries. We show that, in the absence of a DTD, each simple XPath expression has a unique minimal equivalent expression which can be found in polynomial time. We then consider D-equivalence, the equivalence of expressions with respect to the set of documents valid for a given DTD D. We show that a simple XPath expression P does not necessarily have a unique minimal D-equivalent expression. However, if P is reduced (there are no wildcards in it), then there is a unique minimal equivalent expression, but we show that deciding whether two reduced expressions are D-equivalent is coNP-hard.
761|On the minimization of XPath queries|XML queries are usually expressed by means of XPath expressions identifying portions of the selected documents. An XPath expression defines a way of navigating an XML tree and returns the set of nodes which are reachable from one or more starting nodes through the paths specified by the expression. The problem of efficiently answering XPath queries is very interesting and has recently received increasing attention by the research community. In particular, an increasing effort has been devoted to define effective optimization techniques for XPath queries. One of the main issues related to the optimization of XPath queries is their minimization. The minimization of XPath queries has been studied for limited fragments of XPath, containing only the descendent, the child and the branch operators. In this work, we address the problem of minimizing XPath queries for a more general fragment, containing also the wildcard operator. We characterize the complexity of the minimization of XPath queries, stating that it is NP-hard, and propose an algorithm for computing minimum XPath queries. Moreover, we identify an interesting tractable case and propose an ad hoc algorithm handling the minimization of this kind of queries in polynomial time.
762|Statistical Learning Techniques for Costing XML Queries|Developing cost models for query optimization is significantly  harder for XML queries than for traditional  relational queries. The reason is that XML query  operators are much more complex than relational  operators such as table scans and joins. In this  paper, we propose a new approach, called Comet,  to modeling the cost of XML operators; to our  knowledge, Comet is the first method ever proposed  for addressing the XML query costing problem. As  in relational cost estimation, Comet exploits a set of  system catalog statistics that summarizes the XML  data; the set of &#034;simple path&#034; statistics that we  propose is new, and is well suited to the XML setting.
763|Containment join size estimation: Models and methods|Recent years witnessed an increasing interest in researches in XML, partly due to the fact that XML has now become the de facto standard for data interchange over the inter-net. A large amount of work has been reported on XML storage models and query processing techniques. However, few works have addressed issues of XML query optimiza-tion. In this paper, we report our study on one of the chal-lenges in XML query optimization: containment join size estimation. Containment join is well accepted as an impor-tant operation in XML query processing. Estimating the size of its results is no doubt essential to generate ecient XML query processing plans. We propose two models, the interval model and the position model, and a set of estima-tion methods based on these two models. Comprehensive performance studies were conducted. The results not only demonstrate the advantages of our new algorithms over ex-isting algorithms, but also provide valuable insights into the tradeo  among various parameters. 1.
764|Schema-based query optimization for XQuery queries|Abstract. XQuery is widely used for querying XML documents. Within this paper, we examine optimization rules for XQuery queries that exploit type information of the input XML document given in XML Schema. These optimization rules are applicable for all XQuery expressions and are very useful e.g. in the scenario of XQuery queries on XQuery views. The basic idea is to transform the XML Schema definition into a graph, which is extended to a graph representing the XQuery expression. The latter graph is used to delete subexpressions of the XQuery expression that are not used to retrieve the final result of the given XQuery expression. We further include experimental results that demonstrate the improvement of our optimization. 1
765|CXHist: An On-line Classification-Based Histogram for XML String Selectivity Estimation|Query optimization in IBM&#039;s System RX, the  first truly relational-XML hybrid data management  system, requires accurate selectivity estimation  of path-value pairs, i.e., the number of nodes  in the XML tree reachable by a given path with the  given text value. Previous techniques have been  inadequate, because they have focused mainly on  the tag-labeled paths (tree structure) of the XML  data. For most real XML data, the number of distinct  string values at the leaf nodes is orders of  magnitude larger than the set of distinct rooted tag  paths. Hence, the real challenge lies in accurate  selectivity estimation of the string predicates on  the leaf values reachable via a given path.
766|Exploiting Functional Dependence in Query Optimization|I authorize the University of Waterloo to lend this thesis to other institutions or individuals for the purpose of scholarly research. I further authorize the University of Waterloo to reproduce this thesis by photocopying or by other means, in total or in part, at the request of other institutions or individuals for the purpose of scholarly research. iii The University of Waterloo requires the signatures of all persons using or photocopying this thesis. Please sign below, and give address and date. Functional dependency analysis can be applied to various problems in query optimization: selectivity estimation, estimation of (intermediate) result sizes, order optimization (in particular sort avoidance), cost estimation, and various problems in the area of semantic query optimization. Dependency analysis in an ansi sql relational model, however, is made complex due to the existence of null values, three-valued logic, outer joins, and duplicate rows. In this thesis we define the notions of strict and lax functional dependencies, strict and lax equivalence constraints, and null constraints, which capture both a
767|Extending the Database Relational Model to Capture More Meaning|During the last three or four years several investigators have been exploring “semantic models ” for formatted databases. The intent is to capture (in a more or less formal way) more of the meaning of the data so that database design can become more systematic and the database system itself can behave more intelligently. Two major thrusts are clear: (I) the search for meaningful units that are as small as possible--atomic semantics; (2) the search for meaningful units that are larger than the usual n-ary relation-molecular semantics. In this paper we propose extensions to the relational model to support certain atomic and molecular semantics. These extensions represent a synthesis of many ideas from the published work in semantic modeling plus the introduction of new rules for insertion, update, and deletion, as well as new algebraic operators.
768|Efficiently updating materialized views|Query processmg can be sped up by keeping fre-quently accessed users ’ views materlahzed How-ever, the need to access base relations m response to queues can be avoided only If the materlahzed view ls adequately maintained We propose a method m which all database updates to base relations are first filtered to remove from consideration those that can-not possibly affect the view The condltlons given for the detection of updates of this type, called ar-relevant updates, are necessary and sufficient and are mdependent of the database state For the remam-mg database updates, a dzfferentlal algonthm can be apphed to re-evaluate the view expression The algo-nthm proposed exploits the knowledge provided by both the view defimtlon expression and the database update operations 1
769|A complete axiomatization for functional and multivalued dependencies in database relations|We investigate the inference rules that can be applied to functional and multivalued dependencies that exist in a database relation. Three types of rules are discussed. First, we list the well known rules for functional dependencies. Ther, we investigate the rules for multi-valued dependencies. It is shown that fcr... ach rule for functional dependencies the same rule or a similar rule holds for nultivalued dependencies. There is, howc.ier, one additional rule for multi-valued dependencies that has no parallel amonq t5e rules for functional dependen-cies. Finally, we present rules that invo 1 ve func t iona 1 and mu ltivalued depeidencies together. The main result of the paper is that the rules presented are romplete for the family of function-al and multivalued dependencies.  
770|A Normal Form for Relational Databases that is Based on Domains and Keys|A new normal form for relational databases, called domain-key normal form (DK/NF), is defined. Also, formal definitions of insertion anomaly and deletion anomaly are presented. It is shown that a schema is in DK/NF if and only if it has no insertion or deletion anomalies. Unlike previously defined normal forms, DK/NF is not defined in terms of traditional dependencies (functional, multivalued, or join). Instead, it is defined in terms of the more primitive concepts of domain and key, along with the general concept of a “constraint. ” We also consider how the definitions of traditional normal forms might be modified by taking into consideration, for the first time, the combinatorial consequences of bounded domain sizes. It is shown that after this modification, these traditional normal forms are all implied by DK/NF. In particular, if all domains are infinite, then these traditional normal forms are all implied by DK/NF.
771|Supporting Multiple View Maintenance Policies|Materialized views and view maintenance are becoming increasingly important in practice. In order to satisfy different data currency and performance requirements, a number of view maintenance policies have been proposed. Immediate maintenance involves a potential refresh of the view after every update to the deriving tables. When staleness of views can be tolerated, a view may be refreshed periodically or (on-demand) when it is queried. The maintenance policies that are chosen for views have implications on the validity of the results of queries and affect the performance of queries and updates. In this paper, we investigate a number of issues related to supporting multiple views with different maintenance policies. We develop formal notions of consistency for views with different maintenance policies. We then introduce a model based on view groupings for view maintenance policy assignment, and provide algorithms, based on the viewgroup model, that allow consistency of views to be guar...
772|Functional Dependencies in a Relational Database and Propositional Logic|Abstract: An equivalence is shown between functional dependency statements of a relational database, where “+ ” has the meaning of “determines, ” and implicational statements of propositional logic, where “.$ ” has the meaning of “implies. ” Specifically, it is shown that a dependency statement is a consequence of a set of dependency statements iff the corresponding implicational statement is a con-sequence of the corresponding set of implicational statements. The database designer can take advantage of this equivalence to reduce problems of interest to him to simpler problems in propositional logic. A detailed algorithm is presented for such an application. Two proofs of the equivalence are presented: a “syntactic ” proof and a “semantic ” proof. The syntactic proof proceeds in several steps. It is shown that I) Armstrong’s Dependency Axioms are complete for dependency statements in the usual logical sense that they are strong enough to prove every consequence, and that 2) Armstrong’s Axioms are also complete for implicational statements in proposi-tional logic. The equivalence then follows from 1) and 2). The other proof proceeds by considering appropriate semantic interpreta-tions for the propositional variables. The Delobel-Casey Relational Database Decomposition Theorems, which heretofore have seemed somewhat fortuitous, are immediate and natural corollaries of the equivalence. Furthermore, a counterexample is demonstrat-ed, which shows that what seems to be a mild extension of the equivalence fails.
773|Uniformly-Distributed Random Generation of Join Orders|In this paper we study the space of operator trees that can be used to answer a join query, with the goal of generating elements form this space at random. We solve the problem for queries with acyclic query graphs. First, we count the exact number of trees that can be used to evaluate a given query. Then, we establish a mapping between the n operator trees for a query and the integers 1 through n ---i. e. a ranking---  and describe efficient ranking and unranking procedures. The generation of random, uniformly distributed operator trees follows from the unraking.  
774|The CORDS Multidatabase Project|In virtually every organization, data is stored in a variety of ways and managed by different database and file systems. Applications that require data from multiple sources are complex because they must be aware of and deal with the specifics of each data source. They must also perform any data integration needed, for example, joining data from multiple sources. The objective of a multidatabase system is to provide application developers and end users with an integrated view of and a uniform interface to all the required data. The view and the interface should be independent of where the data is stored and how it is managed.  cords is a research project focussed on distributed applications. It is a collaborative effort involving  ibm and several universities. As part of this project, we are designing and prototyping a multidatabase system. This paper provides an overview of its architecture and describes the approach taken in the following areas: management of catalog information, sch...
776|Sesame: A Generic Architecture for Storing and Querying RDF and RDF Schema|RDF and RDF Schema are two W3C standards aimed at  enriching the Web with machine-processable semantic data.
777|The RDFSuite: Managing Voluminous RDF Description Bases|Metadata are widely used in order to fully exploit information resources available  on corporate intranets or the Internet. The Resource Description Framework (RDF)  aims at facilitating the creation and exchange of metadata as any other Web data. The  growing number of available information resources and the proliferation of description  services in various user communities, lead nowadays to large volumes of RDF metadata.  Managing such RDF resource descriptions and schemas with existing low-level APIs and  file-based implementations does not ensure fast deployment and easy maintenance of realscale  RDF applications. In this paper, we advocate the use of database technology to  support declarative access, as well as, logical and physical independence for voluminous  RDF description bases.  We present RDFSuite, a suite of tools for RDF validation, storage and querying.  Specifically, weintroduce a formal data model for RDF description bases created using  multiple schemas. Compared to ...
778|Querying Community Web Portals|Anewgeneration of information systems suchasorganizational memories, vertical aggregators,  infomediaries, etc. is emerging nowadays. Such systems, termed CommunityWeb  Portals, intend to support specific communities of interest (e.g., enterprise, professional, trading)  on corporate intranets or the Web. More precisely, Portal Catalogs, organize and describe  various information resources (e.g., sites, documents, data) for diverse target audiences  (corporate, inter-enterprise, e-marketplace, etc.), in a multitude of ways, which are far more  flexible and complex than those provided by standard (relational or object) databases. Yet, in  commercial software for deploying CommunityPortals, querying is still limited to full-text (or  attribute-value) retrieval and more advanced information-seeking needs implies navigational  access. Furthermore, recentWeb standards for describing resources are completely ignored.
779|Reduction of Query Optimizer Plan Diagrams |A “plan diagram ” is a pictorial enumeration of the execution plan choices of a database query optimizer over the relational selectivity space. We have shown recently that, for industrial-strength database engines, these diagrams are often remarkably complex and dense, with a large number of plans covering the space. However, they can often be reduced to much simpler pictures, featuring significantly fewer plans, without materially affecting the query processing quality. Plan reduc-tion has useful implications for the design and usage of query optimizers, including quantifying redundancy in the plan search space, enhancing useability of parametric query optimization, iden-tifying error-resistant and least-expected-cost plans, and minimizing the overheads of multi-plan approaches. We investigate here the plan reduction issue from theoretical, statistical and empirical perspec-tives. Our analysis shows that optimal plan reduction, w.r.t. minimizing the number of plans, is an NP-hard problem in general, and remains so even for a storage-constrained variant. We then present a greedy reduction algorithm with tight and optimal performance guarantees, whose complexity scales linearly with the number of plans in the diagram for a given resolution. Next, we devise fast estimators for locating the best tradeoff between the reduction in plan cardinality and the impact on query processing quality. Finally, extensive experimentation with a suite of multi-dimensional TPC-H and TPC-DS based query templates on industrial-strength optimizers demonstrates that complex plan diagrams easily reduce to “anorexic ” (small absolute number of plans) levels incurring only marginal increases in the estimated query processing costs. 1
780|A Threshold of ln n for Approximating Set Cover|Given a collection F of subsets of S = f1; : : : ; ng, set cover is the problem of selecting  as few as possible subsets from F such that their union covers S, and max k-cover  is the problem of selecting k subsets from F such that their union has maximum cardinality.  Both these problems are NP-hard. We prove that (1 \Gamma o(1)) ln n is a threshold  below which set cover cannot be approximated efficiently, unless NP has slightly superpolynomial  time algorithms. This closes the gap (up to low order terms) between  the ratio of approximation achievable by the greedy algorithm (which is (1 \Gamma o(1)) ln n),  and previous results of Lund and Yannakakis, that showed hardness of approximation  within a ratio of (log 2 n)=2 &#039; 0:72 lnn. For max k-cover we show an approximation  threshold of (1 \Gamma 1=e) (up to low order terms), under the assumption that P != NP .  
781|A Tight Analysis of the Greedy Algorithm for Set Cover|We establish significantly improved bounds on the performance of the greedy algorithm for approximating set cover. In particular, we provide the first substantial improvement of the 20 year old classical harmonic upper bound, H(m), of Johnson, Lovasz, and Chv&#039;atal, by showing that the performance ratio of the greedy algorithm is, in fact, exactly ln m \Gamma ln ln m+ \Theta(1), where m is the size of the ground set. The difference between the upper and lower bounds turns out to be less than 1:1. This provides the first tight analysis of the greedy algorithm, as well as the first upper bound that lies below H(m) by a function going to infinity with m.  We also show that the approximation guarantee for the greedy algorithm is better than the guarantee recently established by Srinivasan for the randomized rounding technique, thus improving the bounds on the integrality gap.  Our improvements result from a new approach which might be generally useful for attacking other similar problems.  ...
782|Counting, Enumerating, and Sampling of Execution Plans in a Cost-Based Query Optimizer|Testing an SQL database system by running large sets of deterministic or stochastic SQL  statements is common practice in commercial database development. However, code defects  often remain undetected as the query optimizer&#039;s choice of an execution plan is not only  depending on the query but strongly influenced by a large number of parameters describing  the database and the hardware environment. Modifying these parameters in order to steer  the optimizer to select other plans is di#cult since this means anticipating often complex  search strategies implemented in the optimizer.  In this paper we devise algorithms for counting, exhaustive generation, and uniform  sampling of plans from the complete search space. Our techniques allow extensive validation  of both generation of alternatives, and execution algorithms with plans other than the  optimized one---if two candidate plans fail to produce the same results, then either the  optimizer considered an invalid plan, or the execution ...
783|A Characterization of the Sensitivity of Query Optimization to Storage Access Parameters|Most relational query optimizers make use of information about the costs of accessing tuples and data structures on various storage devices. This information can at times be off by several orders of magnitude due to human error in configuration setup, sudden changes in load, or hardware failure. In this paper, we attempt to answer the following questions: • Are inaccurate access cost estimates likely to cause a typical query optimizer to choose a suboptimal query plan? • If an optimizer chooses a suboptimal plan as a result of inaccurate access cost estimates, how far from optimal is this plan likely to be? To address these issues, we provide a theoretical, vector-based framework for analyzing the costs of query plans under various storage parameter costs. We then use this geometric framework to characterize experimentally a commercial query optimizer. We develop algorithms for extracting detailed information about query plans through narrow optimizer interfaces, and we perform the characterization using database statistics from a published run of the TPC-H benchmark and a wide range of storage parameters. We show that, when data structures such as tables, indexes, and sorted runs reside on different storage devices, the optimizer can derive significant benefits from having accurate and timely information regarding the cost of accessing storage devices.
784|Efficient Query Evaluation on Probabilistic Databases|We describe a system that supports arbitrarily complex SQL queries with ”uncertain” predicates. The query semantics is based on a probabilistic model and the results are ranked, much like in Information Retrieval. Our main focus is efficient query evaluation, a problem that has not received attention in the past. We describe an optimization algorithm that can compute efficiently most queries. We show, however, that the data complexity of some queries is #P-complete, which implies that these queries do not admit any efficient evaluation methods. For these queries we describe both an approximation algorithm and a Monte-Carlo simulation algorithm.
785|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
786|Optimal Aggregation Algorithms for Middleware|Assume that each object in a database has m grades, or scores, one for each of m attributes. For example, an object can have a color grade, that tells how red it is, and a shape grade, that tells how round it is. For each attribute, there is a sorted list, which lists each object and its grade under that attribute, sorted by grade (highest grade first). There is some monotone aggregation function, or combining rule, such as min or average, that combines the individual grades to obtain an overall grade. To determine the top k objects (that have the best overall grades), the naive algorithm must access every object in the database, to find its grade under each attribute. Fagin has given an algorithm (“Fagin’s Algorithm”, or FA) that is much more efficient. For some monotone aggregation functions, FA is optimal with high probability in the worst case. We analyze an elegant and remarkably simple algorithm (“the threshold algorithm”, or TA) that is optimal in a much stronger sense than FA. We show that TA is essentially optimal, not just for some monotone aggregation functions, but for all of them, and not just in a high-probability worst-case sense, but over every database. Unlike FA, which requires large buffers (whose size may grow unboundedly as the database size grows), TA requires only a small, constant-size buffer. TA allows early stopping, which yields, in a precise sense, an approximate version of the top k answers. We distinguish
787|A Guided Tour to Approximate String Matching|We survey the current techniques to cope with the problem of string matching allowing  errors. This is becoming a more and more relevant issue for many fast growing areas such  as information retrieval and computational biology. We focus on online searching and mostly  on edit distance, explaining the problem and its relevance, its statistical behavior, its history  and current developments, and the central ideas of the algorithms and their complexities. We  present a number of experiments to compare the performance of the different algorithms and  show which are the best choices according to each case. We conclude with some future work  directions and open problems.   
788|Evaluating Probabilistic Queries over Imprecise Data|Sensors are often employed to monitor continuously changing entities like locations of moving ob-jects and temperature. The sensor readings are reported to a database system, and are subsequently used to answer queries. Due to continuous changes in these values and limited resources (e.g., net-work bandwidth and battery power), the database may not be able to keep track of the actual values of the entities. Queries that use these old values may produce incorrect answers. However, if the degree of uncertainty between the actual data value and the database value is limited, one can place more confidence in the answers to the queries. More generally, query answers can be augmented with probabilistic guarantees of the validity of the answers. In this paper, we study probabilistic query evaluation based on uncertain data. A classification of queries is made based upon the nature of the result set. For each class, we develop algorithms for computing probabilistic answers, and provide efficient indexing and numeric solutions. We address the important issue of measuring the quality of the answers to these queries, and provide algorithms for efficiently pulling data from relevant sensors or moving objects in order to improve the quality of the executing queries. Extensive experiments
789|XRANK: Ranked Keyword Search over XML Documents|We consider the problem of efficiently producing ranked results for keyword search queries over hyperlinked XML documents. Evaluating
790|Integration of Heterogeneous Databases Without Common Domains Using Queries Based on Textual Similarity|Most databases contain &#034;name constants&#034; like course numbers, personal names, and place names that correspond to entities in the real world. Previous work in integration of heterogeneous databases has assumed that local name constants can be mapped into an appropriate global domain by normalization. However, in many cases, this assumption does not hold; determining if two name constants should be considered identical can require detailed knowledge of the world, the purpose of the user&#039;s query, or both. In this paper, we reject the assumption that global domains can be easily constructed, and assume instead that the names are given in natural language text. We then propose a logic called WHIRL which reasons explicitly about the similarity of local names, as measured using the vector-space model commonly adopted in statistical information retrieval. We describe an efficient implementation of WHIRL and evaluate it experimentally on data extracted from the World Wide Web. We show that WHIR...
791|A Probabilistic Relational Algebra for the Integration of Information Retrieval and Database Systems|We present a probabilistic relational algebra (PRA) which is a generalization of standard relational algebra. Here tuples are assigned probabilistic weights giving the probability that a tuple belongs to a relation. Based on intensional semantics, the tuple weights of the result of a PRA expression always confirm to the underlying probabilistic model. We also show for which expressions extensional semantics yields the same results. Furthermore, we discuss complexity issues and indicate possibilities for optimization. With regard to databases, the approach allows for representing imprecise attribute values, whereas for information retrieval, probabilistic document indexing and probabilistic search term weighting can be modelled. As an important extension, we introduce the concept of vague predicates which yields a probabilistic weight instead of a Boolean value, thus allowing for queries with vague selection conditions. So PRA implements uncertainty and vagueness in combination with the...
792|ProbView: A Flexible Probabilistic Database System|... In this article, we characterize, using postulates, whole classes of strategies for conjunction, disjunction, and negation, meaningful from the viewpoint of probability theory. (1) We propose a probabilistic relational data model and a generic probabilistic relational algebra that neatly captures various strategies satisfying the postulates, within a single unified framework. (2) We show that as long as the chosen strategies can be computed in polynomial time, queries in the positive fragment of the probabilistic relational algebra have essentially the same data complexity as classical relational algebra. (3) We establish various containments and equivalences between algebraic expressions, similar in spirit to those in classical algebra. (4) We develop algorithms for maintaining materialized probabilistic views. (5) Based on these ideas, we have developed  
793|Reasoning about Knowledge and Probability|: We provide a model for reasoning about knowledge and probability together. We allow explicit mention of probabilities in formulas, so that our language has formulas that essentially say &#034;according to agent i, formula  &#039; holds with probability at least b.&#034; The language is powerful enough to allow reasoning about higher-order probabilities, as well as allowing explicit comparisons of the probabilities an agent places on distinct events. We present a general framework for interpreting such formulas, and consider various properties that might hold of the interrelationship between agents&#039; probability assignments at different states. We provide a complete axiomatization for reasoning about knowledge and probability, prove a small model property, and obtain decision procedures. We then consider the effects of adding common knowledge and a probabilistic variant of common knowledge to the language.   A preliminary version of this paper appeared in the Proceedings of the Second Conference on T...
794|Probabilistic Logic Programming|Of all scientific investigations into reasoning with uncertainty and chance, probability theory is perhaps the best understood paradigm. Nevertheless, all studies conducted thus far into the semantics of quantitative logic programming (cf. van Emden [51], Fitting [18, 19, 20], Blair and Subrahmanian [5, 6, 49, 50], Kifer et al [29, 30, 31]) have restricted themselves to non-probabilistic semantical characterizations. In this paper, we take a few steps towards rectifying this situation. We define a logic programming language that is syntactically similar to the annotated logics of [5, 6], but in which the truth values are interpreted probabilistically. A probabilistic model theory and fixpoint theory is developed for such programs. This probabilistic model theory satisfies the requirements proposed by Fenstad [16] for a function to be called probabilistic. The logical treatment of probabilities is complicated by two facts: first, that the connectives cannot be interpreted truth function...
795|An algebra for probabilistic databases |An algebra is presented for a simple probabilistic data model that may be regarded as an extension of the standard relational model. The probabilistic algebra is developed in such a way that (restricted to a-acyclic database schemes) the relational algebra is a homomorphic image of it. Strictly probabilistic results are emphasized. Variations on the basic probabilistic data model are discussed. The algebra is used to explicate a commonly used statistical smoothing procedure and is shown to be potentially very useful for decision support with uncertain information.  
796|Vague: a user interface to relational databases that permits vague queries|A specific query establishes a rigid qualification and is concerned only with data that match it precisely. A vague query establishes a target qualification and is concerned also with data that are close to this target. Most conventional database systems cannot handle vague queries directly, forcing their users to retry specific queries repeatedly with minor modifications until they match data that are satisfactory. This article describes a system called VAGUE that can handle vague queries directly. The principal concept behind VAGUE is its extension to the relational data model with data metrics, which are definitions of distances between values of the same domain. A problem with implementing data distances is that different users may have different interpretations for the notion of distance. VAGUE incorporates several features that enable it to adapt itself to the individual views and priorities of its users.
797|Automated ranking of database query results|We investigate the problem of ranking answers to a database query when many tuples are returned. We adapt and apply principles of probabilistic models from Information Retrieval for structured data. Our proposed solution is domain independent. It leverages data and workload statistics and correlations. Our ranking functions can be further customized for different applications. We present results of preliminary experiments which demonstrate the efficiency as well as the quality of our ranking system. 1.
798|Phonetic String Matching: Lessons from Information Retrieval|Phonetic matching is used in applications such as name retrieval, where the spelling of a name is used to identify other strings that are likely to be of similar pronunciation. In this paper we explain the parallels between information retrieval and phonetic matching, and describe our new phonetic matching techniques. Our experimental comparison with existing techniques such as Soundex and edit distances, which is based on recall and precision, demonstrates that the new techniques are superior. In addition, reasoning from the similarity of phonetic matching and information retrieval, we have applied combination of evidence to phonetic matching. Our experiments with combining demonstrate that it leads to substantial improvements in effectiveness.
799|PXML: A probabilistic semistructured data model and algebra|ehung,getoor,vs£ Despite the recent proliferation of work on semistructured data models, there has been little work to date on supporting uncertainty in these models. In this paper, we propose a model for probabilistic semistructured data (PSD). The advantage of our approach is that it supports a flexible representation that allows the specification of a wide class of distributions over semistructured instances. We provide two semantics for the model and show that the semantics are probabilistically coherent. Next, we develop an extension of the relational algebra to handle probabilistic semistructured data and describe efficient algorithms for answering queries that use this algebra. Finally, we present experimental results showing the efficiency of our algorithms. 1
800|The Complexity of Query Reliability|The reliability of database queries on databases with uncertain information is studied, on the basis of a probabilistic model for unreliable databases. While it was already known that the reliability of quantifierfree queries is computable in polynomial time, we show here that already for conjunctive queries, the reliability may become highly intractable. We exhibit a conjunctive query whose reliability problem is complete for FP  #P  . We further show, that FP  #P  is the typical complexity level for the reliability problems of a very large class of queries, including all second-order queries. We study approximation algorithms and prove that the reliabilities of all polynomial-time evaluable queries can be efficiently approximated by randomized algorithms. Finally we discuss the extension of our approach to the more general metafinite database model where finite relational structures are endowed with functions into an infinite interpreted domain; in addition queries may use aggregate ...
801|Probabilistic Object Bases|There are many applications where an object oriented data model is a good way of representing  and querying data. However, current object database systems are unable to handle the  case of objects whose attributes are uncertain. In this paper, extending previous pioneering work  by Kornatzky and Shimony, we develop an extension of the relational algebra to the case of object  bases with uncertainty. We propose concepts of consistency for such object bases, together  with an NP-completeness result, and classes of probabilistic object bases for which consistency  is polynomially checkable. In addition, as certain operations involve conjunctions and disjunctions  of events, and as the probability of conjunctive and disjunctive events depends both on the  probabilities of the primitive events involved as well as on what is known (if anything) about the  relationship between the events, we show how all our algebraic operations may be performed  under arbitrary probabilistic conjunction and ...
802|Combining DAML+OIL, XSLT and probabilistic logics for uncertain schema mappings in MIND |Abstract. When distributed, heterogeneous digital libraries have to be integrated, one of the crucial tasks is to map between different schemas. As schemas may have different granularities, and as schema attributes do not always match precisely, a general-purpose schema mapping approach requires support for uncertain mappings. In this paper we present one of the very few approaches for defining and using uncertain schema mappings. We combine different technologies like DAML+OIL, probabilistic Datalog (since DAML+OIL—as similar ontology languages—lacks rules) and XSLT for actually transforming queries and documents. This declarative approach is fully implemented in the project MIND (which develops methods for retrieval in networked multimedia digital libraries). However, as DAML+OIL lacks some important features, the proposed approach is only a stepping stone for an integrated solution. 1
