ID|Title|Summary
1|Tabu Search -- Part I|This paper presents the fundamental principles underlying tabu search as a strategy for combinatorial optimization problems. Tabu search has achieved impressive practical successes in applications ranging from scheduling and computer channel balancing to cluster analysis and space planning, and more recently has demonstrated its value in treating classical problems such as the traveling salesman and graph coloring problems. Nevertheless, the approach is still in its infancy, and a good deal remains to be discovered about its most effective forms of implementation and about the range of problems for which it is best suited. This paper undertakes to present the major ideas and findings to date, and to indicate challenges for future research. Part I of this study indicates the basic principles, ranging from the short-term memory process at the core of the search to the intermediate and long term memory processes for intensifying and diversifying the search. Included are illustrative data structures for implementing the tabu conditions (and associated aspiration criteria) that underlie these processes. Part I concludes with a discussion of probabilistic tabu search and a summary of computational experience for a variety of applications. Part I1 of this study (to appear in a subsequent issue) examines more advanced considerations, applying the basic ideas to special settings and outlining a dynamic move structure to insure finiteness. Part I1 also describes tabu search methods for solving mixed integer programming problems and gives a brief summary of additional practical experience, including the use of tabu search to guide other types of processes, such as those
2|Efficient similarity search in sequence databases|We propose an indexing method for time sequences for processing similarity queries. We use the Discrete Fourier Transform (DFT) to map time sequences to the frequency domain, the crucial observation being that, for most sequences of practical interest, only the first few frequencies are strong. Another important observation is Parseval&#039;s theorem, which specifies that the Fourier transform preserves the Euclidean distance in the time or frequency domain. Having thus mapped sequences to a lower-dimensionality space by using only the first few Fourier coe cients, we use R-trees to index the sequences and e ciently answer similarity queries. We provide experimental results which show that our method is superior to search based on sequential scanning. Our experiments show that a few coefficients (1-3) are adequate to provide good performance. The performance gain of our method increases with the number and length of sequences. 
3|A Survey of Image Registration Techniques|Registration is a fundamental task in image processing used to  match two or more pictures taken, for example, at different times,  from different sensors or from different viewpoints. Over the years, a  broad range of techniques have been developed for the various types of  data and problems. These techniques have been independently studied  for several different applications resulting in a large body of research.  This paper organizes this material by establishing the relationship  between the distortions in the image and the type of registration techniques  which are most suitable. Two major types of distortions are  distinguished. The first type are those which are the source of misregistration,  i.e., they are the cause of the misalignment between the two  images. Distortions which are the source of misregistration determine  the transformation class which will optimally align the two images.  The transformation class in turn influences the general technique that  should be taken....
4|Voronoi diagrams -- a survey of a fundamental geometric data structure|This paper presents a survey of the Voronoi diagram, one of the most fundamental data structures in computational geometry. It demonstrates the importance and usefulness of the Voronoi diagram in a wide variety of fields inside and outside computer science and surveys the history of its development. The paper puts particular emphasis on the unified exposition of its mathematical and algorithmic properties. Finally, the paper provides the first comprehensive bibliography on Voronoi diagrams and related structures.
5|Database Mining: A Performance Perspective|We present our perspective of database mining as the confluence of machine learning techniques and the performance emphasis of database technology. We describe three classes of database mining problems involving classification, associations, and sequences, and argue that these problems can be uniformly viewed as requiring discovery of rules embedded in massive data. We describe a model and some basic operations for the process of rule discovery. We show how the database mining problems we consider map to this model and how they can be solved by using the basic operations we propose. We give an example of an algorithm for classification obtained by combining the basic rule discovery operations. This algorithm not only is efficient in discovering classification rules but also has accuracy comparable to ID3, one of the current best classifiers.  Index Terms. database mining, knowledge discovery, classification, associations, sequences, decision trees   Current address: Computer Science De...
6|Temporal databases|A temporal database (see Temporal Database) contains time-varying data. Time is an important aspect of all real-world phenomena. Events occur at specific points in time; objects and the relationships among objects exist over time. The ability to model this temporal dimension of the real world is essential to many computer applications, such as accounting, banking, econometrics, geographical information systems, inventory control, law, medical records, multi-media, process control, reservation systems, and scientific data analysis. Conventional databases represent the state of an enterprise at a single moment of time. Although the contents of the database continue to change as new information is added, these changes are viewed as modifications to the state, with the old, out-of-date data being deleted from the database. The current contents of the database may be viewed as a snapshot of the enterprise. When a conventional database is used, the attributes involving time are manipulated solely by the application programs, with little help
7|The hB-tree: A multiattribute indexing method with good guaranteed performance|A new multiattribute index structure called the hB-tree is introduced. It is derived from the K-D-B-tree of Robinson [15] but has additional desirable properties. The hB-tree internode search and growth processes are precisely analogous to the corresponding processes in B-trees [l]. The intranode processes are unique. A k-d tree is used as the structure within nodes for very efficient searching. Node splitting requires that this k-d tree be split. This produces nodes which no longer represent brick-like regions in k-space, but that can be characterized as holey bricks, bricks in which subregions have been extracted. We present results that guarantee hB-tree users decent storage utilization, reasonable size index terms, and good search and insert performance. These results guarantee that the hB-tree copes well with arbitrary distributions of keys.
8|Vague: a user interface to relational databases that permits vague queries|A specific query establishes a rigid qualification and is concerned only with data that match it precisely. A vague query establishes a target qualification and is concerned also with data that are close to this target. Most conventional database systems cannot handle vague queries directly, forcing their users to retry specific queries repeatedly with minor modifications until they match data that are satisfactory. This article describes a system called VAGUE that can handle vague queries directly. The principal concept behind VAGUE is its extension to the relational data model with data metrics, which are definitions of distances between values of the same domain. A problem with implementing data distances is that different users may have different interpretations for the notion of distance. VAGUE incorporates several features that enable it to adapt itself to the individual views and priorities of its users.
9|New Techniques for Best-Match Retrieval|A scheme to answer best-match queries from a file containing a collection of objects is described. A best-match query is to find the objects in the file that are closest (according to some (dis)similarity measure) to a given target. Previous work [5, 331 suggests that one can reduce the number of comparisons required to achieve the desired results using the triangle inequality, starting with a data structure for the file that reflects some precomputed intrafile distances. We generalize the technique to allow the optimum use of any given set of precomputed intrafile distances. Some empirical results are presented which illustrate the effectiveness of our scheme, and its performance relative to previous algorithms.
10|Costly search and mutual fund flows|This paper studies the flows of funds into and out of equity mutual funds. Consumers base their fund purchase decisions on prior performance information, but do so asymmetrically, investing disproportionately more in funds that performed very well the prior period. Search costs seem to be an important determinant of fund flows. High performance appears to be most salient for funds that exert higher marketing effort, as measured by higher fees. Flows are directly related to the size of the fund’s complex as well as the current media attention received by the fund, which lower consumers ’ search costs. ALTHOUGH MUCH ACADEMIC RESEARCH on mutual funds addresses issues of performance measurement and attribution, we can learn more from this industry than whether fund managers can consistently earn risk-adjusted excess returns. Researchers studying funds have shed light on how incentives affect fund managers ’ behavior, 1 how board structure affects oversight activities, 2 and how scale and scope economies affect mutual fund costs and fees. 3 More generally, the fund industry is a laboratory in which to study the actions of individual investors who buy fund shares. In this paper, we study the flows of funds into and out of individual U.S. equity mutual funds to better understand the behavior of households that buy funds and the fund complexes and marketers that sell them.
13|The performance of mutual funds in the period 1945-1964|In this paper I derive a risk-adjusted measure of portfolio performance (now known as &#034;Jensen&#039;s Alpha&#034;) that estimates how much a manager&#039;s forecasting ability contributes to the fund&#039;s returns. The measure is based on the theory of the pricing of capital assets by Sharpe (1964), Lintner (1965a) and Treynor (Undated). I apply the measure to estimate the predictive ability of 115 mutual fund managers in the period 1945-1964—that is their ability to earn returns which are higher than those we would expect given the level of risk of each of the portfolios. The foundations of the model and the properties of the performance measure suggested here are discussed in Section II. The evidence on mutual fund performance indicates not only that these 115 mutual funds were on average not able to predict security prices well enough to outperform a buy-the-marketand-hold policy, but also that there is very little evidence that any individual fund was able to do significantly better than that which we expected from mere random chance. It is also important to note that these conclusions hold even when we measure the fund returns gross of management expenses (that is assume their bookkeeping, research, and other expenses except brokerage commissions were obtained free). Thus on average the funds apparently were not quite successful enough in their trading activities to recoup even their brokerage expenses.  
14|Cognitive Dissonance and Mutual Fund Investors|We present evidence from questionnaire studies of mutual fund  investors about recollections of past fund performance. We find  that investor memories exhibit a positive bias, consistent with  current psychological models. We find that the degree of bias is  conditional upon previous investor choice, a phenomenon related  to the well known theory of cognitive dissonance.
15|Pushing the Envelope: Planning, Propositional Logic, and Stochastic Search|Planning is a notoriously hard combinatorial search problem. In many interesting domains, current planning algorithms fail to scale up gracefully. By combining a general, stochastic search algorithm and appropriate problem encodings based on propositional logic, we are able to solve hard planning problems many times faster than the best current planning systems. Although stochastic methods have been shown to be very e ective on a wide range of scheduling problems, this is the rst demonstration of its power on truly challenging classical planning instances. This work also provides a new perspective on representational issues in planning.
16|Fast Planning Through Planning Graph Analysis|We introduce a new approach to planning in STRIPS-like domains based on constructing and analyzing a compact structure we call a Planning Graph. We describe a new planner, Graphplan, that uses this paradigm. Graphplan always returns a shortest possible partial-order plan, or states that no valid plan exists. We provide empirical evidence in favor of this approach, showing that Graphplan outperforms the total-order planner, Prodigy, and the partial-order planner, UCPOP, on a variety of interesting natural and artificial planning problems. We also give empirical evidence that the plans produced by Graphplan are quite sensible. Since searches made by this approach are fundamentally different from the searches of other common planning methods, they provide a new perspective on the planning problem.
17|Using Temporal Logic to Control Search in a Forward Chaining Planner|. Over the years increasingly sophisticated planning algorithms have been developed. These have made for more efficient planners, but unfortunately these planners still suffer from combinatorial explosion. Indeed, recent theoretical results demonstrate that such an explosion is inevitable. It has long been acknowledged that domain independent planners need domain dependent information to help them plan effectively. In this work we describe how natural domain information, of a &#034;strategic&#034; nature, can be expressed in a temporal logic, and then utilized to effectively control a forward-chaining planner. There are numerous advantages to our approach, including a declarative semantics for the search control knowledge; a high degree of modularity (the more search control knowledge utilized the more efficient search becomes); and an independence of this knowledge from the details of the planning algorithm. We have implemented our ideas in the TLPLAN system, and have been able to demonstrate i...
18|Partial-Order Planning: Evaluating Possible Efficiency Gains|Although most people believe that planners that delay step-ordering decisions as long as possible are more efficient than those that manipulate totally ordered sequences of actions, this intuition has received little formal justification or empirical validation. In this paper we do both, characterizing the types of domains that offer performance differentiation and the features that distinguish the relative overhead of three planning algorithms. As expected, the partial-order (nonlinear) planner often has an advantage when confronted with problems in which the specific order of the plan steps is critical. We argue that the observed performance differences are best understood with an extension of Korf&#039;s taxonomy of subgoal collections. Each planner quickly solved problems whose subgoals were independent or trivially serializable, but problems with laboriously serializable or nonserializable subgoals were intractable for all planners. Since different plan representations induce distinct ...
19|Planning as Temporal Reasoning|This paper describes a reasoning system based on a temporal logic that can solve planning problems along the lines of traditional planning systems. Because it is cast as inference in a general representation, however, the ranges of problems that can be described is considerably greater than in traditional planning systems. In addition, other modes of plan reasoning, such as plan recognition or plan monitoring, can be formalized within the same framework. 1
20|Fast Parallel Algorithms for Short-Range Molecular Dynamics|Three parallel algorithms for classical molecular dynamics are presented. The first assigns each  processor a fixed subset of atoms; the second assigns each a fixed subset of inter-atomic forces to compute;  the third assigns each a fixed spatial region. The algorithms are suitable for molecular dynamics models  which can be difficult to parallelize efficiently -- those with short-range forces where the neighbors of  each atom change rapidly. They can be implemented on any distributed--memory parallel machine which  allows for message--passing of data between independently executing processors. The algorithms are  tested on a standard Lennard-Jones benchmark problem for system sizes ranging from 500 to 100,000,000  atoms on several parallel supercomputers -- the nCUBE 2, Intel iPSC/860 and Paragon, and Cray T3D.  Comparing the results to the fastest reported vectorized Cray Y-MP and C90 algorithm shows that  the current generation of parallel machines is competitive with conventi...
21|A Fast Algorithm for Particle Simulations|this paper to the case where  the potential (or force) at a point is a sum of pairwise An algorithm is presented for the rapid evaluation of the potential and force fields in systems involving large numbers of particles interactions. More specifically, we consider potentials of  whose interactions are Coulombic or gravitational in nature. For a the form  system of N particles, an amount of work of the order O(N  2  ) has traditionally been required to evaluate all pairwise interactions, un- F5F far 1 (F near 1F external ), less some approximation or truncation method is used. The algorithm of the present paper requires an amount of work proportional to N to evaluate all interactions to within roundoff error, making it where F near (when present) is a rapidly decaying potential  con
22|The Torus-Wrap Mapping For Dense Matrix Calculations On Massively Parallel Computers| Dense linear systems of equations are quite common in science and engineering, arising in boundary element methods, least squares problems and other settings. Massively parallel computers will be necessary to solve the large systems required by scientists and engineers, and scalable parallel algorithms for the linear algebra applications must be devised for these machines. A critical step in these algorithms is the mapping of matrix elements to processors. In this paper, we study the use of the torus--wrap mapping in general dense matrix algorithms, from both theoretical and practical viewpoints. We prove that, under reasonable assumptions, this assignment scheme leads to dense matrix algorithms that achieve (to within a constant factor) the lower bound on interprocessor communication. We also show that the torus--wrap mapping allows algorithms to exhibit less idle time, better load balancing and less memory overhead than the more common row and column mappings. Finally, we discuss ...
23|A New Parallel Method for Molecular Dynamics Simulation of Macromolecular Systems|Short--range molecular dynamics simulations of molecular systems are commonly parallelized by  replicated--data methods, where each processor stores a copy of all atom positions. This enables computation  of bonded 2--, 3--, and 4--body forces within the molecular topology to be partitioned among  processors straightforwardly. A drawback to such methods is that the inter--processor communication  scales as N , the number of atoms, independent of P , the number of processors. Thus, their parallel efficiency  falls off rapidly when large numbers of processors are used. In this article a new parallel method  for simulating macromolecular or small--molecule systems is presented, called force--decomposition. Its  memory and communication costs scale as N=  p  P , allowing larger problems to be run faster on greater  numbers of processors. Like replicated--data techniques, and in contrast to spatial--decomposition approaches,  the new method can be simply load--balanced and performs well eve...
24|Parallel Many-Body Simulations Without All-to-All Communication|Simulations of interacting particles are common in science and engineering, appearing in such diverse disciplines as astrophysics, fluid dynamics, molecular physics, and materials science. These simulations are often computationally intensive and so natural candidates for massively  parallel computing. Many-body simulations that directly compute interactions between pairs  of particles, be they short-range or long-range interactions, have been parallelized in several  standard ways. The simplest approaches require all-to-all communication, an expensive communication  step. The fastest methods assign a group of nearby particles to a processor, which  can lead to load imbalance and be difficult to implement efficiently. We present a new approach,  suitable for direct simulations, that avoids all-to-all communication without requiring  any geometric clustering. For some computations we find the new method to be the fastest  parallel algorithm available; we demonstrate its utility...
25|A High Performance Communications and Memory Caching Scheme for Molecular Dynamics on the CM-5|We present several techniques that we have used to optimize the performance of a message-passing C code for molecular dynamics on the CM-5. We describe our use of the CM-5 vector units and a parallel memory caching scheme that we have developed to speed up the code by more than 50%. A modification that decreases our communication time by 35% is also presented along with a discussion of how we have been able to take advantage of the CM-5 hardware without significantly compromising code portability. We have been able to speed up our original code by a factor of ten and we feel that our modifications may be useful in optimizing the performance of other message-passing C applications on the CM-5. 1 Introduction  For several decades, the method of molecular dynamics (MD)[1] has been a useful technique for studying the dynamical properties of solids and liquids. In a molecular dynamics simulation, the motion of a large collection of N atoms is modeled directly by solving Newton&#039;s equations o...
26|A Parallel Scalable Approach to Short-Range Molecular Dynamics on the CM-5|We present an scalable algorithm for short-range Molecular Dynamics which minimizes interprocessor communications at the expense of a modest computational redundancy. The method combines Verlet neighbor lists with coarse-grained cells. Each processing node is associated with a cubic volume of space and the particles it owns are those initially contained in the volume. Data structures for &#034;own&#034; and &#034;visitor &#034; particle coordinates are maintained in each node. Visitors are particles owned by one of the 26 neighboring cells but lying within an interaction range of a face. The Verlet neighbor list includes pointers to own--own and own--visitor interactions. To communicate, each of the 26 neighbor cells sends a corresponding block of particle coordinates using message-passing calls. The algorithm has the numerical properties of the standard serial Verlet method and is efficient for hundreds to thousands of particles per node allowing the simulation of large systems with millions of particles...
27|A Fast Quantum Mechanical Algorithm for Database Search|Imagine a phone directory containing N names arranged in completely random order. In order to find someone&#039;s phone number with a probability of , any classical algorithm (whether deterministic or probabilistic)
will need to look at a minimum of names. Quantum mechanical systems can be in a superposition of states and simultaneously examine multiple names. By properly adjusting the phases of various operations, successful computations reinforce each other while others interfere randomly. As a result, the desired phone number can be obtained in only steps. The algorithm is within a small constant factor of the fastest possible quantum mechanical algorithm.
28|Algorithms for Quantum Computation: Discrete Logarithms and Factoring|A computer is generally considered to be a universal computational device; i.e., it is believed able to simulate any physical computational device with a cost in com-putation time of at most a polynomial factol: It is not clear whether this is still true when quantum mechanics is taken into consideration. Several researchers, starting with David Deutsch, have developed models for quantum mechanical computers and have investigated their compu-tational properties. This paper gives Las Vegas algorithms for finding discrete logarithms and factoring integers on a quantum computer that take a number of steps which is polynomial in the input size, e.g., the number of digits of the integer to be factored. These two problems are generally considered hard on a classical computer and have been used as the basis of several proposed cryptosystems. (We thus give the first examples of quantum cryptanulysis.)
29|Quantum theory, the Church-Turing principle and the universal quantum computer|computer
30|Quantum complexity theory|Abstract. In this paper we study quantum computation from a complexity theoretic viewpoint. Our first result is the existence of an efficient universal quantum Turing machine in Deutsch’s model of a quantum Turing machine (QTM) [Proc. Roy. Soc. London Ser. A, 400 (1985), pp. 97–117]. This construction is substantially more complicated than the corresponding construction for classical Turing machines (TMs); in fact, even simple primitives such as looping, branching, and composition are not straightforward in the context of quantum Turing machines. We establish how these familiar primitives can be implemented and introduce some new, purely quantum mechanical primitives, such as changing the computational basis and carrying out an arbitrary unitary transformation of polynomially bounded dimension. We also consider the precision to which the transition amplitudes of a quantum Turing machine need to be specified. We prove that O(log T) bits of precision suffice to support a T step computation. This justifies the claim that the quantum Turing machine model should be regarded as a discrete model of computation and not an analog one. We give the first formal evidence that quantum Turing machines violate the modern (complexity theoretic) formulation of the Church–Turing thesis. We show the existence of a problem, relative to an oracle, that can be solved in polynomial time on a quantum Turing machine, but requires superpolynomial time on a bounded-error probabilistic Turing machine, and thus not in the class BPP. The class BQP of languages that are efficiently decidable (with small error-probability) on a quantum Turing machine satisfies BPP ? BQP ? P ?P. Therefore, there is no possibility of giving a mathematical proof that quantum Turing machines are more powerful than classical probabilistic Turing machines (in the unrelativized setting) unless there is a major breakthrough in complexity theory.
31|Rapid solution of problems by quantum computation|A class of problems is described which can be solved more efficiently by quantum computation than by any classical or stochastic method. The quantum computation solves the problem with certainty in exponentially less time than any classical deterministic computation.
33|Strengths and Weaknesses of quantum computing|  Recently a great deal of attention has been focused on quantum computation following a
34|Quantum Circuit Complexity|We study a complexity model of quantum circuits analogous to the standard (acyclic) Boolean circuit model. It is shown that any function computable in polynomial time by a quantum Turing machine has a polynomial-size quantum circuit. This result also enables us to construct a universal quantum computer which can simulate, with a polynomial factor slowdown, a broader class of quantum machines than that considered by Bernstein and Vazirani [BV93], thus answering an open question raised in [BV93]. We also develop a theory of quantum communication complexity, and use it as a tool to prove that the majority function does not have a linear-size quantum formula. Keywords. Boolean circuit complexity, communication complexity, quantum communication complexity, quantum computation  AMS subject classifications. 68Q05, 68Q15 1  This research was supported in part by the National Science Foundation under grant CCR-9301430.  1 Introduction One of the most intriguing questions in computation theroy ...
35|Matching is as Easy as Matrix Inversion|A new algorithm for finding a maximum matching in a general graph is presented; its special feature being that the only computationally non-trivial step required in its execution is the inversion of a single integer matrix. Since this step can be parallelized, we get a simple parallel (RNC2) algorithm. At the heart of our algorithm lies a probabilistic lemma, the isolating lemma. We show applications of this lemma to parallel computation and randomized reductions. 
36|Oracle quantum computing|\Because nature isn&#039;t classical, dammit...&#034;
37|A fast quantum mechanical algorithm for estimating the median. Quantum Physics e-Print archive |Consider the problem of estimating the median of N items to a precision e, i.e. the estimate µ should be such that, with a large probability, the number of items with values smaller than µ is less than and those with values greater than µ is also less than. Any classical algorithm to do this will need at least samples. Quantum mechanical systems can simultaneously carry out multiple computations due to their wave like properties. This paper gives an step algorithm for the above problem. 1
38|M-tree: An Efficient Access Method for Similarity Search in Metric Spaces|A new access meth d, called M-tree, is proposed to organize and search large data sets from a generic &#034;metric space&#034;, i.e. whE4 object proximity is only defined by a distance function satisfyingth positivity, symmetry, and triangle inequality postulates. We detail algorith[ for insertion of objects and split management, whF h keep th M-tree always balanced - severalheralvFV split alternatives are considered and experimentally evaluated. Algorithd for similarity (range and k-nearest neigh bors) queries are also described. Results from extensive experimentationwith a prototype system are reported, considering as th performance criteria th number of page I/O&#039;s and th number of distance computations. Th results demonstratethm th Mtree indeed extendsth domain of applicability beyond th traditional vector spaces, performs reasonably well inhE[94Kv#E44V[vh data spaces, and scales well in case of growing files. 1 
39|Nearest Neighbor Queries|A frequently encountered type of query in Geographic Information Systems is to find the k nearest neighbor objects to a given point in space. Processing such queries requires substantially different search algorithms than those for location or range queries. In this paper we present an efficient branch-and-bound R-tree traversal algorithm to find the nearest neighbor object to a point, and then generalize it to finding the k nearest neighbors. We also discuss metrics for an optimistic and a pessimistic search ordering strategy as well as for pruning. Finally, we present the results of several experiments obtained using the implementation of our algorithm and examine the behavior of the metrics and the scalability of the algorithm. 
40|Fast subsequence matching in time-series databases|We present an efficient indexing method to locate 1-dimensional subsequences within a collection of sequences, such that the subsequences match a given (query) pattern within a specified tolerance. The idea is to map each data sequence into a small set of multidimensional rectangles in feature space. Then, these rectangles can be readily indexed using traditional spatial access methods, like the R*-tree [9]. In more detail, we use a sliding window over the data sequence and extract its features; the result is a trail in feature space. We propose an ecient and eective algorithm to divide such trails into sub-trails, which are subsequently represented by their Minimum Bounding Rectangles (MBRs). We also examine queries of varying lengths, and we show how to handle each case efficiently. We implemented our method and carried out experiments on synthetic and real data (stock price movements). We compared the method to sequential scanning, which is the only obvious competitor. The results were excellent: our method accelerated the search time from 3 times up to 100 times.  
41|Efficient and Effective Querying by Image Content|In the QBIC (Query By Image Content) project we are studying methods to query large  on-line image databases using the images&#039; content as the basis of the queries. Examples of  the content we use include color, texture, and shape of image objects and regions. Potential  applications include medical (&#034;Give me other images that contain a tumor with a texture like this  one&#034;), photo-journalism (&#034;Give me images that have blue at the top and red at the bottom&#034;),  and many others in art, fashion, cataloging, retailing, and industry.  We describe a set of novel features and similarity measures allowing query by color, texture,  and shape of image object. We demonstrate the effectiveness of the QBIC system with normalized  precision and recall experiments on test databases containing over 1000 images and 1000  objects populated from commercially available photo clip art images, and of images of airplane  silhouettes. We also consider the efficient indexing of these features, specifically addre...
42|FastMap: A Fast Algorithm for Indexing, Data-Mining and Visualization of Traditional and Multimedia Datasets|A very promising idea for fast searching in traditional and multimedia databases is to map objects into points in k-d  space, using k feature-extraction functions, provided by a domain expert [25]. Thus, we can subsequently use highly fine-tuned spatial access methods (SAMs), to answer several types of queries, including the `Query By Example&#039; type (which translates to a range query); the `all pairs&#039; query (which translates to a spatial join [8]); the nearest-neighbor or best-match query, etc. However, designing feature extraction functions can be hard. It is relatively easier for a domain expert to assess the similarity/distance of two objects. Given only the distance information though, it is not obvious how to map objects into points. This is exactly the topic of this paper. We describe a fast algorithm to map objects into points in some k-dimensional  space (k is user-defined), such that the dis-similarities are preserved. There are two benefits from this mapping: (a) efficient ret...
43|The R + -tree: A dynamic index for multidimensional objects|The problem of indexing multidimensional objects is considered. First, a classification of existing methods is given along with a discussion of the major issues involved in multidimensional data indexing. Second, a variation to Guttman’s R-trees (R +-trees) that avoids overlapping rectangles in intermediate nodes of the tree is introduced. Algorithms for searching, updating, initial packing and reorganization of the structure are discussed in detail. Finally, we provide analytical results indicating that R +-trees achieve up to 50 % savings in disk accesses compared to an R-tree when searching files of thousands of rectangles. 1
44|Content-based classification, search, and retrieval of audio|say that it belongs to the class of speech sounds or the class of applause sounds, where the system has previously been trained on other sounds in this class. I Acoustical/perceptual features: describing the sounds in terms of commonly understood physical characteristics such as brightness, pitch, and loudness. I Subjective features: describing the sounds using personal descriptive language. This requires training the system (in our case, by example) to understand the meaning of these descriptive terms. For example, a user might be looking for a “shimmering ” sound.
45|Near neighbor search in large metric spaces|Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically difficult case where each data element is high dimensional, or more generally, is represented by a point in a large metric spaceand distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT- Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data that does not use its intrinsic geometry. In experiments, we find that GNAT’s outperform previous data structures in a number of applications.
46|Distance-based indexing for high-dimensional metric spaces|In many database applications, one of the common queries is to find approximate matches to a given query item from a collection of data items. For example, given an image database, one may want to retrieve all images that are similar to a given query image. Distance based index structures are proposed for applications where the data domain is high dimensional, or the distance function used to compute distances between data objects is non-Euclidean. In this paper, we introduce a distance based index structure called multi-vantage point (mvp) tree for similarity queries on high-dimensional metric spaces. The mvptree uses more than one vantage point to partition the space into spherical cuts at each level. It also utilizes the pre-computed (at construction time) distances between the data points and the vantage points. We have done experiments to compare mvp-trees with vp-trees which have a similar partitioning strategy, but use only one vantage point at each level, and do not make use of the pre-computed distances. Empirical studies show that mvptree outperforms the vp-tree 20 % to 80 % for varying query ranges and different distance distributions. 1.
47|The FF planning system: Fast plan generation through heuristic search|We describe and evaluate the algorithmic techniques that are used in the FF planning system. Like the HSP system, FF relies on forward state space search, using a heuristic that estimates goal distances by ignoring delete lists. Unlike HSP&#039;s heuristic, our method does not assume facts to be independent. We introduce a novel search strategy that combines Hill-climbing with systematic search, and we show how other powerful heuristic information can be extracted and used to prune the search space. FF was the most successful automatic planner at the recent AIPS-2000 planning competition. We review the results of the competition, give data for other benchmark domains, and investigate the reasons for the runtime performance of FF compared to HSP.  
48|Systematic Nonlinear Planning|This paper presents a simple, sound, complete, and systematic algorithm for domain independent STRIPS planning. Simplicity is achieved by starting with a ground procedure and then applying a general, and independently verifiable, lifting transformation. Previous planners have been designed directly as lifted procedures. Our ground procedure is a ground version of Tate&#039;s NONLIN procedure. In Tate&#039;s procedure one is not required to determine whether a prerequisite of a step in an unfinished plan is guaranteed to hold in all linearizations. This allows Tate&#039;s procedure to avoid the use of Chapman&#039;s modal truth criterion. Systematicity is the property that the same plan, or partial plan, is never examined more than once. Systematicity is achieved through a simple modification of Tate&#039;s procedure.
49|The Computational Complexity of Propositional STRIPS Planning|I present several computational complexity results for propositional STRIPS planning, i.e., STRIPS planning restricted to ground formulas. Different planning problems can be defined by restricting the type of formulas, placing limits on the number of pre- and postconditions, by restricting negation in pre- and postconditions, and by requiring optimal plans. For these types of restrictions, I show when planning is tractable (polynomial) and intractable (NPhard) . In general, it is PSPACE-complete to determine if a given planning instance has any solutions. Extremely severe restrictions on both the operators and the formulas are required to guarantee polynomial time or even NP-completeness. For example, when only ground literals are permitted, determining plan existence is PSPACE-complete even if operators are limited to two preconditions and two postconditions. When definite Horn ground formulas are permitted, determining plan existence is PSPACE-complete even if operators are limited t...
50|Unifying SAT-based and Graph-based Planning|The Blackbox planning system unifies the plan-ning as satisfiability framework (Kautz and Sel-man 1992, 1996) with the plan graph approach to STRIPS planning (Blum and Furst 1995). We show that STRIPS problems can be directly translated into SAT and efficiently solved using new random-ized systematic solvers. For certain computation-ally challenging benchmark problems this unified approach outperforms both SATPLAN and Graph-plan alone. We also demonstrate that polynomial-time SAT simplification algorithms applied to the encoded problem instances are a powerful com-plement to the “mutex ” propagation algorithm that works directly on the plan graph. 1
51|Hard and Easy Distributions of SAT Problems|We report results from large-scale experiments in satisfiability testing. As has been observed by others, testing the satisfiability of random formulas often appears surprisingly easy. Here we show that by using the right distribution of instances, and appropriate parameter values, it is possible to generate random formulas that are hard, that is, for which satisfiability testing is quite difficult. Our results provide a benchmark for the evaluation of satisfiability-testing procedures.  Introduction  Many computational tasks of interest to AI, to the extent that they can be precisely characterized at all, can be shown to be NP-hard in their most general form. However, there is fundamental disagreement, at least within the AI community, about the implications of this. It is claimed on the one hand that since the performance of algorithms designed to solve NP-hard tasks degrades rapidly with small increases in input size, something will need to be given up to obtain acceptable behavior....
52|Planning as Heuristic Search: New Results|In the recent AIPS98 Planning Competition, the hsp planner,  based on a forward state search and a domain-independent heuristic,  showed that heuristic search planners can be competitive with state of  the art Graphplan and Satisfiability planners. hsp solved more problems  than the other planners but it often took more time or produced longer  plans. The main bottleneck in hsp is the computation of the heuristic  for every new state. This computation may take up to 85% of the processing  time. In this paper, we present a solution to this problem that  uses a simple change in the direction of the search. The new planner,  that we call hspr, is based on the same ideas and heuristic as hsp, but  searches backward from the goal rather than forward from the initial  state. This allows hspr to compute the heuristic estimates only once. As  a result, hspr can produce better plans, often in less time. For example,  hspr solves each of the 30 logistics problems from Kautz and Selman in  less than 3 seconds. This is two orders of magnitude faster than blackbox. At the same time
53|Extending planning graphs to an ADL subset| We describe an extension of graphplan to a subset of ADL that allows conditional and universally quantified effects in operators in such away that almost all interesting properties of the original graphplan algorithm are preserved.
54|A Robust and Fast Action Selection Mechanism for Planning|The ability to plan and react in dynamic environments is central to intelligent behavior yet few algorithms have managed to combine fast planning with a robust execution. In this paper we develop one such algorithm by looking at planning as real time search. For that we develop a variation of Korf&#039;s Learning Real Time A algorithm together with a suitable heuristic function. The resulting algorithm interleaves lookahead with execution and never builds a plan. It is an action selection mechanism that decides at each time point what to do next. Yet it solves hard planning problems faster than any domain independent planning algorithm known to us, including the powerful SAT planner recently introduced by Kautz and Selman. It also works in the presence of perturbations and noise, and can be given a fixed time window to operate. We illustrate each of these features by running the algorithm on a number of benchmark problems. 1 Introduction The ability to plan and react ...
55|The automatic inference of state invariants in TIM|As planning is applied to larger and richer domains the e ort involved in constructing domain descriptions increases and becomes a signi cant burden on the human application designer. If general planners are to be applied successfully to large and complex domains it is necessary to provide the domain designer with some assistance in building correctly encoded domains. One way of doing this is to provide domain-independent techniques for extracting, from a domain description, knowledge that is implicit in that description and that can assist domain designers in debugging domain descriptions. This knowledge can also be exploited to improve the performance of planners: several researchers have explored the potential of state invariants in speeding up the performance of domain-independent planners. In this paper we describe a process by which state invariants can be extracted from the automatically inferred type structure of a domain. These techniques are being developed for exploitation by stan, a Graphplan based planner that employs state analysis techniques to enhance its performance. 1.
56|Combining the expressivity of UCPOP with the efficiency of Graphplan|  There has been a great deal of recent work on new approaches to efficiently generating plans in systems such as Graphplan and SATplan. However, these systems only provide an impoverished representation language compared to other planners, such as UCPOP, ADL, or Prodigy. This makes it difficult to represent planning problems using these new planners. This paper addresses this problem by providing a completely automated set of transformations for converting a UCPOP domain representation into a Graphplan representation. The set of transformations extends the Graphplan representation language to include disjunctions, negations, universal quantification, conditional effects, and axioms. We tested the resulting planner on the 18 test domains and 41 problems that come with the UCPOP 4.0 distribution. Graphplan with the new preprocessor is able to solve every problem in the test set and on the hard problems (i.e., those that require more than one second of CPU time) it can solve them significantly faster than UCPOP. While UCPOP was unable to solve 7 of the test problems within a search limit of 100,000 nodes (which requires 414 to 980 CPU seconds), Graphplan with the preprocessor solved them all in under 15 CPU seconds (including the preprocessing time). 
57|A Heuristic Estimator for Means-Ends Analysis in Planning|Means-ends analysis is a seemingly well understood search technique, which can be described, using planning terminology, as: keep adding actions that are feasible and achieve pieces of the goal. Unfortunately, it is often the case that no action is both feasible and relevant in this sense. The traditional answer is to make subgoals out of the preconditions of relevant but infeasible actions. These subgoals become part of the search state. An alternative, surprisingly good, idea is to recompute the entire subgoal hierarchy after every action. This hierarchy is represented by a greedy regression-match graph. The actions near the leaves of this graph are feasible and relevant to a sub. . . subgoals of the original goal. Furthermore, each subgoal is assigned an estimate of the number of actions required to achieve it. This number can be shown in practice to be a useful heuristic estimator for domains that are otherwise intractable.  Keywords: planning, search, means-ends analysis   Reinven...
58|On the compilability and expressive power of propositional planning formalisms|The recent approaches of extending the GRAPHPLAN algorithm to handle more expressive planning formalisms raise the question of what the formal meaning of “expressive power ” is. We formalize the intuition that expressive power is a measure of how concisely planning domains and plans can be expressed in a particular formalism by introducing the notion of “compilation schemes ” between planning formalisms. Using this notion, we analyze the expressiveness of a large family of propositional planning formalisms, ranging from basic STRIPS to a formalism with conditional effects, partial state specifications, and propositional formulae in the preconditions. One of the results is that conditional effects cannot be compiled away if plan size should grow only linearly but can be compiled away if we allow for polynomial growth of the resulting plans. This result confirms that the recently proposed extensions to the GRAPHPLAN algorithm concerning conditional effects are optimal with respect to the “compilability ” framework. Another result is that general propositional formulae cannot be compiled into conditional effects if the plan size should be preserved linearly. This implies that allowing general propositional formulae in preconditions and effect conditions adds another level of difficulty in generating a plan.
59|Ignoring Irrelevant Facts and Operators in Plan Generation|It is traditional wisdom that one should start from the goals when  generating a plan in order to focus the plan generation process on potentially  relevant actions. The graphplan system, however, which is the  most efficient planning system nowadays, builds a &#034;planning graph&#034; in a  forward-chaining manner. Although this strategy seems to work well, it  may possibly lead to problems if the planning task description contains irrelevant  information. Although some irrelevant information can be filtered  out by graphplan, most cases of irrelevance are not noticed.  In this paper, we analyze the effects arising from &#034;irrelevant&#034; information  to planning task descriptions for different types of planners. Based  on that, we propose a family of heuristics that select relevant information  by minimizing the number of initial facts that are used when approximating  a plan by backchaining from the goals ignoring any conflicts. These  heuristics, although not solution-preserving, turn out to be v...
60|Using Regression-Match Graphs to Control Search in Planning|Classical planning is the problem of finding a sequence of actions to achieve a goal given an exact characterization of a domain. An algorithm to solve this problem is presented, which searches a space of plan prefixes, trying to extend one of them to a complete sequence of actions. It is guided by a heuristic estimator based on regression-match graphs, which attempt to characterize the entire subgoal structure of the remaining part of the problem. These graphs simplify the structure by neglecting goal interactions and by assuming that variables in goal conjunctions should be bound in such a way as to make as many conjuncts as possible true without further work. In some domains, these approximations work very well, and experiments show that many classical-planning problems can solved with very little search. 1 Definition of the Problem  The classical planning problem is to generate a sequence of actions that make a given proposition true, in a domain in which there is perfect informati...
61|Efficient Implementation of the Plan Graph in STAN|Stan is a Graphplan-based planner, so-called because it uses a variety of STate ANalysis techniques to enhance its performance. Stan competed in the AIPS-98 planning competition where it compared well with the other competitors in terms of speed, finding solutions fastest to many of the problems posed. Although the domain analysis techniques  Stan exploits are an important factor in its overall performance, we believe that the speed at which Stan solved the competition problems is largely due to the implementation of its plan graph. The implementation is based on two insights: that many of the graph construction operations can be implemented as bit-level logical operations on bit vectors, and that the graph should not be explicitly constructed beyond the fix point. This paper describes the implementation of Stan&#039;s plan graph and provides experimental results which demonstrate the circumstances under which advantages can be obtained from using this implementation. 1. Introduction  Stan ...
62|When Gravity Fails: Local Search Topology|Local search algorithms for combinatorial search problems frequently encounter a sequence of states in which it is impossible to improve the value of the objective function; moves through these regions, called plateau moves, dominate the time spent in local search. We analyze and characterize plateaus for three different classes of randomly generated Boolean Satisfiability problems. We identify several interesting features of plateaus that impact the performance of local search algorithms. We show that local minima tend to be small but occasionally may be very large. We also show that local minima can be escaped without unsatisfying a large number of clauses, but that systematically searching for an escape route may be computationally expensive if the local minimum is large. We show that plateaus with exits, called benches, tend to be much larger than minima, and that some benches have very few exit states which local search can use to escape. We show that the solutions (i.e., global m...
63|On Reasonable and Forced Goal Orderings and their Use in an Agenda-Driven Planning Algorithm|The paper addresses the problem of computing goal orderings, which is one of the longstanding issues in AI planning. It makes two new contributions. First, it formally defines and discusses two different goal orderings, which are called the reasonable and the forced ordering. Both orderings are defined for simple STRIPS operators as well as for more complex ADL operators supporting negation and conditional effects. The complexity of these orderings is investigated and their practical relevance is discussed. Secondly, two different methods to compute reasonable goal orderings are developed. One of them is based on planning graphs, while the other investigates the set of actions directly. Finally, it is shown how the ordering relations, which have been derived for a given set of goals G, can be used to compute a so-called goal agenda that divides G into an ordered set of subgoals. Any planner can then, in principle, use the goal agenda to plan for increasing sets of subgoals. This ...
64|A Heuristic for Domain Independent Planning and its Use in an Enforced Hill-climbing Algorithm |  We present a new heuristic method to evaluate planning states, which is based on solving a relaxation of the planning problem. The solutions to the relaxed problem give a good estimate for the length of a real solution, and they can also be used to guide action selection during planning. Using these informations, we employ a search strategy that combines Hill-climbing with systematic search. The algorithm is complete on what we call deadlock-free domains. Though it does not guarantee the solution plans to be optimal, it does find close to optimal plans in most cases. Often, it solves the problems almost without any search at all. In particular, it outperforms all state-of-the-art planners on a large range of domains. 
65|Conditional Effects in Graphplan|Graphplan has attracted considerable interest because of its extremely high performance, but the algorithm&#039;s inability to handle action representations more expressive than STRIPS is a major limitation. In particular, extending Graphplan to handle conditional effects is a surprisingly subtle enterprise. In this paper, we describe the space of possible alternatives, and then concentrate on one particular approach we call factored expansion. Factored expansion splits an action with conditional effects into several new actions called components, one for each conditional effect. Because these action components are not independent, factored expansion complicates both the mutual exclusion and backward chaining phases of Graphplan. As compensation, factored expansion often produces dramatically smaller domain models than does the more obvious full-expansion into exclusive STRIPS actions. We present experimental...
66|Hybrid STAN: Identifying and Managing Combinatorial Optimisation Sub-problems in Planning|It is well-known that planning is hard but it  is less well-known how to approach the hard  parts of a problem instance eectively. Using  static domain analysis techniques we can identify  and abstract certain combinatorial subproblems  from a planning instance, and deploy  specialised technology to solve these subproblems  in a way that is integrated with the  broader planning activities. We have developed  a hybrid planning system (STAN4) which  brings together alternative planning strategies  and specialised algorithms and selects between  them according to the structure of the planning  domain. STAN4 participated successfully  in the AIPS-2000 planning competition. We  describe how sub-problem abstraction is done,  with particular reference to route-planning abstraction,  and present some of the competition  data to demonstrate the potential power of the  hybrid approach.  1 Introduction  The knowledge-sparse, or domain-independent, planning community is often criticised for its o...
67|GRT: A Domain Independent Heuristic for STRIPS Worlds based on Greedy Regression Tables|This paper presents Greedy Regression Tables (GRT), a new domain  independent heuristic for STRIPS worlds. The heuristic can be used to guide  the search process of any state-space planner, estimating the distance between  each intermediate state and the goals. At the beginning of the problem solving  process a table is created, the records of which contain the ground facts of the  domain, among with estimates for their distances from the goals. Additionally,  the records contain information about interactions that occur while trying to  achieve different ground facts simultaneously. During the search process, the  heuristic, using this table, extracts quite accurate estimates for the distances  between intermediate states and the goals. A simple best-first search planner  that uses this heuristic has been implemented in C++ and has been tested on  several &#034;classical&#034; problem instances taken from the bibliography and on some  new taken from the AIPS-98 planning competition. Our planner has proved to  be faster in all of the cases, finding also in most (but not all) of the cases shorter  solutions.
68|Goal ordering in partially ordered plans|t-o. _E
69|Elevator Control as a Planning Problem|The synthesis of elevator control commands is a difficult problem when new service requirements such as VIP service, access restrictions, nonstop travel etc. have to be individually tailored to each passenger. AI planning technology offers a very elegant and flexible solution because the possible actions of a control system can be made explicit and their preconditions and e ects can be speci ed using expressive representation formalisms. Based on the specification, a planner can flexibly synthesize the required control and changes in the specification do not require any reimplementation of the control software. In this paper, we describe the application and investigate how currently available domain-independent planning formalisms can cope with it.
70|Ordering problem subgoals|Most past research work on problem subgoal ordering are of a heuristic nature and very little attempt has been made to reveal the inherent relationship between subgoal ordering constraints and problem operator schemata. As a result, subgoal ordering strategies which have been developed tend to be either overly committed, imposing ordering on subgoals subjectively or randomly, or overly restricted, ordering subgoals only after a violation of ordering constraints becomes explicit during the development of a problem solution or plan. This paper proposes a new approach characterized by a formal representation of subgoal ordering constraints which makes explicit the relationship between the constraints and the problem operator schemata. Following this approach, it becomes straightforward to categorize various types of subgoal ordering constraints, to manipulate or extend the relational representation of the constraints, to systematically detect important subgoal ordering constraints from problem specifications, and to apply the detected constraints to multiple problem instances. 1
71|Plateaus and Plateau Search in Boolean Satisfiability Problems: When to Give Up Searching and Start Again|: We empirically investigate the properties of the search space and the behavior of hill-climbing search for solving hard, random Boolean satisfiability problems. In these experiments it was frequently observed that rather than attempting to escape from plateaus by extensive search, it was better to completely restart from a new random initial state. The optimum point to terminate search and restart was determined empirically over a range of problem sizes and complexities. The growth rate of the optimum cutoff is faster than linear with the number of features, although the exact growth rate was not determined. Based on these empirical results, a simple run-time heuristic is proposed to determine when to give up searching a plateau and restart. This heuristic closely approximates the empirically determined optimum values over a range of problem sizes and complexities, and consequently allows the search algorithm to automatically adjust its strategy for each particular problem without pr...
72|Solving Complex Planning Tasks Through Extraction of Subproblems|The paper introduces an approach to derive a total ordering  between increasing sets of subgoals by defining  a relation over atomic goals. The ordering is represented  in a so-called goal agenda that is used by the  planner to incrementally plan for the increasing sets  of subgoals. This can lead to an exponential complexity  reduction because the solution to a complex planning  problem is found by solving easier subproblems.  Since only a polynomial overhead is caused by the goal  agenda computation, a potential exists to dramatically  speed up planning algorithms as we demonstrate in the  empirical evaluation.  Introduction  How to effectively plan for interdependent subgoals has been in the focus of AI planning research for a very long time (Chapman 1987). But until today planners have made only some progress to solve larger sets of subgoals and scalability of classical planning systems is still a problem.  Previous approaches fell into two categories: On one hand, one can focus on...
73|Solving the Entailment Problem in the Fluent Calculus using Binary Decision Diagrams|The paper is an exercise in formal program development. We rigorously show how planning  problems encoded as entailment problems in the fluent calculus can be mapped onto  satisfiability problems for propositional formulas, which in turn can be mapped to the problem  to find models using binary decision diagrams. The mapping is shown to be sound and  complete. Preliminary experimental results of an implementation are discussed.
74|Extracting Route-Planning: First Steps in Automatic Problem Decomposition|The divide between knowledge-intensive, domaindependent  planning and knowledge-sparse, searchintensive  domain-independent planning has a long history  in the planning community. It has been argued  that the only route to ecient planning with real application  domains is to exploit human expert knowledge,  encoded in the domain structures used by a planner.  On the other hand, it has also been observed  that the exploitationof knowledge in this way restricts  planners to domains in which the knowledge is valid  and available, and signicantly reduces the extent to  which the planners can be exibly redeployed. In this  paper, we present the rst steps in a direction which  might bridge this gap: the automatic identication  and exploitation of fundamental behaviours within a  knowledge-sparse encoding of a planning domain, with  the objective of allowing specialised problem-solving  technology access to the components of a problem for  which they are best suited, while not increasing ...
75|Heuristic search planning with BDDs|Abstract. In this paper we study traditional and enhanced BDDbased exploration procedures capable of handling large planning problems. On the one hand, reachability analysis and model checking have eventually approached AI-Planning. Unfortunately, they typically rely on uninformed blind search. On the other hand, heuristic search and especially lower bound techniques have matured in effectively directing the exploration even for large problem spaces. Therefore, with heuristic symbolic search we address the unexplored middle ground between single state and symbolic planning engines to establish algorithms that can gain from both sides. To this end we implement and evaluate heuristics found in state-of-the-art heuristic single-state search planners. 1
76|On the Instantiation of ADL Operators Involving Arbitrary First-Order Formulas | The generation of the set of all ground actions for a given set of ADL operators, which are allowed to have conditional effects and preconditions that can be represented using arbitrary first-order formulas is a complex process which heavily influences the performance of any planner or pre-planning analysis method. The paper describes a sophisticated instantiation procedure that determines so-called inertia in a given problem representation and uses them to perform simplifications of formulas during the instantiation process. As a result, many inapplicable actions are detected and ruled out from the domain representation yielding a much smaller search space for the planner. 
77|Suffix arrays: A new method for on-line string searches|A new and conceptually simple data structure, called a suffix array, for on-line string searches is intro-duced in this paper. Constructing and querying suffix arrays is reduced to a sort and search paradigm that employs novel algorithms. The main advantage of suffix arrays over suffix trees is that, in practice, they use three to five times less space. From a complexity standpoint, suffix arrays permit on-line string searches of the type, &#034;Is W a substring of A?&#034; to be answered in time O(P + log N), where P is the length of W and N is the length of A, which is competitive with (and in some cases slightly better than) suffix trees. The only drawback is that in those instances where the underlying alphabet is finite and small, suffix trees can be constructed in O(N) time in the worst case, versus O(N log N) time for suffix arrays. However, we give an augmented algorithm that, regardless of the alphabet size, constructs suffix arrays in O(N) expected time, albeit with lesser space efficiency. We believe that suffix arrays will prove to be better in practice than suffix trees for many applications.  
78|Linear pattern matching algorithms|In 1970, Knuth, Pratt, and Morris [1] showed how to do basic pattern matching in linear time. Related problems, such as those discussed in [4], have previously been solved by efficient but sub-optimal algorithms. In this paper, we introduce an interesting data structure called a bi-tree. A linear time algorithm  for obtaining a compacted version of a bi-tree associated with a given string is presented. With this construction as the basic tool, we indicate how to solve several pattern matching problems, including some from [4], in linear time. 
79|All-Against-All Sequence Matching |In this paper we present an algorithm which attempts to align pairs of subsequences from a database of DNA sequences. The algorithm simulates the classical dynamic programming alignment algorithm over a digital index of the database. The running time of the algorithm is subquadratic on average with respect to the database size. A similar algorithm solves the approximate string matching problem in sublinear average time. 1 Introduction  An all-against-all matching is defined as an attempt to find an alignment for each possible subsequence against each other possible subsequence in a DNA or peptide database. These alignments are done with the standard methods of dynamic programming (Needleman &amp; Wunsch algorithm [NW70]) and are based on a matrix describing the likelihood of homology between pairs of entries in the sequence (e.g. a Dayhoff matrix [DSO78]). The output of this matching will consist of all pairs of subsequences which achieve a given level of similarity. Note that we are not a...
81|The principles of psychology|This Thesis is brought to you for free and open access. It has been accepted for inclusion in University Honors Theses by an authorized administrator of
82|A Model of Saliency-based Visual Attention for Rapid Scene Analysis|A visual attention system, inspired by the behavior and the neuronal architecture of the early primate visual system, is presented. Multiscale image features are combined into a single topographical saliency map. A dynamical neural network then selects attended locations in order of decreasing saliency. The system breaks down the complex problem of scene understanding by rapidly selecting, in a computationally efficient manner, conspicuous locations to be analyzed in detail. Index terms: Visual attention, scene analysis, feature extraction, target detection, visual search. \Pi I. Introduction Primates have a remarkable ability to interpret complex scenes in real time, despite the limited speed of the neuronal hardware available for such tasks. Intermediate and higher visual processes appear to select a subset of the available sensory information before further processing [1], most likely to reduce the complexity of scene analysis [2]. This selection appears to be implemented in the ...
83|The Laplacian Pyramid as a Compact Image Code| We describe a technique for image encoding in which local operators of many scales but identical shape serve as the basis functions. The representation differs from established techniques in that the code elements are localized in spatial frequency as well as in space. Pixel-to-pixel correlations are first removed by subtracting a lowpass filtered copy of the image from the image itself. The result is a net data compression since the difference, or error, image has low variance and entropy, and the low-pass filtered image may represented at reduced sample density. Further data compression is achieved by quantizing the difference image. These steps are then repeated to compress the low-pass image. Iteration of the process at appropriately expanded scales generates a pyramid data structure. The encoding process is equivalent to sampling the image with Laplacian operators of many scales. Thus, the code tends to enhance salient image features. A further advantage of the present code is that it is well suited for many image analysis tasks as well as for image compression. Fast algorithms are described for coding and decoding. A
84|Shiftable Multi-scale Transforms|Orthogonal wavelet transforms have recently become a popular representation for multiscale signal and image analysis. One of the major drawbacks of these representations is their lack of translation invariance: the content of wavelet subbands is unstable under translations of the input signal. Wavelet transforms are also unstable with respect to dilations of the input signal, and in two dimensions, rotations of the input signal. We formalize these problems by defining a type of translation invariance that we call &#034;shiftability&#034;. In the spatial domain, shiftability corresponds to a lack of aliasing; thus, the conditions under which the property holds are specified by the sampling theorem. Shiftability may also be considered in the context of other domains, particularly orientation and scale. We explore &#034;jointly shiftable&#034; transforms that are simultaneously shiftable in more than one domain. Two examples of jointly shiftable transforms are designed and implemented: a one-dimensional tran...
85|Preattentive texture discrimination with early vision mechanisms|mechanisms
86|Sustained and transient components of focal visual attention|Abstract-Human observers fixated the center of a search array and were required to discriminate the color of an odd target if it was present. The array consisted of horizontal or vertical black or white bars. In the simple case, only orientation was necessary to define the odd target, whereas in the conjunctive case, both orientation and color were necessary. A cue located at the critical target position was either visible all the time (sustained cuing) or it appeared at a short variable delay before the array presentation (transient cuing). Sustained visual cuing enhanced perception greatly in the conjunctive, but not in the simple condition. Perception of the odd target in the conjunctive display was improved even further by transient cuing, and peak discrimination performance occurred if the cue preceded the target array by 70-150 msec. Longer delays led to a marked downturn in performance. Control experiments indicated that this transient attentional component was independent of the observers ’ prior knowledge of target position and was not subject to voluntary control. We provide evidence to suggest hat the transient component does not originate at the earliest stages of visual processing, since it could not be extended in duration by flickering the cue, nor did it require a local sensory transient o trigger its onset. Neither the variation in retinal eccentricity nor changing the paradigm to a vernier acuity task altered the basic pattern of results. Our findings indicate the existence of a sustained and a transient component of attention, and we hypothesize that of the two, the transient component is operative at an earlier stage of visual cortical processing. Focal attention Visual search Pattern recognition Vernier acuity
87|A Comparison of Feature Combination Strategies for Saliency-Based Visual Attention Systems|Bottom-up or saliency-based visual attention allows primates to detect non-specific conspicuous targets in cluttered scenes. A classical metaphor, derived from electrophysiological and psychophysical studies, describes attention as a rapidly shiftable &#034;spotlight&#034;. The model described here reproduces the attentional scanpaths of this spotlight: Simple multi-scale &#034;feature maps&#034; detect local spatial discontinuities in intensity, color, orientation or optical flow, and are combined into a unique &#034;master&#034; or &#034;saliency&#034; map. The saliency map is sequentially scanned, in order of decreasing saliency, by the focus of attention. We study the problem of combining feature maps, from different visual modalities and with unrelated dynamic ranges (such as color and motion), into a unique saliency map. Four combination strategies are compared using three databases of natural color images: (1) Simple normalized summation, (2) linear combination with learned weights, (3) global non-linear normalization...
88|An Active Vision Architecture based on Iconic Representations|Active vision systems have the capability of continuously interacting with the environment. The rapidly changing environment of such systems means that it is attractive to replace static representations with visual routines that compute information on demand. Such routines place a premium on image data structures that are easily computed and used. The purpose of this paper is to propose a general active vision architecture based on efficiently computable iconic representations. This architecture employs two primary visual routines, one for identifying the visual image near the fovea (object identification), and another for locating a stored prototype on the retina (object location). This design allows complex visual behaviors to be obtained by composing these two routines with different parameters. The iconic representations are comprised of high-dimensional feature vectors obtained from the responses of an ensemble of Gaussian derivative spatial filters at a number of orientations and...
89|Clustered intrinsic connections in cat visual cortex|The intrinsic connections of the cortex have long been known to run vertically, across the cortical layers. In the present study we have found that individual neurons in the cat primary visual cortex can communicate over suprisingly long distances horizontally (up to 4 mm), in directions parallel to the cortical surface. For all of the cells having widespread projections, the collaterals within their axonal fields were distributed in repeating clusters, with an average periodicity of 1 mm. This pattern of extensive clustered projections has been revealed by combining the techniques of intracellular recording and injection of horseradish peroxidase with three-dimensional computer graphic reconstructions. The clustering pattern was most apparent when the cells were rotated to present a view parallel to the cortical surface. The pattern was observed in more than half of the pyramidal and spiny stellate cells in the cortex and was seen in all cortical layers. In our sample, cells made distant connections within their own layer and/or within another layer. The axon of one cell had clusters covering the same area in two layers, and the clusters in the deeper layer were located under those in the upper layer, suggesting a relationship between the clustering phenomenon and columnar cortical architecture. Some pyramidal cells did not project into the white matter,
90|Overcomplete steerable pyramid filters and rotation invariance|A given (overcomplete) discrete oriented pyramid may be converted into a steerable pyramid by interpolation. We present a technique for deriving the optimal interpolation functions (otherwise called steering coefficients). The proposed scheme is demonstrated on a computationally efficient oriented pyramid, which is a variation on the Burt and Adelson pyramid. We apply the generated steerable pyramid to orientation-invarianttexture analysis to demonstrate its excellent rotational isotropy. High classification rates and precise rotation identification are demonstrated. 1
91|Functional anatomy of macaque striate cortex. V. Spatial frequency|Macaque monkeys were shown retinotopically-specific vi-sual stimuli during %-2-deoxy-&amp;glucose (DG) infusion in a study of the retinotopic organization of primary visual cortex (Vl). In the central half of VI, the cortical magnification was found to be greater along the vertical than along the hori-zontal meridian, and overall magnification factors appeared to be scaled proportionate to brain size across different species. The cortical magnification factor (CMF) was found to reach a maximum of about 15 mm/deg at the represen-tation of the fovea, at a point of acute curvature in the Vl-V2 border. We find neither a duplication nor an overrepre-sentation of the vertical meridian. The magnification factor did not appear to be doubled in a direction perpendicular to the ocular dominance strips; it may not be increased at all. The DG borders in parvorecipient layer 4Cb were found to
92|Eye position effects on visual, memory, and saccade-related activity in areas LIP and 7a of macaque|We studied the effect of eye position on the light-sensitive, memory, and saccade-related activities of neurons of the lateral intraparietal area and area 7a in the posterior parietal cortex of rhesus monkeys. A majority of the cells showed significant effects of eye position, for each of the 3 types of response. The direction tuning of the light-sensitive, memory and saccade responses did not change with eye position but the magnitude of the response did. Since previous work showed a similar effect for the light-sensitive response of area 7a neurons (Andersen and Mountcastle, 1983; Ander-sen et al., 1985b), the present results indicate that this mod-ulating effect of eye position may be a general one, as it is found in 3 types of responses in 2 cortical areas. Gain fields were mapped by measuring the effect of eye position on the magnitude of the response at 9 different eye positions for
93|Incorporating Prior Information in Machine Learning by Creating Virtual Examples|One of the key problems in supervised learning is the insufficient size of the training set. The natural way for an intelligent learner to counter this problem and successfully generalize is to exploit prior information that may be available about the domain or that can be learned from prototypical examples. We discuss the notion of using prior knowledge by creating virtual examples and thereby expanding the effective training set size. We show that in some contexts, this idea is mathematically equivalent to incorporating the prior knowledge as a regularizer, suggesting that the strategy is well-motivated. The process of creating virtual examples in real world pattern recognition tasks is highly non-trivial. We provide demonstrative examples from object recognition and speech recognition to illustrate the idea.  1 Learning from Examples  Recently, machine learning techniques have become increasingly popular as an alternative to knowledge-based approaches to artificial intelligence pro...
94|Control of selective visual attention: Modelling the “where” pathway|Intermediate and higher vision processes require selection of a sub-set of the available sensory information before further processing. Usually, this selection is implemented in the form of a spatially circumscribed region of the visual field, the so-called &#034;focus of at-tention &#034; which scans the visual scene dependent on the input and on the attentional state of the subject. We here present a model for the control of the focus of attention in primates, based on a saliency map. This mechanism is not only expected to model the functional-ity of biological vision but also to be essential for the understanding of complex scenes in machine vision. 1 Introduction: &#034;What &#034; and &#034;Where &#034; In Vision It is a generally accepted fact that the computations of early vision are massively parallel operations, i.e., applied in parallel to all parts of the visual field. This high degree of parallelism cannot be sustained in in~ermediate and higher vision because
95|Multimodal integration for the representation of space in the posterior parietal cortex|The posterior parietal cortex has long been considered an a`ssociation&#039;area that combines information from di¡erent sensory modalities to form a cognitive representation of space. However, until recently little has been known about the neural mechanisms responsible for this important cognitive process. Recent experi-ments from the author&#039;s laboratory indicate that visual, somatosensory, auditory and vestibular signals are combined in areas LIP and 7a of the posterior parietal cortex. The integration of these signals can repre-sent the locations of stimuli with respect to the observer and within the environment. Area MSTd combines visual motion signals, similar to those generated during an observer&#039;s movement through the environment, with eye-movement and vestibular signals. This integration appears to play a role in specifying the path on which the observer is moving. All three cortical areas combine di¡erent modalities into common spatial frames by using a gain-¢eld mechanism. The spatial representations in areas LIP and 7a appear to be important for specifying the locations of targets for actions such as eye movements or reaching; the spatial representation within area MSTd appears to be important for navigation and the perceptual stabi-lity of motion signals. 1.
96|Withdrawing attention at little or no cost: detection and discrimination tasks. Percept Psychophys|Weused a concurrent-task paradigm to investigate the attentional cost of simple visual tasks. As in earlier studies, we found that detecting a unique orientation in an array of oriented elements (&#034;pop-out&#034;) carries little or no attentional cost. Surprisingly, this is true at all levels of performance and holds even when pop-out is barely discriminable. Wediscuss this finding in the context of our previous re-port that the attentional cost of stimulus detection is strongly influenced by the presence and nature of other stimuli in the display (Braun, 1994b). For discrimination tasks, we obtained a similarly mixed outcome: Discrimination of letter shape carried a high attentional cost whereas discrimination of color and orientation did not. Taken together, these findings lead us to modify our earlier position on the at-tentional costs of detection and discrimination tasks (Sagi &amp; Julesz, 1985). We now believe that ob-servers enjoy a significant degree of &#034;ambient &#034; visual awareness outside the focus of attention, per-mitting them to both detect and discriminate certain visual information. We hypothesize that the information in question is selected by a competition for saliency at the level of early vision. It has long been recognized that visual perception is in-fluenced by the observer&#039;s attentional state (Helmholtz, 1850/1962; James, 1890/1981). Psychophysical studies show that attention enhances visual sensitivity for stim-uli that are relevant to the observer and his/her behavior
97|Gapped Blast and PsiBlast: a new generation of protein database search programs|The BLAST programs are widely used tools for searching protein and DNA databases for sequence similarities. For protein comparisons, a variety of definitional, algorithmic and statistical refinements described here permits the execution time of the BLAST programs to be decreased substantially while enhancing their sensitivity to weak similarities. A new criterion for triggering the extension of word hits, combined with a new heuristic for generating gapped alignments, yields a gapped BLAST program that runs at approximately three times the speed of the original. In addition, a method is introduced for automatically combining statistically significant alignments produced by BLAST into a position-specific score matrix, and searching the database using this matrix. The resulting Position-Specific Iterated BLAST (PSIBLAST) program runs at approximately the same speed per iteration as gapped BLAST, but in many cases is much more sensitive to weak but biologically relevant sequence similarities. PSI-BLAST is used to uncover several new and interesting members of the BRCT superfamily.
98|Searching Distributed Collections With Inference Networks|The use of information retrieval systems in networked environments raises a new set of issues that have received little attention. These issues include ranking document collections for relevance to a query, selecting the best set of collections from a ranked list, and merging the document rankings that are returned from a set of collections. This paper describes methods of addressing each issue in the inference network model, discusses their implementation in the INQUERY system, and presents experimental results demonstrating their effectiveness.  
99|Okapi at TREC-3|this document length correction factor is #global&#034;: it is added at the end, after the weights for the individual terms have been summed, and is independentofwhich terms match.
100|The INQUERY Retrieval System|As larger and more heterogeneous text databases become available, information retrieval research will depend on the development of powerful, efficient and flexible retrieval engines. In this paper, we describe a retrieval system (INQUERY) that is based on a probabilistic retrieval model and provides support for sophisticated indexing and complex query formulation. INQUERY has been used successfully with databases containing nearly 400,000 documents. 1 Introduction  The increasing interest in sophisticated information retrieval (IR) techniques has led to a number of large text databases becoming available for research. The size of these databases, both in terms of the number of documents in them, and the length of the documents that are typically full text, has presented significant challenges to IR researchers who are used to experimenting with two or three thousand document abstracts. In order to carry out research with different types of text representations, retrieval models, learni...
101|Evaluation of an Inference Network-Based Retrieval Model|The use of inference networks to support document retrieval is introduced. A network-based retrieval model is described and compared to conventional probabilistic and Boolean models. The performance of a retrieval system based on the inference network model is evaluated and compared to performance with conventional retrieval models,
102|Latent Semantic Indexing (LSI) and TREC-2  (1994) |this paper. The &#034;ltc&#034; weights were computed on this matrix. 3.2 SVD analysis
103|Information Retrieval Systems for Large Document Collections|Practical information retrieval systems must manage large volumes of data, often divided into several collections that may be held on separate machines. Techniques for locating matches to queries must therefore consider identification of probable collections as well as identification of documents that are probable answers. Furthermore, the large amounts of data involved motivates the use of compression, but in a dynamic environment compression is problematic, because as new text is added the compression model slowly becomes inappropriate. In this paper we describe solutions to both of these problems. We show that use of centralised blocked indexes can reduce overall query processing costs in a multi-collection environment, and that careful application of text compression techniques allow collections to grow by several orders of magnitude without recompression becoming necessary. 1 Introduction  Practical information systems are required to store many gigabytes of data while supporting ...
104|Distributed Indexing: A Scalable Mechanism for Distributed Information Retrieval|Despite blossoming computer network bandwidths and the emergence of hypertext and CD-ROM databases, little progress has been made towards uniting the world&#039;s library-style bibliographic databases. While a few advanced distributed retrieval systems can broadcast a query to hundreds of participating databases, experience shows that local users almost always clog library retrieval systems. Hence broadcast remote queries will clog nearly every system. The premise of this work is that broadcast-based systems do not scale to world-wide systems. This project describes an indexing scheme that will permit thorough yet efficient searches of millions of retrieval systems. Our architecture will work with an arbitrary number of indexing companies and information providers, and, in the market place, could provide economic incentive for cooperation between database and indexing services. We call our scheme distributed indexing, and believe it will help researchers disseminate and locate both publishe...
105|TREC-3 Ad-Hoc, Routing Retrieval and Thresholding Experiments using PIRCS|The PIRCS retrieval system has been upgraded in TREC-3 to handle the full English collections of 2 GB in an efficient manner. For ad-hoc retrieval, we use recurrent spreading of activation in our network to implement query learning and expansion based on the best-ranked subdocuments of an initial retrieval. We also augment our standard retrieval algorithm with a soft-Boolean component. For routing, we use learning from signal-rich short documents or subdocument segments. For the optional thresholding experiment, we tried two approaches to transforming retrieval status values (RSV&#039;s) so that they could be used to partition documents into retrieved and nonretrieved sets. The first method normalizes RSV&#039;s using a query self-retrieval score. The second, which requires training data, uses logistic regression to convert RSV&#039;s into estimates of probability of relevance. Overall, our results are highly competitive with those of other participants. 1. INTRODUCTION  PIRCS is an experimental info...
106|Controlled and automatic human information processing: II. Perceptual learning, automatic attending and a general theory|The two-process theory of detection, search, and attention presented by Schneider and Shiffrin is tested and extended in a series of experiments. The studies demonstrate the qualitative difference between two modes of information processing: automatic detection and controlled search. They trace the course of the learning of automatic detection, of categories, and of automaticattention responses. They show the dependence of automatic detection on attending responses and demonstrate how such responses interrupt controlled processing and interfere with the focusing of attention. The learning of categories is shown to improve controlled search performance. A general framework for human information processing is proposed; the framework emphasizes the roles of automatic and controlled processing. The theory is compared to and contrasted with extant models of search and attention.
107|Controlled and automatic human information processing|A two-process theory of human information processing is proposed and applied to detection, search, and attention phenomena. Automatic processing is activa-tion of a learned sequence of elements in long-term memory that is initiated by appropriate inputs and then proceeds automatically—without subject control, without stressing the capacity limitations of the system, and without necessarily demanding attention. Controlled processing is a temporary activation of a se-quence of elements that can be set up quickly and easily but requires attention, is capacity-limited (usually serial in nature), and is controlled by the subject. A series of studies using both reaction time and accuracy measures is presented, which traces these concepts in the form of automatic detection and controlled, search through the areas of detection, search, and attention. Results in these areas are shown to arise from common mechanisms. Automatic detection is shown to develop following consistent mapping of stimuli to responses over trials. Controlled search is utilized in varied-mapping paradigms, and in our studies, it takes the form of serial, terminating search. The approach resolves a number of apparent conflicts in the literature.
108|Test stimulus representation and experimental context effects in memory scanning |The 5s performed a memory-scanning task in which they indicated whether or not a given test stimulus (letter or picture) matched one of a previously memorized set of letters. The test stimuli presented during a given session were either exclusively letters (a letter session), exclusively pictures (a picture session), or a random sequence of both (a mixed session). Reactiontime functions relating response latency to the size of the memorized set of letters were plotted, and the data are discussed in the context of the scanning models previously proposed by S. Sternberg. The reaction time functions of letter sessions and picture sessions were found to be consistent with the exhaustive model for memory scanning. However, the functions for mixed sessions deviated markedly from the predictions of such a model. The context in which a scanning task is imbedded appears to have a substantial effect on reaction time functions. Evidence that scans of information stored in short-term memory are serial and exhaustive
109|Stimulus probability and stimulus set size in memory scanning|In many recent studies of speeded scanning of immediate memory, variations in the size of the positive set (s) were confounded with variations in the probability (P) of the individual items within the positive set: As s increased, P decreased. The present experiment sought to determine whether the effect on RT attributed to s could be accounted for by variations in P. This was accomplished by factorially varying both s and P. Probability effects were confined to items in the positive set and were insufficient to account for the effect of s. The results are discussed in terms of a model in which s and P affect different information-processing stages. The s affects the number of compari-sons between the encoded item and the items stored in the memory of the positive set, as proposed by Sternberg. The P affects response selection— information as to the particular digit that was presented is available to the mechanisms for response selection along with the knowledge that there was or was not a match. The response selection mechanisms are assumed to be biased in tune with the P values of the items within the positive set. The number of things that one has to think about and the expectancy as to the likelihood of occurrence of these things— stimulus number and stimulus probability —have long been regarded as fundamental variables in the study of cognition. The common rinding that longer RTs would be produced by an increase in the number of possible stimuli or a decrease in stimulus probability was a result that was compati-ble with most theories of stimulus recogni-tion. Discriminating among the various theoretical accounts for these effects has been a more elusive task. One class of models holds that variations in stimulus probability and stimulus num-ber affect only a single commodity such as information (in bits) or repetitions. Ex-amples of such models are those that posit
110|An algorithm for finding best matches in logarithmic expected time|An algorithm and data structure are presented for searching a file containing N records, each described by k real valued keys, for the m closest matches or nearest neighbors to a given query record. The computation required to organize the file is proportional to kNlogN. The expected number of records examined in each search is independent of the file size. The expected computation to perform each search is proportional-to 1ogN. Empirical evidence suggests that except for very small files, this algorithm is considerably faster than other methods.
111|Efficient Processing of Spatial Joins Using R-Trees|Abstract: In this paper, we show that spatial joins are very suitable to be processed on a parallel hardware platform. The parallel system is equipped with a so-called shared virtual memory which is well-suited for the design and implementation of parallel spatial join algorithms. We start with an algorithm that consists of three phases: task creation, task assignment and parallel task execu-tion. In order to reduce CPU- and I/O-cost, the three phases are processed in a fashion that pre-serves spatial locality. Dynamic load balancing is achieved by splitting tasks into smaller ones and reassigning some of the smaller tasks to idle processors. In an experimental performance compar-ison, we identify the advantages and disadvantages of several variants of our algorithm. The most efficient one shows an almost optimal speed-up under the assumption that the number of disks is sufficiently large. Topics: spatial database systems, parallel database systems 1
112|Hilbert R-tree: An Improved R-tree Using Fractals|We propose a new R-tree structure that outperforms all the older ones. The heart of the idea is to facilitate the deferred splitting approach in R-trees. This is done by proposing an ordering on the R-tree nodes. This ordering has to be &#039;good&#039;, in the sense that it should group &#039;similar &#039; data rectangles together, to minimize the area and perimeter of the resulting minimum bounding rectangles (MBRs). Following [19] we have chosen the so-called &#039;2D-c &#039; method, which sorts rectangles according to the Hilbert value of the center of the rectangles. Given the ordering, every node has a well-de ned set of sibling nodes; thus, we can use deferred splitting. By adjusting the split policy, the Hilbert R-tree can achieve as high utilization as desired. To the contrary, the R-tree has no control over the space utilization, typically achieving up to 70%. We designed the manipulation algorithms in detail, and we did a full implementation of the Hilbert R-tree. Our experiments show that the &#039;2-to-3 &#039; split policy provides a compromise between the insertion complexity and the search cost, giving up to 28 % savings over the R  tree [3] on real data. 1
113|Direct spatial search on pictorial databases using packed r-trees|Pictorial databases require efficient and duect spatml search based on the analog form of spatial obJects and relatlonshlps instead of search based on some cumbersome alphanumeric encodings of the pmtures R-trees (two-dimensional B-trees) are excellent devices for indexing spatial ObJects and relationships found on pictures Their most important feature 1s that they provide high level ObJect onented search rather than search based on the low level elements of spatial ObJects This paper presents an efficient initial packing technique for creatmg R-trees to index spatial ObJects Since pictorial databases are not update mtensive but rather static, the beneflts of this technique are very significant 1.
114|Refinements to Nearest-Neighbor Searching in k-Dimensional Trees| This note presents a simplification and generalization of an algorithm for searching k-dimensional trees for nearest neighbors reported by Friedman et al. I-3]. If the distance between records is measured using Lz, the Euclidean orm, the data structure used by the algorithm to determine the bounds of the search space can be simplified to a single number. Moreover, because distance measurements in L2 are rotationally invariant, the algorithm can be generalized to allow a partition plane to have an arbitrary orientation, rather than insisting that it be perpendicular to a coordinate axis, as in the original algorithm. When a k-dimensional tree is built, this plane can be found from the principal eigenvector f the covariance matrix of the records to be partitioned. These techniques and others yield variants of k-dimensional trees customized for specific applications. It is wrong to assume that k-dimensional trees guarantee that a nearest-neighbor query completes in logarithmic expected time. For small k, logarithmic behavior isobserved on all but tiny trees. However, for larger k, logarithmic behavior is achievable only with extremely large numbers of records. For k = 16, a search of a k-dimensional tree of 76,000 records examines almost every record.
115|Web Server Workload Characterization: The Search for Invariants (Extended Version)  (1996) |The phenomenal growth in popularity of the World Wide Web (WWW, or the Web) has made WWW traffic the largest contributor to packet and byte traffic on the NSFNET backbone. This growth has triggered recent research aimed at reducing the volume of network traffic produced by Web clients and servers, by using caching, and reducing the latency for WWW users, by using improved protocols for Web interaction. Fundamental to the goal of improving WWW performance is an understanding of WWW workloads. This paper presents a workload characterization study for Internet Web servers. Six different data sets are used in this study: three from academic environments, two from scientific research organizations, and one from a commercial Internet provider. These data sets represent three different orders of magnitude in server activity, and two different orders of magnitude in time duration, ranging from one week of activity to one year of activity. Throughout the study, emphasis is placed on finding wor...
116|On the Self-similar Nature of Ethernet Traffic (Extended Version)  (1994) | We demonstrate that Ethernet LAN traffic is statistically self-similar, that none of the commonly used traffic models is able to capture this fractal-like behavior, that such behavior has serious implications for the design, control, and analysis of high-speed, cell-based networks, and that aggregating streams of such traffic typically intensifies the self-similarity (“burstiness”) instead of smoothing it. Our conclusions are supported by a rigorous statistical analysis of hundreds of millions of high quality Ethernet traffic measurements collected between 1989 and 1992, coupled with a discussion of the underlying mathematical and statistical properties of self-similarity and their relationship with actual network behavior. We also present traffic models based on self-similar stochastic processes that provide simple, accurate, and realistic descriptions of traffic scenarios expected during B-ISDN deployment. 
117|Wide-Area Traffic: The Failure of Poisson Modeling|Network arrivals are often modeled as Poisson processes for analytic simplicity, even though a number of traffic studies have shown that packet interarrivals are not exponentially distributed. We evaluate 24 wide-area traces, investigating a number of wide-area TCP arrival processes (session and connection arrivals, FTP data connection arrivals within FTP sessions, and TELNET packet arrivals) to determine the error introduced by modeling them using Poisson processes. We find that user-initiated TCP session arrivals, such as remotelogin and file-transfer, are well-modeled as Poisson processes with fixed hourly rates, but that other connection arrivals deviate considerably from Poisson; that modeling TELNET packet interarrivals as exponential grievously underestimates the burstiness of TELNET traffic, but using the empirical Tcplib [Danzig et al, 1992] interarrivals preserves burstiness over many time scales; and that FTP data connection arrivals within FTP sessions come bunched into “connection bursts,” the largest of which are so large that they completely dominate FTP data traffic. Finally, we offer some results regarding how our findings relate to the possible self-similarity of widearea traffic.  
118|Characteristics of WWW Client-based Traces|The explosion of WWW traffic necessitates an accurate picture of WWW use, and in particular requires a good understanding of client requests for WWW documents. To address this need, we have collectedtraces of actual executions of NCSA Mosaic, reflecting over half a million user requests for WWW documents. In this paper we describe the methods we used to collect our traces, and the formats of the collected data. Next, we present a descriptive statistical summary of the traces we collected, which identifies a number of trends and reference patterns in WWW use. In particular, we show that many characteristics of WWW use can be modelled using power-law distributions, including the distribution of document sizes, the popularity of documents as a function of size, the distribution of user requests for documents, and the number of references to documents as a function of their overall rank in popularity (Zipf&#039;s law). Finally, we show how the power-law distributions derived from our traces can beused to guide system designers interested in caching WWW documents.
119|Empirically-Derived Analytic Models of Wide-Area TCP Connections: Extended Report|We analyze 2.5 million TCP connections that occurred during 14 wide-area traffic traces. The traces were gathered at five &#034;stub&#034; networks and two internetwork gateways, providing a diverse look at wide-area traffic. We derive analytic models describing the random variables associated with telnet, nntp, smtp, and ftp connections, and present a methodology for comparing the effectiveness of the analytic models with empirical models such as tcplib [DJ91]. Overall we find that the analytic models provide good descriptions, generally modeling the various distributions as well as empirical models and in some cases better.
120|A Caching Relay for the World Wide Web|We describe the design and performance of a caching relay for the World Wide Web. We model the distribution of requests for pages from the web and see how this distribution affects the performance of a cache. We use the data gathered from the relay to make some general characterizations about the web. (A version of this paper is available at http://www.research.digital.com/- SRC/personal/Steve Glassman/-  CachingTheWeb.html or .../CachingTheWeb.ps)  1 Overview  In January 1994, we set up a caching World Wide Web [10] relay for Digital Equipment Corporation &#039;s facilities in Palo Alto, California. We use a relay to reach the Web because Digital has a security firewall that restricts direct interaction between Digital internal computers and machines outside of Digital. We added caching to the relay because we wanted to improve the relay&#039;s performance and reduce its external network traffic. Clients use the relay for accessing the Web outside of Digital; requests for internal Digital pages...
121|Characteristics of Wide-Area TCP/IP Conversations|In this paper, we characterize wide-area network applications that use the TCP transport protocol. We also describe a new way to model the wide-area traffic generated by a stub network. We believe the traffic model presented here will be useful in studying congestion control, routing algorithms, and other resource management schemes for existing and future networks.  Our model is based on trace analysis of TCP/IP widearea internetwork traffic. We collected the TCP/IP packet headers of USC, UCB, and Bellcore networks at the point they connect with their respective regional access networks. We then wrote a handful of programs to analyze the traces. Our model characterizes individual TCP conversations by the distributions of: number of bytes transferred, duration, number of packets transferred, packet size, and packet interarrival time.  Our trace analysis shows that both interactive and bulk transfer traffic from all sites reflect a large number of short conversations. Similarly, it shows that a very large percentage of traffic is bidirectional, even for bulk transfer. We observed that interactive applications send significantly different amounts of data in each direction of a conversation, and that interarrival times for interactive applications closely follow a constant plus exponential model. Half of the conversations are directed to a handful of networks, but the other half are directed to hundreds of networks. Many of these observations contradict commonly held beliefs regarding wide-area traffic.  
122|Explaining World Wide Web Traffic Self-Similarity|Recently the notion of self-similarity has been shown to apply to wide-area and local-area network traffic. In this paper we examine the mechanisms that give rise to self-similar network traffic. We present an explanation for traffic self-similarity by using a particular subset of wide area traffic: traffic due to the World Wide Web (WWW). Using an extensive set of traces of actual user executions of NCSA Mosaic, reflecting over half a million requests for WWW documents, we show evidence that WWW traffic is selfsimilar. Then we show that the self-similarity in such traffic can be explained based on the underlying distributions of WWW document sizes, the effects of caching and user preference in file transfer, the effect of user &#034;think time&#034;, and the superimposition of many such transfers in a local area network. To do this we rely on empirically measured distributions both from our traces and from data independently collected at over thirty WWW sites. 1 Introduction  Understanding the ...
123|Growth Trends in Wide-Area TCP Connections|We analyze the growth of a medium-sized research laboratory &#039;s wide-area TCP connections over a period of more than two years. Our data consisted of six month-long traces of all TCP connections made between the site and the rest of the world. We find that smtp, ftp, and X11 traffic all exhibited exponential growth in the number of connections and bytes transferred, at rates significantly greater than that at which the site&#039;s overall computing resources grew; that individual users increasingly affected the site&#039;s traffic profile by making wide-area connections from background scripts; that the proportion of local computers participating in wide-area traffic outpaces the site&#039;s overall growth; that use of the network by individual computers appears to be constant for some protocols  (telnet) and growing exponentially for others (ftp, smtp);  and that wide-area traffic geography is diverse and dynamic. 1 Introduction  To properly design future networks, we need a thorough understanding of...
124|Application-level document caching in the Internet|With the increasing demand for document transfer services such as the World Wide Web comes a need for better resource management to reduce the latency of documents in these systems. To address this need, we analyze the potential for documentcaching at the application level in document transfer services. Wehave collected traces of actual executions of Mosaic, reflecting over half a million user requests for WWW documents. Using those traces, we study the tradeoffs between caching at three levels in the system, and the potential for use of application-level information in the caching system. Our traces show that while a high hit rate in terms of URLs is achievable, a muchlower hit rate is possible in terms of bytes, because most profitably-cached documents are small. We consider the performance of caching when applied at the level of individual user sessions, at the level of individual hosts, and at the level of a collection of hosts on a single LAN. We show that the performance gain achievable bycaching at the session level (which is straightforward to implement) is nearly all of that achievable at the LAN level (where caching is more difficult to implement). However, when resource requirements are considered, LAN level caching becomes much more desirable, since it can achieveagiven level of caching performance using a much smaller amountofcache space. Finally,we consider the use of organizational boundary information as an example of the potential for use of application-level information in caching. Our results suggest that distinguishing between documents produced locally and those produced remotely can provide useful leverage in designing caching policies, because of differences in the potential for sharing these two documenttypes among multiple users.
125|The effect of client caching on file server workloads|A distributed file systetn provides file service from one or rnore &amp;red file servers to a community of clierit workstations o’ver Q network. Wlaile the client-server paradigm has many advantages, it also presents new challenges to system designers concerning perfor-rnance and reliability. As both client workstations and file servers become increasingly well-resourced, a nu~n-ber of system design decisions need to be re-examined. This research concerus the caching of disk blocks in a distributed client-server enviromnent. Some recent re-search has suggested that various strategies for cache rnauagernent may not be equally suited to the circurn-stances at both the client and the server. Since any caching strategy is based on assumptions concerning the characteristics of the denland, the performance of the strategy is only as good as the accuracy of this assurnp-tion. The perfornlance of a caching strategy at a file server is strongly influenced by the presence of client cnches since these caches alter the characteristics of the stream of requests that reaches the server. This paper presents the results of an investigation of the effect of client caching on the nature of the server workload as a step towards understanding the performnnce of caching strategies at the server. The results demonstrate that client caches alter worklond characteristics in a way that will have CI profound impact on server cnche per-forrnance, and suggest worthwhile directions for future development of server caching strategies. 1
126|Using a Wide-Area File System Within the World-Wide Web|This paper proposes the use of a wide-area file system for storing and retrieving documents.  We demonstrate that most of the functionality of the World-Wide Web (WWW)  information service can be provided by storing documents in AFS. The approach addresses  several performance problems experienced by WWW servers and clients, such  as increased server and network load, network latency and inadequate security. In addition,  the mechanism demonstrates the value of a global, general purpose file sharing  system and the advantage of layering existing technologies.  1 Introduction  The dramatic increase in networked information access through the World-Wide Web [1] demonstrates the value of wide-area information sharing. However, the Web faces many challenges as it continues to scale to accommodate the ever increasing number of users. Among those are increased server and network load, network latency and inadequate security.  Several decades of distributed systems research address the proble...
127|A theory of memory retrieval|A theory of memory retrieval is developed and is shown to apply over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe-memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe-memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with experimental data. The theory is applied to four item recognition paradigms (Sternberg, prememorized list, study-test, and continuous) and to speed-accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme.  
128|Distinctive features, categorical perception, and probability learning: some applications of a neural model|A previously proposed model for memory based on neurophysiological considerations is reviewed. We assume that (a) nervous system activity is usefully represented as the set of simultaneous individual neuron activities in a group of neurons; (b) different memory traces make use of the same synapses; and (c) synapses associate two patterns of neural activity by incrementing synaptic connectivity proportionally to the product of pre- and postsynaptic activity, forming a matrix of synaptic connectivities. We extend this model by (a) introducing positive feedback of a set of neurons onto itself and (b) allowing the individual neurons to saturate. A hybrid model, partly analog and partly binary, arises. The system has certain characteristics reminiscent of analysis by distinctive features. Next, we apply the model to &amp;quot;categorical perception. &amp;quot; Finally, we discuss probability learning. The model can predict overshooting, recency data, and probabilities occurring in systems with more than two events with reasonably good accuracy. In the beginner&#039;s mind there are many possibilities, but in the expert&#039;s there are few. —Shunryu Suzuki 1970 I.
129|Retrieval processes in recognition memory|A method of analyzing reaction time data in recognition memory is presented, which uses an explicit model of latency distributions. This distributional method allows us to distinguish between processes in a way that the traditional measure, mean latency, can not. The behavior of latency distributions is described, and four experiments are reported that show how recognition accuracy and latency vary with independent variables such as study and test position, rate of presentation, and list length. These data are used to develop and test the empirical model. The resulting analyses, together with functional relationships derived from the experimental data, are used to test several theories of recognition memory. The theories examined all show problems in light of these stringent tests, and general properties required by a model to account for the data are suggested. As well as arguing for distributional analyses of reaction time data, this paper presents a wide range of phenomena that any theory of recognition memory must explain. Over the last few years, researchers have been developing theories of recognition memory based not only on accuracy measures but also on latency measures. In this article, we consider latency measures in recognition memory. Results from four experiments are presented, and an empirical model for latency distributions is developed. Latency distributions are shown to provide much more information than can be obtained from mean latency, the most common dependent variable in reaction time measurements. From this, a strong case is made for the study of distributional properties by showing how some current theories are inadequate or wrong when examined in the light of distributional analyses. These recent theories are further evaluated using functional relationships extracted from results of the four experiments presented.
130|A threshold theory for simple detection experiments |The two-state &#034;high &#034; threshold model is generalized by assuming that (with low probability) the threshold may be exceeded when there is no stimulus. Existing Yes-No data (that rejected the high threshold theory) are compatible with the resulting isosensitivity (ROC) curves, namely, 2 line segments that intersect at the true threshold prob-abilities. The corresponding 2-alternative forced-choice curve is a 45° line through this intersection. A simple learning process is suggested to predict S&#039;s location along these curves, asymptotic means are derived, and comparisons are made with data. These asymptotic biases are coupled with the von Bdk&amp;y-Stevens neural quantum model to show how the theoretical linear psychometric functions are distorted into nonsymmetric, nonlinear response curves. A classic postulate of psychophysics is that some stimuli or differences between stimuli never manage to affect the central decision making centers; others, of course, do. In a phrase, peripheral thresholds were assumed to exist. At least three types have been distinguished: absolute, difference, and detection. It is not, however, clear that there is any real difference among them. Absolute thresholds seem to be the same as detection ones except that the only noise is internal, and many difference threshold experiments differ from de-tection experiments only in the nature of the background stimulus, e.g., a pure tone or noise. Recently the literal interpretation of the threshold postulate has been
131|Chaff: Engineering an Efficient SAT Solver|Boolean Satisfiability is probably the most studied of combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in Electronic Design Automation (EDA), as well as in Artificial Intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff, which achieves significant performance gains through careful engineering of all aspects of the search – especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO.
132|GRASP: A Search Algorithm for Propositional Satisfiability|AbstractÐThis paper introduces GRASP (Generic seaRch Algorithm for the Satisfiability Problem), a new search algorithm for Propositional Satisfiability (SAT). GRASP incorporates several search-pruning techniques that proved to be quite powerful on a wide variety of SAT problems. Some of these techniques are specific to SAT, whereas others are similar in spirit to approaches in other fields of Artificial Intelligence. GRASP is premised on the inevitability of conflicts during the search and its most distinguishing feature is the augmentation of basic backtracking search with a powerful conflict analysis procedure. Analyzing conflicts to determine their causes enables GRASP to backtrack nonchronologically to earlier levels in the search tree, potentially pruning large portions of the search space. In addition, by ªrecordingº the causes of conflicts, GRASP can recognize and preempt the occurrence of similar conflicts later on in the search. Finally, straightforward bookkeeping of the causality chains leading up to conflicts allows GRASP to identify assignments that are necessary for a solution to be found. Experimental results obtained from a large number of benchmarks indicate that application of the proposed conflict analysis techniques to SAT algorithms can be extremely effective for a large number of representative classes of SAT instances. Index TermsÐSatisfiability, search algorithms, conflict diagnosis, conflict-directed nonchronological backtracking, conflict-based equivalence, failure-driven assertions, unique implication points. 1
133|Using CSP look-back techniques to solve real-world SAT instances|We report on the performance of an enhanced version of the “Davis-Putnam ” (DP) proof procedure for propositional satisfiability (SAT) on large instances derived from realworld problems in planning, scheduling, and circuit diagnosis and synthesis. Our results show that incorporating CSP lookback techniques-- especially the relatively new technique of relevance-bounded learning-- renders easy many problems which otherwise are beyond DP’s reach. Frequently they make DP, a systematic algorithm, perform as well or better than stochastic SAT algorithms such as GSAT or WSAT. We recommend that such techniques be included as options in implementations of DP, just as they are in systematic algorithms for the more general constraint satisfaction problem.
134|Evidence for Invariants in Local Search|It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies.
135|Improvements To Propositional Satisfiability Search Algorithms|... quickly across a wide range of hard SAT problems than any other SAT tester in the literature on comparable platforms. On a Sun SPARCStation 10 running SunOS 4.1.3 U1, POSIT can solve hard random 400-variable 3-SAT problems in about 2 hours on the average. In general, it can solve hard n-variable random 3-SAT problems with search trees of size O(2  n=18:7  ).  In addition to justifying these claims, this dissertation describes the most significant achievements of other researchers in this area, and discusses all of the widely known general techniques for speeding up SAT search algorithms. It should be useful to anyone interested in NP-complete problems or combinatorial optimization in general, and it should be particularly useful to researchers in either Artificial Intelligence or Operations Research. 
136|Using Randomization and Learning to Solve Hard Real-World Instances of Satisfiability|This paper addresses the interaction between randomization, with restart strategies, and learning, an often crucial technique for proving unsatisfiability. We use instances of SAT from the hardware verification domain to provide evidence that randomization can indeed be essential in solving real-world satis able instances of SAT. More interestingly, our results indicate that randomized restarts and learning may cooperate in proving both satisfiability and unsatisfiability. Finally, we utilize and expand the idea of algorithm portfolio design to propose an alternative approach for solving hard unsatisfiable instances of SAT.
137|The impact of branching heuristics in propositional satisfiability algorithms|Abstract. This paper studies the practical impact of the branching heuristics used in Propositional Satisfiability (SAT) algorithms, when applied to solving real-world instances of SAT. In addition, different SAT algorithms are experimentally evaluated. The main conclusion of this study is that even though branching heuristics are crucial for solving SAT, other aspects of the organization of SAT algorithms are also essential. Moreover, we provide empirical evidence that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics.
138|Improved algorithms for optimal winner determination in combinatorial auctions and generalizations|Combinatorial auctions can be used to reach efficient resource and task allocations in multiagent systems where the items are complementary. Determining the winners is NP-complete and inapproximable, but it was recently shown that optimal search algorithms do very well on average. This paper presents a more sophisticated search algorithm for optimal (and anytime) winner determination, including structural improvements that reduce search tree size, faster data structures, and optimizations at search nodes based on driving toward, identifying and solving tractable special cases. We also uncover a more general tractable special case, and design algorithms for solving it as well as for solving known tractable special cases substantially faster. We generalize combinatorial auctions to multiple units of each item, to reserve prices on singletons as well as combinations, and to combinatorial exchanges -- all allowing for substitutability. Finally, we present algorithms for determining the winners in these generalizations.
139|Computationally Manageable Combinatorial Auctions|There is interest in designing simultaneous auctions for situations in which the value of assets to a bidder depends upon which other assets he or she wins. In such cases, bidders may well wish to submit bids for combinations of assets. When this is allowed, the problem of determining the revenue maximizing set of nonconflicting bids can be a difficult one. We analyze this problem, identifying several different structures of combinatorial bids for which computational tractability is constructively demonstrated and some structures for which computational tractability  1 Introduction  Some auctions sell many assets simultaneously. Often these assets, like U.S. treasury bills, are interchangeable. However, sometimes the assets and the bids for them are distinct. This happens frequently, as in the U.S. Department of the Interior&#039;s simultaneous sales of off-shore oil leases, in some private farm land auctions, and in the Federal Communications Commission&#039;s recent multi-billion dollar sales...
140|Taming the computational complexity of combinatorial auctions: Optimal and approximate approaches|In combinatorial auctions, multiple goods are sold simultaneously and bidders may bid for arbitrary combinations of goods. Determining the outcome of such an auction is an optimization problem that is NP-complete in the general case. We propose two methods of overcoming this apparent intractability. The first method, which is guaranteed to be optimal, reduces running time by structuring the search space so that a modified depth-first search usually avoids even considering allocations that contain conflicting bids. Caching and pruning are also used to speed searching. Our second method is a heuristic, market-based approach. It sets up a virtual multi-round auction in which a virtual agent represents each original bid bundle and places bids, according to a fixed strategy, for each good in that bundle. We show through experiments on synthetic data that (a) our first method finds optimal allocations quickly and offers good anytime performance, and (b) in many cases our second method, despite lacking guarantees regarding optimality or running time, quickly reaches solutions that are nearly optimal. 1 Combinatorial Auctions Auction theory has received increasing attention from computer scientists in recent years. 1 One reason is the explosion of internet-based auctions. The use of auctions in business-to-business trades is also increasing rapidly [Cortese and Stepanek, 1998]. Within AI there is growing interest in using auction mechanisms to solve distributed resource allocation problems. For example, auctions and other market mechanisms are used in network bandwidth allocation, distributed configuration design, factory scheduling, and operating system memory allocation [Clearwater, 1996]. Market-oriented programming has
141|Limitations of the Vickrey Auction in Computational Multiagent Systems|Auctions provide an efficient distributed  mechanism for solving problems such as task  and resource allocation in multiagent systems.
142|eMediator: A Next Generation Electronic Commerce Server|This paper presents eMediator, an electronic commerce server prototype that demonstrates ways in which algorithmic support and game-theoretic incentive engineering can jointly improve the efficiency of ecommerce. eAuctionHouse, the configurable auction server, includes a variety of generalized combinatorial auctions and exchanges, pricing schemes, bidding languages, mobile agents, and user support for choosing an auction type. We introduce two new logical bidding languages for combinatorial markets: the XOR bidding language and the OR-of-XORs bidding language. Unlike the traditional OR bidding language, these are fully expressive. They therefore enable the use of the Clarke-Groves pricing mechanism for motivating the bidders to bid truthfully. eAuctionHouse also supports supply/demand curve bidding. eCommitter, the leveled commitment contract optimizer, determines the optimal contract price and decommitting penalties for a variety of leveled commitment contracting mechanisms, taking into account that rational agents will decommit strategically in Nash equilibrium. It also determines the optimal decommitting strategies for any given leveled commitment contract. eExchangeHouse, the safe exchange planner, enables unenforced anonymous exchanges by dividing the exchange into chunks and sequencing those chunks to be delivered safely in alternation between the buyer and the seller.
143|Some Tractable Combinatorial Auctions|Auctions are the most widely used strategic gametheoretic mechanism in the Internet. Auctions have been mostly studied from a game-theoretic and economic perspective, although recent work in AI and OR has been concerned with computational aspects of auctions as well. When faced from a computational perspective, combinatorial auctions are perhaps the most challenging type of auctions. Combinatorial auctions are auctions where agents may submit bids for bundles of goods. Given that finding an optimal allocation of the goods in a combinatorial auction is intractable, researchers have been concerned with exposing tractable instances of combinatorial auctions.
144|Networks versus Markets in International Trade|I propose a network/search view of international trade in differentiated products. I present evidence that supports the view that proximity and common language/colonial ties are more important for differentiated products than for products traded on organized exchanges in matching international buyers and sellers, and that search barriers to trade are higher for differentiated than for homogeneous products. I also discuss alternative
145|CONTINENTAL TRADING BLOCS: ARE THEY NATURAL, OR SUPER-NATURAL?|Using the gravity model, we find evidence of three continental trading blocs: the Americas, Europe and Pacific Asia. Intra-regional trade exceeds what can be explained by the proximity of a pair of countries, their sizes and GNP/capitas, and whether they share a common border or language. We then turn from the econometrics to the economic welfare implications. Krugman has supplied an argument against a three-bloc world, assuming no transport costs, and another argument in favor, assuming prohibitively high transportation costs between continents. We complete the model for the realistic case where intercontinental transport costs are neither prohibitive nor zero. If transport costs are low, continental Free Trade Areas can reduce welfare. We call such blocs super-natural. Partial liberalization is better than full liberalization within regional Preferential Trading Arrangements, despite the GATT&#039;s Article 24. The super-natural zone occurs when the regionalization of trade policy exceeds what is justified by natural factors.
146|Visual detection in relation to display size and redundancy of critical elements |Visual detection was studied in relation to displays of discrete elements, randomly selected consonant letters, distributed in random subsets of cells of a matrix, the subject being required on each trial to indicate only which member of a predesignated pair of critical elements was present in a given display. Experimental variables were number of elements per display and number of redundant critical elements per display. Estimates of the number of elements effectively processed by a subject during a 50 ms. exposure increased with display size, but not in the manner that would be expected if the subject sampled a fixed proportion of the elements present in a display of given area. Test-retest data indicated substantial correlations over long intervals of time in the particular elements sampled by a subject
147|Adaptive Constraint Satisfaction|Many different approaches have been applied to constraint satisfaction. These range from complete backtracking algorithms to sophisticated distributed configurations. However, most research effort in the field of constraint satisfaction algorithms has concentrated on the use of a single algorithm for solving all problems. At the same time, a consensus appears to have developed to the effect that it is unlikely that any single algorithm is always the best choice for all classes of problem. In this paper we argue that an adaptive approach should play an important part in constraint satisfaction. This approach relaxes the commitment to using a single algorithm once search commences. As a result, we claim that it is possible to undertake a more focused approach to problem solving, allowing for the correction of bad algorithm choices and for capitalising on opportunities for gain by dynamically changing to more suitable candidates.
148|Minimizing conflicts: a heuristic repair method for constraint satisfaction and scheduling problems| This paper describes a simple heuristic approach to solving large-scale constraint satisfaction and scheduling problems. In this approach one starts with an inconsistent assignment for a set of variables and searches through the space of possible repairs. The search can be guided by a value-ordering heuristic, the min-conflicts heuristic, that attempts to minimize the number of constraint violations after each step. The heuristic can be used with a variety of different search strategies. We demonstrate empirically that on the n-queens problem, a technique based on this approach performs orders of magnitude better than traditional backtracking techniques. We also describe a scheduling application where the approach has been used successfully. A theoretical analysis is presented both to explain why this method works well on certain types of problems and to predict when it is likely to be One of the most promising general approaches for solving combinatorial search problems is to generate an
149|Algorithms for Constraint-Satisfaction Problems: A Survey|A large number of problems in AI and other areas of computer science can be viewed as special cases of the constraint-satisfaction problem. Some examples are machine vision, belief maintenance, scheduling, temporal reasoning, graph problems, floor plan design, the planning of genetic experiments, and the satisfiability problem. A number of different approaches have been developed for solving these problems. Some of them use constraint propagation to simplify the original problem. Others use backtracking to directly search for possible solutions. Some are a combination of these two techniques. This article overviews many of these approaches in a tutorial fashion.  
150|Noise strategies for improving local search|It has recently been shown that local search issurprisingly good at nding satisfying assignments for certain computationally hard classes of CNF formulas. The performance of basic local search methods can be further enhanced by introducing mechanisms for escaping from local minima in the search space. We will compare three such mechanisms: simulated annealing, random noise, and a strategy called \mixed random walk&#034;. We show that mixed random walk is the superior strategy. Wealso present results demonstrating the e ectiveness of local search withwalk for solving circuit synthesis and circuit diagnosis problems. Finally, wedemonstrate that mixed random walk improves upon the best known methods for solving MAX-SAT problems.
151|Hybrid Algorithms for the Constraint Satisfaction Problem|problem (csp), namely, naive backtracking (BT), backjumping (BJ), conflict-directed backjumping
152|Domain-Independent Extensions to GSAT: Solving Large Structured Satisfiability Problems|GSAT is a randomized local search procedure  for solving propositional satisfiability  problems (Selman et al. 1992). GSAT can  solve hard, randomly generated problems that  are an order of magnitude larger than those  that can be handled by more traditional approaches  such as the Davis-Putnam procedure.  GSAT also efficiently solves encodings  of graph coloring problems, N-queens, and  Boolean induction. However, GSAT does not  perform as well on handcrafted encodings of  blocks-world planning problems and formulas  with a high degree of asymmetry. We  present three strategies that dramatically improve  GSAT&#039;s performance on such formulas.  These strategies, in effect, manage to uncover  hidden structure in the formula under considerations,  thereby significantly extending the  applicability of the GSAT algorithm.  
153|Practical Applications of Constraint Programming|Constraint programming is newly flowering in industry. Several companies have recently started up to exploit the technology, and the number of industrial applications is now growing very quickly. This survey will seek, by examples,
154|Genet: A connectionist architecture for solving constraint satisfaction problems by iterative improvement|New approaches to solving constraint satisfaction problems using iterative improvement techniques have been found to be successful on certain, very large problems such as the million queens. However, on highly constrained problems it is possible for these methods to get caught in local minima. In this paper we present genet, a connectionist architecture for solving binary and general constraint satisfaction problems by iterative improvement. genet incorporates a learning strategy to escape from local minima. Although genet has been designed to be implemented on vlsi hardware, we present empirical evidence to show that even when simulated on a single processor genet can outperform existing iterative improvement techniques on hard instances of certain constraint satisfaction problems.
155|The Birth of Prolog|The programming language, Prolog, was born of a project aimed not at producing a programming language but at processing natural languages; in this case, French. The project gave rise to a preliminary version of Prolog at the end of 1971 and a more definitive version at the end of 1972. This article gives the history of this project and describes in detail the preliminary and then the final versions of Prolog. The authors also felt it appropriate to describe the Q-systems since it was a language which played a prominent part in Prolog’s genesis.
156|Guided local search and its application to the traveling salesman problem|The Traveling Salesman Problem (TSP) is one of the most famous problems in combinatorial optimization. In this paper, we are going to examine how the techniques of Guided Local Search (GLS) and Fast Local Search (FLS) can be applied to the problem. Guided Local Search sits on top of local search heuristics and has as a main aim to guide these procedures in exploring efficiently and effectively the vast search spaces of combinatorial optimization problems. Guided Local Search can be combined with the neighborhood reduction scheme of Fast Local Search which significantly speeds up the operations of the algorithm. The combination of GLS and FLS with TSP local search heuristics of different efficiency and effectiveness is studied in an effort to determine the dependence of GLS on the underlying local search heuristic used. Comparisons are made with some of the best TSP heuristic algorithms and general optimization techniques which demonstrate the advantages of GLS over alternative heuristic approaches suggested for the problem.
157|An Empirical Analysis of Search in GSAT|We describe an extensive study of search in GSAT, an approximation procedure for  propositional satisfiability. GSAT performs greedy hill-climbing on the number of satisfied  clauses in a truth assignment. Our experiments provide a more complete picture of GSAT&#039;s  search than previous accounts. We describe in detail the two phases of search: rapid hillclimbing  followed by a long plateau search. We demonstrate that when applied to randomly  generated 3-SAT problems, there is a very simple scaling with problem size for both the  mean number of satisfied clauses and the mean branching rate. Our results allow us to  make detailed numerical conjectures about the length of the hill-climbing phase, the average  gradient of this phase, and to conjecture that both the average score and average branching  rate decay exponentially during plateau search. We end by showing how these results can  be used to direct future theoretical analysis. This work provides a case study of how  computer experiments can be used to improve understanding of the theoretical properties  of algorithms.  1. 
159|Adaptive Constraint Satisfaction: The Quickest First Principle|The choice of a particular algorithm for solving a given class of constraint satisfaction problems is often confused by exceptional behaviour of algorithms. One method of reducing the impact of this exceptional behaviour is to adopt an adaptive philosophy to constraint satisfaction problem solving. In this report we describe one such adaptive algorithm, based on the principle of chaining. It is designed to avoid the phenomenon of exceptionally hard problem instances. Our algorithm shows how the speed of more naïve algorithms can be utilised safe in the knowledge that the exceptional behaviour can be bounded. Our work clearly demonstrates the potential benefits of the adaptive approach and opens a new front of research for the constraint satisfaction community.
160|Partial constraint satisfaction problems and guided local search|A largely unexplored aspect of Constraint Satisfaction Problem (CSP) is that of over-constrained instances for which no solution exists that satisfies all the constraints. In these problems, mentioned in the literature as Partial Constraint Satisfaction Problems (PCSPs), we are often looking for solutions which violate the minimum number of constraints. In more realistic settings, constraints violations incur different costs and solutions are sought that minimize the total cost from constraint violations and possibly other criteria. Problems in this category present enormous difficulty to complete search algorithms. In practical terms, complete search has more or less to resemble the traditional Branch and Bound taking no advantage of the efficient pruning techniques recently developed for CSPs. In this report, we examine how the stochastic search method of Guided Local Search (GLS) can be applied to these problems. The effectiveness of the method is demonstrated on instances of the Radio Link Frequency Assignment Problem (RLFAP), which is a real-world Partial CSP.
161|Solving Constraint Satisfaction Problems with Heuristic-based Evolutionary Algorithms|Evolutionary algorithms (EAs) for solving constraint satisfaction problems  (CSPs) can be roughly divided into two classes: EAs using adaptive  fitness functions and EAs using heuristics. In [5] the most effective EAs of the  first class have been compared experimentally using a large set of benchmark  instances consisting of randomly generated binary CSPs. In this paper we  complete this comparison by studying the most effective EAs that use heuristics.
162|Constraint Logic Programming for Scheduling and Planning|This paper provides an introduction to Finite-domain Constraint Logic Programming (CLP) and its application to problems in scheduling and planning. We cover the fundamentals of CLP and indicate recent developments and trends in the field. Some current limitations are identified, and areas of research that may contribute to addressing these limitations are suggested.
163|An attempt to map the performance of a range of algorithm and heuristic combinations|Constraint satisfaction is the core of many AI and real life problems and much research has been done in this field in recent years. Work has been done in the past on comparing the performance of different algorithms and heuristics. Much of such work has focused on finding &#034;the best&#034; algorithm and heuristic combination for all problems. The objective of this paper is to prove that there is no universally best algorithm and heuristic for all problems-- different problems can be solved most efficiently by different algorithm and heuristic combinations. The implication of this is important because it means that instead of trying to find &#034;the best &#034; algorithms and heuristics, future research should try to identify the application domain of each algorithm and heuristic (i.e. when they are most effective). Furthermore our results point to future research which focuses on how to retrieve the most efficient algorithm for a given problem. The results in this paper provide a first step towards achieving such goals. 
164|On the Selection of Constraint Satisfaction Problem Formulations|This paper outlines a possible method for discriminating between formulations of the same problem. We attempt to relate different ZDC formulations in terms of their relative difficulty. This difficulty is quantified in terms of a new measure known as the T-factor. The result of our work is to demonstrate that in some cases, when very different formulations of the same problem exist, it is possible to identify the formulation that is most likely to be easiest to solve. In the next section we define the T-factors of formulations. In section 3 we present alternative formulations of the well known N-Queens and Zebra problems, together with evaluation of their T-factors. Finally in section 4 we discuss our findings and propose directions for future work.
165|Tackling car sequencing problems using a generic genetic algorithm|The car sequencing problem (CarSP) was seen as a challenge to artificial intelligence. The CarSP is a version of the job-shop scheduling problem which is known to be NP-complete. The task in the CarSP is to schedule a given number of cars (of different types) in a sequence to allow the teams in each work station on the assembly line to fit the required options (e.g. radio, sunroof) on the cars within the capacity of that work station. In unsolvable problems, one would like to minimize the penalties associated to the violation of the capacity constraints. Previous attempts to tackle the problem have either been unsuccessful or restricted to solvable CarSPs only. In this paper, we report on promising results in applying a generic genetic algorithm, which we call GAcSP, to tackle both solvable and unsolvable CarSPs.
166|Ng-Backmarking - an Algorithm for Constraint Satisfaction|Ng-backmarking with Min-conflict repair, a hybrid algorithm for solving constraint satisfaction problems, is presented in the context of the four main approaches to constraint satisfaction and optimisation: tree-search, domainfiltering, solution repair, and learning while searching. Repair-based techniques are often designed to use local gradients to direct the search for a solution to a constraint satisfaction problem. It has been shown experimentally that such techniques are often well suited to solving large scale problems. One drawback is that they do not guarantee a (optimal) solution if one exists. The motivation behind ng-backmarking is to allow the search to follow local gradients in the search space whilst ensuring a (optimal) solution if one exists. The search space of this combined approach is controlled by the ng-backmarking process, a method of learning constraints during search (at each failure point  1  ) that may be used to avoid the repeated traversing of failed paths ...
167|The architecture of complexity|A number of proposals have been advanced in recent years for the development of “general systems theory ” that, abstracting from properties peculiar to physical, biological, or social systems, would be applicable to all of them. 1 We might well feel that, while the goal is laudable, systems of such diverse kinds could hardly be expected to have any nontrivial properties in common. Metaphor and analogy can be helpful, or they can be misleading. All depends on whether the similarities the metaphor captures are significant or superficial. It may not be entirely vain, however, to search for common properties among diverse kinds of complex systems. The ideas that go by the name of cybernetics constitute, if not a theory, at least a point of view that has been proving fruitful over a wide range of applications. 2 It has been useful to look at the behavior of adaptive systems in terms of the concepts of feedback and homeostasis, and to analyze adaptiveness in terms of the theory of selective information. 3 The ideas of feedback and information provide a frame of reference for viewing a wide range of situations, just as do the ideas of evolution, of relativism, of axiomatic method, and of
168|An Efficient Boosting Algorithm for Combining Preferences|The problem of combining preferences arises in several applications, such as combining the results of different search engines. This work describes an efficient algorithm for combining multiple preferences. We first give a formal framework for the problem. We then describe and analyze a new boosting algorithm for combining preferences called RankBoost. We also describe an efficient implementation of the algorithm for certain natural cases. We discuss two experiments we carried out to assess the performance of RankBoost. In the first experiment, we used the algorithm to combine different WWW search strategies, each of which is a query expansion for a given domain. For this task, we compare the performance of RankBoost to the individual search strategies. The second experiment is a collaborative-filtering task for making movie recommendations. Here, we present results comparing RankBoost to nearest-neighbor and regression algorithms.
169|The Nature of Statistical Learning Theory| Statistical learning theory was introduced in the late 1960’s. Until the 1990’s it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990’s new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more
170|Support-Vector Networks|The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the supportvector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.
171|Bagging Predictors|Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy. 1. Introduction A learning set of L consists of data f(y n ; x n ), n = 1; : : : ; Ng where the y&#039;s are either class labels or a numerical response. We have a procedure for using this learning set to form a predictor &#039;(x; L) --- if the input is x we ...
173|Experiments with a New Boosting Algorithm|In an earlier paper, we introduced a new “boosting” algorithm called AdaBoost which, theoretically, can be used to significantly reduce the error of any learning algorithm that consistently generates classifiers whose performance is a little better than random guessing. We also introduced the related notion of a “pseudo-loss ” which is a method for forcing a learning algorithm of multi-label conceptsto concentrate on the labels that are hardest to discriminate. In this paper, we describe experiments we carried out to assess how well AdaBoost with and without pseudo-loss, performs on real learning problems. We performed two sets of experiments. The first set compared boosting to Breiman’s “bagging ” method when used to aggregate various classifiers (including decision trees and single attribute-value tests). We compared the performance of the two methods on a collection of machine-learning benchmarks. In the second set of experiments, we studied in more detail the performance of boosting using a nearest-neighbor classifier on an OCR problem. 
174|A Theory of the Learnable|  Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems.
175|A training algorithm for optimal margin classifiers|A training algorithm that maximizes the margin between the training patterns and the decision boundary is presented. The technique is applicable to a wide variety of classifiaction functions, including Perceptrons, polynomials, and Radial Basis Functions. The effective number of parameters is adjusted automatically to match the complexity of the problem. The solution is expressed as a linear combination of supporting patterns. These are the subset of training patterns that are closest to the decision boundary. Bounds on the generalization performance based on the leave-one-out method and the VC-dimension are given. Experimental results on optical character recognition problems demonstrate the good generalization obtained when compared with other learning algorithms.  
176|Additive Logistic Regression: a Statistical View of Boosting|Boosting (Freund &amp; Schapire 1996, Schapire &amp; Singer 1998) is one of the most important recent developments in classification methodology. The performance of many classification algorithms can often be dramatically improved by sequentially applying them to reweighted versions of the input data, and taking a weighted majority vote of the sequence of classifiers thereby produced. We show that this seemingly mysterious phenomenon can be understood in terms of well known statistical principles, namely additive modeling and maximum likelihood. For the two-class problem, boosting can be viewed as an approximation to additive modeling on the logistic scale using maximum Bernoulli likelihood as a criterion. We develop more direct approximations and show that they exhibit nearly identical results to boosting. Direct multi-class generalizations based on multinomial likelihood are derived that exhibit performance comparable to other recently proposed multi-class generalizations of boosting in most...
177|GroupLens: An Open Architecture for Collaborative Filtering of Netnews|Collaborative filters help people make choices based on the opinions of other people. GroupLens is a system for collaborative filtering of netnews, to help people find articles they will like in the huge stream of available articles. News reader clients display predicted scores and make it easy for users to rate articles after they read them. Rating servers, called Better Bit Bureaus, gather and disseminate the ratings. The rating servers predict scores based on the heuristic that people who agreed in the past will probably agree again. Users can protect their privacy by entering ratings under pseudonyms, without reducing the effectiveness of the score prediction. The entire architecture is open: alternative software for news clients and Better Bit Bureaus can be developed independently and can interoperate with the components we have developed.
178|Empirical Analysis of Predictive Algorithm for Collaborative Filtering|1
179|Social Information Filtering: Algorithms for Automating &#034;Word of Mouth&#034;|This paper describes a technique for making personalized recommendations from any type of database to a user based on similarities between the interest profile of that user and those of other users. In particular, we discuss the implementation of a networked system called Ringo, which makes personalized recommendations for music albums and artists. Ringo&#039;s database of users and artists grows dynamically as more people use the system and enter more information. Four different algorithms for making recommendations by using social information filtering were tested and compared. We present quantitative and qualitative results obtained from the use of Ringo by more than 2000 people.
180|Boosting the margin: A new explanation for the effectiveness of voting methods|One of the surprising recurring phenomena observed in experiments with boosting is that the test error of the generated classifier usually does not increase as its size becomes very large, and often is observed to decrease even after the training error reaches zero. In this paper, we show that this phenomenon is related to the distribution of margins of the training examples with respect to the generated voting classification rule, where the margin of an example is simply the difference between the number of correct votes and the maximum number of votes received by any incorrect label. We show that techniques used in the analysis of Vapnik’s support vector classifiers and of neural networks with small weights can be applied to voting methods to relate the margin distribution to the test error. We also show theoretically and experimentally that boosting is especially effective at increasing the margins of the training examples. Finally, we compare our explanation to those based on the bias-variance decomposition.  
181|The Weighted Majority Algorithm|We study the construction of prediction algorithms in a situation in which a learner faces a sequence of trials, with a prediction to be made in each, and the goal of the learner is to make few mistakes. We are interested in the case that the learner has reason to believe that one of some pool of known algorithms will perform well, but the learner does not know which one. A simple and effective method, based on weighted voting, is introduced for constructing a compound algorithm in such a circumstance. We call this method the Weighted Majority Algorithm. We show that this algorithm is robust in the presence of errors in the data. We discuss various versions of the Weighted Majority Algorithm and prove mistake bounds for them that are closely related to the mistake bounds of the best algorithms of the pool. For example, given a sequence of trials, if there is an algorithm in the pool A that makes at most m mistakes then the Weighted Majority Algorithm will make at most c(log jAj + m) mi...
182|The Strength of Weak Learnability|Abstract. This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a Source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent. A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error e.
183|An experimental comparison of three methods for constructing ensembles of decision trees|Abstract. Bagging and boosting are methods that generate a diverse ensemble of classifiers by manipulating the training data given to a “base ” learning algorithm. Breiman has pointed out that they rely for their effectiveness on the instability of the base learning algorithm. An alternative approach to generating an ensemble is to randomize the internal decisions made by the base algorithm. This general approach has been studied previously by Ali and Pazzani and by Dietterich and Kong. This paper compares the effectiveness of randomization, bagging, and boosting for improving the performance of the decision-tree algorithm C4.5. The experiments show that in situations with little or no classification noise, randomization is competitive with (and perhaps slightly superior to) bagging but not as accurate as boosting. In situations with substantial classification noise, bagging is much better than boosting, and sometimes better than randomization.
184|Boosting a Weak Learning Algorithm By Majority|We present an algorithm for improving the accuracy of algorithms for learning binary concepts. The improvement is achieved by combining a large number of hypotheses, each of which is generated by training the given learning algorithm on a different set of examples. Our algorithm is based on ideas presented by Schapire in his paper &#034;The strength of weak learnability&#034;, and represents an improvement over his results. The analysis of our algorithm provides general upper bounds on the resources required for learning in Valiant&#039;s polynomial PAC learning framework, which are the best general upper bounds known today. We show that the number of hypotheses that are combined by our algorithm is the smallest number possible. Other outcomes of our analysis are results regarding the representational power of threshold circuits, the relation between learnability and compression, and a method for parallelizing PAC learning algorithms. We provide extensions of our algorithms to cases in which the conc...
185|Learning to Order Things|There are many applications in which it is desirable to order rather than classify  instances. Here we consider the problem of learning how to order, given feedback  in the form of preference judgments, i.e., statements to the effect that one instance  should be ranked ahead of another. We outline a two-stage approach in which one  first learns by conventional means a preference function, of the form PREF(u; v),  which indicates whether it is advisable to rank u before v. New instances are  then ordered so as to maximize agreements with the learned preference function.  We show that the problem of finding the ordering that agrees best with  a preference function is NP-complete, even under very restrictive assumptions.  Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a  good approximation. We then discuss an on-line learning algorithm, based on the  &#034;Hedge&#034; algorithm, for finding a good linear combination of ranking &#034;experts.&#034;  We use the ordering algorith...
186|Cryptographic Limitations on Learning Boolean Formulae and Finite Automata|In this paper we prove the intractability of learning several classes of Boolean functions in the distribution-free model (also called the Probably Approximately Correct or PAC model) of learning from examples. These results are representation independent, in that they hold regardless of the syntactic form in which the learner chooses to represent its hypotheses. Our methods reduce the problems of cracking a number of well-known public-key cryptosystems to the learning problems. We prove that a polynomial-time learning algorithm for Boolean formulae, deterministic finite automata or constant-depth threshold circuits would have dramatic consequences for cryptography and number theory: in particular, such an algorithm could be used to break the RSA cryptosystem, factor Blum integers (composite numbers equivalent to 3 modulo 4), and detect quadratic residues. The results hold even if the learning algorithm is only required to obtain a slight advantage in prediction over random guessing. The techniques used demonstrate an interesting duality between learning and cryptography. We also apply our results to obtain strong intractability results for approximating a generalization of graph coloring.
187|Discriminative Reranking for Natural Language Parsing|This article considers approaches which rerank the output of an existing probabilistic parser. The base parser produces a set of candidate parses for each input sentence, with associated probabilities that define an initial ranking of these parses. A second model then attempts to improve upon this initial ranking, using additional features of the tree as evidence. The strength of our approach is that it allows a tree to be represented as an arbitrary set of features, without concerns about how these features interact or overlap and without the need to define a derivation or a generative model which takes these features into account. We introduce a new method for the reranking task, based on the boosting approach to ranking problems described in Freund et al. (1998). We apply the boosting method to parsing the Wall Street Journal treebank. The method combined the log-likelihood under a baseline model (that of Collins [1999]) with evidence from an additional 500,000 features over parse trees that were not included in the original model. The new model achieved 89.75 % F-measure, a 13 % relative decrease in F-measure error over the baseline model’s score of 88.2%. The article also introduces a new algorithm for the boosting approach which takes advantage of the sparsity of the feature space in the parsing data. Experiments show significant efficiency gains for the new algorithm over the obvious implementation of the boosting approach. We argue that the method is an appealing alternative—in terms of both simplicity and efficiency—to work on feature selection methods within log-linear (maximum-entropy) models. Although the experiments in this article are on natural language parsing (NLP), the approach should be applicable to many other NLP problems which are naturally framed as ranking tasks, for example, speech recognition, machine translation, or natural language generation.  
188|Bagging, Boosting, and C4.5|Breiman&#039;s bagging and Freund and Schapire&#039;s  boosting are recent methods for improving the predictive power of classifier learning systems. Both form a set of classifiers that are combined by voting, bagging by generating replicated bootstrap samples of the data, and boosting by adjusting the weights of training instances. This paper reports results of applying both techniques to a system that learns decision trees and testing on a representative collection of datasets. While both approaches substantially improve predictive accuracy, boosting shows the greater benefit. On the other hand, boosting also produces severe degradation on some datasets. A small change to the way that boosting combines the votes of learned classifiers reduces this downside and also leads to slightly better results on most of the datasets considered. Introduction  Designers of empirical machine learning systems are concerned with such issues as the computational cost of the learning method and the accuracy and ...
189|Pranking with Ranking|We discuss the problem of ranking instances. In our framework each instance is associated with a rank or a rating, which is an integer from 1 to k. Our goal is to find a rank-prediction rule that assigns each instance a rank which is as close as possible to the instance&#039;s true rank. We describe a simple and efficient online algorithm, analyze its performance in the mistake bound model, and prove its correctness. We describe two sets of experiments, with synthetic data and with the EachMovie dataset for collaborative filtering. In the experiments we performed, our algorithm outperforms online algorithms for regression and classification applied to ranking.
190|The Sample Complexity of Pattern Classification With Neural Networks: The Size of the Weights is More Important Than the Size of the Network|Sample complexity results from computational learning theory, when applied to neural network learning for pattern classification problems, suggest that for good generalization performance the number of training examples should grow at least linearly with the number of adjustable parameters in the network. Results in this paper show that if a large neural network is used for a pattern classification problem and the learning algorithm finds a network with small weights that has small squared error on the training patterns, then the generalization performance depends on the size of the weights rather than the number of weights. For example, consider a two-layer feedforward network of sigmoid units, in which the sum of the magnitudes of the weights associated with each unit is bounded by A and the input dimension is n. We show that the misclassification probability is no more than a certain error estimate (that is related to squared error on the training set) plus A³ p  (log n)=m (ignori...
191|Bias plus variance decomposition for zero-one loss functions|We present a bias-variance decomposition of expected misclassi cation rate, the most commonly used loss function in supervised classi cation learning. The bias-variance decomposition for quadratic loss functions is well known and serves as an important tool for analyzing learning algorithms, yet no decomposition was o ered for the more commonly used zero-one (misclassi cation) loss functions until the recent work of Kong &amp; Dietterich (1995) and Breiman (1996). Their decomposition su ers from some major shortcomings though (e.g., potentially negative variance), which our decomposition avoids. We show that, in practice, the naive frequency-based estimation of the decomposition terms is by itself biased and show how to correct for this bias. We illustrate the decomposition on various algorithms and datasets from the UCI repository. 1
192|Error-Correcting Output Coding Corrects Bias and Variance|Previous research has shown that a technique called error-correcting output coding (ECOC) can dramatically improve the classification accuracy of supervised learning algorithms that learn to classify data points into one of k AE 2 classes. This paper presents an investigation of why the ECOC technique works, particularly when employed with decision-tree learning algorithms. It shows that the ECOC method--- like any form of voting or committee---can reduce the variance of the learning algorithm. Furthermore---unlike methods that simply combine multiple runs of the same learning algorithm---ECOC can correct for errors caused by the bias of the learning algorithm. Experiments show that this bias correction ability relies on the non-local behavior of C4.5. 1 Introduction  Error-correcting output coding (ECOC) is a method for applying binary (two-class) learning algorithms to solve k-class supervised learning problems. It works by converting the k-class supervised learning problem into a la...
193|Adaptive game playing using multiplicative weights|We present a simple algorithm for playing a repeated game. We show that a player using this algorithm suffers average loss that is guaranteed to come close to the minimum loss achievable by any fixed strategy. Our bounds are nonasymptotic and hold for any opponent. The algorithm, which uses the multiplicative-weight methods of Littlestone and Warmuth, is analyzed using the Kullback–Liebler divergence. This analysis yields a new, simple proof of the min–max theorem, as well as a provable method of approximately solving a game. A variant of our game-playing algorithm is proved to be optimal in a very strong sense.  
194|Automatic Combination of Multiple Ranked Retrieval Systems|Retrieval performance can often be improved significantly by using a number of different retrieval algorithms and combining the results, in contrast to using just a single retrieval algorithm. This is because different retrieval algorithms, or retrieval experts, often emphasize different document and query features when determining relevance and therefore retrieve different sets of documents. However, it is unclear how the different experts are to be combined, in general, to yield a superior overall estimate. We propose a method by which the relevance estimates made by different experts can be automatically combined to result in superior retrieval performance. We apply the method to two expert combination tasks. The applications demonstrate that the method can identify high performance combinations of experts and also is a novel means for determining the combined effectiveness of experts.  1 Introduction  In text retrieval, two heads are definitely better than one. Retrieval performanc...
195|Game Theory, On-line Prediction and Boosting|We study the close connections between game theory, on-line prediction and boosting. After a brief review of game theory, we describe an algorithm for learning to play repeated games based on the on-line prediction methods of Littlestone and Warmuth. The analysis of this algorithm yields a simple proof of von Neumann’s famous minmax theorem, as well as a provable method of approximately solving a game. We then show that the on-line prediction model is obtained by applying this game-playing algorithm to an appropriate choice of game and that boosting is obtained by applying the same algorithm to the “dual” of this game. 
196|A Game of Prediction with Expert Advice|We consider the following problem. At each point of discrete time the learner must make a prediction; he is given the predictions made by a pool of experts. Each prediction and the outcome, which is disclosed after the learner has made his prediction, determine the incurred loss. It is known that, under weak regularity, the learner can ensure that his cumulative loss never exceeds cL+ a ln n, where c and a are some constants, n is the size of the pool, and L is the cumulative loss incurred by the best expert in the pool. We find the set of those pairs (c; a) for which this is true.
197|Pruning Adaptive Boosting|The boosting algorithm AdaBoost, developed by Freund and Schapire, has exhibited outstanding performance on several benchmark problems when using C4.5 as the &#034;weak&#034; algorithm to be &#034;boosted.&#034; Like other ensemble learning approaches, AdaBoost constructs a composite hypothesis by voting many individual hypotheses. In practice, the large amount of memory required to store these hypotheses can make ensemble methods hard to deploy in applications. This paper shows that by selecting a subset of the hypotheses, it is possible to obtain nearly the same levels of performance as the entire set. The results also provide some insight into the behavior of AdaBoost.
198|Using Output Codes to Boost Multiclass Learning Problems|This paper describes a new technique for solving multiclass learning problems by combining Freund and Schapire&#039;s boosting algorithm with the main ideas of Dietterich and Bakiri&#039;s method of error-correcting output codes (ECOC). Boosting is a general method of improving the accuracy of a given base or &#034;weak&#034; learning algorithm. ECOC is a robust method of solving multiclass learning problems by reducing to a sequence of two-class problems. We show that our new hybrid method has advantages of both: Like ECOC, our method only requires that the base learning algorithm work on binary-labeled data. Like boosting, we prove that the method comes with strong theoretical guarantees on the training and generalization error of the final combined hypothesis assuming only that the base learning algorithm perform slightly better than random guessing. Although previous methods were known for boosting multiclass problems, the new method may be significantly faster and require less programming effort in creating the base
learning algorithm. We also compare the new algorithm
experimentally to other voting methods.
199|An Adaptive Version of the Boost By Majority Algorithm|We propose a new boosting algorithm. This boosting algorithm is an adaptive version of the boost by  majority algorithm and combines bounded goals of the boost by majority algorithm with the adaptivity  of AdaBoost.
200|An empirical evaluation of bagging and boosting|An ensemble consists of a set of independently trained classi ers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble as a whole is often more accurate than any of the single classiers in the ensemble. Bagging (Breiman 1996a) and Boosting (Freund &amp; Schapire 1996) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods using both neural networks and decision trees as our classi cation algorithms. Our results clearly showtwo important facts. The rst is that even though Bagging almost always produces a better classi er than any of its individual component classi ers and is relatively impervious to over tting, it does not generalize any better than a baseline neural-network ensemble method. The second is that Boosting is apowerful technique that can usually produce better ensembles than Bagging ? however, it is more susceptible to noise and can quickly over t a data set.
201|A New Family of Online Algorithms for Category Ranking|We describe a new family of topic-ranking algorithms for multi-labeled documents. The motivation for the algorithms stems from recent advances in online learning algorithms. The algorithms we present are simple to implement and are time and memory ecient. We evaluate the algorithms on the Reuters-21578 corpus and the new corpus released by Reuters in 2000. On both corpora the algorithms we present outperform adaptations to topic-ranking of Rocchio&#039;s algorithm and the Perceptron algorithm. We also outline the formal analysis of the algorithm in the mistake bound model. To our knowledge, this work is the  rst to report performance results with the entire new Reuters corpus.
202|Arcing the edge|Recent work has shown that adaptively reweighting the training set, growing a classifier using the new weights, and combining the classifiers constructed to date can significantly decrease generalization error. Procedures of this type were called arcing by Breiman[1996]. The first successful arcing procedure was introduced by Freund and Schapire[1995,1996] and called Adaboost. In an effort to explain why Adaboost works, Schapire et.al. [1997] derived a bound on the generalization error of a convex combination of classifiers in terms of the margin. We introduce a function called the edge, which differs from the margin only if there are more than two classes. A framework for understanding arcing algorithms is defined. In this framework, we see that the arcing algorithms currently in the literature are optimization algorithms which minimize some function of the edge. A relation is derived between the optimal reduction in the maximum value of the edge and the PAC concept of weak learner. Two algorithms are described which achieve the optimal reduction. Tests on both synthetic and real data cast doubt on the Schapire et.al. explanation.
203|T.: Using the future to sort out the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
204|Cranking: Combining Rankings Using Conditional Probability Models on Permutations|A new approach to ensemble learning is introduced  that takes ranking rather than classification  as fundamental, leading to models on the symmetric  group and its cosets. The approach uses a  generalization of the Mallows model on permutations  to combine multiple input rankings. Applications  include the task of combining the output  of multiple search engines and multiclass or multilabel  classification, where a set of input classifiers  is viewed as generating a ranking of class labels.
205|Direct Optimization of Margins Improves Generalization in Combined Classifiers|Sonar Cumulative training margin distributions  for AdaBoost versus  our &#034;Direct Optimization Of  Margins&#034; (DOOM) algorithm.
206|Data Filtering and Distribution Modeling Algorithms for Machine Learning|vi Acknowledgments vii 1. Introduction 1  1.1 Boosting by majority : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 4 1.2 Query By Committee : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 7 1.3 Learning distributions of binary vectors : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 8  2. Boosting a weak learning algorithm by majority 10  2.1 Introduction : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 10 2.2 The majority-vote game : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : 14 2.2.1 Optimality of the weighting scheme : : : : : : : : : : : : : : : : : : : : : : : : : : : 19 2.2.2 The representational power of majority gates : : : : : : : : : : : : : : : : : : : : : : 20 2.3 Boosting a weak learner using a majority vote : : : : : : : : : : : : : : : : : : : : : : : : : : 22 2.3.1 Preliminaries : : : : : : : : : : : : : : : : : : : : : : : : : :...
207|Using the future to \sort out&amp;quot; the present: Rankprop and multitask learning for medical risk evaluation|A patient visits the doctor; the doctor reviews the patient&#039;s history, asks questions, makes basic measurements (blood pressure,...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk|patients at higher risk are given more and faster attention. It is also sequential|it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable. 1
208|SIMPLIcity: Semantics-Sensitive Integrated Matching for Picture LIbraries|The need for efficient content-based image retrieval has increased tremendously in many application areas such as biomedicine, military, commerce, education, and Web image classification and searching. We present here SIMPLIcity (Semanticssensitive Integrated Matching for Picture LIbraries), an image retrieval system, which uses semantics classification methods, a wavelet-based approach for feature extraction, and integrated region matching based upon image segmentation. As in other regionbased retrieval systems, an image is represented by a set of regions, roughly corresponding to objects, which are characterized by color, texture, shape, and location. The system classifies images into semantic categories, such as textured-nontextured, graphphotograph. Potentially, the categorization enhances retrieval by permitting semantically-adaptive searching methods and narrowing down the searching range in a database. A measure for the overall similarity between images is developed using a region-matching scheme that integrates properties of all the regions in the images. Compared with retrieval based on individual regions, the overall similarity approach 1) reduces the adverse effect of inaccurate segmentation, 2) helps to clarify the semantics of a particular region, and 3) enables a simple querying interface for region-based image retrieval systems. The application of SIMPLIcity to several databases, including a database of about 200,000 general-purpose images, has demonstrated that our system performs significantly better and faster than existing ones. The system is fairly robust to image alterations.
209|Photobook: Content-Based Manipulation of Image Databases|We describe the Photobook system, which is a set of interactive tools for browsing and searching images and image sequences. These query tools differ from those used in standard image databases in that they make direct use of the image content rather than relying on text annotations. Direct search on image content is made possible by use of semantics-preserving image compression, which reduces images to a small set of perceptually-significant coefficients. We describe three types of Photobook descriptions in detail: one that allows search based on appearance, one that uses 2-D shape, and a third that allows search based on textural properties. These image content descriptions can be combined with each other and with textbased descriptions to provide a sophisticated browsing and search capability. In this paper we demonstrate Photobook on databases containing images of people, video keyframes, hand tools, fish, texture swatches, and 3-D medical data.  
210|NeTra: A toolbox for navigating large image databases|. We present here an implementation of NeTra, a prototype image retrieval system that uses color, texture, shape and spatial location information in segmented image regions to search and retrieve similar regions from the database. A distinguishing aspect of this system is its incorporation of a robust automated image segmentation algorithm that allows object- or region-based search. Image segmentation significantly improves the quality of image retrieval when images contain multiple complex objects. Images are segmented into homogeneous regions at the time of ingest into the database, and image attributes that represent each of these regions are computed. In addition to image segmentation, other important components of the system include an efficient color representation, and indexing of color, texture, and shape features for fast search and retrieval. This representation allows the user to compose interesting queries such as &#034;retrieve all images that contain regions that have the colo...
211|Blobworld: A System for Region-Based Image Indexing and Retrieval|. Blobworld is a system for image retrieval based on finding coherent image regions which roughly correspond to objects. Each image is automatically segmented into regions (&#034;blobs&#034;) with associated color and texture descriptors. Querying is based on the attributes of one or two regions of interest, rather than a description of the entire image. In order to make large-scale retrieval feasible, we index the blob descriptions using a tree. Because indexing in the high-dimensional feature space is computationally prohibitive, we use a lower-rank approximation to the high-dimensional distance. Experiments show encouraging results for both querying and indexing. 1 Introduction  From a user&#039;s point of view, the performance of an information retrieval system can be measured by the quality and speed with which it answers the user&#039;s information need. Several factors contribute to overall performance:  -- the time required to run each individual query,  -- the quality (precision/recall) of each i...
212|Visual Information Retrieval|ND BUSINESSMAN CALVIN MOORES COINED the term information retrieval [10] to describe the process through which a prospective user of information can convert a request for information into a useful collection of references. &#034;Information retrieval,&#034; he wrote, &#034;embraces the intellectual aspects of the description of information and its specification for search, and also whatever systems, techniques, or machines that are employed Amarnath Gupta and Ramesh Jain 72 May 1997/Vol. 40, No. 5 COMMUNICATIONS OF THE ACM lar expressions to describe a clip. There is also a deeper reason: The information sought is inherently in the form of imagery that a textual language, however powerful, is unable to express adequately, making query processing inefficient. HE ROLE OF THE EMERGING FIELD OF visual information retrieval (VIR) systems is to go beyond text-based descri
213|A probabilistic approach to object recognition using local photometry and global geometry|Abstract. Many object classes, including human faces, can be modeled as a set of characteristic parts arranged in a variable spatial con guration. We introduce a simpli ed model of a deformable object class and derive the optimal detector for this model. However, the optimal detector is not realizable except under special circumstances (independent part positions). A cousin of the optimal detector is developed which uses \soft &#034; part detectors with a probabilistic description of the spatial arrangement of the parts. Spatial arrangements are modeled probabilistically using shape statistics to achieve invariance to translation, rotation, and scaling. Improved recognition performance over methods based on \hard &#034; part detectors is demonstrated for the problem of face detection in cluttered scenes. 1
214|WALRUS: A Similarity Retrieval Algorithm for Image Databases|Traditional approaches for content-based image querying typically compute a single signature for each image based on color histograms, texture, wavelet transforms etc., and return as the query result, images whose signatures are closest to the signature of the query image. Therefore, most traditional methods break down when images contain similar objects that are scaled differently or at different locations, or only certain regions of the image match.  In this paper, we propose WALRUS (WAveLet-based Retrieval of User-specified Scenes), a novel similarity retrieval algorithm that is robust to scaling and translation of objects within an image. WALRUS employs a novel similarity model in which each image is first decomposed into its regions, and the similarity measure between a pair of images is then defined to be the fraction of the area of the two images covered by matching regions from the images. In order to extract regions for an image, WALRUS considers sliding windows of varying siz...
215|Content-based image indexing and searching using Daubechies&#039; wavelets|This paper describes WBIIS (Wavelet-Based Image Indexing and Searching), a new image indexing and retrieval algorithm with partial sketch image searching capability for large image databases. The algorithm characterizes the color variations over the spatial extent of the image in a manner that provides semantically meaningful image comparisons. The indexing algorithm applies a Daubechies&#039; wavelet transform for each of the three opponent color components. The wavelet coefficients in the lowest few frequency bands, and their variances, are stored as feature vectors. To speed up retrieval, a two-step procedure is used that first does a crude selection based on the variances, and then renes the search by performing a feature vector match between the selected images and the query. For better accuracy in searching, two-level multiresolution matching may also be used. Masks are used for partial-sketch queries. This technique performs much better in capturing coherence of image, object granular...
216|The earth mover’s distance, multi-dimensional scaling, and color-based image retrieval|In this paper we present a novel approach tothe problem of navigating through a database of color images. We consider the images as points in a metric space in which we wish to move around so as to locate image neighborhoods of interest, based on color information. The data base images are mapped to distributions in color space, these distributions are appropriately compressed, and then the distances between all pairs I;J of images are computed based on the work needed to rearrange the mass in the compressed distribution representing I to that of J. We also propose the use of multi-dimensional scaling (MDS) techniques to embed a group of images as points in a two- or three-dimensional Euclidean space so that their distances are preserved as much as possible. Such geometric embeddings allow the user to perceive the dominant axes of variation in the displayed image group. In particular, displays of 2-d MDS embeddings can be used to organize and re ne the results of a nearest-neighbor query in a perceptually intuitive way. By iterating this process, the user is able to quickly navigate to the portion of the image space of interest. 1
217|Image Classification and Querying using Composite Region Templates|The tremendous growth in digital imagery is driving the need for more sophisticated  methods for automatic image analysis, cataloging, and searching.  We present a method for classifying and querying images based on the spatial  orderings of regions or objects using composite region templates (CRTs). The  CRTs capture the spatial information statistically and provide a robust way to  measure similarity in the presence of region insertions, deletions, substitutions, replications and relocations. The CRTs can be used for classifying and annotating  images by assigning symbols to the regions or objects and by extracting  symbol strings from spatial scans of the images. The symbol strings can be  decoded using a library of annotated CRTs to automatically label and classify  the images. The CRTs can also be used for searching bysketch or example by  measuring image similarity based on relative counts of the CRTs.  
218|Finding Similar Patterns in Large Image Databases|We address a new and rapidly growing application, automated searching through large sets of images to find a pattern &#034;similar to this one.&#034; Classical matched filtering fails at this problem since patterns, particularly textures, can differ in every pixel and still be perceptually similar. Most potential recognition methods have not been tested on large sets of imagery. This paper evaluates a key recognition method on a library of almost 1000 images, based on the entire Brodatz texture album. The features used for searching rely on a significant improvement to the traditional Karhunen-Lo&#039;eve (KL) transform which makes it shift-invariant. Results are shown for a variety of false alarm rates and for different subsets of KL features.  1 Introduction  As vastly increasing amounts of image and video are stored in computers it becomes harder for humans to locate a particular scene or video clip. It is currently impossible, in the general case, to semantically describe an image to the computer...
219|Unsupervised Multiresolution Segmentation for Images with Low Depth of Field|This paper describes a novel multiresolution image  segmentation algorithm for low DOF images. The algorithm is designed to  separate a sharply focused object-of-interest from other foreground or background objects. The algorithm is fully automatic in that all parameters are image  independent. A multiscale approach based on high frequency wavelet coefficients and their statistics is used to perform context-dependent classification of individual blocks of the image. Unlike other edge-based approaches, our algorithm does not rely on the process of connecting object boundaries. The algorithm has achieved high accuracy when tested on more than 100 low DOF images, many with  inhomogeneous foreground or background distractions. Compared with the state of the art algorithms, this new algorithm provides better accuracy at higher speed. Index TermsContent-based image retrieval, image region segmentation, low  depth-of-field, wavelet, multiresolution image analysis
220|Semantic Clustering and Querying on Heterogeneous Features for Visual data|  The effectiveness of the content-based image retrieval can be enhanced using the heterogeneous features embedded in the images. However, since the features in texture, color, and shape are generated using different computation methods and thus may require different similarity measurements, the integration of the...
221|System for Screening Objectionable Images|As computers and Internet become more and more available to families, access of objectionable graphics by children is increasingly a problem that many parents are concerned about. This paper describes WIPE TM (Wavelet Image Pornography Elimination), a system capable of classifying an image as objectionable or benign. The algorithm uses a combination of an icon filter, a graph-photo detector, a color histogram filter, a texture filter, and a wavelet-based shape matching algorithm to provide robust screening of on-line objectionable images. Semantically-meaningful feature vector matching is carried out so that comparisons between a given on-line image and images in a pre-marked training data set can be performed efficiently and effectively. The system is practical for real-world applications, processing queries at the speed of less than 2 seconds each, including the time to compute the feature vector for the query, on a Pentium Pro PC. Besides its exceptional speed, it has demonstrated 9...
222|Visual Similarity, Judgmental Certainty and Stereo Correspondence|Normal human vision is nearly infallible in modeling the visually sensed physical environment in which it evolved. In contrast, most currently available computer vision systems fall far short of human performance in this task, and further, they are generally not capable of being able to assert the correctness of their judgments. In computerized stereo matching systems, correctness of the similarity/identity-matching is almost never guaranteed. In this paper, we explore the question of the extent to which judgments of similarity/identity can be made essentially error-free in support of obtaining a relatively dense depth model of a natural outdoor scene. We argue for the necessity of simultaneously producing a crude scene-specific semantic &#034;overlay&#034;. For our experiments, we designed awavelet-based stereo matching algorithm and use &#034;classification-trees&#034; to create a primitive semantic overlay of the scene. A series of mutually independent filters has been designed and implemented based on the study of different error sources. Photometric appearance, camera imaging geometry and scene constraints are utilized in these filters. When tested on different sets of stereo images, our system has demonstrated above 97% correctness on asserted matches. Finally,we provide a principled basis for relatively dense depth recovery.
223|A learning algorithm for Boltzmann machines|The computotionol power of massively parallel networks of simple processing elements resides in the communication bandwidth provided by the hardware connections between elements. These connections con allow a significant fraction of the knowledge of the system to be applied to an instance of a problem in o very short time. One kind of computation for which massively porollel networks appear to be well suited is large constraint satisfaction searches, but to use the connections efficiently two conditions must be met: First, a search technique that is suitable for parallel networks must be found. Second, there must be some way of choosing internal representations which allow the preexisting hardware connections to be used efficiently for encoding the con-straints in the domain being searched. We describe a generol parallel search method, based on statistical mechanics, and we show how it leads to a gen-eral learning rule for modifying the connection strengths so as to incorporate knowledge obout o task domain in on efficient way. We describe some simple examples in which the learning algorithm creates internal representations thot ore demonstrobly the most efficient way of using the preexisting connectivity structure. 1.
225|Understanding Line Drawings of Scenes with Shadows|this paper, how can we recognize the identity of Figs. 2.1 and 2.2? Do we use&#039; learning and knowledge to interpret what we see, or do we somehow automatically see the world as stable and independent bf lighting? What portions of scenes can we understand from local features alone, and what configurations require the use of 1obal hypotheses?  19 In this essay I describe a working collection of computer programs which reconstruct three-dimensional descriptions from line drawings which are obtained from scenes composed of plane-faced objects under various lighting conditions. The system identifies shadow lines and regions, groups regions which belong to the same object, and notices such relations as contact or lack of contact between the objects, support and in-front-of/behind relations between the objects as well as information about the spatial orientation of various regions, all using the description it has generated
226|Optimal perceptual inference|When a vision system creates an interpretation of some input datn, it assigns truth values or probabilities to intcrnal hypothcses about the world. We present a non-dctcrministic method for assigning truth values that avoids many of the problcms encountered by existing relaxation methods. Instead of rcprcscnting probabilitics with real-numbers, we usc a more dircct encoding in which thc probability associated with a hypotlmis is rcprcscntcd by the probability hat it is in one of two states, true or false. Wc give a particular non-deterministic operator, based on statistical mechanics, for updating the truth values of hypothcses. The operator ensures that the probability of discovering a particular combination of hypothcscs is a simplc function of how good that combination is. Wc show that thcrc is a simple relationship bctween this operator and Bayesian inference, and we describe a learning rule which allows a parallel system to converge on a set ofweights that optimizes its perccptt~al inferences. lnt roduction One way of interpreting images is to formulate hypotheses about parts or aspects of the imagc and then decide which of these hypotheses are likely to be correct. Thc probability that each hypothesis is correct is determined partly by its fit to the imagc and partly by its fit to other hypothcses (hat are taken to be correct, so the truth&#039;value of an individual hypothesis cannot be decided in isolation. One method of searching for the most plausible combination of hypotheses is to use a rclaxation process in which a probability is associated with each hypothesis, and the probabilities arc then iteratively modified on the basis of the fit to the imagc and the known relationships bctwcen hypotheses. An attractive property of rclaxation methods is that they can be implemented in parallel hardwarc where one computational unit is used for each possible hypothcsis, and the interactions betwcen hypotheses are implemented by dircct hardwarc connections betwcen the units. Many variations of the basic relaxation idea have becn However, all the current methods suffer from one or more of the following problems:
227|Schema selection and stochastic inference in modular environments|Given a set of stimuli presenting views of some environment, how can one characterize the natural modules or “objects ” that compose the environment? Should a given set of items be encoded as a collection of instances or as a set of rules? Restricted formulations of these questions are addressed by analysis within a new mathematical framework that describes stochastic parallel computation. An algorithm is given for simulating this computation once schemas encoding the modules of the environment have been seIected. The concept of computational temperature is introduced. As this temperature is Iowered, the system appears to display a dramatic tendency to interpret input, even if the evidence for any particular interpretation is very weak. IIltrodoction Our sensory systems are capabIe of representing a vast number of possible stimuli. Our environment presents us with only a smaI1 fraction of the possibilities; this se&amp;ted subset is characterized by many regularities. Our minds encode these regularities, and this gives us some ability to infer the probable current condition of unknown portions of the environment given some Iimited information about the current state. What kind of regularities exist in the environment, and how should they be encoded? This paper presents preliminary results of research founded on the hypothesis that in real environments there exist reguIarities that can be idealized as mathematical structures that are simpIe enough to be anaIyxabIe. Only the simpIest kind of reguhuity is considered here: I will assume that the environment contains modules (objects) that recur exactly, with various states of the environment being comprised of various combinations of these modules. Even this simplest kind of environmental regularity offers interesting Iearning problems and results. It also serves to introduce a general framework capable of treating more subtle types of regularities. And the probIem considered is an important one, for the delineation of moduIes at one level of conceptual representation is a major step in the construction of higher Ievel representations.
228|Dynamic topic models|Scientists need new tools to explore and browse large collections of scholarly literature. Thanks to organizations such as JSTOR, which scan and index the original bound archives of many journals, modern scientists can search digital libraries spanning hundreds of years. A scientist, suddenly
229|Latent dirichlet allocation|We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model. 1.
230|A bayesian hierarchical model for learning natural scene categories|We propose a novel approach to learn and recognize natural scene categories. Unlike previous work [9, 17], it does not require experts to annotate the training set. We represent the image of a scene by a collection of local regions, denoted as codewords obtained by unsupervised learning. Each region is represented as part of a “theme”. In previous work, such themes were learnt from hand-annotations of experts, while our method learns the theme distributions as well as the codewords distribution over the themes without supervision. We report satisfactory categorization performances on a large set of 13 categories of complex scenes. 1.
231|The Author-Topic Model for Authors and Documents |We introduce the author-topic model, a generative model for documents that extends Latent Dirichlet Allocation (LDA; Blei, Ng, &amp; Jordan, 2003) to include authorship information. Each author is associated with a multinomial distribution over topics and each topic is associated with a multinomial distribution over words. A document with multiple authors is modeled as a distribution over topics
that is a mixture of the distributions associated with the authors. We apply the model to a collection of 1,700 NIPS conference papers and 160,000 CiteSeer abstracts. Exact
inference is intractable for these datasets and
we use Gibbs sampling to estimate the topic
and author distributions. We compare the performance with two other generative models for documents, which are special cases of the author-topic model: LDA (a topic model)
and a simple author model in which each author is associated with a distribution over words rather than a distribution over topics. We show topics recovered by the author-topic model, and demonstrate applications
to computing similarity between authors and
entropy of author output.
232|Sparse Gaussian processes using pseudo-inputs|We present a new Gaussian process (GP) regression model whose covariance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M « N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M 2 N) training cost and O(M 2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime. 1
233|Discovering object categories in image collections|Given a set of images containing multiple object categories, we seek to discover those categories and their image locations without supervision. We achieve this using generative models from the statistical text literature: probabilistic Latent Semantic Analysis (pLSA), and Latent Dirichlet Allocation (LDA). In text analysis these are used to discover topics in a corpus using the bag-of-words document representation. Here we discover topics as object categories, so that an image containing instances of several categories is modelled as a mixture of topics. The models are applied to images by using a visual analogue of a word, formed by vector quantizing SIFT like region descriptors. We investigate a set of increasingly demanding scenarios, starting with image sets containing only two object categories through to sets containing multiple categories (including airplanes, cars, faces, motorbikes, spotted cats) and background clutter. The object categories sample both intra-class and scale variation, and both the categories and their approximate spatial layout are found without supervision. We also demonstrate classification of unseen images and images containing multiple objects. Performance of the proposed unsupervised method is compared to the semi-supervised approach of [7].  
234|Integrating topics and syntax|Statistical approaches to language learning typically focus on either short-range syntactic dependencies or long-range semantic dependencies between words. We present a generative model that uses both kinds of dependencies, and can be used to simultaneously find syntactic classes and semantic topics despite having no representation of syntax or semantics beyond statistical dependency. This model is competitive on tasks like part-of-speech tagging and document classification with models that exclusively use short- and long-range dependencies respectively. 1
235|A Generalized Mean Field Algorithm for Variational Inference in Exponential Families|We present a class of generalized mean field (GMF) algorithms for approximate inference in exponential family graphical models which is analogous to the generalized belief propagation (GBP) or cluster variational methods. While those methods are based on...
236|Collaborative Filtering: A Machine Learning Perspective|Collaborative filtering was initially proposed as a framework for filtering information based on the preferences of users, and has since been refined in many different ways. This thesis is a comprehensive study of rating-based, pure, non-sequential collaborative filtering. We analyze existing methods for the task of rating prediction from a machine learning perspective. We show that many existing methods proposed for this task are simple applications or modi  cations of one or more standard machine learning methods for classifi cation, regression, clustering, dimensionality reduction, and density estimation. We introduce new prediction methods in all of these classes. We introduce a new experimental procedure for testing stronger forms of generalization than has been used previously. We implement a total of nine prediction methods, and conduct large scale prediction accuracy experiments. We show interesting new results on the relative performance of these methods.
237|Applying Discrete PCA in Data Analysis|Methods for analysis of principal components in discrete data have existed for some time under various names such as grade of membership modelling, probabilistic latent semantic analysis, and genotype inference with admixture. In this paper we explore a number of extensions to the common theory, and present some application of these methods to some common statistical tasks. We show that these methods can be interpreted as a discrete version of ICA. We develop a hierarchical version yielding components at different levels of detail, and additional techniques for Gibbs sampling. We compare the algorithms on a text prediction task using support vector machines, and to information retrieval.  
238|The authorrecipienttopic model for topic and role discovery in social networks: Experiments with Enron and academic email|Previous work in social network analysis (SNA) has modeled the existence of links from one entity to another, but not the language content or topics on those links. We present the Author-Recipient-Topic (ART) model for social network analysis, which learns topic distributions based on the the directionsensitive messages sent between entities. The model builds on Latent Dirichlet Allocation and the Author-Topic (AT) model, adding the key attribute that distribution over topics is conditioned distinctly on both the sender and recipient—steering the discovery of topics according to the relationships between people. We give results on both the Enron email corpus and a researcher’s email archive, providing evidence not only that clearly relevant topics are discovered, but that the ART model better predicts people’s roles. 1
240|Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles|To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a lineartime distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be ‘explained ’ by a relatively small number of structurally simple common behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing lowcomplexity and intuitively interpretable representations. 1
241|Ontology Development 101: A Guide to Creating Your First Ontology|In recent years the development of ontologies—explicit formal specifications of the terms in the domain and relations among them (Gruber 1993)—has been moving from the realm of Artificial-Intelligence laboratories to the desktops of domain experts. Ontologies have become common on the World-Wide Web. The ontologies on the Web range from large taxonomies categorizing Web sites (such as on Yahoo!) to categorizations of products for sale and their features (such as on Amazon.com). The WWW Consortium (W3C) is developing the Resource Description Framework (Brickley and Guha 1999), a language for encoding knowledge on Web pages to make it understandable to electronic agents searching for information. The Defense Advanced Research Projects Agency (DARPA), in conjunction with the W3C, is developing DARPA Agent Markup Language (DAML) by extending RDF with more expressive constructs aimed at facilitating agent interaction on the Web (Hendler and McGuinness 2000). Many disciplines now develop standardized ontologies that domain experts can use to share and annotate information in their fields. Medicine, for example, has produced large, standardized, structured vocabularies such as SNOMED (Price and Spackman 2000) and the semantic network of the Unified Medical Language System (Humphreys and Lindberg 1993). Broad general-purpose ontologies are
242|A translation approach to portable ontology specifications|To support the sharing and reuse of formally represented knowledge among AI systems, it is useful to define the common vocabulary in which shared knowledge is represented. A specification of a representational vocabulary for a shared domain of discourse — definitions of classes, relations, functions, and other objects — is called an ontology. This paper describes a mechanism for defining ontologies that are portable over representation systems. Definitions written in a standard format for predicate calculus are translated by a system called Ontolingua into specialized representations, including frame-based systems as well as relational languages. This allows researchers to share and reuse ontologies, while retaining the computational benefits of specialized implementations. We discuss how the translation approach to portability addresses several technical problems. One problem is how to accommodate the stylistic and organizational differences among representations while preserving declarative content. Another is how to translate from a very expressive language into restricted languages, remaining system-independent while preserving the computational efficiency of implemented systems. We describe how these problems are addressed by basing Ontolingua itself on an ontology of domain-independent, representational idioms. 
243|Methodology for the Design and Evaluation of Ontologies|This paper describes the methodology used in the Enterprise Integration Laboratory for the design and evaluation of integrated ontologies, including the proposal of new ontologies and the extension of existing ontologies (see Figure 1). We illustrate these ideas with examples from our activity and organisation ontologies. 2 Motivating Scenarios
244|Living with CLASSIC: When and How to Use a KL-ONE-Like Language|classic is a recently-developed knowledge representation system that follows the  paradigm originally set out in the kl-one system: it concentrates on the definition  of structured concepts, their organization into taxonomies, the creation and manipulation  of individual instances of such concepts, and the key inferences of subsumption  and classification. Rather than simply presenting a description of classic, we complement  a brief system overview with a discussion of how to live within the confines  of a limited object-oriented deductive system. By analyzing the representational  strengths and weaknesses of classic, we consider the circumstances under which it  is most appropriate to use (or not use) it. We elaborate a knowledge-engineering  methodology for building kl-one-style knowledge bases, with emphasis on the modeling  choices that arise in the process of describing a domain. We also address some  of the key difficult issues encountered by new users, including primitive vs. d...
245|Reusable Ontologies, Knowledge-Acquisition Tools, and Performance Systems: PROTÉGÉ-II Solutions to Sisyphus-2|This paper describes how we applied the PROTG-II architecture to build a knowledgebased system that configures elevators. The elevator-configuration task was solved originally with a system that employed the propose-and-revise problem-solving method (VT; Marcus, Stout &amp; McDermott, 1988). A variant of this task, here named the Sisyphus-2 problem, is used by the knowledge-acquisition community for comparative studies. PROTG-II is a knowledge-engineering environment that focuses on the use of reusable ontologies and problem-solving methods to generate task-specific knowledge-acquisition tools and executable problem solvers. The main goal of this paper is to describe in detail how we used PROTG-II to model the elevator-configuration task. This description provides a starting point for comparison with other frameworks that use abstract problem-solving methods. Starting from a detailed description of the elevator-configuration knowledge (Yost, 1992), we analyzed the domain knowledge and developed a general, reusable domain ontology. We selected, from PROTG-II&#039;s library of preexisting methods, a propose-and-revise method based on chronological backtracking. We then configured this method to solve the elevator-configuration task in a knowledge-based system named ELVIS. We entered domain-specific knowledge about elevator configuration into the knowledge base with the help of a task-specific knowledge-acquisition tool that was generated from the ontologies. After we constructed mapping relations to connect the domain and method ontologies, PROTG-II generated the executable problem solver. We have found that the development of ELVIS has provided a valuable test case for evaluating  PROTG-II&#039;s suite of system-building tools.
246|Gradient projection for sparse reconstruction: Application to compressed sensing and other inverse problems|Abstract—Many problems in signal processing and statistical inference involve finding sparse solutions to under-determined, or ill-conditioned, linear systems of equations. A standard approach consists in minimizing an objective function which includes a quadratic (squared l2) error term combined with a sparseness-inducing (l1) regularization term.Basis pursuit, the least absolute shrinkage and selection operator (LASSO), waveletbased deconvolution, and compressed sensing are a few wellknown examples of this approach. This paper proposes gradient projection (GP) algorithms for the bound-constrained quadratic programming (BCQP) formulation of these problems. We test variants of this approach that select the line search parameters in different ways, including techniques based on the Barzilai-Borwein method. Computational experiments show that these GP approaches perform well in a wide range of applications, often being significantly faster (in terms of computation time) than competing methods. Although the performance of GP methods tends to degrade as the regularization term is de-emphasized, we show how they can be embedded in a continuation scheme to recover their efficient practical performance. A. Background I.
247|Convex Analysis|In this book we aim to present, in a unified framework, a broad spectrum of mathematical theory that has grown in connection with the study of problems of optimization, equilibrium, control, and stability of linear and nonlinear systems. The title Variational Analysis reflects this breadth. For a long time, ‘variational ’ problems have been identified mostly with the ‘calculus of variations’. In that venerable subject, built around the minimization of integral functionals, constraints were relatively simple and much of the focus was on infinite-dimensional function spaces. A major theme was the exploration of variations around a point, within the bounds imposed by the constraints, in order to help characterize solutions and portray them in terms of ‘variational principles’. Notions of perturbation, approximation and even generalized differentiability were extensively investigated. Variational theory progressed also to the study of so-called stationary points, critical points, and other indications of singularity that a point might have relative to its neighbors, especially in association with existence theorems for differential equations.
248|Regression Shrinkage and Selection Via the Lasso|We propose a new method for estimation in linear models. The &#034;lasso&#034; minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly zero and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described. Keywords: regression, subset selection, shrinkage, quadratic programming. 1 Introduction Consider the usual regression situation: we h...
249|Compressed sensing|We study the notion of Compressed Sensing (CS) as put forward in [14] and related work [20, 3, 4]. The basic idea behind CS is that a signal or image, unknown but supposed to be compressible by a known transform, (eg. wavelet or Fourier), can be subjected to fewer measurements than the nominal number of pixels, and yet be accurately reconstructed. The samples are nonadaptive and measure ‘random ’ linear combinations of the transform coefficients. Approximate reconstruction is obtained by solving for the transform coefficients consistent with measured data and having the smallest possible `1 norm. We perform a series of numerical experiments which validate in general terms the basic idea proposed in [14, 3, 5], in the favorable case where the transform coefficients are sparse in the strong sense that the vast majority are zero. We then consider a range of less-favorable cases, in which the object has all coefficients nonzero, but the coefficients obey an `p bound, for some p ? (0, 1]. These experiments show that the basic inequalities behind the CS method seem to involve reasonable constants. We next consider synthetic examples modelling problems in spectroscopy and image pro-
250|ATOMIC DECOMPOSITION BY BASIS PURSUIT|The Time-Frequency and Time-Scale communities have recently developed a large number of overcomplete waveform dictionaries -- stationary wavelets, wavelet packets, cosine packets, chirplets, and warplets, to name a few. Decomposition into overcomplete systems is not unique, and several methods for decomposition have been proposed, including the Method of Frames (MOF), Matching Pursuit (MP), and, for special dictionaries, the Best Orthogonal Basis (BOB). Basis Pursuit (BP) is a principle for decomposing a signal into an &#034;optimal&#034; superposition of dictionary elements, where optimal means having the smallest l 1 norm of coefficients among all such decompositions. We give examples exhibiting several advantages over MOF, MP and BOB, including better sparsity, and super-resolution. BP has interesting relations to ideas in areas as diverse as ill-posed problems, in abstract harmonic analysis, total variation de-noising, and multi-scale edge denoising. Basis Pursuit in highly overcomplete dictionaries leads to large-scale optimization problems. With signals of length 8192 and a wavelet packet dictionary, one gets an equivalent linear program of size 8192 by 212,992. Such problems can be attacked successfully only because of recent advances in linear programming by interior-point methods. We obtain reasonable success with a primal-dual logarithmic barrier method and conjugate-gradient solver.
251|Robust Uncertainty Principles: Exact Signal Reconstruction From Highly Incomplete Frequency Information|This paper considers the model problem of reconstructing an object from incomplete frequency samples. Consider a discrete-time signal and a randomly chosen set of frequencies. Is it possible to reconstruct from the partial knowledge of its Fourier coefficients on the set? A typical result of this paper is as follows. Suppose that is a superposition of spikes @ Aa @ A @ A obeying @?? ? A I for some constant H. We do not know the locations of the spikes nor their amplitudes. Then with probability at least I @ A, can be reconstructed exactly as the solution to the I minimization problem I aH @ A s.t. ” @ Aa ”  @ A for all
252|Nonlinear total variation based noise removal algorithms|A constrained optimization type of numerical algorithm for removing noise from images is presented. The total variation of the image is minimized subject to constraints involving the statistics of the noise. The constraints are imposed using Lagrange multipliers. The solution is obtained using the gradient-projection method. This amounts to solving a time dependent partial differential equation on a manifold determined by the constraints. As t--- ~ 0o the solution converges to a steady state which is the denoised image. The numerical algorithm is simple and relatively fast. The results appear to be state-of-the-art for very noisy images. The method is noninvasive, yielding sharp edges in the image. The technique could be interpreted as a first step of moving each level set of the image normal to itself with velocity equal to the curvature of the level set divided by the magnitude of the gradient of the image, and a second step which projects the image back onto the constraint set. 
253|Near Optimal Signal Recovery From Random Projections: Universal Encoding Strategies?|Suppose we are given a vector f in RN. How many linear measurements do we need to make about f to be able to recover f to within precision ? in the Euclidean (l2) metric? Or more exactly, suppose we are interested in a class F of such objects— discrete digital signals, images, etc; how many linear measurements do we need to recover objects from this class to within accuracy ?? This paper shows that if the objects of interest are sparse or compressible in the sense that the reordered entries of a signal f ? F decay like a power-law (or if the coefficient sequence of f in a fixed basis decays like a power-law), then it is possible to reconstruct f to within very high accuracy from a small number of random measurements. typical result is as follows: we rearrange the entries of f (or its coefficients in a fixed basis) in decreasing order of magnitude |f | (1)  = |f | (2)  =... = |f | (N), and define the weak-lp ball as the class F of those elements whose entries obey the power decay law |f | (n)  = C · n -1/p. We take measurements <f, Xk>, k = 1,..., K, where the Xk are N-dimensional Gaussian
254|Least angle regression |The purpose of model selection algorithms such as All Subsets, Forward Selection and Backward Elimination is to choose a linear model on the basis of the same set of data to which the model will be applied. Typically we have available a large collection of possible covariates from which we hope to select a parsimonious set for the efficient prediction of a response variable. Least Angle Regression (LARS), a new model selection algorithm, is a useful and less greedy version of traditional forward selection methods. Three main properties are derived: (1) A simple modification of the LARS algorithm implements the Lasso, an attractive version of ordinary least squares that constrains the sum of the absolute regression coefficients; the LARS modification calculates all possible Lasso estimates for a given problem, using an order of magnitude less computer time than previous methods. (2) A different LARS modification efficiently implements Forward Stagewise linear regression, another promising
255|De-Noising By Soft-Thresholding|Donoho and Johnstone (1992a) proposed a method for reconstructing an unknown function f on [0; 1] from noisy data di = f(ti)+ zi, iid i =0;:::;n 1, ti = i=n, zi N(0; 1). The reconstruction fn ^ is de ned in the wavelet domain by translating all the empirical wavelet coe cients of d towards 0 by an amount p 2 log(n)  = p n. We prove two results about that estimator. [Smooth]: With high probability ^ fn is at least as smooth as f, in any of a wide variety of smoothness measures. [Adapt]: The estimator comes nearly as close in mean square to f as any measurable estimator can come, uniformly over balls in each of two broad scales of smoothness classes. These two properties are unprecedented in several ways. Our proof of these results develops new facts about abstract statistical inference and its connection with an optimal recovery model.
256|Greed is Good: Algorithmic Results for Sparse Approximation|This article presents new results on using a greedy algorithm, orthogonal matching pursuit (OMP), to solve the sparse approximation problem over redundant dictionaries. It provides a sufficient condition under which both OMP and Donoho’s basis pursuit (BP) paradigm can recover the optimal representation of an exactly sparse signal. It leverages this theory to show that both OMP and BP succeed for every sparse input signal from a wide class of dictionaries. These quasi-incoherent dictionaries offer a natural generalization of incoherent dictionaries, and the cumulative coherence function is introduced to quantify the level of incoherence. This analysis unifies all the recent results on BP and extends them to OMP. Furthermore, the paper develops a sufficient condition under which OMP can identify atoms from an optimal approximation of a nonsparse signal. From there, it argues that OMP is an approximation algorithm for the sparse problem over a quasi-incoherent dictionary. That is, for every input signal, OMP calculates a sparse approximant whose error is only a small factor worse than the minimal error that can be attained with the same number of terms. 
257|The Dantzig Selector: Statistical Estimation When p Is Much Larger Than n|In many important statistical applications, the number of variables or parameters p is much larger than the number of observations n. Suppose then that we have observations y = Xß + z, where ß ? Rp is a parameter vector of interest, X is a data matrix with possibly far fewer rows than columns, n « p, and the zi’s are i.i.d. N(0,s2). Is it possible to estimate ß reliably based on the noisy data y? To estimate ß, we introduce a new estimator—we call it the Dantzig selector—which is a solution to the l1-regularization problem min ˜ß?R p ? ˜ß?l1 subject to ?X * r?l 8  = (1 + t-1 v) 2logp · s, where r is the residual vector y - X ˜ß and t is a positive scalar. We show that if X obeys a uniform uncertainty principle (with unit-normed columns) and if the true parameter vector ß is sufficiently sparse (which here roughly guarantees that the model is identifiable), then with very large probability,
258|The Extended Linear Complementarity Problem|We consider an extension of the horizontal linear complementarity problem, which we call the extended linear complementarity problem (XLCP). With the aid of a natural bilinear program, we establish various properties of this extended complementarity problem; these include the convexity of the bilinear objective function under a monotonicity assumption, the polyhedrality of the solution set of a monotone XLCP, and an error bound result for a nondegenerate XLCP. We also present a finite, sequential linear programming algorithm for solving the nonmonotone XLCP.  
260|LSQR: An Algorithm for Sparse Linear Equations and Sparse Least Squares|An iterative method is given for solving Ax ~ffi b and minU Ax- b 112, where the matrix A is large and sparse. The method is based on the bidiagonalization procedure of Golub and Kahan. It is analytically equivalent to the standard method of conjugate gradients, but possesses more favorable numerical properties. Reliable stopping criteria are derived, along with estimates of standard errors for x and the condition number of A. These are used in the FORTRAN implementation of the method, subroutine LSQR. Numerical tests are described comparing I~QR with several other conjugate-gradient algorithms, indicating that I~QR is the most reliable algorithm when A is ill-conditioned. Categories and Subject Descriptors: G.1.2 [Numerical Analysis]: ApprorJmation--least squares approximation; G.1.3 [Numerical Analysis]: Numerical Linear Algebra--linear systems (direct and
261|An Algorithm for Total Variation Minimization and Applications| We propose an algorithm for minimizing the total variation of an image, and provide a proof of convergence. We show applications to image denoising, zooming, and the computation of the mean curvature motion of interfaces. 
262|Interior-point Methods|The modern era of interior-point methods dates to 1984, when Karmarkar proposed his algorithm for linear programming. In the years since then, algorithms and software for linear programming have become quite sophisticated, while extensions to more general classes of problems, such as convex quadratic programming, semidefinite programming, and nonconvex and nonlinear problems, have reached varying levels of maturity. We review some of the key developments in the area, including comments on both the complexity theory and practical algorithms for linear programming, semidefinite programming, monotone linear complementarity, and convex programming over sets that can be characterized by self-concordant barrier functions. 
263|Just Relax: Convex Programming Methods for Identifying Sparse Signals in Noise|This paper studies a difficult and fundamental problem that arises throughout electrical engineering, applied mathematics, and statistics. Suppose that one forms a short linear combination of elementary signals drawn from a large, fixed collection. Given an observation of the linear combination that has been contaminated with additive noise, the goal is to identify which elementary signals participated and to approximate their coefficients. Although many algorithms have been proposed, there is little theory which guarantees that these algorithms can accurately and efficiently solve the problem. This paper studies a method called convex relaxation, which attempts to recover the ideal sparse signal by solving a convex program. This approach is powerful because the optimization can be completed in polynomial time with standard scientific software. The paper provides general conditions which ensure that convex relaxation succeeds. As evidence of the broad impact of these results, the paper describes how convex relaxation can be used for several concrete signal recovery problems. It also describes applications to channel coding, linear regression, and numerical analysis.
264|Stable recovery of sparse overcomplete representations in the presence of noise| Overcomplete representations are attracting interest in signal processing theory, particularly due to their potential to generate sparse representations of signals. However, in general, the problem of finding sparse representations must be unstable in the presence of noise. This paper establishes the possibility of stable recovery under a combination of sufficient sparsity and favorable structure of the overcomplete system. Considering an ideal underlying signal that has a sufficiently sparse representation, it is assumed that only a noisy version of it can be observed. Assuming further that the overcomplete system is incoherent, it is shown that the optimally sparse approximation to the noisy data differs from the optimally sparse decomposition of the ideal noiseless signal by at most a constant multiple of the noise level. As this optimal-sparsity method requires heavy (combinatorial) computational effort, approximation algorithms are considered. It is shown that similar stability is also available using the basis and the matching pursuit algorithms. Furthermore, it is shown that these methods result in sparse approximation of the noisy data that contains only terms also appearing in the unique sparsest representation of the ideal noiseless sparse signal. 
265|An EM Algorithm for Wavelet-Based Image Restoration|This paper introduces an expectation-maximization (EM) algorithm for image restoration (deconvolution) based on a penalized likelihood formulated in the wavelet domain. Regularization is achieved by promoting a reconstruction with low-complexity, expressed in terms of the wavelet coecients, taking advantage of the well known sparsity of wavelet representations. Previous works have investigated wavelet-based restoration but, except for certain special cases, the resulting criteria are solved approximately or require very demanding optimization methods. The EM algorithm herein proposed combines the efficient image representation oered by the discrete wavelet transform (DWT) with the diagonalization of the convolution operator obtained in the Fourier domain. The algorithm alternates between an E-step based on the fast Fourier transform (FFT) and a DWT-based M-step, resulting in an ecient iterative process requiring O(N log N) operations per iteration. Thus, it is the  rst image restoration algorithm that optimizes a wavelet-based penalized likelihood criterion and has computational complexity comparable to that of standard wavelet denoising or frequency domain deconvolution methods. The convergence behavior of the algorithm is investigated, and it is shown that under mild conditions the algorithm converges to a globally optimal restoration. Moreover, our new approach outperforms several of the best existing methods in benchmark tests, and in some cases is also much less computationally demanding.
266|Signal reconstruction from noisy random projections|Recent results show that a relatively small number of random projections of a signal can contain most of its salient information. It follows that if a signal is compressible in some orthonormal basis, then a very accurate reconstruction can be obtained from random projections. We extend this type of result to show that compressible signals can be accurately recovered from random projections contaminated with noise. We also propose a practical iterative algorithm for signal reconstruction, and briefly discuss potential applications to coding, A/D conversion, and remote wireless sensing. Index Terms sampling, signal reconstruction, random projections, denoising, wireless sensor networks
267|Nonmonotone spectral projected gradient methods on convex sets|Abstract. Nonmonotone projected gradient techniques are considered for the minimization of differentiable functions on closed convex sets. The classical projected gradient schemes are extended to include a nonmonotone steplength strategy that is based on the Grippo–Lampariello–Lucidi nonmonotone line search. In particular, the nonmonotone strategy is combined with the spectral gradient choice of steplength to accelerate the convergence process. In addition to the classical projected gradient nonlinear path, the feasible spectral projected gradient is used as a search direction to avoid additional trial projections during the one-dimensional search process. Convergence properties and extensive numerical results are presented.
268|Why simple shrinkage is still relevant for redundant representations|Abstract—Shrinkage is a well known and appealing denoising technique, introduced originally by Donoho and Johnstone in 1994. The use of shrinkage for denoising is known to be optimal for Gaussian white noise, provided that the sparsity on the signal’s representation is enforced using a unitary transform. Still, shrinkage is also practiced with nonunitary, and even redundant representations, typically leading to very satisfactory results. In this correspondence we shed some light on this behavior. The main argument in this work is that such simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. While the desired solution of BPDN is hard to obtain in general, we develop a simple iterative procedure for the BPDN minimization that amounts to stepwise shrinkage. We demonstrate how the simple shrinkage emerges as the first iteration of this novel algorithm. Furthermore, we show how shrinkage can be iterated, turning into an effective algorithm that minimizes the BPDN via simple shrinkage steps, in order to further strengthen the denoising effect. Index Terms—Basis pursuit, denoising, frame, overcomplete, redundant, sparse representation, shrinkage, thresholding.
269|Spectral bounds for sparse PCA: Exact and greedy algorithms|Sparse PCA seeks approximate sparse “eigenvectors ” whose projections capture the maximal variance of data. As a cardinality-constrained and non-convex optimization problem, it is NP-hard and yet it is encountered in a wide range of applied fields, from bio-informatics to finance. Recent progress has focused mainly on continuous approximation and convex relaxation of the hard cardinality constraint. In contrast, we consider an alternative discrete spectral formulation based on variational eigenvalue bounds and provide an effective greedy strategy as well as provably optimal solutions using branch-and-bound search. Moreover, the exact methodology used reveals a simple renormalization step that improves approximate solutions obtained by any continuous method. The resulting performance gain of discrete algorithms is demonstrated on real-world benchmark data and in extensive Monte Carlo evaluation trials. 1
270|WARM-START STRATEGIES IN INTERIOR-POINT METHODS FOR LINEAR PROGRAMMING | We study the situation in which, having solved a linear program with an interior-point method, we are presented with a new problem instance whose data is slightly perturbed from the original. We describe strategies for recovering a &#034;warm-start&#034; point for the perturbed problem instance from the iterates of the original problem instance. We obtain worst-case estimates of the number of iterations required to converge to a solution of the perturbed instance from the warm-start points, showing that these estimates depend on the size of the perturbation and on the conditioning and other properties of the problem instances.
271|Image denoising with shrinkage and redundant representations|Shrinkage is a well known and appealing denoising technique. The use of shrinkage is known to be optimal for Gaussian white noise, provided that the sparsity on the signal’s representation is enforced using a unitary transform. Still, shrinkage is also practiced successfully with nonunitary, and even redundant representations. In this paper we shed some light on this behavior. We show that simple shrinkage could be interpreted as the first iteration of an algorithm that solves the basis pursuit denoising (BPDN) problem. Thus, this work leads to a novel iterative shrinkage algorithm that can be considered as an effective pursuit method. We demonstrate this algorithm, both on synthetic data, and for the image denoising problem, where we learn the image prior parameters directly from the given image. The results in both cases are superior to several popular alternatives. 1
273|More on sparse representations in arbitrary bases|Abstract: The purpose of this contribution is to generalize some recent results on sparse representations of signals in redundant bases. The question that is considered is the following: let A be a known (n, m) matrix with m&gt; n, one observes b = AX where X is known to have p &lt; n nonzero components, under which conditions on A and p is it possible to recover X by solving a convex optimization problem such as a linear or quadratic program? The solution is known when A is the concatenation of two unitary matrices, we extend it to arbitrary matrices.
275|On the convergence properties of the projected gradient method for convex optimization |Abstract. When applied to an unconstrained minimization problem with a convex objective, the steepest descent method has stronger convergence properties than in the noncovex case: the whole sequence converges to an optimal solution under the only hypothesis of existence of minimizers (i.e. without assuming e.g. boundedness of the level sets). In this paper we look at the projected gradient method for constrained convex minimization. Convergence of the whole sequence to a minimizer assuming only existence of solutions has also been already established for the variant in which the stepsizes are exogenously given and square summable. In this paper, we prove the result for the more standard (and also more efficient) variant, namely the one in which the stepsizes are determined through an Armijo search. Mathematical subject classification: 90C25, 90C30. Key words: projected gradient method, convex optimization, quasi-Fejér convergence.
276|Learning Stochastic Logic Programs|Stochastic Logic Programs (SLPs) have been shown to  be a generalisation of Hidden Markov Models (HMMs),  stochastic context-free grammars, and directed Bayes&#039;  nets. A stochastic logic program consists of a set of  labelled clauses p:C where p is in the interval [0,1] and  C is a first-order range-restricted definite clause. This  paper summarises the syntax, distributional semantics  and proof techniques for SLPs and then discusses how a  standard Inductive Logic Programming (ILP) system,  Progol, has been modied to support learning of SLPs.  The resulting system 1) nds an SLP with uniform  probability labels on each definition and near-maximal  Bayes posterior probability and then 2) alters the probability  labels to further increase the posterior probability.  Stage 1) is implemented within CProgol4.5, which  differs from previous versions of Progol by allowing  user-defined evaluation functions written in Prolog. It  is shown that maximising the Bayesian posterior function  involves nding SLPs with short derivations of the  examples. Search pruning with the Bayesian evaluation  function is carried out in the same way as in previous  versions of CProgol. The system is demonstrated with  worked examples involving the learning of probability  distributions over sequences as well as the learning of  simple forms of uncertain knowledge.  
277|Learning logical definitions from relations| This paper describes FOIL, a system that learns Horn clauses from data expressed as relations. FOIL is based on ideas that have proved effective in attribute-value learning systems, but extends them to a first-order formalism. This new system has been applied successfully to several tasks taken from the machine learning literature.
278|Inverse entailment and Progol|This paper firstly provides a re-appraisal of the development of  techniques for inverting deduction, secondly introduces Mode-Directed Inverse  Entailment (MDIE) as a generalisation and enhancement of previous approaches  and thirdly describes an implementation of MDIE in the Progol system. Progol is  implemented in C and available by anonymous ftp. The re-assessment of previous  techniques in terms of inverse entailment leads to new results for learning from  positive data and inverting implication between pairs of clauses. 
279|Qualitative Simulation|Qualitative simulation predicts the set of possible behaviors...
280|Clausal Discovery|  The clausal discovery engine Claudien is presented. Claudien is an inductive logic programming engine that fits in the descriptive data mining paradigm. Claudien addresses characteristic induction from interpretations, a task which is related to existing formalisations of induction in logic. In characteristic induction from interpretations, the regularities are represented by clausal theories, and the data using Herbrand interpretations. Because Claudien uses clausal logic to represent hypotheses, the regularities induced typically involve multiple relations or predicates. Claudien also employs a novel declarative bias mechanism to define the set of clauses that may appear in a hypothesis.
281|Generating Production Rules From Decision Trees|Many inductive knowledge acquisition algorithms generate classifiers in the form of decision trees. This paper describes a technique for transforming such trees to small sets of production rules, a common formalism for expressing knowledge in expert systems. The method makes use of the training set of cases from which the decision tree was generated, first to generalize and assess the reliability of individual rules extracted from the tree, and subsequently to refine the collection of rules as a whole. The final set of production rules is usually both simpler than the decision tree from which it was obtained, and more accurate when classifying unseen cases. Transformation to production rules also provides a way of combining different decision trees for the same classification domain.
282|Relational bayesian networks|A new method is developed to represent probabilistic relations on multiple random events. Where previously knowledge bases containing probabilistic rules were used for this purpose, here a probabilitydistributionover the relations is directly represented by a Bayesian network. By using a powerful way of specifying conditional probability distributions in these networks, the resulting formalism is more expressive than the previous ones. Particularly, it provides for constraints on equalities of events, and it allows to define complex, nested combination functions. 1
283|Applications of Machine Learning and Rule Induction|An important area of application for machine learning is in automating the acquisition of knowledge bases required for expert systems. In this paper, we review the major paradigms for machine learning, including neural networks, instance-based methods, genetic learning, rule induction, and analytic approaches. We consider rule induction in greater detail and review some of its recent applications, in each case stating the problem, how rule induction was used, and the status of the resulting expert system. In closing, we identify the main stages in fielding an applied learning system and draw some lessons from successful applications. Introduction  Machine learning is the study of computational methods for improving performance by mechanizing the acquisition of knowledge from experience. Expert performance requires much domainspecific knowledge, and knowledge engineering has produced hundreds of AI expert systems that are now used regularly in industry. Machine learning aims to provide ...
284|Mutagenesis: ILP experiments in a non-determinate biological domain|This paper describes the use of Inductive Logic Programming as a scientific  assistant. In particular, it details the application of the ILP system  Progol to discovering structural features that can result in mutagenicity in  small molecules. To discover these concepts, Progol only had access to the  atomic and bond structure of the molecules. With such a primitive description  and no further assistance from chemists, Progol corroborated some  existing knowledge and proposed a new structural alert for mutagenicity in  compounds. In the process, the experiments act as a case study in which,  even with extremely limited background knowledge, an Inductive Logic  Programming tool firstly, complements a complex statistical model developed  by skilled chemists, and secondly, continues to provide understandable  theories when the statistical model fails. The experiments also constitute  the first demonstrations of a prototype of the Progol system. Progol  allows the construction of hypotheses with bounded non-determinacy by  performing a best-first search within the subsumption lattice. The results  here provide evidence that such searches are both viable and desirable.  1 
285|Learning concepts by asking questions|Tw o important issues in machine learning are explored: the role that memory plays in acquiring new concepts; and the extent to which the learner can take anactive part in acquiring these concepts. This chapter describes a program, called Marvin, which uses concepts it has learned previously to learn new concepts. The program forms hypotheses about the concept being learned and tests the hypotheses by asking the trainer questions. Learning begins when the trainer shows Marvin an example of the concept to be learned. The program determines which objects in the example belong to concepts stored in the memory. A description of the new concept is formed by using the information obtained from the memory to generalize the description of the training example. The generalized description is tested when the program constructs new examples and shows these to the trainer, asking if they belong to the target concept. 1.
286|Logical Depth and Physical Complexity|Some mathematical and natural objects (a random sequence, a sequence  of zeros, a perfect crystal, a gas) are intuitively trivial, while  others (e.g. the human body, the digits of #) contain internal evidence  of a nontrivial causal history. We formalize this
287|Biochemical knowledge discovery using Inductive Logic Programming|Machine Learning algorithms are being increasingly used for knowledge discovery tasks.  Approaches can be broadly divided by distinguishing discovery of procedural from that of  declarative knowledge. Client requirements determine which of these is appropriate. This  paper discusses an experimental application of machine learning in an area related to drug  design. The bottleneck here is in finding appropriate constraints to reduce the large number of  candidate molecules to be synthesisedand tested. Such constraints canbe viewed as declarative  specifications of the structural elements necessary for high medicinal activity and low toxicity.  The first-order representation used within Inductive Logic Programming (ILP) provides an  appropriate description language for such constraints. Within this application area knowledge  accreditation requires not only a demonstration of predictive accuracy but also, and crucially, a  certification of novel insight into the structural chemistry. Thi...
288|Uncertainty, Belief, and Probability|: We introduce a new probabilistic approach to dealing with uncertainty, based on the observation that probability theory does not require that every event be assigned a probability. For a nonmeasurable event (one to which we do not assign a probability), we can talk about only the inner measure and outer measure of the event. In addition to removing the requirement that every event be assigned a probability, our approach circumvents other criticisms of probability-based approaches to uncertainty. For example, the measure of belief in an event turns out to be represented by an interval (defined by the inner and outer measure), rather than by a single number. Further, this approach allows us to assign a belief (inner measure) to an event E without committing to a belief about its negation :E (since the inner measure of an event plus the inner measure of its negation is not necessarily one). Interestingly enough, inner measures induced by probability measures turn out to correspond in a ...
289|Learning Qualitative Models of Dynamic Systems|A technique is described for learning qualitative models of dynamic systems. The QSIM formalism is used as a representation for learned qualitative models. The problem of learning QSIM-type models is formulated in logic, and the GOLEM learning program is used for induction. An experiment in learning a qualitative model of the connected containers system, also called U-tube, is described in detail. 1 Introduction  It has been shown that qualitative models are better suited for several tasks than the traditional quantitative, or numerical models. These tasks include diagnosis (e.g. Bratko, Mozetic and Lavrac 1989), generating explanation of the system&#039;s behaviour (e.g. Forbus and Falkenheiner 1990) and designing novel devices from first principles (e.g. Williams 1990). We believe that system identification, a fundamental problem in the theory of dynamic systems, is also a task that is done easier at the qualitative level. This paper presents a case study in how this can be done using a l...
290|Loglinear models for first-order probabilistic reasoning|Recent work on loglinear models in probabilistic constraint logic programming is applied to firstorder probabilistic reasoning. Probabilities are defined directly on the proofs of atomic formulae, and by marginalisation on the atomic formulae themselves. We use Stochastic Logic Programs (SLPs) composed of labelled and unlabelled definite clauses to define the proof probabilities. We have a conservative extension of first-order reasoning, so that, for example, there is a one-one mapping between logical and random variables. We show how, in this framework, Inductive Logic Programming (ILP) can be used to induce the features of a loglinear model from data. We also compare the presented framework with other approaches to first-order probabilistic reasoning.
291|A Strategy for Constructing New Predicates in First Order Logic|There is increasing interest within the Machine Learning community in systems which automatically reformulate their problem representation by defining and constructing new predicates. A previous paper discussed such a system, called CIGOL, and gave a derivation for the mechanism of inverting individual steps in first order resolution proofs. In this paper we describe an enhancement to CIGOL&#039;s learning strategy which strongly constrains the formation of new concepts and hypotheses. The new strategy is based on results from algorithmic information theory. Using these results it is possible to compute the probability that the simplifications produced by adopting new concepts or hypotheses are not based on chance regularities within the examples. This can be derived from the amount of information compression produced by replacing the examples with the hypothesised concepts. CIGOL&#039;s improved performance, based on an approximation of this strategy, is demonstrated by way of the automatic &#034;di...
292|The Role of Databases in Knowledge-Based Systems|This paper explores the requirements for database techniques in the construction of  knowledge-based systems. Three knowledge-based systems are reviewed: XCN/R1,  ISIS and Callisto in order to ascertain database requirements. These requirements result  in the introduction of the Organization level, an extension to the symbol and knowledge  levels introduced by Newell. An implementation of these requirements is explored in the  SRL knowledge representation and problem-solving system.
293|The Cyclical Behavior of Equilibrium Unemployment and Vacancies|This paper argues that a broad class of search models cannot generate the observed business-cycle-frequency fluctuations in unemployment and job vacancies in response to shocks of a plausible magnitude. In the U.S., the vacancy-unemployment ratio is 20 times as volatile as average labor productivity, while under weak assumptions, search models predict that the vacancy-unemployment ratio and labor productivity have nearly the same variance. I establish this claim both using analytical comparative statics in a very general deterministic search model and using simulations of a stochastic version of the model. I show that a shock that changes average labor productivity primarily alters the present value of wages, generating only a small movement along a downward sloping Beveridge curve (unemployment-vacancy locus). A shock to the job destruction rate generates a counterfactually positive correlation between unemployment and vacancies. In both cases, the shock is only slightly amplified and the model exhibits virtually no propagation. I reconcile these findings with an existing literature and argue that the source of the model’s failure is lack of wage rigidity, a consequence of the assumption that wages are determined by Nash bargaining. * This is a major revision of ‘Equilibrium Unemployment Fluctuations’. I thank Daron Acemoglu, Olivier
294|Job Destruction and Propagation of Shocks |This paper considers propagation of aggregate shocks in a dynamic general-equilibrium model with labor-market matching and endogenous job destruction. Cyclical fluctuations in the job-destruction rate magnify the output effects of shocks, as well as making them much more persistent. Interactions between capital adjust-ment and the job-destruction rate play an important role in generating persistence. Propagation effects are shown to be quantitatively substantial when the model is calibrated using job-flow data. Incorporating costly capital adjustment leads to significantly greater propagation. (JEL E24, E32) It has been well documented that the cyclical adjustment of labor input chiefly represents move-ment of workers into and out of employment, rather than adjustment of hours at given jobs. Thus, in understanding business cycles, it is cen-trally important to understand the formation and breakdown of employment relationships. The na-
295|Measuring the Cyclicality of Real Wages: How Important Is Composition Bias?&#034; NBER Working Paper No |In the period since the 1960s, as in other periods, aggregate time series on real wages have displayed only modest cyclicality. Macroeconomists therefore have described weak cyclicality of real wages as a salient feature of the business cycle. Contrary to this conventional wisdom, our analysis of longitudinal microdata indicates that real wages have been substantially procyclical since the 1960s. We show that the true procyclicality of real wages is obscured in aggregate time series because of a composition bias: the aggregate statistics are constructed in a way that gives more weight to low-skill workers during expansions than during recessions. I.
296|2001): “Pricing and matching with frictions |Suppose that n buyers each want one unit and m sellers each have one or more units of a good. Sellers post prices, and then buyers choose sellers. In symmetric equilibrium, similar sellers all post one price, and buyers randomize. Hence, more or fewer buyers may arrive than a seller can accommodate. We call this frictions. We solve for prices and the endogenous matching function for finite n and m and consider the limit as n and m grow. The matching function displays decreasing returns but converges to constant returns. We argue that the standard matching function in the literature is misspecified and discuss implications for the Beveridge curve. I.
297|Equilibrium Unemployment|A search-theoretic model of equilibrium unemployment is constructed and shown  to be consistent with the key regularities of the labor market and business cycle.
298|Changes in Unemployment Duration and Labor Force Attachment|This paper accounts for the observed increase in unemployment duration  relative to the unemployment rate in the U.S. over the past thirty years, typified  by the record low level of short-term unemployment. We show that part of the  increase is due to changes in how duration is measured, a consequence of the  1994 Current Population Survey redesign. Another part is due to the passage  of the baby boomers into their prime working years. After accounting for these  shifts, most of the remaining increase in unemployment duration relative to the  unemployment rate is concentrated among women, whose unemployment rate  has fallen sharply in the last two decades while their unemployment duration  has increased. Using labor market transition data, we show that this is a  consequence of the increase in women&#039;s labor force attachment.  # We are grateful to Giuseppe Bertola, Robert Solow, David Weiman, and participants in the Sustainable Employment Initiative conference for their comments and to Fran Horvath, Randy Ilg, Rowena Johnson, Bob McIntire, and Anne Polivka at the Bureau of Labor Statistics (BLS) for their help compiling the data. Ron Tucker of the Census Bureau and Clyde Tucker of the BLS provided useful information concerning the Current Population Survey redesign. Joydeep Roy provided valuable research assistance. Shimer acknowledges financial support from National Science Foundation grant SES-0079345 and the hospitality of the University of Chicago while part of this paper was written. Please address correspondence to shimer@princeton.edu.  1 
299|Contractual Fragility, Job Destruction and Business Cycles|. We develop a theory of labor contracting in which negative  productivity shocks lead to costly job loss, despite unlimited possibilities  for renegotiating wage contracts. Such fragile contracts emerge from ...rms&#039;  tradeos between robustness of incentives in ongoing employment relationships  and costly speci...c investment. In a matching market equilibrium, contractual  fragility serves as a powerful mechanism for propagating underlying productivity  shocks: in our benchmark calibration, i.i.d. shocks are magni...ed seven times  in their eect on aggregate output, and the eect is highly persistent. We also  explore novel motivations for government policies that strengthen employment  relationships.  1. Introduction  Popular discussion of recessions emphasizes the high costs borne by workers as a consequence of increased job loss. This view of recessions has in turn been amply documented by empirical research. Blanchard and Diamond (1990), for example, ...nd that gross #ows of workers...
300|The Roaring Nineties: Can Full Employment be Sustained|ences, a measure of rents, declined in a two-step sequence with a pattern and timing similar to movements in the Beveridge curve-a measure of matching efficiency. These comovements also match in some important ways the spotty data on the adoption of innovative work practices. This last point-parallel timing-is a key criterion of explanatory success. Any full account of the decline in the U.S. NAIRU should explain the timing and cause of the discrete inward shifts in the Beveridge curve that took place in
301|The Beveridge Curve, Job Creation and the Propagation of Shocks|This paper proposes modifications to the popular model of equilibrium unemployment by Mortensen and Pissarides [30]. I augment the model by introducing (1) costly planning for brand-new jobs, and (2) the option to mothball preexisting jobs; to develop new jobs requires a time-consuming planning process, whereas firms with preexisting jobs are allowed to mothball (temporarily freeze) their jobs, and to reactivate them with no planning lags. These modifications greatly improve the model’s ability to replicate the Beveridge curve as well as observed correlations between vacancies and job creation. It is also shown that persistent behavior of vacancies in the model serves to enhance the model’s propagation mechanism.  
302|Modern theory of unemployment fluctuations: Empirics and policy applications |Strong and widely accepted evidence shows that the natural rate of unemployment varies over time with substantial amplitude. The frictions in the labor market that account for positive normal levels of unemployment are not simple and mechanical. Instead, as a rich modern body of theory demonstrates, the natural rate of unemployment is an equilibrium in which the volumes of job-seeking by workers and worker-seeking by employers reach a balance controlled by fundamental determinants of the relative prices of the two activities. In recessions, unemployment rises, and job vacancies fall. The natural explanation is an economywide fall in labor demand. But a compelling model that generates a fall in labor demand without a counterfactual fall in productivity has eluded theorists to date. Nonetheless, policymakers have appropriately adopted the view that the natural rate varies over time and is not a simple benchmark for setting monetary instruments.
303|A new approach to the maximum flow problem|  All previously known efficient maximum-flow algorithms work by finding augmenting paths, either one path at a time (as in the original Ford and Fulkerson algorithm) or all shortest-length augmenting paths at once (using the layered network approach of Dinic). An alternative method based on the preflow concept of Karzanov is introduced. A preflow is like a flow, except that the total amount flowing into a vertex is allowed to exceed the total amount flowing out. The method maintains a preflow in the original network and pushes local flow excess toward the sink along what are estimated to be shortest paths. The algorithm and its analysis are simple and intuitive, yet the algorithm runs as fast as any other known method on dense. graphs, achieving an O(n³) time bound on an n-vertex graph. By incorporating the dynamic tree data structure of Sleator and Tarjan, we obtain a version of the algorithm running in O(nm log(n²/m)) time on an n-vertex, m-edge graph. This is as fast as any known method for any graph density and faster on graphs of moderate density. The algorithm also admits efticient distributed and parallel implementations. A parallel implementation running in O(n²log n) time using n processors and O(m) space is obtained. This time bound matches that of the Shiloach-Vishkin algorithm, which also uses n processors but requires O(n²) space.
304|A Structural Theory of Explanation-Based Learning|The impact of Explanation-Based Learning (EBL) on problem-solving efficiency varies greatly from one problem space to another. In fact, seemingly minute modifications to problem space encoding can drastically alter EBL&#039;s impact. For example, while prodigy/ebl  (a state-of-the-art EBL system) significantly speeds up the prodigy problem solver in the Blocksworld, prodigy/ebl actually slows prodigy down in a representational variant of the Blocksworld constructed by adding a single, carefully chosen, macro-operator to the Blocksworld operator set. Although EBL has been tested experimentally, no theory has been put forth that accounts for such phenomena. This paper presents such a theory. The theory exhibits a correspondence between a graph representation of problem spaces and the proofs used by EBL systems to generate search-control knowledge. The theory relies on this correspondence to account for the variations in EBL&#039;s impact. This account is validated by static, a program that extract...
305|Generating Parallel Execution Plans with a Partial-Order Planner|Many real-world planning problems require generating  plans that maximize the parallelism inherentina  problem. There are a number of partial-order planners  that generate such plans# however, in most of  these planners it is unclear under what conditions the  resulting plans will be correct and whether the planner  can even find a plan if one exists. This paper identifies  the underlying assumptions about when a partial plan  can be executed in parallel, defines the classes of parallel  plans that can be generated by different partial-order  planners, and describes the changes required to  turn ucpop into a parallel execution planner. In addition,  we describe how this planner can be applied to  the problem of query access planning, where parallel  execution produces substantial reductions in overall  execution time.  
306|The Need for Different Domain-Independent Heuristics|PRODIGY&#039;s planning algorithm uses domain-independent search heuristics. In this paper, we support our belief that there is no single search heuristic that performs more efficiently than others for all problems or in all domains. The paper presents three different domin-independent search heuristics of increasing complexity. We run PRODIGY with these heuristics in a series of artificial domains (introduced in (Barrett &amp; Weld 1994)) where in act one of the heuristics performs more efficiently than the others. However, we introduce an additional simple domain where the apparently worst heuristic outperforms the other two. The results we obtained...
307|Linkability: Examining Causal Link Commitments in Partial-Order Planning|Recently, several researchers have demonstrated domains where partially-ordered planners outperform totally-ordered planners. In (Barrett &amp; Weld 1994), Barrett and Weld build a series of artificial domains exploring the concepts of trivial and laborious serializability, in which a partially-ordered planner, SNLP, consistently outperforms two totally-ordered planners. In this paper, we demonstrate that...
308|Comparison of Methods for Improving Search Efficiency in a Partial-Order Planner|The search space in partial-order planning grows quickly with the number of subgoals and initial conditions, as well as less countable factors such as operator ordering and subgoal interactions. For partial-order planners to solve more than simple problems, the expansion of the search space will need to be controlled. This paper presents four new approaches to controlling search space expansion by exploiting commonalities in emerging plans. These approaches are described in terms of their algorithms, their effect on the completeness and correctness of the underlying planner and their expected performance. The four new and two existing approaches are compared on several metrics of search space and planning overhead. 1 Improving Search Efficiency in Planners  Partial order planning is becoming a common method of planning. Unfortunately, but hardly unexpectedly, the search space in partial order planning expands quickly as the problem size increases. Unfortunately, but less expectedly, se...
309|Statistical mechanics of complex networks |Complex networks describe a wide range of systems in nature and society, much quoted examples including the cell, a network of chemicals linked by chemical reactions, or the Internet, a network of routers and computers connected by physical links. While traditionally these systems were modeled as random graphs, it is increasingly recognized that the topology and evolution of real
312|Learning to rank using gradient descent|We investigate using gradient descent methods for learning ranking functions; we propose a simple probabilistic cost function, and we introduce RankNet, an implementation of these ideas using a neural network to model the underlying ranking function. We present test results on toy data and on data from a commercial internet search engine. 1.
313|IR evaluation methods for retrieving highly relevant documents|This paper proposes evaluation methods based on the use of non-dichotomous relevance judgements in IR experiments. It is argued that evaluation methods should credit IR methods for their ability to retrieve highly relevant documents. This is desirable from the user point of view in moderu large IR environments. The proposed methods are (1) a novel application of P-R curves and average precision computations based on separate recall bases for documents of different degrees of relevance, and (2) two novel measures computing the cumulative gain the user obtains by examining the retrieval result up to a given ranked position. We then demonstrate the use of these evaluation methods in a case study on the effectiveness of query types, based on combinations of query structures and expansion, in retrieving documents of various degrees of relevance. The test was run with a best match retrieval system (In- Query ) in a text database consisting of newspaper articles. The results indicate that the tested strong query structures are most effective in retrieving highly relevant documents. The differences between the query types are practically essential and statistically significant. More generally, the novel evaluation methods and the case demonstrate that non-dichotomous rele- vance assessments are applicable in IR experiments, may reveal interesting phenomena, and allow harder testing of IR methods. 1. 
314|Classification by pairwise coupling|We discuss a strategy for polychotomous classification that involves estimating class probabilities for each pair of classes, and then coupling the estimates together. The coupling model is similar to the Bradley-Terry method for paired comparisons. We study the nature of the class probability estimates that arise, and examine the performance of the procedure in real and simulated datasets. Classifiers used include linear discriminants, nearest neighbors, and the support vector machine. 
315|Boosting Algorithms as Gradient Descent|Much recent attention, both experimental and theoretical, has been focussed on classification algorithms which produce voted combinations of classifiers. Recent theoretical work has shown that the impressive generalization performance of algorithms like AdaBoost can be attributed to the classifier having large margins on the training data. We present an abstract algorithm for finding linear combinations of functions that minimize arbitrary cost functionals (i.e functionals that do not necessarily depend on the margin). Many existing voting methods can be shown to be special cases of this abstract algorithm. Then, following previous theoretical results bounding the generalization performance of convex combinations of classifiers in terms of general cost functions of the margin, we present a new algorithm (DOOM II) for performing a gradient descent optimization of such cost functions. Experiments on
316|Log-Linear Models for Label Ranking|Label ranking is the task of inferring a total order over a predefined set of  labels for each given instance. We present a general framework for batch  learning of label ranking functions from supervised data. We assume that  each instance in the training data is associated with a list of preferences  over the label-set, however we do not assume that this list is either complete  or consistent. This enables us to accommodate a variety of ranking  problems. In contrast to the general form of the supervision, our goal is  to learn a ranking function that induces a total order over the entire set  of labels. Special cases of our setting are multilabel categorization and  hierarchical classification. We present a general boosting-based learning  algorithm for the label ranking problem and prove a lower bound on the  progress of each boosting iteration. The applicability of our approach is  demonstrated with a set of experiments on a large-scale text corpus.
317|Online ranking/collaborative filtering using the perceptron algorithm|In this paper we present a simple to implement truly online large margin version of the Perceptron ranking (PRank) algorithm, called the OAP-BPM (Online Aggregate Prank-Bayes Point Machine) algorithm, which finds a rule that correctly ranks a given training sequence of instance and target rank pairs. PRank maintains a weight vector and a set of thresholds to define a ranking rule that maps each instance to its respective rank. The OAP-BPM algorithm is an extension of this algorithm by approximating the Bayes point, thus giving a good generalization performance. The Bayes point is approximated by averaging the weights and thresholds associated with several PRank algorithms run in parallel. In order to ensure diversity amongst the solutions of the PRank algorithms we randomly subsample the stream of incoming training examples. We also introduce two new online versions of Bagging and the voted Perceptron using the same randomization trick as OAP-BPM, hence are referred to as OAP with extension-Bagg and-VP respectively. A rank learning experiment was conducted on a synthetic data set and collaborative filtering experiments on a number of real world data sets were conducted, showing that OAP-BPM has a better performance compared to PRank and a pure online regression algorithm, albeit with a higher computational cost, though is not too prohibitive. 1.
318|Probabilistic Visual Learning for Object Representation|We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for unimodal distributions) and a Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition and coding. Our learning technique is applied to the probabilistic visual modeling, detection, recognition, and coding of human faces and non-rigid objects such as hands.
319|Snakes: Active contour models|A snake is an energy-minimizing spline guided by external constraint forces and influenced by image forces that pull it toward features such as lines and edges. Snakes are active contour models: they lock onto nearby edges, localizing them accurately. Scale-space continuation can be used to enlarge the cap-ture region surrounding a feature. Snakes provide a unified account of a number of visual problems, in-cluding detection of edges, lines, and subjective contours; motion tracking; and stereo matching. We have used snakes successfully for interactive interpretation, in which user-imposed constraint forces guide the snake near features of interest.
320|View-Based and Modular Eigenspaces for Face Recognition|In this work we describe experiments with eigenfaces for recognition and interactive search in a large-scale face database. Accurate visual recognition is demonstrated using a database of o(10^3) faces. The problem of  recognition under general viewing orientation is also explained. A view-based multiple-observer eigenspace technique is proposed for use in face recognition under variable pose. In addition, a modular eigenspace description technique is used which incorporates salient features such as the eyes, nose, mouth, in a eigenfeature layer. This modular representation yields higher recognition rates as well as a more robust framework for face recognition. An automatic feature extraction technique using feature eigentemplates is also demostrated. 
321|Active Shape Models -- &#034;Smart Snakes&#034;|We describe &#039;Active Shape Models&#039; which iteratively adapt to refine estimates of the pose, scale and shape of models of image objects. The method uses flexible models derived from sets of training examples. These models, known as Point Distribution Models, represent objects as sets of labelled points. An initial estimate of the location of the model points in an image is improved by attempting to move each point to a better position nearby. Adjustments to the pose variables and shape parameters are calculated. Limits are placed on the shape parameters ensuring that the example can only deform into shapes conforming to global constraints imposed by the training set. An iterative procedure deforms the model example to find the best fit to the image object. Results of applying the method are described. The technique is shown to be a powerful method for refining estimates of object shape and location. 
322|Modal Matching for Correspondence and Recognition|Modal matching is a new method for establishing correspondences and computing canonical descriptions. The method is based on the idea of describing objects in terms of generalized symmetries, as defined by each object&#039;s eigenmodes. The resulting modal description is used for object recognition and categorization, where shape similarities are expressed as the amounts of modal deformation energy needed to align the two objects. In general, modes provide a global-to-local ordering of shape deformation and thus allow for selecting which types of deformations are used in object alignment and comparison. In contrast to previous techniques, which required correspondence to be computed with an initial or prototype shape, modal matching utilizes a new type of finite element formulation that allows for an object&#039;s eigenmodes to be computed directly from available image information. This improved formulation provides greater generality and accuracy, and is applicable to data of any dimensionality. Correspondence results with 2-D contour and point feature data are shown, and recognition experiments with 2-D images of hand tools and airplanes are described.
323|Closed-Form Solutions for Physically Based Shape Modeling and Recognition| We present a closed-form, physically based solution for recovering a 3-D solid model from collections of 3-D surface measurements. Given a sufficient number of independent mea-surements, the solution is overconstrained and unique except for rotational symmetries. We then present a physically based object-recognition method that allows simple, closed-form comparisons of recovered 3-D solid models. The performance of these methods is evaluated using both synthetic range data with various signal-to-noise ratios and using laser rangefinder data. 
324|Human Face Detection in Visual Scenes|We present a neural network-based face detection system. A retinally connected neural network examines small windows of an image, and decides whether each window contains a face. The system arbitrates between multiple networks to improve performance over a single network. We use a bootstrap algorithm for training the networks, which adds false detections into the training set as training progresses. This eliminates the difficult task of manually selecting non-face training examples, which must be chosen to span the entire space of non-face images. Comparisons with other state-of-the-art face detection systems are presented; our system has better performance in terms of detection and false-positive rates. This work was partially supported by a grant from Siemens Corporate Research, Inc., by the Department of the Army, Army Research Office under grant number DAAH04-94-G-0006, and by the Office of Naval Research under grant number N00014-95-1-0591. This work was started while Shumeet Balu...
325|Face Recognition using View-Based and Modular Eigenspaces|In this paper we describe experiments using eigenfaces for recognition and interactive search in the FERET face database. A recognition accuracy of 99.35% is obtained using frontal views of 155 individuals. This figure is consistent with the 95% recognition rate obtained previously on a much larger database of 7,562 &#034;mugshots&#034; of approximately 3,000 individuals, consisting of a mix of all age and ethnic groups. We also demonstrate that we can automatically determine head pose without significantly lowering recognition accuracy; this is accomplished by use of a viewbased multiple-observer eigenspace technique. In addition, a modular eigenspace description is used which incorporates salient facial features such as the eyes, nose and mouth, in an eigenfeature layer. This modular representation yields slightly higher recognition rates as well as a more robust framework for face recognition. In addition, a robust and automatic feature detection technique using eigentemplates is demonstra...
326|Surface Learning with Applications to Lipreading|Most connectionist research has focused on learning mappings from one space to another (eg. classification and regression). This paper introduces the more general task of learning constraint surfaces. It describes a simple but powerful architecture for learning and manipulating nonlinear surfaces from data. We demonstrate the technique on low dimensional synthetic surfaces and compare it to nearest neighbor approaches. We then show its utility in learning the space of lip images in a system for improving speech recognition by lip reading. This learned surface is used to improve the visual tracking performance during recognition.  
327|Human Face Recognition and the Face Image Set&#039;s Topology|If we consider an n x n image as an n  2  dimensional vector, then images of faces can be considered as points in this n  2  -dimensional image space. Our previous studies of physical transformations of the face, including translation, small rotations and illumination changes, showed that the set of face images consists of relatively simple connected sub-regions in image space [1]. Consequently linear matching techniques can be used to obtain reliable face recognition. However for more general transformations, such as large rotations or scale changes, the face subregions become highly non-convex. We have therefore developed a scale-space matching technique that allows us to take advantage of knowledge about important geometrical transformations and about the topology of the face subregion in image space. While recognition of faces is the focus of this paper, the algorithm is sufficiently general to be applicable to a large variety of object recognition tasks.  List of Symbols  ffi: Gr...
328|Automating the Hunt for Volcanoes on Venus|Our long-term goal is to develop a trainable tool for locating patterns of interest in large image databases. Toward this goal we have developed a prototype system, based on classical filtering and statistical pattern recognition techniques, for automatically locating volcanoes in the Magellan SAR database of Venus. Training for the specific volcano-detection task is obtained by synthesizing feature templates (via normalization and principal components analysis) from a small number of examples provided by experts. Candidate regions identified by a focus of attention (FOA) algorithm are classified based on correlations with the feature templates. Preliminary tests show performance comparable to trained human observers. 1 Introduction  Many geological studies use surface features to deduce processes that have occurred on a planet. The recent JPL Magellan mission, which was successful in imaging over 95% of the surface of Venus with synthetic aperture radar (SAR), has provided planetary s...
329|On Comprehensive Visual Learning|1  Comprehensive visual learning is the treatment of theories and techniques for computer vision systems to automatically learn to understand comprehensive visual information with minimal human-imposed rules about the visual world. This article discusses some major performance difficulties encountered by currently prevailing approaches to computer vision and introduces the promising direction of comprehensive learning towards overcoming these difficulties. It also indicates why the direction may have a profound impact on the performance of computer vision algorithms for real world problems. Some example techniques for comprehensive visual learning are presented.  1 Introduction  An image of a real-world scene depends on a series of factors, illumination, object shape, surface reflectance, viewing geometry, sensor type, etc. The image is a result of compound interactions among these factors. In the real world, change in these factors is ubiquitous and mostly is not known a priori. This ...
330|A Guided Tour to Approximate String Matching|We survey the current techniques to cope with the problem of string matching allowing  errors. This is becoming a more and more relevant issue for many fast growing areas such  as information retrieval and computational biology. We focus on online searching and mostly  on edit distance, explaining the problem and its relevance, its statistical behavior, its history  and current developments, and the central ideas of the algorithms and their complexities. We  present a number of experiments to compare the performance of the different algorithms and  show which are the best choices according to each case. We conclude with some future work  directions and open problems.   
331|Modern Information Retrieval|Information retrieval (IR) has changed considerably in the last years with the expansion of the Web (World Wide Web) and the advent of modern and inexpensive graphical user interfaces and mass storage devices. As a result, traditional IR textbooks have become quite out-of-date which has led to the introduction of new IR books recently. Nevertheless, we believe that there is still great need of a book that approaches the field in a rigorous and complete way from a computer-science perspective (in opposition to a user-centered perspective). This book is an effort to partially fulfill this gap and should be useful for a first course on information retrieval as well as for a graduate course on the topic. The book
332|A New Approach to Text Searching |We introduce a family of simple and fast algorithms for solving the classical string matching problem, string matching with classes of symbols, don&#039;t care symbols and complement symbols, and multiple patterns. In addition we solve the same problems allowing up to k mismatches. Among the features of these algorithms are that they don&#039;t need to buffer the input, they are real time algorithms (for constant size patterns), and they are suitable to be implemented in hardware. 1 Introduction  String searching is a very important component of many problems, including text editing, bibliographic retrieval, and symbol manipulation. Recent surveys of string searching can be found in [17, 4]. The string matching problem consists of finding all occurrences of a pattern of length  m in a text of length n. We generalize the problem allowing &#034;don&#039;t care&#034; symbols, the complement of a symbol, and any finite class of symbols. We solve this problem for one or more patterns, with or without mismatches. Fo...
333|An O(ND) Difference Algorithm and Its Variations  (1986) |The problems of finding a longest common subsequence of two sequences A and B and a shortest edit script for transforming A into B have long been known to be dual problems. In this paper, they are shown to be equivalent to finding a shortest/longest path in an edit graph. Using this perspective, a simple O(ND) time and space algorithm is developed where N is the sum of the lengths of A and B and D is the size of the minimum edit script for A and B. The algorithm performs well when differences are small (sequences are similar) and is consequently fast in typical applications. The algorithm is shown to have O(N +D    expected-time performance under a basic stochastic model. A refinement of the algorithm requires only O(N) space, and the use of suffix trees leads to an O(NlgN +D    ) time variation.
334|A fast bit-vector algorithm for approximate string matching based on dynamic programming|Abstract. The approximate string matching problem is to find all locations at which a query of length m matches a substring of a text of length n with k-or-fewer differences. Simple and practical bit-vector algorithms have been designed for this problem, most notably the one used in agrep. These algorithms compute a bit representation of the current state-set of the k-difference automaton for the query, and asymptotically run in either O(nmk/w) orO(nm log ?/w) time where w is the word size of the machine (e.g., 32 or 64 in practice), and ? is the size of the pattern alphabet. Here we present an algorithm of comparable simplicity that requires only O(nm/w) time by virtue of computing a bit representation of the relocatable dynamic programming matrix for the problem. Thus, the algorithm’s performance is independent of k, and it is found to be more efficient than the previous results for many choices of k and small m. Moreover, because the algorithm is not dependent on k, it can be used to rapidly compute blocks of the dynamic programming matrix as in the 4-Russians algorithm of Wu et al. [1996]. This gives rise to an O(kn/w) expected-time algorithm for the case where m may be arbitrarily large. In practice this new algorithm, that computes a region of the dynamic programming (d.p.) matrix w entries at a time using the basic algorithm as a subroutine, is significantly faster than our previous 4-Russians algorithm, that computes the same region 4 or 5 entries at a time using table lookup. This performance improvement yields a code that is either superior or competitive with all existing algorithms except for some filtration algorithms that are superior when k/m is sufficiently small.
335|Approximate string matching|Approximate matching of strings is reviewed with the aim of surveying techniques suitable for finding an item in a database when there may be a spelling mistake or other error in the keyword. The methods found are classified as either equivalence or similarity problems. Equivalence problems are seen to be readily solved using canonical forms. For sinuiarity problems difference measures are surveyed, with a full description of the well-establmhed dynamic programming method relating this to the approach using probabilities and likelihoods. Searches for approximate matches in large sets using a difference function are seen to be an open problem still, though several promising ideas have been suggested. Approximate matching (error correction) during parsing is briefly reviewed.
336|Practical fast searching in strings|The problem of searching through text to find a specified substring is considered in a practical setting. It is discovered that a method developed by Boyer and Moore can outperform even special-purpose search instructions that may be built into the, computer hardware. For very short substrings however, these special purpose instructions are fastest-provided that they are used in an optimal way. KEY WORDS String searching Pattern matching Text editing Bibliographic search
337|Speeding Up Two String-Matching Algorithms| We show how to speed up two string-matching algorithms: the Boyer-Moore algorithm (BM algorithm), and its version called here the reverse factor algorithm (RF algorithm). The RF algorithm is based on factor graphs for the reverse of the pattern.The main feature of both algorithms is that they scan the text right-to-left from the supposed right position of the pattern. The BM algorithm goes as far as the scanned segment (factor) is a suffix of the pattern. The RF algorithm scans while the segment is a factor of the pattern. Both algorithms make a shift of the pattern, forget the history, and start again. The RF algorithm usually makes bigger shifts than BM, but is quadratic in the worst case. We show that it is enough to remember the last matched segment (represented by two pointers to the text) to speed up the RF algorithm considerably (to make a linear number of inspections of text symbols, with small coefficient), and to speed up the BM algorithm (to make at most 2.n comparisons). Only a constant additional memory is needed for the search phase. We give alternative versions of an accelerated RF algorithm: the first one is based on combinatorial properties of primitive words, and the other two use the power of suffix trees extensively. The paper demonstrates the techniques to transform algorithms, and also shows interesting new applications of data structures representing all subwords of the pattern in compact form.
338|Transducers and repetitions|Abstract. The factor transducer of a word associates to each of its factors (or subwc~rds) their first occurrence. Optimal bounds on the size of minimal factor transducers together with an algorithm for building them are given. Analogue results and a simple algorithm are given for the case of subsequential suffix transducers. Algorithms are applied to repetition searching in words. Rl~sum~. Le transducteur des facteurs d&#039;un mot associe a chacun de ses facteurs leur premiere occurrence. On donne des bornes optimales sur la taille du transducteur minimal d&#039;un mot ainsi qu&#039;un algorithme pour sa construction. On donne des r6sultats analogues et un algorithme simple dans le cas du transducteur sous-s~luentiel des suffixes d&#039;un mot. On donne une application la d6tection de r6p6titions dans les mots. Contents
339|Faster Approximate String Matching|We present a new algorithm for on-line approximate string matching. The algorithm is based on the simulation of a non-deterministic finite automaton built from the pattern and using the text as input. This simulation uses bit operations on a RAM machine with word length w = \Omega\Gamma137 n) bits, where n is the text size. This is essentially similar to the model used in Wu and Manber&#039;s work, although we improve the search time by packing the automaton states differently. The running time achieved is O(n) for small patterns (i.e. whenever mk = O(log n)),  where m is the pattern length and k ! m the number of allowed errors. This is in contrast with the result of Wu and Manber, which is O(kn) for m = O(log n). Longer patterns can be processed by partitioning the automaton into many machine words, at O(mk=w n) search cost. We allow generalizations in the pattern, such as classes of characters, gaps and others, at essentially the same search cost. We then explore other novel techniques t...
341|Text Retrieval: Theory and Practice|We present the state of the art of the main component of text retrieval systems: the searching engine. We outline the main lines of research and issues involved. We survey recently published results for text searching and we explore the gap between theoretical vs. practical algorithms. The main observation is that simpler ideas are better in practice.  1597 Shaks. Lover&#039;s Compl. 2 From off a hill whose concaue wombe reworded A plaintfull story from a sistring vale.  OED2, reword, sistering  1 1 Introduction  Full text retrieval systems are becoming a popular way of providing support for on-line text. Their main advantage is that they avoid the complicated and expensive process of semantic indexing. From the end-user point of view, full text searching of on-line documents is appealing because a valid query is just any word or sentence of the document. However, when the desired answer cannot be obtained with a simple query, the user must perform his/her own semantic processing to guess w...
342|Block Edit Models for Approximate String Matching|In this paper we examine string block edit distance, in which two strings A and B  are compared by extracting collections of substrings and placing them into correspondence. This model accounts for certain phenomena encountered in important real-world applications, including pen computing and molecular biology. The basic problem admits a family of variations depending on whether the strings must be matched in their entireties, and whether overlap is permitted. We show that several variants are NPcomplete, and give polynomial-time algorithms for solving the remainder. Keywords: block edit distance, approximate string matching, sequence comparison, approximate ink matching, dynamic programming. 1 Introduction  The edit distance model for string comparison [Lev66, NW70, WF74] has found widespread application in fields ranging from molecular biology to bird song classification [SK83]. A great deal of research has been devoted to this area, and numerous algorithms have been proposed for com...
343|Incremental String Comparison|The problem of comparing two sequences A and B to determine their LCS or the edit distance between them has been much studied. In this paper we consider the following incremental version of these problems: given an appropriate encoding of a comparison between A and B, can one incrementally compute the answer for A and bB, and the answer for A and Bb with equal efficiency, where b is an additional symbol? Our main result is a theorem exposing a surprising relationship between the dynamic programming solutions for two such &#034;adjacent&#034; problems. Given a threshold k  on the number of differences to be permitted in an alignment, the theorem leads directly to an O(k)  algorithm for incrementally computing a new solution from an old one, as contrasts the O(k²) time required to compute a solution from scratch. We further show with a series of applications that this algorithm is indeed more powerful than its non-incremental counterpart by solving the applications with greater asymptotic ef...
344|A Comparison of Approximate String Matching Algorithms|Experimental comparison of the running time of approximate string matching algorithms for the?differences problem is presented. Given a pattern string, a text string, and integer?, the task is to find all approximate occurrences of the pattern in the text with at most?differences (insertions, deletions, changes). We consider seven algorithms based on different approaches including dynamic programming, Boyer-Moore string matching, suffix automata, and the distribution of characters. It turns out that none of the algorithms is the best for all values of the problem parameters, and the speed differences between the methods can be considerable. 2??? KEY WORDS String matching Edit distance k differences problem
345|Block Addressing Indices for Approximate Text Retrieval|Although the issue of approximate text retrieval is gaining importance in the last years, it is currently addressed by only a few indexing schemes. To reduce space requirements, the indices may point to text blocks instead of exact word positions. This is called &#034;block addressing&#034;. The most notorious index of this kind is Glimpse. However, block addressing has not been well studied yet, especially regarding approximate searching. Our main contribution is an analytical study of the spacetime trade-offs related to the block size. We find that, under reasonable assumptions, it is possible to build an index which is simultaneously sublinear in space overhead and in query time. We validate the analysis with extensive experiments, obtaining typical performance figures. These results are valid not only for approximate searching queries but also for classical ones. Finally, we propose a new strategy for approximate searching on block addressing indices, which we experimentally find 4-5 times f...
346|A Suboptimal Lossy Data Compression Based On Approximate Pattern Matching|A practical suboptimal (variable source coding) algorithm for lossy data compression is presented. This scheme is based on approximate string matching, and it naturally extends the lossless Lempel-Ziv data compression scheme. Among others we consider the typical length of approximately repeated pattern within the first n positions of a stationary mixing sequence where D% of mismatches is allowed. We prove that there exists a constant r 0 (D) such that the length of such an approximately repeated pattern converges in probability to 1=r 0 (D) log n (pr.) but it almost surely oscillates between 1=r \Gamma1 (D) log n and 2=r 1 (D) log n,  where r \Gamma1 (D) ? r 0 (D) ? r 1 (D)=2 are some constants. These constants are natural generalizations of R&#039;enyi entropies to the lossy environment. More importantly, we show that the compression ratio of a lossy data compression scheme based on such an approximate pattern matching is asymptotically equal to r 0 (D). We also establish the asymptotic be...
347|NR-grep: A Fast and Flexible Pattern Matching Tool|We present nrgrep (&#034;nondeterministic reverse grep&#034;), a new pattern matching tool designed  for efficient search of complex patterns. Unlike previous tools of the grep family, such as agrep  and Gnu grep, nrgrep is based on a single and uniform concept: the bit-parallel simulation  of a nondeterministic suffix automaton. As a result, nrgrep can find from simple patterns to  regular expressions, exactly or allowing errors in the matches, with an efficiency that degrades  smoothly as the complexity of the searched pattern increases. Another concept fully integrated  into nrgrep and that contributes to this smoothness is the selection of adequate subpatterns for  fast scanning, which is also absent in many current tools. We show that the efficiency of nrgrep  is similar to that of the fastest existing string matching tools for the simplest patterns, and by  far unpaired for more complex patterns.
348|Approximate String Matching: A Simpler Faster Algorithm|Abstract. We give two algorithms for finding all approximate matches of a pattern in a text, where the edit distance between the pattern and the matching text substring is at most k. The first algorithm, which is quite simple, runs in time O ( nk3 + n + m) on all patterns except k-break periodic m strings (defined later). The second algorithm runs in time O ( nk4 + n + m) onk-break periodic m patterns. The two classes of patterns are easily distinguished in O(m) time.
349|Large Text Searching Allowing Errors|. We present a full inverted index for exact and approximate string matching in large texts. The index is composed of a table containing the vocabulary of words of the text and a list of positions in the text corresponding to each word. The size of the table of words is usually much less than 1% of the text size and hence can be kept in main memory, where most query processing takes place. The text, on the other hand, is not accessed at all. The algorithm permits a large number of variations of the exact and approximate string search problem, such as phrases, string matching with sets of characters (range and arbitrary set of characters, complements, wild cards), approximate search with nonuniform costs and arbitrary regular expressions. The whole index can be built in linear time, in a single sequential pass over the text, takes near 1=3 the space of the text, and retrieval times are near O(  p  n)  for typical cases. Experimental results show that the algorithm works well in practice...
350|Approximate multiple string search|Abstract. This paper presents a fast algorithm for searching a large text for multiple strings allowing one error. On a fast workstation, the algo-rithm can process a megabyte of text searching for 1000 patterns (with one error) in less than a second. Although we combine several interest-ing techniques, overall the algorithm is not deep theoretically. The emphasis of this paper is on the experimental side of algorithm design. We show the importance of careful design, experimentation, and utiliza-tion of current architectures. In particular, we discuss the issues of locality and cache performance, fast hash functions, and incremental hashing techniques. We introduce the notion of two-level hashing, which utilizes cache behavior to speed up hashing, especially in cases where unsuccessful searches are not uncommon. Two-level hashing may be useful for many other applications. The end result is also interesting by itself. We show that multiple search with one error is fast enough for most text applications. 1.
351|A Practical q-Gram Index for Text Retrieval Allowing Errors|We propose an indexing technique for approximate text searching, which is practical and powerful, and especially optimized for natural language text. Unlike other indices of this kind, it is able to retrieve any string that approximately matches the search pattern, not only words. Every text substring of a fixed length q is stored in the index, together with pointers to all the text positions where it appears. The search pattern is partitioned into pieces which are searched in the index, and all their occurrences in the text are verified for a complete match. To reduce space requirements, pointers to blocks instead of exact positions can be used, which increases querying costs. We design an algorithm to optimize the pattern partition into pieces so that the total number of verifications is minimized. This is especially well suited for natural language texts, and allows to know in advance the expected cost of the search and the expected relevance of the query to the user. We show experi...
352|Episode matching |Abstract. Given two words, text T of length n and episode P of length m, the episode matching problem is to find all minimal length substrings of text T that contain episode P as a subsequence. The respective optimization problem is to find the smallest number w, s.t. text T has a subword of length w which contains episode P. In this paper, we introduce a few efficient off-line as well as on-line algorithms for the entire problem, where by on-line algorithms we mean algorithms which search from left to right consecutive text symbols only once. We present two alphabet independent algorithms which work in time O(nm). The off-line algorithm operates in O(1) additional space while the on-line algorithm pays for its property with O(m) additional space. Two other on-line algorithms have subquadratic time complexity. One of them works in time O(nm/log m) and O(m) additional space. The other one gives a time/space trade-off, i.e., it works in time O(n + s +nm log log s ~ log(s/m)) when additional space is limited to O(s). Finally, we present two approximation algorithms for the optimization problem. The off-line algorithm is alphabet independent, it has superlinear time complexity O(n/e + nloglog(n/m)) and it uses only constant space. The on-line algorithm works in time O(n/e + n) and uses space O(m). Both approximation algorithms achieve 1 + e approximation ratio, for any e&gt; 0. 1
353|Pattern Matching with Swaps|Let a text string T of n symbols and a pattern string P of m symbols from alphabet \Sigma be given. A swapped version T  0  of T is a length n string derived from T by a series of local swaps,  (i.e. t  0  ` / t `+1 and t  0  `+1 / t ` ) where each element can participate in no more than one swap.  The Pattern Matching with Swaps problem is that of finding all locations i for which there exists a swapped version T  0  of T where there is an exact matching of P in location i of T  0  . It has been an open problem whether swapped matching can be done in less than O(mn) time. In this paper we show the first algorithm that solves the pattern matching with swaps problem in time o(mn). We present an algorithm whose time complexity is O(nm  1=3  log m log  2  min(m; j\Sigmaj))  for a general alphabet \Sigma.  Key Words: Design and analysis of algorithms, combinatorial algorithms on words, pattern matching, pattern matching with swaps, non-standard pattern matching.   Department of Mathematics...
354|Applications of Approximate Word Matching in Information Retrieval|As more online databases are integrated into digital libraries, the issue of quality control of the data becomes increasingly important, especially as it relates to the effective retrieval of information. The need to discover and reconcile variant forms of strings in bibliographic entries, i.e., authority work, will become more critical in the future. Spelling variants, misspellings, and transllteration differences will all increase the difficulty of retrieving information. Approximate string matching has traditionally been used to help with this problem. In this paper we introduce the notion of approximate word matching and show how it can be used to improve detection and categorization of variant forms.
355|On the Searchability of Electronic Ink|Pen-based computers and personal digital assistant&#039;s (PDA&#039;s) are new technologies that are growing in importance. In previous papers, we have espoused a philosophy we call &#034;Computing in the Ink Domain&#034; that treats ink as a first-class datatype. One of the most important questions that arises under this model concerns the searching of large quantities of previously stored pen-stroke data. In this paper, we examine the ink search problem. We present an algorithm based on a known dynamic programming technique, and examine its performance under a variety of circumstances.  Keywords: pen computing, approximate string matching, edit distance. 1 Introduction  Despite several early, high-profile &#034;flops,&#034; pen-based computers and personal digital assistants (PDA&#039;s) are important technologies that are now starting to find acceptance. This synthesis of new hardware and software raises many systems-level issues, including the possibility of new paradigms for human-computer interaction. In previous ...
356|Multiple Approximate String Matching|We present two new algorithms for on-line multiple approximate string matching. These are extensions of previous algorithms that search for a single pattern. The single-pattern version of the first one is based on the simulation with bits of a non-deterministic finite automaton built from the pattern and using the text as input. To search for multiple patterns, we superimpose their automata, using the result as a filter. The second algorithm partitions the pattern in sub-patterns that are searched with no errors, with a fast exact multipattern search algorithm. To handle multiple patterns, we search the sub-patterns of all of them together. The average running time achieved is in both cases O(n) for moderate error level, pattern length and number of patterns. They adapt (with higher costs) to the other cases. However, the algorithms differ in speed and thresholds of usefulness. We analyze theoretically when each algorithm should be used, and show experimentally that they are faster ...
357|Multiple Approximate String Matching by Counting|. We present a very simple and efficient algorithm for online multiple approximate string matching. It uses a previously known counting-based filter [9] that searches for a single pattern by quickly discarding uninteresting parts of the text. Our multi-pattern algorithm is based on the simulation of many parallel filters using bits of the computer word. Our average complexity to search r patterns of length m is  O(rn log m= log n), being n is the text size. We can search patterns of different length, each one with a different number of errors. We show experimentally that our algorithm is competitive with the fastest known algorithms, being the fastest for a wide range of intermediate error ratios. We give the first average-case analysis of the filtering efficiency of the counting method, applicable also to [9]. 1 Introduction  A number of important problems related to string processing lead to algorithms for approximate string matching: text searching, pattern recognition, computationa...
358|Fast String Matching with Mismatches|We describe and analyze three simple and fast algorithms on the average for solving the problem of string matching with a bounded number of mismatches. These are the naive algorithm, an algorithm based on the Boyer-Moore approach, and ad-hoc deterministic finite automata searching. We include simulation results that compare these algorithms to previous works. 1 Introduction  The problem of string matching with k mismatches consists of finding all occurrences of a pattern of length m in a text of length n such that in at most k positions the text and the pattern have different symbols. In the following, we assume that 0 ! k ! m and m  n. The case of k = 0 is the well known exact string matching problem, and if k = m the solution is trivial. Landau and Vishkin [LV86] gave the first efficient algorithm to solve this particular problem. Their algorithm uses O(kn + km log m)) time and O(k(n + m)) space. While it is fast, the space required is unacceptable for most practical purposes. Galil ...
359|Improving an Algorithm for Approximate Pattern Matching|We study a recent algorithm for fast on-line approximate string matching. This is the  problem of searching a pattern in a text allowing errors in the pattern or in the text. The  algorithm is based on a very fast kernel which is able to search short patterns using a nondeterministic  finite automaton, which is simulated using bit-parallelism. A number of techniques  to extend this kernel for longer patterns are presented in that work. However, the techniques  can be integrated in many ways and the optimal interplay among them is by no means obvious.  The solution to this problem starts at a very low level, by obtaining basic probabilistic  information about the problem which was not previously known, and ends integrating analytical  results with empirical data to obtain the optimal heuristic. The conclusions obtained via analysis  are experimentally confirmed. We also improve many of the techniques and obtain a combined  heuristic which is faster than the original work.  This work sho...
360|New and Faster Filters for Multiple Approximate String Matching|We present three new algorithms for on-line multiple string matching allowing errors. These  are extensions of previous algorithms that search for a single pattern. The average running  time achieved is in all cases linear in the text size for moderate error level, pattern length and number of patterns. They adapt (with higher costs) to the other cases. However, the algorithms  differ in speed and thresholds of usefulness. We analyze theoretically when each algorithm should be used, and show experimentally their performance. The only previous solution for this  problem allows only one error. Our algorithms are the first to allow more errors, and are faster  than previous work for a moderate number of patterns (e.g. less than 50-100 on English text, depending on the pattern length). 
361|Approximate string searching under weighted edit distance|Abstract. Let p ? S * be a string of length m and t ? S * be a string of length n. The approximate string searching problem is to find all approximate matches of p in t having weighted edit distance at most k from p. We present a new method that preprocesses the pattern into a DFA which scans t online in linear time, thereby recognizing all positions in t where an approximate match ends. We show how to reduce the exponential preprocessing effort and propose two practical algorithms. The first algorithm constructs the states of the DFA up to a certain depth r = 1. It runs in O(|S | r+1 · m + q · m + n) time and O(|S | r+1 + |S | r ·m) space where q = n decreases as r increases. The second algorithm constructs the transitions of the DFA when they are demanded. It runs in O(qs·|S|+qt·m+n) time and O(qs·(|S|+m)) space where qs = qt = n depend on the problem instance. Practical measurements show that our algorithms work well in practice and beat previous methods for problems of interest in molecular biology. 1
362|A Unified View to String Matching Algorithms|  We present a unified view to sequential algorithms for many  pattern matching problems, using a finite automaton built from the pattern  which uses the text as input. We show the limitations of deterministic  finite automata (DFA) and the advantages of using a bitwise  simulation of non-deterministic finite automata (NFA). This approach  gives very fast practical algorithms which have good complexity for small  patterns on a RAM machine with word length O(log n), where n is the  size of the text. For generalized string matching the time complexity is  O(mn= log n) which for small patterns is linear. For approximate string  matching we show that the two main known approaches to the problem  are variations of the NFA simulation. For this case we present a different  simulation technique which gives a running time of O(n) independently  of the maximum number of errors allowed, k, for small patterns. This  algorithm improves the best bit-wise or comparison based algorithms of  running ti...
363|A Partial Deterministic Automaton for Approximate String Matching|. One of the simplest approaches to approximate string matching is to consider the associated non-deterministic finite automaton and make it deterministic. Besides automaton generation, the search time is  O(n) in the worst case, where n is the text size. This solution is mentioned in the classical literature but has not been further pursued, due to the large number of automaton states that may be generated. We study the idea of generating the deterministic automaton on the fly. That is, we only generate the states that are actually reached when the text is traversed. We show that this limits drastically the number of states actually generated. Moreover, the algorithm is competitive, being the fastest one for intermediate error ratios and pattern lengths. 1 Introduction  Approximate string matching is one of the main problems in classical string algorithms, with applications to text searching, computational biology, pattern recognition, etc. The problem is defined as follows: given a t...
364|Improved Approximate Pattern Matching on Hypertext|. The problem of approximate pattern matching on hypertext is defined and solved by Amir et al. in O(m(n log m + e)) time, where  m is the length of the pattern, n is the total text size and e is the total number of edges. Their space complexity is O(mn). We present a new algorithm which is O(mk(n + e)) time and needs only O(n) extra space, where k ! m is the number of allowed errors in the pattern. If the graph is acyclic, our time complexity drops to O(m(n + e)), improving Amir&#039;s results. 1 Introduction  Approximate string matching problems appear in a number of important areas related to string processing: text searching, pattern recognition, computational biology, audio processing, etc. The edit distance between two strings a and b, ed(a; b), is defined as the minimum number of edit operations that must be carried out to make them equal. The allowed operations are insertion, deletion and substitution of characters in  a or b. The problem of approximate string matching is defined as...
365|Estimating the Probability of Approximate Matches|this paper addresses how to define S k (P ) and how to solve the algorithmic sub-problems involved in an efficient realization with respect to this definition. Section 2 introduces as our choice for S k (P ) the set of what we call the condensed, canonical edit scripts. Our choice attempts to keep small, both (i) the number of edit scripts for which X(s) = 0, and (ii) the size of g(v). Doing so improves the convergence of the estimator as it places S k (P ) and CN k (P ) in closer correspondence. The remaining sections present dynamic programming algorithms for the following subtasks:
366|Efficient Algorithms for Approximate String Matching with Swaps|this paper we include the swap operation that interchanges two adjacent characters  into the set of allowable edit operations, and we present an O(t min(m, n))-time  algorithm for the extended edit distance problem, where t is the edit distance  between the given strings, and an O(kn)-time algorithm for the extended k-differ-  ences problem. That is, we add swaps into the set of edit operations without  increasing the time complexities of previous algorithms that consider only changes,  insertions, and deletions for the edit distance and k-differences problems. # 1999  Academic Press  1. INTRODUCTION  Given two strings A[1}}}m] and B[1}}}n] over an alphabet 7, the edit distance between A and&lt;F12
367|Fast Multi-Dimensional Approximate Pattern Matching|. We address the problem of approximate string matching in  d dimensions, that is, to find a pattern of size m  d  in a text of size n  d  with at most k ! m  d  errors (substitutions, insertions and deletions along any dimension). We use a novel and very flexible error model, for which there exists only an algorithm to evaluate the similarity between two elements in two dimensions at O(m  4  ) time. We extend the algorithm to d dimensions, at O(d!m  2d  ) time and O(d!m  2d\Gamma1  ) space. We also give the first search algorithm for such model, which is O(d!m  d  n  d  ) time and O(d!m  d  n  d\Gamma1  ) space. We show how to reduce the space cost to O(d!3  d  m  2d\Gamma1  ) with little time penalty. Finally, we present the first sublinear-time (on average) searching algorithm (i.e. not all text cells are inspected), which is O(kn  d  =m  d\Gamma1  ) for k ! (m=(d(log oe m \Gamma log oe d)))  d\Gamma1  , where oe is the alphabet size. After that error level the filter still remains ...
368|Eraser: a dynamic data race detector for multithreaded programs|Multi-threaded programming is difficult and error prone. It is easy to make a mistake in synchronization that produces a data race, yet it can be extremely hard to locate this mistake during debugging. This paper describes a new tool, called Eraser, for dynamically detecting data races in lock-based multi-threaded programs. Eraser uses binary rewriting techniques to monitor every shared memory reference and verify that consistent locking behavior is observed. We present several case studies, including undergraduate coursework and a multi-threaded Web search engine, that demonstrate the effectiveness of this approach. 1
369|Time, Clocks, and the Ordering of Events in a Distributed System|The concept of one event happening before another in a distributed system is examined, and is shown to define a partial ordering of the events. A distributed algorithm is given for synchronizing a system of logical clocks which can be used to totally order the events. The use of the total ordering is illustrated with a method for solving synchronization problems. The algorithm is then specialized for synchronizing physical clocks, and a bound is derived on how far out of synchrony the clocks can become.  
370|Atom: A system for building customized program analysis tools|research relevant to the design and application of high performance scientific computers. We test our ideas by designing, building, and using real systems. The systems we build are research prototypes; they are not intended to become products. There is a second research laboratory located in Palo Alto, the Systems Research Center (SRC). Other Digital research groups are located in Paris (PRL) and in Cambridge,
371|Monitors: An Operating System Structuring Concept|This is a digitized copy derived from an ACM copyrighted work. It is not guaranteed to be an accurate copy of the author&#039;s original work. This paper develops Brinch-Hansen&#039;s concept of a monitor as a method of structuring an operating system. It introduces a form of synchronization, describes a possible rnctltotl of implementation in terms of semaphorcs and gives a suitable proof rule. Illustrative examples include a single rcsourcc scheduler, a bounded buffer, an alarm clock, a buffer pool, a disk head optimizer, and a version of the problem of readers and writers. Key Words and Phrases: monitors, operating systems,schcduling, mutual exclusion, synchronization, system implementation langua yes, structured multiprogramming CR Categories:
372|Extensibility, safety and performance in the SPIN operating system|This paper describes the motivation, architecture and performance of SPIN, an extensible operating system. SPIN provides an extension infrastructure, together with a core set of extensible services, that allow applications to safely change the operating system&#039;s interface and implementation. Extensions allow an application to specialize the underlying operating system in order to achieve a particular level of performance and functionality. SPIN uses language and link-time mechanisms to inexpensively export ne-grained interfaces to operating system services. Extensions are written in a type safe language, and are dynamically linked into the operating system kernel. This approach o ers extensions rapid access to system services, while protecting the operating system code executing within the kernel address space. SPIN and its extensions are written in Modula-3 and run on DEC Alpha workstations. 1
373|Petal: Distributed Virtual Disks|The ideal storage system is globally accessible, always available, provides unlimited performance and capacity for a large number of clients, and requires no management. This paper describes the design, implementation, and performance of Petal, a system that attempts to approximate this ideal in practice through a novel combination of features. Petal consists of a collection of networkconnected servers that cooperatively manage a pool of physical disks. To a Petal client, this collection appears as a highly available block-level storage system that provides large abstract containers called virtual disks. A virtual disk is globally accessible to all Petal clients on the network. A client can create a virtual disk on demand to tap the entire capacity and performance of the underlying physical resources. Furthermore, additional resources, such as servers and disks, can be automatically incorporated into Petal. We have an initial Petal prototype consisting of four 225 MHz DEC 3000/700 work...
375|Shasta: A LowOverhead Software-Only Approach to Fine-Grain Shared Memory|Digital Equipment Corporation This paper describes Shasta, a system that supports a shared address space in software on clusters of computers with physically distributed memory. A unique aspect of Shasta compared to most other software distributed shared memory systems is that shared data can be kept coherent at a fine granularity. In addition, the system allows the coherence granularity to vary across different shared data structures in a single application. Shasta implements the shared address space by transparently rewriting the application executable to intercept loads and stores. For each shared load or store, the inserted code checks to see if the data is available locally and communicates with other processors if necessary. The system uses numerous techniques to reduce the run-time overhead of these checks. Since Shasta is implemented entirely in software,
376|Experience with Processes and Monitors in Mesa|The use of monitors for describing concurrency has been much discussed in the literature. When monitors are used in real systems of any size, however, a number of problems arise which have not been adequately dealt with: the semantics of nested monitor calls; the various ways of defining the meaning of WAIT; priority scheduling; handling of timeouts, aborts and other exceptional conditions; interactions with process creation and destruction; monitoring large numbers of small objects. These problems are addressed by the facilities described here for concurrent programming in Mesa. Experience with several substantial applications gives us some confidence in the validity of our solutions.
377|On-the-fly Detection of Data Races for Programs with Nested Fork-Join Parallelism|Detecting data races in shared-memory parallel programs is an important debugging problem. This paper presents a new protocol for run-time detection of data races in executions of shared-memory programs with nested fork-join parallelism and no other interthread synchronization. This protocol has significantly smaller worst-case run-time overhead than previous techniques. The worst-case space required by our protocol when monitoring an execution of a program P is O(V N), where V is the number of shared variables in P , and N is the maximum dynamic nesting of parallel constructs in P &#039;s execution. The worst-case time required to perform any monitoring operation is O(N). We formally prove that our new protocol always reports a non-empty subset of the data races in a monitored program execution and describe how this property leads to an e#ective debugging strategy.
378|Online Data-Race Detection via Coherency Guarantees|We present the design and evaluation of an on-thefly data-race-detection technique that handles applications written for the lazy release consistent (LRC) shared memory model. We require no explicit association between synchronization and shared memory. Hence, shared accesses have to be tracked and compared at the minimum granularity of data accesses, which is typically a single word.  The novel aspect of this system is that we are able to leverage information used to support the underlying memory abstraction to perform on-the-fly data-race detection, without compiler support. Our system consists of a minimally modified version of the CVM distributed shared memory system, and instrumentation code inserted by the ATOM code re-writer.  We present an experimental evaluation of our technique by using our system to look for data races in four unaltered programs. Our system correctly found read-write data races in a program that allows unsynchronized read access to a global tour bound, and a write-write race in a program from a standard benchmark suite. Overall, our mechanism reduced program performance by approximately a factor of two.   
379|Compile-time Support for Efficient Data Race Detection in Shared-Memory Parallel Programs|Data race detection strategies based on software runtime monitoring are notorious for dramatically inflating execution times of shared-memory parallel programs. Without significant reductions in the execution overhead incurred when using these techniques, there is little hope that they will be widely used. A promising approach to this problem is to apply compile-time analysis to identify variable references that need not be monitored at run time because they will never be involved in a data race. In this paper we describe eraser, a data race instrumentation tool that uses aggressive program analysis to prune the number of references to be monitored. To quantify the effectiveness of our analysis techniques, we compare the overhead of race detection with three levels of compile-time analysis ranging from little analysis to aggressive interprocedural analysis for a suite of test programs. For the programs tested, using interprocedural analysis and dependence analysis dramatically reduced ...
380|Learning the Kernel Matrix with Semi-Definite Programming|Kernel-based learning algorithms work by embedding the data into a Euclidean space, and  then searching for linear relations among the embedded data points. The embedding is performed  implicitly, by specifying the inner products between each pair of points in the embedding  space. This information is contained in the so-called kernel matrix, a symmetric and positive  definite matrix that encodes the relative positions of all points. Specifying this matrix amounts  to specifying the geometry of the embedding space and inducing a notion of similarity in the  input space---classical model selection problems in machine learning. In this paper we show how  the kernel matrix can be learned from data via semi-definite programming (SDP) techniques. When applied
381|Online Learning with Kernels|  Kernel based algorithms such as support vector machines have achieved considerable success in various problems in the batch setting where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper we consider online learning in a Reproducing Kernel Hilbert Space. By considering classical stochastic gradient descent within a feature space, and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst case loss bounds and moreover we show the convergence of the hypothesis to the minimiser of the regularised risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection. In addition
382|Using SeDuMi 1.02, a MATLAB toolbox for optimization over symmetric cones|SeDuMi is an add-on for MATLAB, that lets you solve optimization problems with linear, quadratic and semidefiniteness constraints. It is possible to have complex valued data and variables in SeDuMi. Moreover, large scale optimization problems are solved efficiently, by exploiting sparsity. This paper describes how to work with this toolbox.
384|Arcing Classifiers|Recent work has shown that combining multiple versions of unstable classifiers such as trees or neural nets results in reduced test set error. One of the more effective is bagging (Breiman [1996a] ) Here, modified training sets are formed by resampling from the original training set, classifiers constructed using these training sets and then combined by voting. Freund and Schapire [1995,1996] propose an algorithm the basis of which is to adaptively resample and combine  (hence the acronym--arcing) so that the weights in the resampling are increased for those cases most often misclassified and the combining is done by weighted voting. Arcing is more successful than bagging in test set error reduction. We explore two arcing algorithms, compare them to each other and to bagging, and try to understand how arcing works. We introduce the definitions of bias and variance for a classifier as components of the test set error. Unstable classifiers can have low bias on a large range of data sets....
385|On kernel-target alignment|Editor: Kernel based methods are increasingly being used for data modeling because of their conceptual simplicity and outstanding performance on many tasks. However, the kernel function is often chosen using trial-and-error heuristics. In this paper we address the problem of measuring the degree of agreement between a kernel and a learning task. A quantitative measure of agreement is important from both a theoretical and practical point of view. We propose a quantity to capture this notion, which we call Alignment. We study its theoretical properties, and derive a series of simple algorithms for adapting a kernel to the labels and vice versa. This produces a series of novel methods for clustering and transduction, kernel combination and kernel selection. The algorithms are tested on two publicly available datasets and are shown to exhibit good performance.
386|DETERMINANT MAXIMIZATION WITH LINEAR MATRIX INEQUALITY CONSTRAINTS   | The problem of maximizing the determinant of a matrix subject to linear matrix inequalities arises in many fields, including computational geometry, statistics, system identification, experiment design, and information and communication theory. It can also be considered as a generalization of the semidefinite programming problem. We give an overview of the applications of the determinant maximization problem, pointing out simple cases where specialized algorithms or analytical solutions are known. We then describe an interior-point method, with a simplified analysis of the worst-case complexity and numerical results that indicate that the method is very efficient, both in theory and in practice. Compared to existing specialized algorithms (where they are available), the interior-point method will generally be slower; the advantage is that it handles a much wider variety of problems.  
387|Diffusion kernels on graphs and other discrete input spaces|The application of kernel-based learning algorithms has, so far, largely been confined to realvalued data and a few special data types, such as strings. In this paper we propose a general method of constructing natural families of kernels over discrete structures, based on the matrix exponentiation idea. In particular, we focus on generating kernels on graphs, for which we propose a special class of exponential kernels called diffusion kernels, which are based on the heat equation and can be regarded as the discretization of the familiar Gaussian kernel of Euclidean space.
388|Empirical margin distributions and bounding the generalization error of combined classifiers|Dedicated to A.V. Skorohod on his seventieth birthday We prove new probabilistic upper bounds on generalization error of complex classifiers that are combinations of simple classifiers. Such combinations could be implemented by neural networks or by voting methods of combining the classifiers, such as boosting and bagging. The bounds are in terms of the empirical distribution of the margin of the combined classifier. They are based on the methods of the theory of Gaussian and empirical processes (comparison inequalities, symmetrization method, concentration inequalities) and they improve previous results of Bartlett (1998) on bounding the generalization error of neural networks in terms of l1-norms of the weights of neurons and of Schapire, Freund, Bartlett and Lee (1998) on bounding the generalization error of boosting. We also obtain rates of convergence in Lévy distance of empirical margin distribution to the true margin distribution uniformly over the classes of classifiers and prove the optimality of these rates.
389|Text Categorization by Boosting Automatically Extracted Concepts|Term-based representations of documents have found widespread use in information retrieval. However, one of the main shortcomings of such methods is that they largely disregard lexical semantics and, as a consequence, are not sufficiently robust with respect to variations in word usage. In this paper we investigate the use of concept-based document representations to supplement word- or phrase-based features. The utilized concepts are automatically extracted from documents via probabilistic latent semantic analysis. We propose to use AriaBoost to optimally combine weak hypotheses based on both types of features. Experimental results on standard benchmarks confirm the validity of our approach, showing that AriaBoost achieves consistent improvements by including additional semantic features in the learned ensemble.
391|Support Vector Machines for Text Categorization Based on Latent Semantic Indexing|Text Categorization(TC) is an important component in  many information organization and information  management tasks. Two key issues in TC are feature  coding and classifier design. In this paper Text  Categorization via Support Vector Machines(SVMs)  approach based on Latent Semantic Indexing(LSI) is  described. Latent Semantic Indexing[1][2] is a method for  selecting informative subspaces of feature spaces with the  goal of obtaining a compact representation of document. Support Vector Machines[3] are powerful machine  learning systems, which combine remarkable performance with an elegant theoretical framework. The SVMs well  fits the Text Categorization task due to the special  properties of text itself. Experiments show that the  LSI+SVMs frame improves clustering performance by  focusing attention of Support Vector Machines onto  informative subspaces of the feature spaces.
392|Convex tuning of the soft margin parameter|In order to deal with known limitations of the hard margin support vector machine (SVM) for binary classication | such as overtting and the fact that some data sets are not linearly separable |, a soft margin approach has been proposed in literature [2, 4, 5]. The soft margin SVM allows training data to be misclassied to a certain extent, by introducing slack variables and penalizing the cost function with an error term, i.e., the 1-norm or 2-norm of the corre-sponding slack vector. A regularization parameter C trades o  the importance of maximizing the margin versus minimizing the error. While the 2-norm soft margin algorithm itself is well understood, and a generalization bound is known [4, 5], no computationally tractable method for tuning the soft margin parameter C has been proposed so far. In this report we present a convex way to optimize C for the 2-norm soft margin SVM, by maximizing this generalization bound. The resulting problem is a quadratically constrained quadratic programming (QCQP) problem, which can be solved in polynomial time O(l3) with l the number of training samples. 1
393|Multiple sequence alignment with the Clustal series of programs|The Clustal series of programs are widely used in molecular biology for the multiple alignment of both nucleic acid and protein sequences and for preparing phylogenetic trees. The popularity of the programs depends on a number of factors, including not only the accuracy of the results, but also the robustness, portability and user-friendliness of the programs. New features include NEXUS and FASTA format output, printing range numbers and faster tree calculation. Although, Clustal was originally developed to run on a local computer, numerous Web servers have been set up, notably at the EBI
394|The Pfam protein families database|Pfam is a large collection of protein families and domains. Over the past 2 years the number of families in Pfam has doubled and now stands at 6190 (version 10.0). Methodology improvements for searching the Pfam collection locally as well as via the web are described. Other recent innovations include modelling of discontinuous domains allow-ing Pfam domain de®nitions to be closer to those found in structure databases. Pfam is available on the web in the UK
395|Optimal alignments in linear space|Space, not time, is often the limiting factor when computing optimal sequence alignments, and a number of recent papers in the biology literature have proposed space-saving strategies. However, a 1975 computer science paper by Hirschberg presented a method that is superior to the newer proposals, both in theory and in practice. The goal of this note is to give Hirschberg’s idea the visibility it deserves by developing a linear-space version of Gotoh’s algorithm, which accommodates affine gap penalties. A portable C-software package implementing this algorithm is available on the BIONET free of charge.
396|NEXUS: an extensible file format for systematic information|Abstract.—NEXUS is a file format designed to contain systematic data for use by computer pro-grams. The goals of the format are to allow future expansion, to include diverse kinds of infor-mation, to be independent of particular computer operating systems, and to be easily processed by a program. To this end, the format is modular, with a file consisting of separate blocks, each containing one particular kind of information, and consisting of standardized commands. Public blocks (those containing information utilized by several programs) house information about taxa, morphological and molecular characters, distances, genetic codes, assumptions, sets, trees, etc.; private blocks contain information of relevance to single programs. A detailed description of commands in public blocks is given. Guidelines are provided for reading and writing NEXUS files and for extending the format. [Computer program; file format; NEXUS; systematics.] NEXUS is a file format designed to house systematic data. Although it is cur-rently in use by several computer pro-grams, including MacClade 3.07 (Maddi-
397|A Roadmap of Agent Research and Development|  This paper provides an overview of research and development activities in the field of autonomous agents and multi-agent systems. It aims to identify key concepts and applications, and to indicate how they relate to one-another. Some historical context to the field of agent-based computing is given, and contemporary research directions are presented. Finally, a range of open issues and future challenges are highlighted.
398|The tragedy of the commons|At the end of a thoughtful article on the future of nuclear war, Wiesner and York (1) concluded that: “Both sides in the arms race are... confronted by the dilemma of steadily increasing military power and steadily de-creasing national security. It is our considered professional judgment that this dilemma has no technical solution. If the great powers continue to look for solutions in the area of science and technology only, the result will be to worsen the situation.” I would like to focus your attention not on the subject of the article (national secu-rity in a nuclear world) but on the kind of conclusion they reached, namely that there
399|Intelligence Without Representation|Artificial intelligence research has foundered on the issue of representation. When intelligence is approached in an incremental manner, with strict reliance on interfacing to the real world through perception and action, reliance on representation disappears. In this paper we outline our approach to incrementally building complete intelligent Creatures. The fundamental decomposition of the intelligent system is not into independent information processing units which must interface with each other via representations. Instead, the intelligent system is decomposed into independent and parallel activity producers which all interface directly to the world through perception and action, rather than interface to each other particularly much. The notions of central and peripheral systems evaporateeverything is both central and peripheral. Based on these principles we have built a very successful series of mobile robots which operate without supervision as Creatures in standard office environ...
401|Intelligence without reason|Computers and Thought are the two categories that together define Artificial Intelligence as a discipline. It is generally accepted that work in Artificial Intelligence over the last thirty years has had a strong influence on aspects of computer architectures. In this paper we also make the converse claim; that the state of computer architecture has been a strong influence on our models of thought. The Von Neumann model of computation has lead Artificial Intelligence in particular directions. Intelligence in biological systems is completely different. Recent work in behavior-based Artificial Intelligence has produced new models of intelligence that are much closer in spirit to biological systems. The non-Von Neumann computational models they use share many characteristics with biological computation.
404|The Role of Emotion in Believable Agents|Articial intelligence researchers attempting to create engaging  apparently living creatures may nd important insight in the work of artists who have explored the idea of believable character  In particular  appropriately timed and clearly expressed emotion is a central requirement for believable characters  We discuss these ideas and suggest how they may apply to believable interactive characters  which we call believable agents This work was supported in part by Fujitsu Laboratories and Mitsubishi Electric Research Laborato ries  The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the ocial policies  either expressed or implied  of any other parties Keywords  articial intelligence  emotion  believable agents art  animation  believable characters  BELIEVABILITY   Believability There is a notion in the Arts of believable character  It does not mean an honest or reliable character  but one that provides the illusion of life  and thus permits the audience s suspension of disbelief The idea of believability has long been studied and explored in literature  theater lm  radio drama  and other media  Traditional character animators are among those artists who have sought to create believable characters  and the Disney animators of the   	 s made great strides toward this goal  The rst page of the enormous classic reference work on Disney animation Thomas and Johnston     begins with these words Disney animation makes audiences really believe in   characters  whose adventures and misfortunes make people laugh  and even cry  There is a special ingredient in our type of animation that produces drawings that appear to think and make decisions and act of their own volition  it is what creates the illusion of life Many articial intelligence researchers have long wished to build robots  and their cousins called agents  that seem to think  feel  and live  These are creatures with whom you	d want to share some of your life  as with a companion  or a social pet For instance  in his 
405|Collaborative plans for complex group action|The original formulation of SharedPlans by B. Grosz and C. Sidner ( 1990) was developed to provide a model of collaborative planning in which it was not necessary for one agent to have intentions-to toward an act of a different agent. Unlike other contemporaneous approaches (J.R. Searle, 1990), this formulation provided for two agents to coordinate their activities without introducing any notion of irreducible joint intentions. However, it only treated activities that directly decomposed into single-agent actions, did not address the need for agents to commit to their joint activity, and did not adequately deal with agents having only partial knowledge of the way in which to perform an action. This paper provides a revised and expanded version of SharedPlans that addresses these shortcomings. It also reformulates Pollack’s ( 1990) definition of individual plans to handle cases in which a single agent has only partial knowledge; this reformulation meshes with the definition of SharedPlans. The new definitions also allow for contracting out certain actions. The formalization that results has the features required by Bratrnan’s ( 1992) account of shared cooperative activity and is more general than alternative accounts (H. Levesque et al., 1990; E. Sonenberg et al., 1992).  
406|Plans And Resource-Bounded Practical Reasoning|An architecture for a rational agent must allow for means-end reasoning, for the weighing of competing alternatives, and for interactions between these two forms of reasoning. Such an architecture must also address the problem of resource boundedness. We sketch a solution of the first problem that points the way to a solution of the second. In particular, we present a high-level specification of the practical-reasoning component of an architecture for a resource-bounded rational agent. In this architecture, a major role of the agent&#039;s plans is to constrain the amount of further practical reasoning she must perform.
407|Kasbah: An Agent Marketplace for Buying and Selling Goods|While there are many Web services which help users find things to buy,we know of none which actually try to automate the process of buying and selling. Kasbah is a virtual marketplace on the Web where users create autonomous agents to buy and sell goods on their behalf. Users specify parameters to guide and constrain an agent&#039;s overall behavior. A simple prototype has been built to test the viability of this concept.
408|Experiences with an Architecture for Intelligent, Reactive Agents  |This paper describes an implementation of the 3T robot architecture  which has been under development for the last eightyears. The architecture  uses three levels of abstraction and description languages whichare  compatible between levels. The makeup of the architecture helps to coordinate  planful activities with real-time behaviors for dealing with dynamic  environments. In recent years, other architectures have been created with  similar attributes but two features distinguish the 3T architecture: 1) a  variety of useful software tools have been created to help implement this  architecture on multiple real robots;, and 2) this architecture, or parts of it, have been implemented on a varietyofvery different robot systems  using different processors, operating systems, effectors and sensor suites.
409|A Scalable Comparison-Shopping Agent for the World-Wide Web|The Web is less agent-friendly than we might hope. Most information on the Web is presented in loosely structured natural language text with no agent-readable semantics. HTML annotations structure the display of Web pages, but provide virtually no insight into their content. Thus, the designers of intelligent Web agents need to address the following questions: (1) To what extent can an agent understand information published at Web sites? (2) Is the agent&#039;s understanding sufficient to provide genuinely useful assistance to users? (3) Is site-specific hand-coding necessary, or can the agent automatically extract information from unfamiliar Web sites? (4) What aspects of the Web facilitate this competence? In this paper we investigate these issues with a case study using the ShopBot. ShopBot is a fullyimplemented, domain-independent comparison-shopping agent. Given the home pages of several on-line stores, ShopBot autonomously learns how to shop at those vendors. After its learning is com...
410|Controlling Cooperative Problem Solving in Industrial Multi-Agent Systems using Joint Intentions|One reason why Distributed AI (DAI) technology has been deployed in relatively few real-size applications is that it lacks a clear and implementable model of cooperative problem solving which specifies how agents should operate and interact in complex, dynamic and unpredictable environments. As a consequence of the experience gained whilst building a number of DAI systems for industrial applications, a new principled model of cooperation has been developed. This model, called Joint Responsibility, has the notion of joint intentions at its core. It specifies pre-conditions which must be attained before collaboration can commence and prescribes how individuals should behave both when joint activity is progressing satisfactorily and also when it runs into difficulty. The theoretical model has been used to guide the implementation of a general-purpose cooperation framework and the qualitative and quantitative benefits of this implementation have been assessed through a series of comparativ...
411|Commitments and conventions: The foundation of coordination in multi-agent systems|Distributed Artificial Intelligence systems, in which multiple agents interact to improve their individual performance and to enhance the system’s overall utility, are becoming an increasingly pervasive means of conceptualising a diverse range of applications. As the discipline matures, researchers are beginning to strive for the underlying theories and principles which guide the central processes of coordination and cooperation. Here agent communities are modelled using a distributed goal search formalism and it is argued that commitments (pledges to undertake a specified course of action) and conventions (means of monitoring commitments in changing circumstances) are the foundation of coordination in multi-agent systems. An analysis of existing coordination models which use concepts akin to commitments and conventions is undertaken before a new unifying framework is presented. Finally a number of prominent coordination techniques which do not explicitly involve commitments or conventions are reformulated in these terms to demonstrate their compliance with the central hypothesis of this paper. 1
412|Middle-Agents for the Internet|Like middle-men in physical commerce, middleagents  support the flow of information in electronic  commerce, assisting in locating and connecting the  ultimate information provider with the ultimate information  requester. Many different types of middleagents  will be useful in realistic, large, distributed,  open multi-agent problem solving systems. These  include matchmakers or yellow page agents that process  advertisements, blackboard agents that collect requests,  and brokers that process both. The behaviors  of each type of middle-agent have certain performance  characteristics---privacy, robustness, and  adaptiveness qualities---that are related to characteristics  of the external environment and of the agents  themselves. For example, while brokered systems are  more vulnerable to certain failures, they are also able  to cope more quickly with a rapidly fluctuating agent  workforce and meet certain privacy considerations.  This paper identifies a spectrum of middle-agents,  cha...
413|WebMate: A Personal Agent for Browsing and Searching|The World-Wide Web is developing very fast. Currently, finding useful information on the Web is a time consuming process. In this paper, we present WebMate, an agent that helps users to effectively browse and search the Web. WebMate extends the state of the art in Web-based information retrieval in many ways. First, it uses multiple TF-IDF vectors to keep track of user interests in different domains. These domains are automatically learned by WebMate. Second, WebMate uses the Trigger Pair Model to automatically extract keywords for refining document search. Third, during search, the user can provide multiple pages as similarity/relevance guidance for the search. The system extracts and combines relevant keywords from these relevant pages and uses them for keyword refinement. Using these techniques, WebMate provides effective browsing and searching help and also compiles and sends to users personal newspaper by automatically spiding news sources. We have experimentally evaluated the per...
414|Designing a Family of Coordination Algorithms|Many researchers have shown that there is no single best organization or coordination mechanism  for all environments. This paper discusses the design and implementation of an extendable  family of coordination mechanisms, called Generalized Partial Global Planning (GPGP). The set  of coordination mechanisms described here assists in scheduling activities for teams of cooperative  computational agents. The GPGP approach has several unique features. First, it is not tied to  a single domain. Each mechanism is defined as a response to certain features in the current task  environment. We show that different combinations of mechanisms are appropriate for different  task environments. Secondly, the approach works in conjunction with an agent&#039;s existing local  planner/scheduler. Finally, the initial set of five mechanisms presented here generalizes and extends  the Partial Global Planning (PGP) algorithm. In comparison to PGP, GPGP allows more  agent heterogeneity, it exchanges less global ...
415|editors. Blackboard Systems|Retroviral vectors containing putative internal ribosome entry sites: development of a polycistronic gene transfer
416|Trends in Cooperative Distributed Problem Solving|Introduction Cooperative Distributed Problem-Solving (CDPS) studies how a loosely-coupled network of problem solvers can work together to solve problems that are beyond their individual capabilities. Each problem-solving node in the network is capable of sophisticated problem solving and can work independently, but the problems faced by the nodes cannot be completed without cooperation. Cooperation is necessary because no single node has sufficient expertise, resources, and information to solve a problem, and different nodes might have expertise for solving different parts of the problem. For example, if the problem is to design a house, one node might have expertise on the strength of structural materials, another on the space requirements for different types of rooms, another on plumbing, another on electrical wiring, and so on. Different nodes might have different resources: some might be very fast at computation, others might have connections that speed communication, whil
417|Multiagent negotiation under time constraints|Research in distributed artificial intelligence (DAI) is concerned with how automated agents can be designed to interact effectively. Negotiation is proposed as a means for agents to communicate and compromise to reach mutually beneficial agreements. The paper examines the problems of resource allocation and task distribution among autonomous agents which can benefit from sharing a common resource or distributing a set of common tasks. We propose a strategic model of negotiation that takes the passage of time during the negotiation process itself into account. A distributed negotiation mechanism is introduced that is simple, efficient, stable, and flexible in various situations. The model considers situations characterized by complete as well as incomplete information, and ones in which some agents lose over time while others gain over time. Using this negotiation mechanism autonomous agents have simple and stable negotiation strategies that result in efficient agreements without delays even when there are dynamic changes in the environment.  
418|The behavior of computational ecologies|We describe a form of distributed computation in which agents have incomplete knowledge and imperfect information on the state of the system, and an instantiation of such systems based on market mechanisms. When agents can choose among several resources, the dynamics of the system can be oscillatory and even chaotic. A mechanism is described for achieving global stability through local controls. 1
419|A Retrospective View of FA/C Distributed Problem Solving|The Functionally-Accurate, Cooperative (FA/C) paradigm provides a model for task decomposition and agent interaction in a distributed problem-solving system. In this model, agents need not have all the necessary information locally to solve their subproblems, and agents interact through the asynchronous, co-routine exchange of partial results. This model leads to the possibility that agents may behave in an uncoordinated manner. This paper traces the development of a series of increasingly sophisticated cooperative control mechanisms for coordinating agents. They include integrating data- and goal-directed control, using static meta-level information specified by an organizational structure, and using dynamic meta-level information developed in partial global planning. The framework of distributed search motivates these developments. Major themes of this work are the importance of sophisticated local control, the interplay between local control and cooperative control, and the use of s...
420|The use of meta-level control for coordination in a distributed problem solving network|This paper was presented at IJCAI-83. Distributed problem-solving networks provide an interesting application area for meta-level control through the use of organizational structuring. We describe a decentralized approach to network coordination that relies on each node making sophisticated local decisions that balance its own perceptions of appropriate problem-solving activity with activities deemed important by other nodes. Each node is guided by a high-level strategic plan for cooperation among the nodes in the network. The high-level strategic plan, which is a form of meta-level control, is represented as a network organizational structure that specifies in a general way the information and control relationships among the nodes. An implementation of these ideas is briefly described along with the results of preliminary experiments with various network problem-solving strategies specified via organizational structuring. In addition to its application to Distributed Artificial Intelligence, this research has implications for organizing and controlling complex knowledge-based systems that involve semi-autonomous problem solving agents. 1
421|Agent-Based Business Process Management|This paper describes work undertaken in the ADEPT (Advanced Decision Environment for Process Tasks) project towards developing an agent-based infrastructure for managing business processes. We describe how the key technology of negotiating, service providing, autonomous agents was realised and demonstrate how this was applied to the BT (British Telecom) business process of providing a customer quote for network services.
422|Moving Up the Information Food Chain: Deploying Softbots on the World Wide Web|I view the World Wide Web as an information food chain (figure 1). The maze of pages and hyperlinks that comprise the Web are at the very bottom of the chain. The WebCrawlers and Alta Vistas of the world are information herbivores; they graze on Web pages and regurgitate them as searchable indices. Today, most Web users feed near the bottom of the information food chain, but the time is ripe to move up. Since 1991, we have been building information carnivores, which intelligently hunt and feast on herbivores
423|Designing behaviors for information agents|To facilitate the rapid development and open system interoperability of autonomous agents we need to carefully specify and effectively implement various classes of agent behaviors. Our current focus is on the behaviors and underlying architecture of WWW-based autonomous software agents that collect and supply information to humans and other computational agents. This paper discusses a set of architectural building blocks that support the specification of behaviors for these information agents in a way that allows periodic actions, interleaving of planning and execution, and the concurrent activation of multiple behaviors with asynchronous components. We present an initial set of information agent behaviors, including responding to repetitive queries, monitoring information sources, advertising capabilities, and self cloning. We have implemented and tested these behaviors on the WWW in the context of WAR-REN, an open multi-agent organization for financial portfolio management.
425|TouringMachines: An Architecture for Dynamic, Rational, Mobile Agents|ion-Partitioned Evaluator (APE) architecture which has been tested in a simulated, single-agent, indoor navigation domain [SH90].  The APE architecture is composed of a number of concurrent, hierarchically abstract action control layers, each representing and reasoning about some particular aspect of the agent&#039;s task domain. Implemented as a parallel blackboard-based planner, the five layers --- sensor/motor, spatial, temporal, causal, and conventional (general knowledge) --- effectively partition the agent&#039;s data processing duties along a number of dimensions including temporal granularity, information/resource use, and functional abstraction. Perceptual information flows strictly from the agent sensors (connected to the sensor /motor level) toward the higher levels, while command or goal-achievement information flows strictly downward towards the agent&#039;s effectors (also connected to the sensor/motor level).  Besides mechanisms for communicating with other layers, each layer in the AP...
426|Cooperative Transportation Scheduling: an Application Domain for DAI|A multiagent approach to designing the transportation domain is presented. The Mars system is described which models cooperative order scheduling within a society of shipping companies. We argue why Distributed Artificial Intelligence (DAI) offers suitable tools to deal with the hard problems in this domain. We present three important instances for DAI techniques that proved useful in the transportation application: cooperation among the agents, task decomposition and task allocation, and decentralised planning. An extension of the contract net protocol for task decomposition and task allocation is presented; we show that it can be used to obtain good initial solutions for complex resource allocation problems. By introducing global information based upon auction protocols, this initial solution can be improved significantly. We demonstrate that the auction mechanism used for schedule optimisation can also be used for implementing dynamic replanning. Experimental results are provided ev...
427|Foundations of a Logical Approach to Agent Programming|This paper describes a novel approach to high-level agent programming based on a highly  developed logical theory of action. The user provides a specification of the agents&#039; basic actions  (preconditions and effects) as well as of relevant aspects of the environment, in an extended  version of the situation calculus. He can then specify behaviors for the agents in terms of these  actions in a programming language where one can refer to conditions in effect in the environment.  When an implementation of the basic actions is provided, the programs can be executed in a  real environment; otherwise, a simulated execution is still possible. The interpreter automatically  maintains the world model required to execute programs based on the specification. The theoretical  framework includes a solution to the frame problem, allows agents to have incomplete knowledge  of their environment, and handles perceptual actions. The theory can also be used to prove  programs correct. A simple meeting sc...
428|Increasing believability in animated pedagogical agents|Animated pedagogical agents o er great promise for knowledge-based learning environments. In addition to coupling feedback capabilities with a strong visual presence, these agents play a critical role in motivating students. The extent to which they exhibit life-like behaviors strongly increases their motivational impact, but these behaviors must always complement and never interfere with students &#039; problem solving. To address this problem, we havedeveloped a framework for dynamically sequencing animated pedagogical agents &#039; believability-enhancing behaviors. By monitoring a student&#039;s problemsolving history and the agent&#039;s past activities, a competition-based behavior sequencing engine produces realtime life-like character animations that are pedagogically appropriate. Behaviors in the agent&#039;s repertoire compete with one another. At each moment, the strongest eligible behavior is heuristically selected as the winner and is exhibited. We haveimplemented this framework in Herman the Bug, an animated pedagogical agent that inhabits a knowledge-based learning environment for the domain of botanical anatomy and physiology.
429|Towards a Social Level Characterisation of Socially Responsible Agents|This paper presents a high-level framework for analysing and designing intelligent agents. The framework&#039;s key abstraction mechanism is a new computer level called the  Social Level. The Social Level sits immediately above the Knowledge Level, as defined by Allen Newell, and is concerned with the inherently social aspects of multiple agent systems. To illustrate the working of this framework, an important new class of agent is identified and then specified. Socially responsible agents retain their local autonomy but still draw from, and provide resources to, the larger community. Through empirical evaluation, it is shown that such agents produce both good system-wide performance and good individual performance. 1. INTRODUCTION The number of multi-agent systems being designed and built is rapidly increasing as software agents gain acceptance as a powerful and useful technology for solving complex problems (Chaib-draa, 1995; Jennings, 1994; PAAM, 1996). As applications become more comple...
430|Representing and Executing Agent-Based Systems|In this paper we describe an approach to the representation and implementation  of agent-based systems where the behaviour of an individual agent is  represented by a set of logical rules in a particular form. This not only provides a  logical specification of the agent, but also allows us to directly execute the rules in  order to implement the agent&#039;s behaviour. Agents communicate with each other  through a simple, and logically well-founded, broadcast communication mechanism.
431|Entertaining Agents: a sociological case study|Traditional AI has not concerned itself extensively with sociology nor with what emotional reactions might be produced in its users. On the other hand, entertainment is very concerned indeed with these issues. AI and ALife programs which are to be used in entertainment must therefore be viewed both as AI/ALife endeavors and as psychological and sociological endeavors. This paper presents a brief description of Julia [Mauldin 94], an implemented software agent, and then examines the sociology of those who encounter her, using both transcripts of interactions with Julia, and direct interviews with users. Julia is designed to pass as human in restricted environments while being both entertaining and informative, and often elicits surprisingly intense emotional reactions in those who encounter her. An introduction to MUDs and Julia Julia [Mauldin 94] is a MUD [Curtis 92] [Bruckman 93] [Evard 93] robot. A MUD is a text-only, multiperson, virtual reality. [Mauldin 94], while describing Julia’s internal structure, gives very little ‘feel ’ for what it like to interact with her outside of the strictures of a formal Turing test; hence, transcripts of many interactions with her appear below as examples. (Since Julia adamantly insists that she is female, I refer to the program here as ‘she’.)
432|Using ARCHON to develop real-world DAI applications for electricity transportation management and particle accelerator control|ARCHON ^TM (ARchitecture for Cooperative Heterogeneous ON-line systems) was Europe&#039;s largest ever project in the area of Distributed Artificial Intelligence (DAI). It devised a general-purpose architecture, software framework, and methodology which has been used to support the development of DAI systems in a number of real world industrial domains. Two of these applications, electricity transportation management and particle accelerator control, have been run successfully on-line in the organisation for which they were developed (respectively, Iberdrola an electricity utility in the north of Spain and CERN the European Centre for high energy physics research near Geneva). This paper recounts the problems, insights and experiences gained whilst deploying ARCHON technology in these real-world industrial applications. Firstly, it gives the rationale for a DAI approach to industrial applications and highlights the key design forces which shape work in this important domain. Secondly, the...
433|An Agent-based Approach to Health Care Management|The provision of medical care typically involves a number of individuals, located in a number of different institutions, whose decisions and actions need to be coordinated if the care is to be effective and efficient. To facilitate this decision making and to ensure the coordination process runs smoothly, the use of software support is becoming increasingly widespread. To this end, this paper describes an agent-based system which was developed to help manage the care process in real world settings. The agents themselves are implemented using a layered architecture, called AADCare, which combines a number of AI and agent techniques: a symbolic decision procedure for decision making with incomplete and conflicting information, a concept of accountability for task allocation, the notions of commitments and conventions for managing coherent cooperation, and a set of communication primitives for interagent interaction. The utility of this approach is demonstrated through the development of ...
434|The Negotiating Agents Approach to Runtime Feature Interaction Resolution|. This article describes how to use the Negotiating Agents approach on a telecommunications platform. Negotiation is used in this approach to resolve conflicts between features of one user and of different users. The theory behind the approach is discussed briefly. Methods for implementing the approach are given along with the methods for defining IN features in terms of the Negotiating Agents approach in order to resolve conflicts between these features. 1 Introduction  Rapid change in the telecommunications industry increases the complexity not only of building but also of using telecommunications services. Much of the complexity arises from the feature interaction problem. When features interact, a user must understand the behavior of features in combination -- even how features of other users may affect the negbehavior of her features. Similarly, a service provider must determine how combinations of features will behave, including combinations of its own features with other provide...
435|Specification and implementation of a belief desire joint-intention architecture for collaborative problem solving|Systems composed of multiple interacting problem solvers are becoming increasingly pervasive and have been championed in some quarters as the basis of the next generation of intelligent information systems. If this technology is to fulfill its true potential then it is important that the systems which are developed have a sound theoretical grounding. One aspect of this foundation, namely the model of collaborative problem solving, is examined in this paper. A synergistic review of existing models of cooperation is presented, their weaknesses are highlighted and a new model (called joint responsibility) is introduced. Joint responsibility is then used to specify a novel high-level agent architecture for cooperative problem solving in which the mentalistic notions of belief, desire, intention and joint intention play a central role in guiding an individual’s and the group’s problem solving behaviour. An implementation of this high-level architecture is then discussed and its utility is illustrated for the real-world domain of electricity transportation management.
436|Industrial Applications of Distributed AI|This article argues that a DAI approach can be used to cope with the complexity of industrial applications. DAI techniques are beginning to have a broad impact; the current introduction of these techniques by an ESPRIT project, a Palo Alto consortium, ARPA, Carnegie Mellon University, MCC, and others are good examples. In the near future, other industrial products will emerge from the application of DAI techniques to other domains, including distributed databases, computer-supported cooperative work, and air traffic control. An important advantage of a DAI approach is the ability to integrate existing standalone knowledge-based systems. This factor is important because software for industrial applications is often developed in an ad hoc fashion. Thus, organizations possess a large number of standalone systems developed at different times by different people using different techniques. These systems all operate in the same physical environment, all have expertise that is related but distinct, and all could benefit from cooperation with other such standalone systems
437|Using mobile agents to support interorganizational workflow-management|This paper argues that the mobile agent approach is well suited for sporadic communication in open distributed systems- especially for rather ‘loose’ cooperations across local and organizational borders: In an increasing number of cases, management of distributed business procedures reaches beyond such borders. This means for most existing workflow management systems that cooperating partners are required to give up their local autonomy. However, for cases in which business partners intend to cooperate but still need to preserve their local autonomy, process participation on the basis of mobile agents represents an attractive and appropriate mechanism. This article shows how such kind of process integration can be achieved. It further demonstrates how the COSM (Common Open Service Market) system software can be extended in order to use petri net based process definitions which realize mobile agents in an integrated distributed system platform.
438|Creatures: Entertainment Software Agents with Artificial Life|We present a technical description of Creatures, a commercial home-entertainment software package. Creatures provides a simulatedenvironment in which exist a number of synthetic agents that a user can interact with in real-time. The agents (known as &#034;creatures&#034;) are intended as sophisticated &#034;virtual pets&#034;. The internal architecture of the creatures is strongly inspired by animal biology. Each creature has a neural network responsible for sensory-motorcoordinationand behavior selection, and an &#034;artificial biochemistry&#034; that models a simple energy metabolism along with a &#034;hormonal&#034; system that interacts with the neural network to model diffuse modulation of neuronal activity and staged ontogenetic development. A biologically inspired learning mechanism allows the neural network to adapt during the lifetime of a creature. Learning includes the ability to acquire a simple verb--object language.
439|Some Issues in the Design of Market-Oriented Agents|. In a computational market, distributed market agents interact with other agents primarily through the exchange of goods and services. Thanks to a welldeveloped underlying economic framework, we can draw on a rich source of analytic tools and theoretical techniques for designing individual agents and predicting aggregate behavior. For many narrowly scoped static problems, design of a computational market is relatively straightforward. We consider some issues that arise in attempting to design computational economies for broadly scoped, dynamic environments. These issues include how to specify the goods and services beingexchanged,how these market-oriented agents should set their exchangepolicies, and how computational market mechanisms appropriate for idealized environments can be adapted to work in a larger class of non-ideal environments. 1 Introduction  Approaches to resource allocation in distributed systems can be bounded by two extremes. At one end (the &#034;software engineering&#034; ap...
440|Application of multi-agent systems in traffic and transportation|Agent-oriented techniques offer a new approach aimed at supporting the whole software development process. All the phases in the software development process are treated with a single uniform concept, namely that of agents, and a system modelled by a collection of agents is called a multi-agent system. AOT as a new advance in information technology can help to respond to the growing interest in making traffic and transportation more efficient, resource-saving and ecological. This article gives an overview of a diverse range of applications where multi-agent systems promise to create a great impact in this domain. To demonstrate the ideas behind AOT and their applicability in this domain, two applications currently under development at Daimler-Benz Research will be described in some detail. 1
441|Managing heterogeneous transaction workflows with cooperating agents|This paper describes how a set of autonomous computational agents can cooperate in providing coherent management of transaction workflows in environments where there are many diverse information resources. The agents use models of themselves and of the resources that are local to them. Resource models may be the schemas of databases, frame systems of knowledge bases, domain models of business environments, or process models of business operations. Models enable the agents and information resources to use the appropriate semantics when they interoperate. This is accomplished by specifying the semantics in terms of a common ontology. We discuss the contents of the models, where they come from, and how the agents acquire them. We then describe a set of agents for telecommunication service provisioning and show how the agents use such models to cooperate. The agents implement virtual state machines, and interact by exchanging state information. Their interactions produce an implementation of relaxed transaction processing. 1
442|Location-Aided Routing (LAR) in mobile ad hoc networks  (1998) |A mobile ad hoc network consists of wireless hosts that may move often. Movement of hosts results in a change in routes, requiring some mechanism for determining new routes. Several routing protocols have already been proposed for ad hoc networks. This paper suggests an approach to utilize location information (for instance, obtained using the global positioning system) to improve performance of routing protocols for ad hoc networks. By using location information, the proposed Location-Aided Routing (LAR) protocols limit the search for a new route to a smaller “request zone” of the ad hoc network. This results in a significant reduction in the number of routing messages. We present two algorithms to determine the request zone, and also suggest potential optimizations to our algorithms.
443|Dynamic source routing in ad hoc wireless networks|An ad hoc network is a collection of wireless mobile hosts forming a temporary network without the aid of any established infrastructure or centralized administration. In such an environment, it may be necessary for one mobile host to enlist the aid of other hosts in forwarding a packet to its destination, due to the limited range of each mobile host’s wireless transmissions. This paper presents a protocol for routing in ad hoc networks that uses dynamic source routing. The protocol adapts quickly to routing changes when host movement is frequent, yet requires little or no overhead during periods in which hosts move less frequently. Based on results from a packet-level simulation of mobile hosts operating in an ad hoc network, the protocol performs well over a variety of environmental conditions such as host density and movement rates. For all but the highest rates of host movement simulated, the overhead of the protocol is quite low, falling to just 1 % of total data packets transmitted for moderate movement rates in a network of 24 mobile hosts. In all cases, the difference in length between the routes used and the optimal route lengths is negligible, and in most cases, route lengths are on average within a factor of 1.01 of optimal. 1.
444|Highly Dynamic Destination-Sequenced Distance-Vector Routing (DSDV) for Mobile Computers  (1994) |An ad-hoc network is the cooperative engagement of a collection of Mobile Hosts without the required intervention of any centralized Access Point. In this paper we present an innovative design for the operation of such ad-hoc networks. The basic idea of the design is to operate each Mobile Host as a specialized router, which periodically advertises its view of the interconnection topology with other Mobile Hosts within the network. This amounts to a new sort of routing protocol. We have investigated modifications to the basic Bellman-Ford routing mechanisms, as specified by RIP [5], to make it suitable for a dynamic and self-starting network mechanism as is required by users wishing to utilize adhoc networks. Our modifications address some of the previous objections to the use of Bellman-Ford, related to the poor looping properties of such algorithms in the face of broken links and the resulting time dependent nature of the interconnection topology describing the links between the Mobile Hosts. Finally, we describe the ways in which the basic network-layer routing can be modified to provide MAC-layer support for ad-hoc networks.  
445|A Performance Comparison of Multi-Hop Wireless Ad Hoc Network Routing Protocols|An ad hoc network is a collection of wireless mobile nodes dynamically forming a temporary network without the use of any existing network infrastructure or centralized administration. Due to the limited transmission range of wireless network interfaces, multiple network &amp;quot;hops &amp;quot; may be needed for one node to exchange data with another across the network. In recent years, a variety of new routing protocols targeted specifically at this environment have been developed, but little performance information on each protocol and no realistic performance comparison between them is available. This paper presents the results of a detailed packet-level simulation comparing four multi-hop wireless ad hoc network routing protocols that cover a range of design choices: DSDV, TORA, DSR, and AODV. We have extended the ns-2 network simulator to accurately model the MAC and physical-layer behavior of the IEEE 802.11 wireless LAN standard, including a realistic wireless transmission channel model, and present the results of simulations of networks of 50 mobile nodes. 1
446|A Highly Adaptive Distributed Routing Algorithm for Mobile Wireless Networks|We present a new distributed routing protocol for mobile, multihop, wireless networks. The protocol is one of a family of protocols which we term &#034;link reversal&#034; algorithms. The protocol&#039;s reaction is structured as a temporally-ordered sequence of diffusing computations; each computation consisting of a sequence of directed l i nk reversals. The protocol is highly adaptive, efficient and scalable; being best-suited for use in large, dense, mobile networks. In these networks, the protocol&#039;s reaction to link failures typically involves only a localized &#034;single pass&#034; of the distributed algorithm. This capability is unique among protocols which are stable in the face of network partitions, and results in the protocol&#039;s high degree of adaptivity. This desirable behavior is achieved through the novel use of a &#034;physical or logical clock&#034; to establish the &#034;temporal order&#034; of topological change events which is used to structure (or order) the algorithm&#039;s reaction to topological changes. We refer to the protocol as the Temporally-Ordered Routing Algorithm (TORA).
447|GeoCast - Geographic Addressing and Routing|In the near future GPS will be widely used, thus allowing a broad variety of location dependent services such as direction giving, navigation, etc. In this paper we propose and evaluate a routing and addressing method to integrate geographic coordinates into the Internet Protocol to enable the creation of location dependent services. The main challenge is to integrate the concept of physical location into the current design of the Internet which relies on logical addressing.
448|Comparative Performance Evaluation of Routing Protocols for Mobile, Ad hoc Networks|We evaluate several routing protocols for mobile, wireless, ad hoc networks via packet level simulations. The protocol suite includes routing protocols specifically designed for ad hoc routing, as well as more traditional protocols, such as link state and distance vector, used for dynamic networks. Performance is evaluated with respect to fraction of packets delivered, end-to-end delay and routing load for a given traffic and mobility model. It is observed that the new generation of on-demand routing protocols use much lower routing load. However, the traditional link state and distance vector protocols provide, in general, better packet delivery and delay performance. 1. Introduction  A mobile, ad hoc network [4] is an autonomous system of mobile hosts connected by wireless links. There is no static infrastructure such as base stations. If two hosts are not within radio range, all message communication between them must pass through one or more intermediate hosts that double as router...
449|Movement-based location update and selective paging for PCS networks| This paper introduces a mobility tracking mechanism that combines a movement-based location update policy with a selective paging scheme. Movement-based location update is selected for its simplicity. It does not require ea &amp; mobile terminal to store information about the arrangement and the distance relationship of all cells. In fact, each mobile terminal only keeps a counter of the number of cells visited. A location update is performed when this counter exceeds a predefined threshold value. This scheme allows the dynamic selection of the movement threshold on a per-user basis. This is desirable as different users may have very different mobility patterns. Selective paging reduces the cost for locating a mobile terminal in the expense of an increase in the paging delay. In this paper, we propose a selective paging scheme which significantly decreases the location tracking cost under a small increase in the allowable paging delay. We introduce an analytical model for the proposed location tracking mechanism which captures the mobility and the incoming call arrival patterns of each mobile terminal. Analytical results are provided to demonstrate the cost-effectiveness of the proposed scheme under various parameters. 
450|Signal Stability based Adaptive Routing (SSA) for Ad-Hoc Mobile Networks  (1997) |Unlike static networks, ad-hoc networks have no spatial hierarchy and suffer from frequent link failures which prevent mobile hosts from using traditional routing schemes. Under these conditions, mobile hosts must find routes to destinations without the use of designated routers and also must dynamically adapt the routes to the current link conditions. This paper proposes a distributed adaptive routing protocol for finding and maintaining stable routes based on signal strength and location stability in an ad-hoc network and presents an architecture for its implementation. 1 Introduction  Mobility is becoming increasingly important for users of computing systems. Technology has made possible wireless devices and smaller, less expensive, and more powerful computers. As a result users gain flexibility and the ability to maintain connectivity to their primary computer while roaming through a large area. The number of users with portable laptops and personal communications devices is increa...
451|A Survey of Routing Techniques for Mobile Communications Networks|Mobile wireless networks pose interesting challenges for routing system design. To produce feasible routes in a  mobile wireless network, a routing system must be able to accommodate roving users, changing network topology, and fluctuating  link quality. We discuss the impact of node mobility and wireless communication on routing system design, and we survey  the set of techniques employed in or proposed for routing in mobile wireless networks.
452|Routing in Ad Hoc Networks Using a Spine|We present a two-level hierarchical routing architecture for ad hoc networks. Within each lower level cluster, we describe a self-organizing, dynamic spine  structure to (a) propagate topology changes, (b) compute updated routes in the background, and (c) provide backup routes in case of transient failures of the primary routes. We analyze and bound the worst case of movements between upper level clusters to show that this hierarchical architecture scales well with network size. 1 Introduction  Ad hoc networks are multihop networks in which mobile hosts share a scarce wireless channel. In ad hoc networks, the network topology changes frequently. Hence, routing algorithms must expend overhead either to maintain current routing and topology tables or to discover up-to-date routes. Currently, most routing algorithms for ad hoc networks are flat, that is, designed with only one level of hierarchy. These flat routing algorithms can suffer from excessive overhead as network sizes increase. I...
453|GPS-Based Addressing and Routing|In the near future GPS will be widely used, thus allowing a broad variety of location dependent services such as direction giving, navigation, etc. In this document we propose a family of protocols and addressing methods to integrate GPS into the Internet Protocol to enable the creation of location dependent services. The solutions which we present are flexible (scalable) in terms of the target accuracy of the GPS. The main challenge is to integrate the concept of physical location into the current design of the Internet which relies on logical addressing. Two solutions are presented in this draft and a third solution is sketched. Figure 1: GPS Satellites Orbiting the Earth  Contents  1 Introduction 3  1.1 Scenarios of Usage and Interface Issues : : : : : : : : : : : : : 4  2 Background 5  2.1 Related Work : : : : : : : : : : : : : : : : : : : : : : : : : : : 5 2.2 Global Positioning System (GPS) : : : : : : : : : : : : : : : : 6 2.2.1 What is GPS? : : : : : : : : : : : : : : : : : : ...
454|Location-Based Multicast in Mobile Ad Hoc Networks|Multicast distribution in mobile wireless networks is a topic that has recently begun to be explored. For multicasting, conventional protocols define a multicast group as a collection of hosts which register to a multicast group address. However, in this paper, we define a location-based multicast group which is based on a specific region (&#034;Multicast  Region&#034;) in a mobile ad hoc network (MANET). Hosts within the multicast region at a given time form the multicast group at that time. We present two algorithms for delivering packets to such a multicast group, and present simulation results. 1 Introduction  Multicast distribution in mobile wireless networks is a topic that has recently begun to be explored [22]. When an application must send the same information to more than one destination, multicasting is often used. Multicasting has played an important role in supporting multimedia applications, such as audio/video broadcasting. Multicasting is much more advantageous than multiple unic...
455|Using Location Information to Improve Routing in Ad Hoc Networks|In ad hoc network environments, any techniques to reduce high routing-related overhead are worth investigating. This brief note explains how to exploit location information to improve ad hoc routing. We also suggest some optimization approaches that can improve the performance of the protocol.    Research reported is supported in part by Texas Advanced Technology Program 009741-052-C.  1 Introduction  Many protocols for routing have been proposed for mobile wireless networks (also known as ad hoc or mobile mesh networks) [5, 6, 8, 9, 11, 12, 14, 16, 18]. As the hosts in such a network are mobile, an inherent drawback of these protocols (more precisely, drawback of the network) is that routing-related overhead tends to be high. Therefore, any techniques to reduce this overhead are worth investigating. In this brief note, we consider how to exploit location information to improve ad hoc routing. 2 Motivation and Related Works  Location information is important in mobile computing enviro...
456|A New Method for Solving Hard Satisfiability Problems|We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the approach are also discussed.  GSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks.  
457|The complexity of theorem-proving procedures|It is shown that any recognition problem solved by a polynomial time-bounded nondeterministic Turing machine can be “reduced” to the problem of determining whether a given propositional formula is a tautology. Here “reduced ” means, roughly speaking, that the first problem can be solved deterministically in polynomial time provided an oracle is available for solving the second. From this notion of reducible, polynomial degrees of difficulty are defined, and it is shown that the problem of determining tautologyhood has the same polynomial degree as the problem of determining whether the first of two given graphs is isomorphic to a subgraph of the second. Other examples are discussed. A method of measuring the complexity of proof procedures for the predicate calculus is introduced and discussed. Throughout this paper, a set of strings 1 means a set of strings on some fixed, large, finite alphabet S. This alphabet is large enough to include symbols for all sets described here. All Turing machines are deterministic recognition devices, unless the contrary is explicitly stated.
458|Model Checking vs. Theorem Proving: A Manifesto| We argue that rather than representing an agent&#039;s knowledge as a collection of formulas, and then doing theorem proving to see if a given formula follows from an agent&#039;s knowledge base, it may be more useful to represent this knowledge by a semantic model, and then do model checking to see if the given formula is true in that model. We discuss how to construct a model that represents an agent&#039;s knowledge in a number of different contexts, and then consider how to approach the model-checking problem.
459|A Continuous Approach to Inductive Inference|In this paper we describe an interior point mathematical programming approach to inductive inference. We list several versions of this problem and study in detail the formulation based on hidden Boolean logic. We consider the problem of identifying a hidden Boolean function  F : f0; 1g  n  ! f0; 1g using outputs obtained by applying a limited number of random inputs to the hidden function. Given this input-output sample, we give a method to synthesize a Boolean function that describes the sample. We pose the Boolean Function Synthesis Problem as a particular type of Satisfiability Problem. The Satisfiability Problem is translated into an integer programming feasibility problem, that is solved with an interior point algorithm for integer programming. A similar integer programming implementation has been used in a previous study to solve randomly generated instances of the Satisfiability Problem. In this paper we introduce a new variant of this algorithm, where the Riemannian metric used...
460|The Complexity of Automated Reasoning|This thesis explores the relative complexity of proofs produced by the automatic theorem proving procedures of analytic tableaux, linear resolution, the connection method, tree resolution and the Davis-Putnam procedure. It is shown that tree resolution simulates the improved tableau procedure and that SL-resolution and the connection method are equivalent to restrictions of the improved tableau method. The theorem by Tseitin that the Davis-Putnam Procedure cannot be simulated by tree resolution is given an explicit and simplified proof. The hard examples for tree resolution are contradictions constructed from simple Tseitin graphs.
461|Comparison of Multiobjective Evolutionary Algorithms: Empirical Results|In this paper, we provide a systematic comparison of various evolutionary approaches to multiobjective optimization using six carefully chosen test functions. Each test function involves a particular feature that is known to cause difficulty in the evolutionary optimization process, mainly in converging to the Pareto-optimal front (e.g., multimodality and deception). By investigating these different problem features separately, it is possible to predict the kind of problems to which a certain technique is or is not well suited. However, in contrast to what was suspected beforehand, the experimental results indicate a hierarchy of the algorithms under consideration. Furthermore, the emerging effects are evidence that the suggested test functions provide sufficient complexity to compare multiobjective optimizers. Finally, elitism is shown to be an important factor for improving evolutionary multiobjective search.
462|A Fast and Elitist Multi-Objective Genetic Algorithm: NSGA-II|Multi-objective evolutionary algorithms which use non-dominated sorting and sharing have been mainly criticized for their (i) O(MN computational complexity (where M is the number of objectives and N is the population size), (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In this paper, we suggest a non-dominated sorting based multi-objective evolutionary algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which alleviates all the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN ) computational complexity is presented. Second, a selection operator is presented which creates a mating pool by combining the parent and child populations and selecting the best (with respect to fitness and spread) N solutions. Simulation results on a number of difficult test problems show that the proposed NSGA-II, in most problems, is able to find much better spread of solutions and better convergence near the true Pareto-optimal front compared to PAES and SPEA - two other elitist multi-objective EAs which pay special attention towards creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems eciently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint non-linear problem, are compared with another constrained multi-objective optimizer and much better performance of NSGA-II is observed. Because of NSGA-II&#039;s low computational requirements, elitist approach, parameter-less niching approach, and simple constraint-handling strategy, NSGA-II should find increasing applications in the coming years.
463|A Fast Elitist Non-Dominated Sorting Genetic Algorithm for Multi-Objective Optimization: NSGA-II|Multi-objective evolutionary algorithms which use non-dominated sorting  and sharing have been mainly criticized for their  (i)  -4  computational  complexity (where    is the number of objectives and    is the population size),  (ii) non-elitism approach, and (iii) the need for specifying a sharing parameter. In  this paper, we suggest a non-dominated sorting based multi-objective evolutionary  algorithm (we called it the Non-dominated Sorting GA-II or NSGA-II) which  alleviates all the above three difficulties. Specifically, a fast non-dominated sorting  approach with    computational complexity is presented. Second, a  selection operator is presented which creates a mating pool by combining the  parent and child populations and selecting the best (with respect to fitness and  spread)    solutions. Simulation results on five difficult test problems show that  the proposed NSGA-II is able to find much better spread of solutions in all problems  compared to PAES---another elitist multi-objective EA which pays special  attention towards creating a diverse Pareto-optimal front. Because of NSGA-II&#039;s  low computational requirements, elitist approach, and parameter-less sharing approach,  NSGA-II should find increasing applications in the years to come.
464|Genetic Algorithms for Multiobjective Optimization: Formulation, Discussion and Generalization|The paper describes a rank-based fitness assignment method for Multiple Objective Genetic Algorithms (MOGAs). Conventional niche formation methods are extended to this class of multimodal problems and theory for setting the niche size is presented. The fitness assignment method is then modified to allow direct intervention of an external decision maker (DM). Finally, the MOGA is generalised further: the genetic algorithm is seen as the optimizing element of a multiobjective optimization loop, which also comprises the DM. It is the interaction between the two that leads to the determination of a satisfactory solution to the problem. Illustrative results of how the DM can interact with the genetic algorithm are presented. They also show the ability of the MOGA to uniformly sample regions of the trade-off surface.
465|Multiobjective Optimization Using Nondominated Sorting in Genetic Algorithms|In trying to solve multiobjective optimization problems, many traditional methods scalarize  the objective vector into a single objective. In those cases, the obtained solution is highly  sensitive to the weight vector used in the scalarization process and demands the user to have  knowledge about the underlying problem. Moreover, in solving multiobjective problems, designers  may be interested in a set of Pareto-optimal points, instead of a single point. Since genetic  algorithms(GAs) work with a population of points, it seems natural to use GAs in multiobjective  optimization problems to capture a number of solutions simultaneously. Although a vector  evaluated GA (VEGA) has been implemented by Schaffer and has been tried to solve a number  of multiobjective problems, the algorithm seems to have bias towards some regions. In this  paper, we investigate Goldberg&#039;s notion of nondominated sorting in GAs along with a niche and  speciation method to find multiple Pareto-optimal points sim...
466|An Overview of Evolutionary Algorithms in Multiobjective Optimization|The application of evolutionary algorithms (EAs) in multiobjective optimization is currently receiving growing interest from researchers with various backgrounds. Most research in this area has understandably concentrated on the selection stage of EAs, due to the need to integrate vectorial performance measures with the inherently scalar way in which EAs reward individual performance, i.e., number of offspring. In this review, current multiobjective evolutionary approaches are discussed, ranging from the conventional analytical aggregation of the different objectives into a single function to a number of populationbased approaches and the more recent ranking schemes based on the definition of Pareto-optimality. The sensitivity of different methods to
467|Evolutionary Algorithms for Multiobjective Optimization|Multiple, often conflicting objectives arise naturally in most real-world optimization scenarios. As evolutionary algorithms possess several characteristics due to which they are well suited to this type of problem, evolution-based methods have been used for multiobjective optimization for more than a decade. Meanwhile evolutionary multiobjective optimization has become established as a separate subdiscipline combining the fields of evolutionary computation and classical multiple criteria decision making. In this paper, the basic principles of evolutionary multiobjective optimization are discussed from an algorithm design perspective. The focus is on the major issues such as fitness assignment, diversity preservation, and elitism in general rather than on particular algorithms. Different techniques to implement these strongly related concepts will be discussed, and further important aspects such as constraint handling and preference articulation are treated as well. Finally, two applications will presented and some recent trends in the field will be outlined.  
468|Multiobjective Evolutionary Algorithms: Analyzing the State-of-the-Art|Solving optimization problems with multiple (often conflicting) objectives is, generally, a  very difficult goal. Evolutionary algorithms (EAs) were initially extended and applied during  the mid-eighties in an attempt to stochastically solve problems of this generic class. During  the past decade, a variety of multiobjective EA (MOEA) techniques have been proposed  and applied to many scientific and engineering applications. Our discussion&#039;s intent is  to rigorously define multiobjective optimization problems and certain related concepts,  present an MOEA classification scheme, and evaluate the variety of contemporary MOEAs.  Current MOEA theoretical developments are evaluated; specific topics addressed include  fitness functions, Pareto ranking, niching, fitness sharing, mating restriction, and secondary  populations. Since the development and application of MOEAs is a dynamic and rapidly  growing activity, we focus on key analytical insights based upon critical MOEA evaluation  of c...
469|A Niched Pareto Genetic Algorithm for Multiobjective Optimization|Many, if not most, optimization problems have multiple objectives. Historically, multiple objectives have been combined ad hoc to form a scalar objective function, usually through a linear combination (weighted sum) of the multiple attributes, or by turning objectives into constraints. The genetic algorithm (GA), however, is readily modified to deal with multiple objectives by incorporating the concept of Pareto domination in its selection operator, and applying a niching pressure to spread its population out along the Pareto optimal tradeoff surface. We introduce the Niched Pareto GA as an algorithm for finding the Pareto optimal set. We demonstrate its ability to find and maintain a diverse &#034;Pareto optimal population&#034; on two artificial problems and an open problem in hydrosystems.
470|A Comprehensive Survey of Evolutionary-Based Multiobjective Optimization Techniques|. This paper presents a critical review of the most important evolutionary-based multiobjective optimization techniques developed over the years, emphasizing the importance of analyzing their Operations Research roots as a way to motivate the development of new approaches that exploit the search capabilities of evolutionary algorithms. Each technique is briefly described mentioning its advantages and disadvantages, their degree of applicability and some of their known applications. Finally, the future trends in this discipline and some of the open areas of research are also addressed.  Keywords: multiobjective optimization, multicriteria optimization, vector optimization, genetic algorithms, evolutionary algorithms, artificial intelligence. 1 Introduction  Since the pioneer work of Rosenberg in the late 60s regarding the possibility of using genetic-based search to deal with multiple objectives, this new area of research (now called evolutionary multiobjective optimization) has grown c...
471|Genetic Algorithms, Noise, and the Sizing of Populations|This paper considers the effect of stochasticity on the quality of convergence of genetic algorithms  (GAs). In many problems, the variance of building-block fitness or so-called collateral noise is the  major source of variance, and a population-sizing equation is derived to ensure that average signal-to-collateral-noise ratios are favorable to the discrimination of the best building blocks required to  solve a problem of bounded deception. The sizing relation is modified to permit the inclusion of other  sources of stochasticity, such as the noise of selection, the noise of genetic operators, and the explicit  noise or nondeterminism of the objective function. In a test suite of five functions, the sizing relation  proves to be a conservative predictor of average correct convergence, as long as all major sources  of noise are considered in the sizing calculation. These results suggest how the sizing equation may  be viewed as a coarse delineation of a boundary between what a physicist might call two distinct  phases of GA behavior. At low population sizes the GA makes many errors of decision, and the  quality of convergence is largely left to the vagaries of chance or the serial fixup of flawed results  through mutation or other serial injection of diversity. At large population sizes, GAs can reliably  discriminate between good and bad building blocks, and parallel processing and recombination of  building blocks lead to quick solution of even difficult deceptive problems. Additionally, the paper  outlines a number of extensions to this work, including the development of more refined models of  the relation between generational average error and ultimate convergence quality, the development  of online methods for sizing populations via the estimation of population-s...
472|The Gambler&#039;s Ruin Problem, Genetic Algorithms, and the Sizing of Populations|This paper presents a model for predicting the convergence quality of genetic algorithms. The model incorporates previous knowledge about decision making in genetic algorithms and the initial supply of building blocks in a novel way. The result is an equation that accurately predicts the quality of the solution found by a GA using a given population size. Adjustments for different selection intensities are considered and computational experiments demonstrate the effectiveness of the model. I. Introduction The size of the population in a genetic algorithm (GA) is a major factor in determining the quality of convergence. The question of how to choose an adequate population size for a particular domain is difficult and has puzzled GA practitioners for a long time. Hard questions are better approached using a divide-and-conquer strategy and the population sizing issue is no exception. In this case, we can identify two factors that influence convergence quality: the initial supply of build...
473|Multiobjective Optimization and Multiple Constraint Handling with Evolutionary Algorithms-Part I: A Unified Formulation|In optimization, multiple objectives and constraints cannot be handled independently of the underlying optimizer. Requirements such as continuity and differentiability of the cost surface add yet another conflicting element to the decision process. While ``better&#039;&#039; solutions should be rated higher than ``worse&#039;&#039; ones, the resulting cost landscape must also comply with such requirements. Evolutionary algorithms (EAs), which have found application in many areas not amenable to optimization by other methods, possess many characteristics desirable in a multiobjective optimizer, most notably the concerted handling of multiple candidate solutions. However, EAs are essentially unconstrained search techniques which require the assignment of a scalar measure of quality, or fitness, to such candidate solutions. After reviewing current evolutionary approaches to multiobjective and constrained optimization, the paper proposes that fitness assignment be interpreted as, or at least related to, a multicriterion decision process. A suitable decision making framework based on goals and priorities is subsequently formulated in terms of a relational operator, characterized, and shown to encompass a number of simpler decision strategies. Finally, the ranking of an arbitrary number of candidates is considered. The effect of preference changes on the cost surface seen by an EA is illustrated graphically for a simple problem. The paper concludes with the formulation of a multiobjective genetic algorithm based on the proposed decision strategy. Niche formation techniques are used to promote diversity among preferable candidates, and progressive articulation of preferences is shown to be possible as long as the genetic algorithm can recover from abrupt changes in the cost landscape.
474|Multiobjective Optimization Using Evolutionary Algorithms - A Comparative Case Study|. Since 1985 various evolutionary approaches to multiobjective optimization have been developed, capable of searching for multiple solutions concurrently in a single run. But the few comparative studies of different methods available to date are mostly qualitative and restricted to two approaches. In this paper an extensive, quantitative comparison is presented, applying four multiobjective evolutionary algorithms to an extended 0/1 knapsack problem.  1 Introduction  Many real-world problems involve simultaneous optimization of several incommensurable and often competing objectives. Usually, there is no single optimal solution, but rather a set of alternative solutions. These solutions are optimal in the wider sense that no other solutions in the search space are superior to them when all objectives are considered. They are known as Pareto-optimal solutions. Mathematically, the concept of Pareto-optimality can be defined as follows: Let us consider, without loss of generality, a multio...
475|Simulated Binary Crossover for Continuous Search Space|The success of binary-coded genetic algorithms (GAs) in problems having discrete search space largely depends on the coding used to represent the problem variables and on the crossover operator that propagates building-blocks from parent strings to children strings. In solving optimization problems having continuous search space, binary-coded GAs discretize the search space by using a coding of the problem variables in binary strings. However, the coding of real-valued variables in finite-length strings causes a number of difficulties---inability to achieve arbitrary precision in the obtained solution, fixed mapping of problem variables, inherent Hamming cliff problem associated with the binary coding, and processing of Holland&#039;s schemata in continuous search space. Although, a number of real-coded GAs are developed to solve optimization problems having a continuous search space, the search powers of these crossover operators are not adequate. In this paper, the search power of a cross...
476|Multi-Objective Genetic Algorithms: Problem Difficulties and Construction of Test Problems|In this paper, we study the problem features that may cause a multi-objective genetic  algorithm (GA) difficulty in converging to the true Pareto-optimal front. Identification  of such features helps us develop difficult test problems for multi-objective optimization.  Multi-objective test problems are constructed from single-objective optimization  problems, thereby allowing known difficult features of single-objective problems (such as  multi-modality, isolation, or deception) to be directly transferred to the corresponding  multi-objective problem. In addition, test problems having features specific to multiobjective  optimization are also constructed. More importantly, these difficult test problems  will enable researchers to test their algorithms for specific aspects of multi-objective  optimization.  Keywords  Genetic algorithms, multi-objective optimization, niching, pareto-optimality, problem difficulties,  test problems.  1 Introduction  After a decade since the pioneering wor...
477|Multiobjective Optimization Using the Niched Pareto Genetic Algorithm|Many, if not most, optimization problems have multiple objectives. Historically, multiple objectives (i.e., attributes or criteria) have been combined ad hoc to form a scalar objective function, usually through a linear combination (weighted sum) of the multiple attributes, or by turning objectives into constraints. The most recent development in the field of decision analysis has yielded a rigorous technique for combining attributes multiplicatively (thereby incorporating nonlinearity), and for handling uncertainty in the attribute values. But MultiAttribute Utility Analysis (MAUA) provides only a mapping from a vector-valued objective function to a scalar-valued function, and does not address the difficulty of searching large problem spaces. Genetic algorithms (GAs), on the other hand, are well suited to searching intractably large, poorly understood problem spaces, but have mostly been used to optimize a single objective. The direct combination of MAUA and GAs is a logical next step...
478|On the Performance Assessment and Comparison of Stochastic Multiobjective Optimizers|Abstract. This work proposes a quantitative, non-parametric interpretation of statistical performance of stochastic multiobjective optimizers, including, but not limited to, genetic algorithms. It is shown that, according to this interpretation, typical performance can be defined in terms analogous to the notion of median for ordinal data, as can other measures analogous to other quantiles. Non-parametric statistical test procedures are then shown to be useful in deciding the relative performance of different multiobjective optimizers on a given problem. Illustrative experimental results are provided to support the discussion. 1
479|On a Multi-Objective Evolutionary Algorithm and Its Convergence to the Pareto Set|Although there are many versions of evolutionary algorithms that are tailored to multi-criteria optimization, theoretical results are apparently not yet available. Here, it is shown that results known from the theory of evolutionary algorithms in case of single criterion optimization do not carry over to the multi-criterion case. At first, three different step size rules are investigated numerically for a selected problem with two conflicting objectives. The empirical results obtained by these experiments lead to the observation that only one of these step size rules may have the property to ensure convergence to the Pareto set. A theoretical analysis finally shows that a special version of an evolutionary algorithm with this step size rule converges with probability one to the Pareto set for the test problem under consideration. 
480|P.: A Spatial Predator-Prey Approach to Multi-Objective Optimization: A Preliminary Study|Abstract. This paper presents a novel evolutionary approach of approximating the shape of the Pareto-optimal set of multi-objective optimization problems. The evolutionary algorithm (EA) uses the predator-prey model from ecology. The prey are the usual individuals of an EA that represent possible solutions to the optimization task. They are placed at vertices of a graph, remain stationary, reproduce, and are chased by predators that traverse the graph. The predators chase the prey only within its current neighborhood and according to one of the optimization criteria. Because there are several predators with different selection criteria, those prey individuals, which perform best with respect to all objectives, are able to produce more descendants than inferior ones. As soon as a vertex for the prey becomes free, it is refilled by descendants from alive parents in the usual way of EA, i.e., by inheriting slightly altered attributes. After a while, the prey concentrate at Pareto-optimal positions. The main objective of this preliminary study is the answer to the question whether the predator-prey approach to multi-objective optimization works at all. The performance of this evolutionary algorithm is examined under several step-size adaptation rules. 1
481|Evolutionary Computation and Convergence to a Pareto Front|Research into solving multiobjective optimization problems (MOP) has as one of its an overall goals that of developing and defining foundations of an Evolutionary Computation (EC)-based MOP theory. In this paper, we introduce relevant MOP concepts, and the notion of Pareto optimality, in particular. Specific notation is defined and theorems are presented ensuring Paretobased Evolutionary Algorithm (EA) implementations are clearly understood. Then, a specific experiment investigating the convergence of an arbitrary EA to a Pareto front is presented. This experiment gives a basis for a theorem showing a specific multiobjective EA statistically converges to the Pareto front. We conclude by using this work to justify further exploration into the theoretical foundations of EC-based MOP solution methods.  1 Introduction  Our research focuses on solving scientific and engineering multiobjective optimization problems (MOPs), contributing to the overall goal of developing and defining foundatio...
482|Continuum structural topology design with genetic algorithms|The genetic algorithm (GA), an optimization technique based on the theory of natural selection, is applied to structural topology design problems. After reviewing the GA and previous research in structural topology optimization, we describe a binary material/void design representation that is encoded in GA chromosome data structures. This representation is intended to approximate a material continuum as opposed to discrete truss structures. Four examples, showing the broad utility of the approach and representation, are then presented. A fifth example suggests an alternate representation that allows continuously-variable material density. Concluding discussion suggests recommended uses of the technique and describes ongoing and possible future work. Ó 2000 Elsevier Science S.A. All rights reserved.
483|Freenet: A Distributed Anonymous Information Storage and Retrieval System|We describe Freenet, an adaptive peer-to-peer network application  that permits the publication, replication, and retrieval of data  while protecting the anonymity of both authors and readers. Freenet operates  as a network of identical nodes that collectively pool their storage  space to store data files and cooperate to route requests to the most  likely physical location of data. No broadcast search or centralized location  index is employed. Files are referred to in a location-independent  manner, and are dynamically replicated in locations near requestors and  deleted from locations where there is no interest. It is infeasible to discover  the true origin or destination of a file passing through the network,  and difficult for a node operator to determine or be held responsible for  the actual physical contents of her own node.
485|Error and attack tolerance of complex networks|Many complex systems display a surprising degree of tolerance against errors. For example, relatively simple organisms grow, persist and reproduce despite drastic pharmaceutical or environmental interventions, an error tolerance attributed to the robustness of the underlying metabolic network [1]. Complex communication networks [2] display a surprising degree of robustness: while key components regularly malfunction, local failures rarely lead to the loss of the global information-carrying ability of the network. The stability of these and other complex systems is often attributed to the redundant wiring of the functional web defined by the systems’ components. In this paper we demonstrate that error tolerance is not shared by all redundant systems, but it is displayed only by a class of inhomogeneously wired networks, called scale-free networks. We find that scale-free networks, describing a number of systems, such as the World Wide Web (www) [3–5], Internet [6], social networks [7] or a cell [8], display an unexpected degree of robustness, the ability of their nodes to communicate being unaffected by even unrealistically high failure rates. However,
486|Private Information Retrieval| Publicly accessible databases are an indispensable resource for retrieving up to date information. But they also pose a significant risk to the privacy of the user, since a curious database operator can follow the user&#039;s queries and infer what the user is after. Indeed, in cases where the users &#039; intentions are to be kept secret, users are often cautious about accessing the database. It can be shown that when accessing a single database, to completely guarantee the privacy of the user, the whole database should be downloaded, namely n bits should be communicated (where n is the number of bits in the database). In this work, we investigate whether by replicating the database, more efficient solutions to the private retrieval problem can be obtained. We describe schemes that enable a user to access k replicated copies of a database (k * 2) and privately retrieve information stored in the database. This means that each individual database gets no information on the identity of the item retrieved by the user. Our schemes use the replication to gain substantial saving. In particular, we have ffl A two database scheme with communication complexity of O(n1=3). ffl A scheme for a constant number, k, of databases with communication complexity O(n1=k). ffl A scheme for 13 log2 n databases with polylogarithmic (in n) communication complexity.
487|The Free Haven Project: Distributed Anonymous Storage Service|We present a design for a system of anonymous storage which resists the attempts of powerful adversaries to find or destroy any stored data. We enumerate distinct notions of anonymity for each party in the system, and suggest a way to classify anonymous systems based on the kinds of anonymity provided. Our design ensures the availability of each document for a publisher-specified lifetime. A reputation system provides server accountability by limiting the damage caused from misbehaving servers. We identify attacks and defenses against anonymous storage services, and close with a list of problems which are currently unsolved.
488|Publius: A robust, tamper-evident, censorship-resistant, web publishing system|Permission is granted for noncommercial reproduction of the work for educational or research purposes.
489|Onion Routing for Anonymous and Private Internet Connections|this article&#039;s publication, the prototype network is processing more than 1 million Web connections per month from more than six thousand IP addresses in twenty countries and in all six main top level domains. [7] Onion Routing operates by dynamically building anonymous connections within a network of real-time Chaum Mixes [3]. A Mix is a store and forward device that accepts a number of fixed-length messages from numerous sources, performs cryptographic transformations on the messages, and then forwards the messages to the next destination in a random order. A single Mix makes tracking of a particular message either by specific bit-pattern, size, or ordering with respect to other messages difficult. By routing through numerous Mixes in the network, determining who is talking to whom becomes even more difficult. Onion Routing&#039;s network of core onion-routers (Mixes) is distributed, faulttolerant, and under the control of multiple administrative domains, so no single onion-router can bring down the network or compromise a user&#039;s privacy, and cooperation between compromised onion-routers is thereby confounded.
490|Web MIXes: A system for anonymous and unobservable Internet access|We present the architecture, design issues and functions of a MIX-based system for anonymous and unobservable real-time Internet access. This system prevents trac analysis as well as ooding attacks. The core technologies include an adaptive, anonymous, time/volumesliced channel mechanism and a ticket-based authentication mechanism. The system also provides an interface to inform anonymous users about their level of anonymity and unobservability.
491|The Eternity Service|The Internet was designed to provide a communications channel that is as resistant to denial of service attacks as human ingenuity can make it. In this note, we propose the construction of a storage medium with similar properties. The basic idea is to use redundancy and scattering techniques to replicate data across a large set of machines (such as the Internet), and add anonymity mechanisms to drive up the cost of selective service denial attacks. The detailed design of this service is an interesting scientific problem, and is not merely academic: the service may be vital in safeguarding individual rights against new threats posed by the spread of electronic publishing.
492|Anonymous Web Transactions with Crowds|This article presents a system called Crowds  that enables the retrieval of information over the  Web without revealing so much potentially private  information to several parties. The goal of  Crowds is to make browsing anonymous, so that  information about either the user or what information  he or she retrieves is hidden from Web  servers and other parties. Crowds prevents a  Web server from learning any potentially identifying  information about the user, including even  the user&#039;s IP address or domain name. Crowds  also prevents Web servers from learning a variety  of other information, such as the page that  referred the user to its site or the user&#039;s computing  platform
493|A Prototype Implementation of Archival Intermemory|An Archival Intermemory solves the problem of highly survivable digital data storage in the spirit of the Internet. In this paper we describe a prototype implementation of Intermemory, including an overall system architecture and implementations of key system components. The result is a working Intermemory that tolerates up to 17 simultaneous node failures, and includes a Web gateway for browser-based access to data. Our work demonstrates the basic feasibility of Intermemory and represents significant progress towards a deployable system.
494|Project &#034;Anonymity and Unobservability in the Internet&#034;|. It is a hard problem to achieve anonymity for real-time services in the Internet (e.g. Web access). All existing concepts fail when we assume a very strong attacker model (i.e. an attacker is able to observe all communication links). We also show that these attacks are realworld attacks. This paper outlines alternative models which mostly render these attacks useless. Our present work tries to increase the efficiency of these measures.  1 The perfect system  1.1 Attacks  The perfect anonymous communication system has to prevent the following attacks:  1.  Message coding attack: If messages do not change their coding during transmission they can be linked or traced.  2.  Timing attack: An opponent can observe the duration of a specific communication by linking its possible endpoints and waiting for a correlation between the creation and/or release event at each possible endpoint.  3.  Message volume attack: The amount of transmitted data (i.e. the message length) can be observed. Thus...
495|TAZ Servers and the Rewebber Network: Enabling Anonymous Publishing on the World Wide Web|The World Wide Web has recently matured enough to provide everyday users with an extremely cheap publishing mechanism. However, the current WWW architecture makes it fundamentally difficult to provide content without identifying yourself. We examine the problem of anonymous publication on the WWW, propose a design suitable for practical deployment, and describe our implementation. Some key features of our design include universal accessibility by pre-existing clients, short persistent names, security against social, legal, and political pressure, protection against abuse, and good performance.
496|The Free Haven Project: Design and Deployment of an Anonymous Secure Data Haven|The Free Haven Project aims to deploy a system for distributed data storage which is robust against attempts by powerful adversaries to find and destroy stored data. Free Haven uses a mixnet for communication, and it emphasizes distributed, reliable, and anonymous storage over efficient retrieval. We provide an outline of a formal definition of anonymity, to help characterize the protection that Free Haven provides and to help compare related services. We also provide some background from case law about anonymous speech and anonymous publication, and examine some of the ethical and moral implications of an anonymous publishing service. In addition, we describe a variety of attacks on the system and ways of protecting against these attacks. Some of the problems Free Haven addresses include providing sufficient accountability without sacrificing anonymity, building trust between servers based entirely on their observed behavior, and providing user interfaces that will make the system easy for end-users.
497|Wrappers for Feature Subset Selection|In the feature subset selection problem, a learning algorithm is faced with the problem of selecting a relevant subset of features upon which to focus its attention, while ignoring the rest. To achieve the best possible performance with a particular learning algorithm on a particular training set, a feature subset selection method should consider how the algorithm and the training set interact. We explore the relation between optimal feature subset selection and relevance. Our wrapper method searches for an optimal feature subset tailored to a particular algorithm and a domain. We study the strengths and weaknesses of the wrapper approach andshow a series of improved designs. We compare the wrapper approach to induction without feature subset selection and to Relief, a filter approach to feature subset selection. Significant improvement in accuracy is achieved for some datasets for the two families of induction algorithms used: decision trees and Naive-Bayes. 
498|Induction of Decision Trees| The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions.  
499|PROBABILITY INEQUALITIES FOR SUMS OF BOUNDED RANDOM VARIABLES|Upper bounds are derived for the probability that the sum S of n independent random variables exceeds its mean ES by a positive number nt. It is assumed that the range of each summand of S is bounded or bounded above. The bounds for Pr(S-ES&gt; nt) depend only on the endpoints of the ranges of the smumands and the mean, or the mean and the variance of S. These results are then used to obtain analogous inequalities for certain sums of dependent random variables such as U statistics and the sum of a random sample without replacement from a finite population. 
500|Instance-based learning algorithms|Abstract. Storing and using specific instances improves the performance of several supervised learning algorithms. These include algorithms that learn decision trees, classification rules, and distributed networks. However, no investigation has analyzed algorithms that use only specific instances to solve incremental learning tasks. In this paper, we describe a framework and methodology, called instance-based learning, that generates classification predictions using only specific instances. Instance-based learning algorithms do not maintain a set of abstractions derived from specific instances. This approach extends the nearest neighbor algorithm, which has large storage requirements. We describe how storage requirements can be significantly reduced with, at most, minor sacrifices in learning rate and classification accuracy. While the storage-reducing algorithm performs well on several realworld databases, its performance degrades rapidly with the level of attribute noise in training instances. Therefore, we extended it with a significance test to distinguish noisy instances. This extended algorithm&#039;s performance degrades gracefully with increasing noise levels and compares favorably with a noise-tolerant decision tree algorithm.
501|A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection|We review accuracy estimation methods and compare the two most common methods: cross-validation and bootstrap. Recent experimental results on artificial data and theoretical results in restricted settings have shown that for selecting a good classifier from a set of classifiers (model selection), ten-fold cross-validation may be better than the more expensive leaveone-out cross-validation. We report on a largescale experiment -- over half a million runs of C4.5 and a Naive-Bayes algorithm -- to estimate the effects of different parameters on these algorithms on real-world datasets. For cross-validation, we vary the number of folds and whether the folds are stratified or not; for bootstrap, we vary the number of bootstrap samples. Our results indicate that for real-word datasets similar to ours, the best method to use for model selection is ten-fold stratified cross validation, even if computation power allows using more folds. 
502|The Perceptron: A Probabilistic Model for Information Storage and Organization in The Brain|If we are eventually to understand the capability of higher organisms for perceptual recognition, generalization, recall, and thinking, we must first have answers to three fundamental questions: 1. How is information about the physical world sensed, or detected, by the biological system? 2. In what form is information stored, or remembered? 3. How does information contained in storage, or in memory, influence recognition and behavior? The first of these questions is in the
503|Irrelevant Features and the Subset Selection Problem|We address the problem of finding a subset of features that allows a supervised induction algorithm to induce small high-accuracy concepts. We examine notions of relevance and irrelevance, and show that the definitions used in the machine learning literature do not adequately partition the features into useful categories of relevance. We present definitions for irrelevance and for two degrees of relevance. These definitions improve our understanding of the behavior of previous subset selection algorithms, and help define the subset of features that should be sought. The features selected should depend not only on the features and the target concept, but also on the induction algorithm. We describe a method for feature subset selection using cross-validation that is applicable to any induction algorithm, and discuss experiments conducted with ID3 and C4.5 on artificial and real datasets.
504|Supervised and unsupervised discretization of continuous features|Many supervised machine learning algorithms require a discrete feature space. In this paper, we review previous work on continuous feature discretization, identify de n-ing characteristics of the methods, and conduct an empirical evaluation of several methods. We compare binning, an unsupervised discretization method, to entropy-based and purity-based methods, which are supervised algorithms. We found that the performance of the Naive-Bayes algorithm signi cantly improved when features were discretized using an entropy-based method. In fact, over the 16 tested datasets, the discretized version of Naive-Bayes slightly outperformed C4.5 on average. We also show that in some cases, the performance of the C4.5 induction algorithm signi cantly improved if features were discretized in advance ? in our experiments, the performance never signi cantly degraded, an interesting phenomenon considering the fact that C4.5 is capable of locally discretizing features. 1
505|Neural network ensembles, cross validation, and active learning|Learning of continuous valued functions using neural network en-sembles (committees) can give improved accuracy, reliable estima-tion of the generalization error, and active learning. The ambiguity is defined as the variation of the output of ensemble members aver-aged over unlabeled data, so it quantifies the disagreement among the networks. It is discussed how to use the ambiguity in combina-tion with cross-validation to give a reliable estimate of the ensemble generalization error, and how this type of ensemble cross-validation can sometimes improve performance. It is shown how to estimate the optimal weights of the ensemble members using unlabeled data. By a generalization of query by committee, it is finally shown how the ambiguity can be used to select new training data to be labeled in an active learning scheme. 1
506|Estimating Attributes: Analysis and Extensions of RELIEF|. In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies among them. Kira and Rendell (1992a,b) developed an algorithm called RELIEF, which was shown to be very efficient in estimating attributes. Original RELIEF can deal with discrete and continuous attributes and is limited to only two-class problems. In this paper RELIEF is analysed and extended to deal with noisy, incomplete, and multi-class data sets. The extensions are verified on various artificial and one well known real-world problem. 1 Introduction  This paper deals with the problem of estimating the quality of attributes with strong dependencies to other attributes which seems to be the key issue of machine learning in general. Namely, for particular problems (e.q. parity problems of higher degrees) the discovering of dependencies between attributes may be unfeasible due to combinatorial explosion. In such cases efficient heuris...
507|An analysis of Bayesian classifiers|In this paper we present anaverage-case analysis of the Bayesian classifier, a simple induction algorithm that fares remarkably well on many learning tasks. Our analysis assumes a monotone conjunctive target concept, and independent, noise-free Boolean attributes. We calculate the probability that the algorithm will induce an arbitrary pair of concept descriptions and then use this to compute the probability of correct classification over the instance space. The analysis takes into account the number of training instances, the number of attributes, the distribution of these attributes, and the level of class noise. We also explore the behavioral implications of the analysis by presenting
508| 	 Beyond Independence: Conditions for the Optimality of the Simple Bayesian Classifier   |The simple Bayesian classifier (SBC) is commonly thought to assume that attributes are independent given the class, but this is apparently contradicted by the surprisingly good performance it exhibits in many domains that contain clear attribute dependences. No explanation for this has been proposed so far. In this paper we show that the SBC does not in fact assume attribute independence, and can be optimal even when this assumption is violated by a wide margin. The key to this finding lies in the distinction between classification and probability estimation: correct classification can be achieved even when the probability estimates used contain large errors. We show that the previously-assumed region of optimality of the SBC is a second-order infinitesimal fraction of the actual one. This is followed by the derivation of several necessary and several sufficient conditions for the optimality of the SBC. For example, the SBC is optimal for learning arbitrary conjunctions and disjunctions, even though they violate the independence assumption. The paper also reports empirical evidence of the SBC&#039;s competitive performance in domains containing substantial degrees of attribute dependence.
509|Induction of Selective Bayesian Classifiers|In this paper, we examine previous work on the naive Bayesian classifier and review its limitations, which include a sensitivity to correlated features. We respond to this problem by embedding the naive Bayesian induction scheme within an algorithm that carries out a greedy search through the space of features. We hypothesize that this approach will improve asymptotic accuracy in domains that involve correlated features without reducing the rate of learning in ones that do not. We report experimental results on six natural domains, including comparisons with decision-tree induction, that support these hypotheses. In closing, we discuss other approaches to extending naive Bayesian classifiers and outline some directions for future research.
510|Learning With Many Irrelevant Features|In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires \Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  +  p ln n]) training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. The paper also presents a quasi-polynomial time algorithm, FOCUS, which implements MIN-FEATURES. Experimental studies are presented that compare FOCUS to the ID3 and FRINGE algorithms. These experiments show that--- contrary to expectations---these algorithms do not implement good approximations of MIN-FEATURES. The coverage, sample complexity, and generalization performance of FOCUS is substantially better than either ID3 or FRINGE on learning problems where the MIN-FEATURE...
511|Training a 3-Node Neural Network is NP-Complete| We consider a 2-layer, 3-node, n-input neural network whose nodes compute linear threshold functions of their inputs. We show that it is NP-complete to decide whether there exist weights and thresholds for this network so that it produces output consistent with a given set of training examples. We extend the result to other simple networks. We also present a network for which training is hard but where switching to a more powerful representation makes training easier. These results suggest that those looking for perfect training algorithms cannot escape inherent computational difficulties just by considering only simple or very regular networks. They also suggest the importance, given a training problem, of finding an appropriate network and input encoding for that problem. It is left as an open problem to extend our result to nodes with nonlinear functions such as sigmoids.
512|Greedy Attribute Selection|Many real-world domains bless us with a wealth of attributes to use for learning. This blessing is often a curse: most inductive methods generalize worse given too many attributes than if given a  good subset of those attributes. We examine this problem for two learning tasks taken from a calendar scheduling domain. We show that ID3/C4.5 generalizes poorly on these tasks if allowed to use all available attributes. We examine five greedy hillclimbing procedures that search for attribute sets that generalize well with ID3/C4.5. Experiments suggest hillclimbing in attribute space can yield substantial improvements in generalization performance. We present a caching scheme that makes attribute hillclimbing more practical computationally. We also compare the results of hillclimbing in attribute space with FOCUS and RELIEF on the two tasks. 1 INTRODUCTION  As machine learning is applied to real-world tasks, difficulties arise that do not occur in simpler textbook experiments. One such diffic...
513|Data Mining using MLC++: A Machine Learning Library in C++|Data mining algorithmsincluding machine learning, statistical analysis, and pattern recognition techniques can greatly improve our understanding of data warehouses that are now becoming more widespread. In this paper, we focus on classification algorithms and review the need for multiple classification algorithms. We describe a system called  MLC++ , which was designed to help choose the appropriate classification algorithm for a given dataset by making it easy to compare the utility of different algorithms on a specific dataset of interest. MLC  ++ not only provides a workbench for such comparisons, but also provides a library of C  ++  classes to aid in the development of new algorithms, especially hybrid algorithms and multi-strategy algorithms. Such algorithms are generally hard to code from scratch. We discuss design issues, interfaces to other programs, and visualization of the resulting classifiers. 1 Introduction  Data warehouses containing massive amounts of data have been b...
514|The Power of Decision Tables|. We evaluate the power of decision tables as a hypothesis space for supervised learning algorithms. Decision tables are one of the simplest hypothesis spaces possible, and usually they are easy to understand. Experimental results show that on artificial and real-world domains containing only discrete features, IDTM, an algorithm inducing decision tables, can sometimes outperform state-of-the-art algorithms such as C4.5. Surprisingly, performance is quite good on some datasets with continuous features, indicating that many datasets used in machine learning either do not require these features, or that these features have few values. We also describe an incremental method for performing crossvalidation that is applicable to incremental learning algorithms including IDTM. Using incremental cross-validation, it is possible to cross-validate a given dataset and IDTM in time that is linear in the number of instances, the number of features, and the number of label values. The time for incre...
515|Efficient algorithms for minimizing cross validation error|Model selection is important in many areas of supervised learning. Given a dataset and a set of models for predicting with that dataset, we must choose the model which is expected to best predict future data. In some situations, such as online learning for control of robots or factories, data is cheap and human expertise costly. Cross validation can then be a highly effective method for automatic model selection. Large scale cross validation search can, however, be computationally expensive. This paper introduces new algorithms to reduce the computational burden of such searches. We show how experimental design methods can achieve this, using a technique similar to a Bayesian version of Kaelbling’s Interval Estimation. Several improvements are then given, including (1) the use of blocking to quickly spot near-identical models, and (2) schemata search: a new method for quickly finding families of relevant features. Experiments are presented for robot data and noisy synthetic datasets. The new algorithms speed up computation without sacrificing reliability, and in some cases are more reliable than conventional techniques. 1
516|Learning classification trees|Algorithms for learning cIassification trees have had successes in ar-tificial intelligence and statistics over many years. This paper outlines how a tree learning algorithm can be derived using Bayesian statis-tics. This iutroduces Bayesian techniques for splitting, smoothing, and tree averaging. The splitting rule is similar to QuinIan’s information gain, while smoothing and averaging replace pruning. Comparative ex-periments with reimplementations of a minimum encoding approach, Quinlan’s C4 (1987) and Breiman et aL’s CART (1984) show the full Bayesian algorithm produces more accurate predictions than versions
517|A Comparative Evaluation of  Sequential Feature Selection Algorithms|Several recent machine learning publications demonstrate the utility of using feature  selection algorithms in supervised learning tasks. Among these, scqucnlial feature  s1ion algorithms are receiving attention. The most frequently studied variants  of these algorithms are forward and backward sequential selection. Many studies  on supervised learning with sequential feature selection report applications of these  algorithms, but do not consider variants of them that might be more appropriate  for some performance tasks. This paper reports positive empirical results on such  variants, and argues for their serious consideration in similar learning tasks.
518|Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation|Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memorybased learning algorithms, but we also argue that it is applicable to any class of model selection problems. 1 Introduction  Model selection addresses &#034;high level&#034; decisions about how best to tune learning algorithm architectures for particular tasks. Such decisions include which...
519|Learning Boolean Concepts in the Presence of Many Irrelevant Features|In many domains, an appropriate inductive bias is the MIN-FEATURES bias, which prefers consistent hypotheses definable over as few features as possible. This paper defines and studies this bias in Boolean domains. First, it is shown that any learning algorithm implementing the MIN-FEATURES bias requires \Theta(  1  ffl ln  1  ffi +  1  ffl [2  p  + p ln n])  training examples to guarantee PAC-learning a concept having p relevant features out of n available features. This bound is only logarithmic in the number of irrelevant features. For implementing the MIN-FEATURES bias, the paper presents five algorithms that identify a subset of features sufficient to construct a hypothesis consistent with the training examples. FOCUS-1 is a straightforward algorithm that returns a minimal and sufficient subset of features in quasi-polynomial time. FOCUS-2 does the same task as FOCUS-1 but is empirically shown to be substantially faster than FOCUS-1. Finally, the Simple-Greedy, Mutual-Information-G...
520|Wrappers For Performance Enhancement And Oblivious Decision Graphs|In this doctoral dissertation, we study three basic problems in machine learning and two new hypothesis spaces with corresponding learning algorithms. The problems we investigate are: accuracy estimation, feature subset selection, and parameter tuning. The latter two problems are related and are studied under the wrapper approach. The hypothesis spaces we investigate are: decision tables with a default majority rule (DTMs) and oblivious read-once decision graphs (OODGs).
521|Using Decision Trees to Improve Case-Based Learning|This paper shows that decision trees can be used to improve the performance of casebased learning (CBL) systems. We introduce a performance task for machine learning systems called semi-flexible prediction that lies between the classification task performed by decision tree algorithms and the flexible prediction task performed by conceptual clustering systems. In semi-flexible prediction, learning should improve prediction of a specific set of features known a priori rather than a single known feature (as in classification) or an arbitrary set of features (as in conceptual clustering). We describe one such task from natural language processing and present experiments that compare solutions to the problem using decision trees, CBL, and a hybrid approach that combines the two. In the hybrid approach, decision trees are used to specify the features to be included in k-nearest neighbor case retrieval. Results from the experiments show that the hybrid approach outperforms both the decision ...
522|On biases in estimating multi-valued attributes|We analyse the biases of eleven measures for estimating the quality of the multi-valued attributes. The values of information gain, J-measure, gini-index, and relevance tend to linearly increase with the number of values of an attribute. The values of gain-ratio, distance measure, Relief, and the weight of evidence decrease for informative attributes and increase for irrelevant attributes. The bias of the statistic tests based on the chi-square distribution is similar but these functions are not able to discriminate among the attributes of different quality. We also introduce a new function based on the MDL principle whose value slightly decreases with the increasing number of attribute’s values. 1
524|Oversearching and Layered Search in Empirical Learning|When learning classifiers, more extensive search for rules is shown to lead to lower predictive accuracy on many of the real-world domains investigated. This counter-intuitive result is particularly relevant to recent systematic search methods that use risk-free pruning to achieve the same outcome as exhaustive search. We propose an iterated search method that commences with greedy search, extending its scope at each iteration until a stopping criterion is satisfied. This layered search is often found to produce theories that are more accurate than those obtained with either greedy search or moderately extensive beam search. 1 Introduction  Mitchell [1982] observes that the generalization implicit in learning from examples can be viewed as a search over the space of possible theories. From this perspective, most machine learning methods carry out a series of local searches in the vicinity of the current theory, selecting at each step the most promising improvement. Covering algorithms ...
525|Selecting a Classification Method by Cross-Validation|If we lack relevant problem-specific knowledge, cross-validation methods may be used to select a classification method empirically. We examine this idea here to show in what senses cross-validation does and does not solve the selection problem. As illustrated empirically, cross-validation may lead to higher average performance than application of any single classification strategy and it also cuts the risk of poor performance. On the other hand, cross-validation is no more or  less a form of bias than simpler strategies and applying it appropriately ultimately depends in the same way on prior knowledge. In fact, cross-validation may be seen as a way of applying partial information about the applicability of alternative classification strategies. Keywords: Cross-validation, classification, decision trees, neural networks.  1 Introduction Machine learning researchers and statisticians have produced a host of approaches to the problem of classification including methods for inducing rul...
526|Feature Selection for Case-Based Classification of Cloud Types: An Empirical Comparison|Accurate weather prediction is crucial for many activities, including Naval operations. Researchers within the meteorological division of the Naval Research Laboratory have developed and fielded several expert systems for problems such as fog and turbulence forecasting, and tropical storm movement. They are currently developing an automated system for satellite image interpretation, part of which involves cloud classification. Their cloud classification database contains 204 high-level features, but contains only a few thousand instances. The predictive accuracy of classifiers can be improved on this task by employing a feature selection algorithm. We explain why non-parametric case-based classifiers are excellent choices for use in feature selection algorithms. We then describe a set of such algorithms that use case-based classifiers, empirically compare them, and introduce novel extensions of backward sequential selection that allows it to scale to this task. Several of the approache...
527|Searching for Dependencies in Bayesian Classifiers|Naive Bayesian classifiers which make independence assumptions perform remarkably well on some data sets but poorly on others. We explore ways to improve the Bayesian classifier by searching for dependencies among attributes. We propose and evaluate two algorithms for detecting dependencies among attributes and show that the backward sequential elimination and joining algorithm provides the most improvement over the naive Bayesian classifier. The domains on which the most improvement occurs are those domains on which the naive Bayesian classifier is significantly less accurate than a decision tree learner. This suggests that the attributes used in some common databases are not independent conditioned on the class and that the violations of the independence assumption that affect the accuracy of the classifier can be detected from training data. 23.1 Introduction The Bayesian classifier (Duda
528|Automatic Parameter Selection by Minimizing Estimated Error|We address the problem of finding the parameter settings that will result in optimal performance of a given learning algorithm using a particular dataset as training data. We describe a &#034;wrapper&#034; method, considering determination of the best parameters as a discrete function optimization problem. The method uses best-first search and crossvalidation to wrap around the basic induction algorithm: the search explores the space of parameter values, running the basic algorithm many times on training and holdout sets produced by cross-validation to get an estimate of the expected error of each parameter setting. Thus, the final selected parameter settings are tuned for the specific induction algorithm and dataset being studied. We report experiments with this method on 33 datasets selected from the UCI and StatLog collections using C4.5 as the basic induction algorithm. At a 90% confidence level, our method improves the performance of C4.5 on nine domains, degrades performance on one, and is...
529|Lookahead and Pathology in Decision Tree Induction|The standard approach to decision tree induction is a top-down, greedy algorithm that makes locally optimal, irrevocable decisions at each node of a tree. In this paper, we study an alternative approach, in which the algorithms use limited lookahead to decide what test to use at a node. We systematically compare, using a very large number of decision trees, the quality of decision trees induced by the greedy approach to that of trees induced using lookahead. The main results of our experiments are: (i) the greedy approach produces trees that are just as accurate as trees produced with the much more expensive lookahead step; and (ii) decision tree induction exhibits pathology, in the sense that lookahead can produce trees that are both larger and less accurate than trees produced without it. 1. Introduction  The standard algorithm for constructing decision trees from a set of examples is greedy induction --- a tree is induced top-down with locally optimal choices made at each node, with...
530|Hybrid learning using genetic algorithms and decision trees for pattern classification|This paper introduces a hybrid learning methodology that integrates genetic algorithms (GAs) and decision tree learning (ID3) in order to evolve optimal subsets of discriminatory features for robust pattern classification. A GA is used to search the space of all possible subsets of a large set of candidate discrimination features. For a given feature subset, ID3 is invoked to produce a decision tree. The classification performance of the decision tree on unseen data is used as a measure of fitness for the given feature set, which, in turn, is used by the GA to evolve better feature sets. This GA-ID3 process iterates until a feature subset is found with satisfactory classification performance. Experimental results are presented which illustrate the feasibility of our approach on difficult problems involving recognizing visual concepts in satellite and facial image data. The results also show improved classification performance and reduced description complexity when compared against standard methods for feature selection. 1
531|Feature subset selection as search with probabilistic estimates|Irrelevant features and weakly relevant features may reduce the comprehensibility and accuracy of concepts induced by supervised learning algorithms. We formulate search problem with probabilistic estimates. Searching a space using an evaluation function that is a random variable requires trading off accuracy of estimates for increased state exploration. We show how recent feature subset selection algorithms in the machine learning literature fit into this search problem as simple hill climbing approaches, and conduct a small experiment using a best-first search technique. 1
532|Inductive Policy: The Pragmatics of Bias Selection|This paper extends the currently accepted model of inductive bias by identifying six categories of bias and separates inductive bias from the policy for its selection (the inductive policy). We analyze existing &amp;quot;blas selection &amp;quot; systems, examining the similarities and differences in their inductive policies, and idemify three techniques useful for building inductive policies. We then present a framework for representing and automaticaIly selecting a wide variety of biases and describe experiments with an instantiation of the framework addressing various pragmatic tradeoffs of time, space, accuracy, and the cost oferrors. The experiments show that a common framework can be used to implement policies for a variety of different types of blas selection, such as parameter selection, term selection, and example selection, using similar techniques. The experiments also show that different tradeoffs can be made by the implementation of different policies; for example, from the same data different rule sets can be learned based on different tradeoffs of accuracy versus the cost of erroneous predictions.
533|Oblivious Decision Trees and Abstract Cases|In this paper, we address the problem of case-based learning in the presence of irrelevant features. We review previous work on attribute selection and present a new algorithm, Oblivion, that carries out greedy pruning of oblivious decision trees, which effectively store a set of abstract cases in memory. We hypothesize that this approach will efficiently identify relevant features even when they interact, as in parity concepts. We report experimental results on artificial domains that support this hypothesis, and experiments with natural domains that show improvement in some cases but not others. In closing, we discuss the implications of our experiments, consider additional work on irrelevant features, and outline some directions for future research.
534|Useful Feature Subsets and Rough Set Reducts|In supervised classification learning, one attempts to induce a classifier that correctly predicts the label of novel instances. We demonstrate that by choosing a useful subset of features for the indiscernibility relation, an induction algorithm based on simple decision table can have high prediction accuracy on artificial and real-world datasets. We show that useful feature subsets are not necessarily maximal independent sets (relative reducts) with respect to the label, and that, in practical situations, using a subset of the relative core features may lead to superior performance. 1 Introduction In supervised classification learning, one is given a training set containing labelled instances (examples) . Each labelled instance contains a list of feature values (attribute values) and a discrete label value. The induction task is to build a classifier that will correctly predict the label of novel instances. Common classifiers are decision trees, neural networks, and nearest-neighbor...
535|A Quantitative Study of Hypothesis Selection|The hypothesis selection problem (or the k-  armed bandit problem) is central to the realization  of many learning systems. This  paper studies the minimization of sampling  cost in hypothesis selection under a probably  approximately optimal (PAO) learning  framework. Hypothesis selection algorithms  could be exploration-oriented or exploitationoriented.
536|Learning Bayesian Networks Using Feature Selection|This paper introduces a novel enhancement for learning Bayesian networks with a bias for small, high-predictive-accuracy networks. The new approach selects a subset of features which maximizes predictive accuracy prior to the network learning phase. We examine explicitly the effects of two aspects of the algorithm, feature selection and node ordering. Our approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all attributes. 1 INTRODUCTION  Bayesian networks are being increasingly recognized as an important representation for probabilistic reasoning. For many domains, the need to specify the probability distributions for a Bayesian network is considerable, and learning these probabilities from data using an algorithm like K2 [8]  1  could alleviate such specification difficulties. We describe an extension to the Bayesian network learning approaches introduced in K2. Rather than ...
537|Probabilistic Hill-Climbing: Theory and Applications|Many learning systems search through a space of possible performance elements, seeking an element with high expected utility. As the task of finding the globally optimal element is usually intractable, many practical learning systems use hill-climbing to find a local optimum. Unfortunately, even this is difficult, as it depends on the distribution of problems, which is typically unknown. This paper addresses the task of approximating this hill-climbing search when the utility function can only be estimated by sampling. We present an algorithm that returns an element that is, with provably high probability, essentially a local optimum. We then demonstrate the generality of this algorithm by sketching three meaningful applications, that respectively find an element whose efficiency, accuracy or completeness is nearly optimal. These results suggest approaches to solving the utility problem from explanation-based learning, the multiple extension problem from nonmonotonic reasoning and the ...
538|Fractals for Secondary Key Retrieval |In this paper we propose the use of fractals and especially the Hilbert curve, in order to design good distance-preserving mappings. Such mappings improve the performance of secondary-key- and spatial- access methods, where multi-dimensional points have to be stored on an 1-dimensional medium (e.g., disk). Good clustering reduces the number of disk accesses on retrieval, improving the response time. Our experiments on range queries and nearest neighbor queries showed that the proposed Hilbert curve achieves better clustering than older methods (&amp;quot;bit-shuffling&amp;quot;, or Peano curve), for every situation we tried.
539|Multi-Step Processing of Spatial Joins   |Spatial joins are one of the most importaot operations for combining spatial objects of several relations. IO this paper, spatial join processing is studied in detail for extended spatial objects in two-dimensional data space. We present an approach for spatial join processing that is based on three steps. First, a spatial join is performed on the minimum bounding rectangles of the objects returning a set of candidates. Various approaches for accelerating this step of join processing have been examined at the last year’s conference [BKS 93a]. In this paper, we focus on the problem how to compute the answers from the set of candidates which is handled by the foliowing two steps. First of all, sophisticated approximations are used to identify answers as well as to filter out false hits from the set of candidates. For this purpose, we investigate various types of conservative and progressive approximations. In the last step, the exact geometry of the remaioing candidates has to be tested against the join predicate. The time required for computing spatial joio predicates can essentially be reduced when objects are adequately organized in main memory. IO our approach, objects are fiist decomposed into simple components which are exclusively organized by a main-memory resident spatial data structure. Overall, we present a complete approach of spatial join processing on complex spatial objects. The performance of the individual steps of our approach is evaluated with data sets from real cartographic applications. The results show that our approach reduces the total execution time of the spatial join by factors.  
540|QBISM: A Prototype 3-D Medical Image Database System|this paper. However, these automatic or semi-automatic warping algorithms are extremely important for this application. It is precisely this technology that permits anatomic structure-based access to acquired medical images as well as comparisons among studies, even of different patients, as long as they have been warped to the same atlas. Furthermore, it enables the database to grow, and be queryable, without time-consuming manual segmentation of the data.
541|Model-Based Clustering, Discriminant Analysis, and Density Estimation|Cluster analysis is the automated search for groups of related observations in a data set. Most clustering done in practice is based largely on heuristic but intuitively reasonable procedures and most clustering methods available in commercial software are also of this type. However, there is little systematic guidance associated with these methods for solving important practical questions that arise in cluster analysis, such as \How many clusters are there?&#034;, &#034;Which clustering method should be used?&#034; and \How should outliers be handled?&#034;. We outline a general methodology for model-based clustering that provides a principled statistical approach to these issues. We also show that this can be useful for other problems in multivariate analysis, such as discriminant analysis and multivariate density estimation. We give examples from medical diagnosis, mineeld detection, cluster recovery from noisy data, and spatial density estimation. Finally, we mention limitations of the methodology, a...
542|Bayes Factors|In a 1935 paper, and in his book Theory of Probability, Jeffreys developed a methodology for quantifying the evidence in favor of a scientific theory. The centerpiece was a number, now called the Bayes factor, which is the posterior odds of the null hypothesis when the prior probability on the null is one-half. Although there has been much discussion of Bayesian hypothesis testing in the context of criticism of P -values, less attention has been given to the Bayes factor as a practical tool of applied statistics. In this paper we review and discuss the uses of Bayes factors in the context of five scientific applications in genetics, sports, ecology, sociology and psychology.
544|Bayesian Density Estimation and Inference Using Mixtures|We describe and illustrate Bayesian inference in models for density estimation using mixtures of Dirichlet processes. These models provide natural settings for density estimation, and are exemplified by special cases where data are modelled as a sample from mixtures of normal distributions. Efficient simulation methods are used to approximate various prior, posterior and predictive distributions. This allows for direct inference on a variety of practical issues, including problems of local versus global smoothing, uncertainty about density estimates, assessment of modality, and the inference on the numbers of components. Also, convergence results are established for a general class of normal mixture models. Keywords: Kernel estimation; Mixtures of Dirichlet processes; Multimodality; Normal mixtures; Posterior sampling; Smoothing parameter estimation * Michael D. Escobar is Assistant Professor, Department of Statistics and Department of Preventive Medicine and Biostatistics, University ...
545|Bayesian Model Selection in Social Research (with Discussion by Andrew Gelman &amp; Donald B. Rubin, and Robert M. Hauser, and a Rejoinder)  (1995) |It is argued that P-values and the tests based upon them give unsatisfactory results, especially in large samples. It is shown that, in regression, when there are many candidate independent variables, standard variable selection procedures can give very misleading results. Also, by selecting a single model, they ignore model uncertainty and so underestimate the uncertainty about quantities of interest. The Bayesian approach to hypothesis testing, model selection and accounting for model uncertainty is presented. Implementing this is straightforward using the simple and accurate BIC approximation, and can be done using the output from standard software. Specific results are presented for most of the types of model commonly used in sociology. It is shown that this approach overcomes the difficulties with P values and standard model selection procedures based on them. It also allows easy comparison of non-nested models, and permits the quantification of the evidence for a null hypothesis...
547|Regularized discriminant analysis|Linear and quadratic discriminant analysis are considered in the small sample high-dimensional setting. Alternatives to the usual maximum likelihood (plug-in) estimates for the covariance matrices are proposed. These alternatives are characterized by two parameters, the values of which are customized to individual situations by jointly minimizing a sample based estimate of future misclassification risk. Computationally fast implementations are presented, and the efficacy of the approach is examined through simulation studies and application to data. These studies indicate that in many circumstances dramatic gains in classification accuracy can be achieved. Submitted to Journal of the American Statistical Association
549|Discriminant Analysis by Gaussian Mixtures|Fisher-Rao linear discriminant analysis (LDA) is a valuable tool for multigroup classification. LDA is equivalent to maximum likelihood classification assuming Gaussian distributions for each class. In this paper, we fit Gaussian mixtures to each class to facilitate effective classification in non-normal settings, especially when the classes are clustered. Low dimensional views are an important by-product of LDA---our new techniques inherit this feature. We are able to control the within-class spread of the subclass centers relative to the between-class spread. Our technique for fitting these models permits a natural blend with nonparametric versions of LDA.  Keywords: Classification, Pattern Recognition, Clustering, Nonparametric, Penalized. 1 Introduction  In the generic classification or discrimination problem, the outcome of interest  G falls into J unordered classes, which for convenience we denote by the set J =  f1; 2; 3; \Delta \Delta \Delta Jg. We wish to build a rule for pred...
550|Practical Bayesian Density Estimation Using Mixtures Of Normals|this paper, we propose some solutions to these problems. Our goal is to come up with a simple, practical method for estimating the density. This is an interesting problem in its own right, as well as a first step towards solving other inference problems, such as providing more flexible distributions in hierarchical models. To see why the posterior is improper under the usual reference prior, we write the model in the following way. Let Z = (Z 1 ; : : : ; Z n ) and X = (X 1 ; : : : ; X n ). The Z
552|Detecting Features in  Spatial Point Processes with . . .|We consider the problem of detecting features in spatial point processes in the presence of substantial clutter. One example is the detection of mine elds using reconnaissance aircraft images that erroneously identify many objects that are not mines. Another is the detection of seismic faults on the basis of earthquake catalogs: earthquakes tend to be clustered close to the faults, but there are many that are farther away. Our solution uses model-based clustering based on a mixture model for the process, in which features are assumed to generate points according to highly linear multivariate normal densities, and the clutter arises according to a spatial Poisson process. Very nonlinear features are represented by several highly linear multivariate normal densities, giving a piecewise linear representation. The model is estimated in two stages. In the rst stage, hierarchical model-based clustering is used to provide a rst estimate of the features. In the second stage, this clustering is re ned using the EM algorithm. The number of features is found using an approximation to the posterior probability of each number of features. For the minefield
553|MCLUST: Software for Model-based Cluster Analysis|MCLUST is a software package for cluster analysis written in Fortran and interfaced to the S-PLUS commercial software package1. It implements parameterized Gaussian hierarchical clustering algorithms [16, 1, 7] and the EM algorithm for parameterized Gaussian mixture models [5, 13, 3, 14] with the possible addition of a Poisson noise term. MCLUST also includes functions that combine hierarchical clustering, EM and the Bayesian Information Criterion (BIC) in a comprehensive clustering strategy [4, 8]. Methods of this type have shown promise in a number of practical applications, including character recognition [16], tissue segmentation [1], mine eld and seismic fault detection [4], identi cation of textile aws from images [2], and classi cation of astronomical data [3, 15]. Aweb page with related links can be found at
554|Algorithms for model-based Gaussian hierarchical clustering|1 Funded by the O ce of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-
555|Regularized Gaussian Discriminant Analysis Through Eigenvalue Decomposition|Friedman (1989) has proposed a regularization technique (RDA) of discriminant analysis in the Gaussian framework. RDA makes use of two regularization parameters to design an intermediate classi cation rule between linear and quadratic discriminant analysis. In this paper, we propose an alternative approach to design classi cation rules which have also a median position between linear and quadratic discriminant analysis. Our approach is based on the reparametrization of the covariance matrix k of a group Gk in terms of its eigenvalue decomposition, k = kDkAkD 0 k where k speci es the volume of Gk, Ak its shape, and Dk its orientation. Variations on constraints concerning k?Ak and Dk lead to 14 discrimination models of interest. For each model, we derived the maximum likelihood parameter estimates and our approach consists in selecting the model among the 14 possible models by minimizing the sample-based estimate of future misclassi cation risk by cross-validation. Numerical experiments show favorable behavior of this approach as compared to RDA.
556|Scaling EM (Expectation-Maximization) Clustering to Large Databases  (1999) |Practical statistical clustering algorithms typically center upon an iterative refinement optimization procedure to compute a locally optimal clustering solution that maximizes the fit to data. These algorithms typically require many database scans to converge, and within each scan they require the access to every record in the data table. For large databases, the scans become prohibitively expensive. We present a scalable implementation of the Expectation-Maximization (EM) algorithm. The database community has focused on distance-based clustering schemes and methods have been developed to cluster either numerical or categorical data. Unlike distancebased algorithms (such as K-Means), EM constructs proper statistical models of the underlying data source and naturally generalizes to cluster databases containing both discrete-valued and continuous-valued data. The scalable method is based on a decomposition of the basic statistics the algorithm needs: identifying regions of the data that...
557|Averaging, Maximum Penalized Likelihood and Bayesian Estimation for Improving Gaussian Mixture Probability Density Estimates|We apply the idea of averaging ensembles of estimators to probability density estimation.
558|Non Parametric Maximum Likelihood Estimation of Features in . . .|This paper addresses the problem of estimating the support domain of a bounded point process in presence of background noise. This situation occurs for example in the detection of a mine field from aerial observations. A Maximum Likelihood Estimator for a mixture of uniform point processes is derived using a natural partition of the space defined by the data themselves: the Voronoï tessellation. The methodology is tested on simulations and compared to a model-based clustering technique.
559|Inference in model-based cluster analysis|A new approach to cluster analysis has been introduced based on parsimonious geometric modelling of the within-group covariance matrices in a mixture of multivariate normal distributions, using hierarchical agglomeration and iterative relocation. It works well and is widely used via the MCLUST software available in S-PLUS and StatLib. However, it has several limitations: there is no assessment of the uncertainty about the classification, the partition can be suboptimal, parameter estimates are biased, the shape matrix has to be specified by the user, prior group probabilities are assumed to be equal, the method for choosing the number of groups is based on a crude approximation, and no formal way of choosing between the various possible models is included. Here, we propose a new approach which overcomes all these difficulties. It consists of exact Bayesian inference via Gibbs sampling, and the calculation of Bayes factors (for choosing the model and the number of groups) from the output using the Laplace-Metropolis estimator. It works well in several real and simulated examples.  
560|Principal curve clustering with noise|was supported by ONR grants N00014-96-1-0192 and N00014-96-1-0330. The authors are Clustering on principal curves combines parametric modeling of noise with nonparametric modeling of feature shape. This is useful for detecting curvilinear features in spatial point patterns, with or without background noise. Applications of this include the detection of curvilinear mine elds from reconnaissance images, some of the points in which represent false detections, and the detection of seismic faults from earthquake catalogs. Our algorithm for principal curve clustering is in two steps: the rst is hierarchical and agglomerative (HPCC), and the second consists of iterative relocation based on the Classi cation EM algorithm (CEM-PCC). HPCC is used to combine potential feature clusters, while CEM-PCC re nes the results and deals with background noise. It is importanttohave a good starting point for the algorithm: this can be found manually or automatically using, for example, nearest neighbor clutter removal or model-based clustering. We choose the number of features and the amount of smoothing simultaneously using approximate Bayes
561|Fitting mixtures of Kent distributions to aid in joint set identification|When examining a rock mass, joint sets and their orientations can play a significant role with regard to how the rock mass will behave. To identify joint sets present in the rock mass, the orientation of individual fracture planes can be measured on exposed rock faces and the resulting data can be examined for heterogeneity. In this article, the expectation–maximization algorithm is used to fit mixtures of Kent component distributions to the fracture data to aid in the identification of joint sets. An additional uniform component is also included in the model to accommodate the noise present in the data.
562|From Massive Data Sets to Science Catalogs: Applications and Challenges|With hardware advances in scientific instruments and data gathering techniques comes the  inevitable flood of data that can render traditional approaches to science data analysis severely  inadequate. The traditional approach of manual and exhaustive analysis of a data set is no longer  feasible for many tasks ranging from remote sensing, astronomy, and atmospherics to medicine,  molecular biology, and biochemistry. In this paper we present our views as practitioners engaged  in building computational systems to help scientists deal with large data sets. We focus on  what we view as challenges and shortcomings of the current state-of-the-art in data analysis in  view of the massive data sets that are still awaiting analysis. The presentation is grounded in  applications in astronomy, planetary sciences, solar physics, and atmospherics that are currently  driving much of our work at JPL.  keywords: science data analysis, limitations of current methods, challenges for massive data sets, ...
563|Face Recognition Based on Fitting a 3D Morphable Model|Abstract—This paper presents a method for face recognition across variations in pose, ranging from frontal to profile views, and across a wide range of illuminations, including cast shadows and specular reflections. To account for these variations, the algorithm simulates the process of image formation in 3D space, using computer graphics, and it estimates 3D shape and texture of faces from single images. The estimate is achieved by fitting a statistical, morphable model of 3D faces to images. The model is learned from a set of textured 3D scans of heads. We describe the construction of the morphable model, an algorithm to fit the model to images, and a framework for face identification. In this framework, faces are represented by model parameters for 3D shape and texture. We present results obtained with 4,488 images from the publicly available CMU-PIE database and 1,940 images from the FERET database. Index Terms—Face recognition, shape estimation, deformable model, 3D faces, pose invariance, illumination invariance. æ 1
564|An iterative image registration technique with an application to stereo vision|Image registration finds a variety of applications in computer vision. Unfortunately, traditional image registration techniques tend to be costly. We present a new image registration technique that makes use of the spatial intensity gradient of the images to find a good match using a type of Newton-Raphson iteration. Our technique is faster because it examines far fewer potential matches between the images than existing techniques. Furthermore, this registration technique can be generalized to handle rotation, scaling and shearing. We show show our technique can be adapted for use in a stereo vision system. 2. The registration problem The translational image registration problem can be characterized as follows: We are given functions F(x) and G(x) which give the respective pixel values at each location x in two images, where x is a vector. We wish to find the disparity vector h which minimizes some measure of the difference between F(x + h) and G(x), for x in some region of interest R. (See figure 1). 1.
565|Determining Optical Flow|Optical flow cannot be computed locally, since only one independent measurement is available from the image sequence at a point, while the flow velocity has two components. A second constraint is needed. A method for finding the optical flow pattern is presented which assumes that the apparent velocity of the brightness pattern varies smoothly almost everywhere in the image. An iterative implementation is shown which successfully computes the optical flow for a number of synthetic image sequences. The algorithm is robust in that it can handle image sequences that are quantized rather coarsely in space and time. It is also insensitive to quantization of brightness levels and additive noise. Examples are included where the assumption of smoothness is violated at singular points or along lines in the image. 
566|Active Appearance Models|AbstractÐWe describe a new method of matching statistical models of appearance to images. A set of model parameters control modes of shape and gray-level variation learned from a training set. We construct an efficient iterative matching algorithm by learning the relationship between perturbations in the model parameters and the induced image errors. Index TermsÐAppearance models, deformable templates, model matching. 1
568|From Few to many: Illumination cone models for face recognition under variable lighting and pose|We present a generative appearance-based method for recognizing human faces under variation in lighting and viewpoint. Our method exploits the fact that the set of images of an object in fixed pose, but under all possible illumination conditions, is a convex cone in the space of images. Using a small number of training images of each face taken with different lighting directions, the shape and albedo of the face can be reconstructed. In turn, this reconstruction serves as a generative model that can be used to render—or synthesize—images of the face under novel poses and illumination conditions. The pose space is then sampled, and for each pose the corresponding illumination cone is approximated by a low-dimensional linear subspace whose basis vectors are estimated using the generative model. Our recognition algorithm assigns to a test image the identity of the closest approximated illumination cone (based on Euclidean distance within the image space). We test our face recognition method on 4050 images from the Yale Face Database B; these images contain 405 viewing conditions (9 poses ¢ 45 illumination conditions) for 10 individuals. The method performs almost without error, except on the most extreme lighting directions, and significantly outperforms popular recognition methods that do not use a generative model.
569|Fitting Parameterized Three-Dimensional Models to Images|Model-based recognition and motion tracking depends upon the ability to solve  for projection and model parameters that will best fit a 3-D model to matching  2-D image features. This paper extends current methods of parameter solving to  handle objects with arbitrary curved surfaces and with any number of internal parameters  representing articulations, variable dimensions, or surface deformations. Numerical
570|Linear Object Classes and Image Synthesis From a Single Example Image|Abstract—The need to generate new views of a 3D object from a single real image arises in several fields, including graphics and object recognition. While the traditional approach relies on the use of 3D models, we have recently introduced [1], [2], [3] simpler techniques that are applicable under restricted conditions. The approach exploits image transformations that are specific to the relevant object class, and learnable from example views of other “prototypical ” objects of the same class. In this paper, we introduce such a technique by extending the notion of linear class proposed by Poggio and Vetter. For linear object classes, it is shown that linear transformations can be learned exactly from a basis set of 2D prototypical views. We demonstrate the approach on artificial objects and then show preliminary evidence that the technique can effectively “rotate ” highresolution face images from a single 2D view. Index Terms—3D object recognition, rotation invariance, deformable templates, image synthesis. 1
571|Face identification across different poses and illumination with a 3D morphable model|We present a novel approach for recognizing faces in im-ages taken from different directions and under different il-lumination. The method is based on a 3D morphable face model that encodes shape and texture in terms of model pa-rameters, and an algorithm that recovers these parameters from a single image of a face. For face identification, we use the shape and texture parameters of the model that are separated from imaging parameters, such as pose and illu-mination. In addition to the identity, the system provides a measure of confidence. We report experimental results for more than 4000 images from the publicly available CMU-PIE database. 1
572|Automatic face identification system using flexible appearance models|We describe the use of flexible models for representing the shape and grey-level appearance of human faces. These models are controlled by a small number of parameters which can be used to code the overall appearance of a face for image compression and classification purposes. The model parameters control both inter-class and within-class variation. Discriminant analysis techniques are employed to enhance the effect of those parameters affecting inter-class variation, which are useful for classification. We have performed experiments on face coding and reconstruction and automatic face identification. Good recognition rates are obtained even when significant variation in lighting, expression and 3D viewpoint, is allowed. 
573|Multidimensional morphable models: A framework for representing and matching object classes|This thesis describes a flexible model for representing images of objects of a certain class, such as faces, and introduces a new algorithm for matching the model to novel images from the class. The model and matching algorithm are very general and can be used for many image analysis tasks. The flexible model, called a multidimensional morphable model, is learned from example images (called prototypes) of objects of a class. In the learning phase, pixelwise correspon-dences between a reference prototype and each of the other prototypes are first computed and then used to obtain shape and texture vectors associated with each prototype. The mor-phable model is then synthesized as a linear combination that spans the linear vector space determined by the prototypical shapes and textures. We next introduce an effective stochastic gradient descent algorithm that automatically matches a model to a novel image by finding the parameters that minimize the error between the image generated by the model and the novel image. Several experiments demonstrate the robustness and the broad range of applicability of the matching algorithm and the underlying
574|SFS Based View Synthesis for Robust Face Recognition|Sensitivity to variations in pose is a challenging problem in face recognition using appearance-based methods. More specifically, the appearance of a face changes dramatically when viewing and/or lighting directions change. Various approaches have been proposed to solve this difficult problem. They can be broadly divided into three classes: 1) multiple image based methods where multiple images of various poses per person are available, 2) hybrid methods where multiple example images are available during learning but only one database image per person is available during recognition, and 3) single image based methods where no example based learning is carried out. In this paper, we present a method that comes under class 3. This method based on shape-from-shading (SFS) improves the performance of a face recognition system in handling variations due to pose and illumination via image synthesis.  
575|Eigen Light-Fields and Face Recognition Across Pose|In many face recognition tasks the pose of the probe and gallery images are different. In other cases multiple gallery or probe images may be available, each captured from a different pose. We propose a face recognition algorithm which can use any number of gallery images per subject captured at arbitrary poses, and any number of probe images, again captured at arbitrary poses. The algorithm operates by estimating the eigen light-field of the subject&#039;s head from the input gallery or probe images. Matching between the probe and gallery is then performed using the eigen light-fields. We present results on the CMU PIE and the FERET face databases.
576|Face Recognition from Unfamiliar Views: Subspace Methods and Pose Dependency|A framework for recognising human faces from unfamiliar views is described and a simple implementation of this framework evaluated. The interaction between training view and testing view is shown to compare with observations in human face recognition experiments. The ability of the system to learn from several training views, as available in video footage, is shown to improve the overall performance of the system as is the use of multiple testing images. 1 Introduction Recognising faces from previously unseen viewpoints is inherently more difficult than matching faces at the same view. Simple image comparisons such as correlation demonstrate that there is a greater difference between different viewpoints of the same subject than between different subjects at the same view which means that the recognition method used must take into account the non-linear variations of faces with viewpoint. In order to achieve recognition of previously unseen views, we require a method of relating the...
577|Distributed Computing in Practice: The Condor Experience|Since 1984, the Condor project has enabled ordinary users to do extraordinary computing. Today, the project continues to explore the social and technical problems of cooperative computing on scales ranging from the desktop to the world-wide computational grid. In this chapter, we provide the history and philosophy of the Condor project and describe how it has interacted with other projects and evolved along with the field of distributed computing. We outline the core components of the Condor system and describe how the technology of computing must correspond to social structures. Throughout, we reflect on the lessons of experience and chart the course traveled by research ideas as they grow into production systems.
578|The Byzantine Generals Problem|Reliable computer systems must handle malfunctioning components that give conflicting information to different parts of the system. This situation can be expressed abstractly in terms of a group of generals of the Byzantine army camped with their troops around an enemy city. Communicating only by messenger, the generals must agree upon a common battle plan. However, one of more of them may be traitors who will try to confuse the others. The problem is to find an algorithm to ensure that the loyal generals will reach agreement. It is shown that, using only oral messages, this problem is solvable if and only if more than two-thirds of the generals are loyal; so a single traitor can confound two loyal generals. With unforgeable written messages, the problem is solvable for any number of generals and possible traitors. Applications of the solutions to reliable computer systems are then discussed.
579|Distributed Snapshots: Determining Global States of Distributed Systems|This paper presents an algorithm by which a process in a distributed system determines a global state of the system during a computation. Many problems in distributed systems can be cast in terms of the problem of detecting global states. For instance, the global state detection algorithm helps to solve an important class of problems: stable property detection. A stable property is one that persists: once a stable property becomes true it remains true thereafter. Examples of stable properties are “computation has terminated,”  “the system is deadlocked” and “all tokens in a token ring have disappeared.” The stable property detection problem is that of devising algorithms to detect a given stable property. Global state detection can also be used for checkpointing. 
580|The Protection of Information in Computer Systems|This tutorial paper explores the mechanics of protecting computer-stored information from unauthorized use or modification. It concentrates on those architectural structures--whether hardware or software--that are necessary to support information protection. The paper develops in three main sections. Section I describes desired functions, design principles, and examples of elementary protection and authentication mechanisms. Any reader familiar with computers should find the first section to be reasonably accessible. Section II requires some familiarity with descriptor-based computer architecture. It examines in depth the principles of modern protection architectures and the relation between capability systems and access control list systems, and ends with a brief analysis of protected subsystems and protected objects. The reader who is dismayed by either the prerequisites or the level of detail in the second section may wish to skip to Section III, which reviews the state of the art and current research projects and provides suggestions for further reading. Glossary The following glossary provides, for reference, brief definitions for several terms as used in this paper in the context of protecting information in computers. Access The ability to make use of information stored in a computer system. Used frequently as a verb, to the horror of grammarians. Access control list A list of principals that are authorized to have access to some object. Authenticate To verify the identity of a person (or other agent external to the protection system) making a request.
581|Kerberos: An Authentication Service for Open Network Systems|In an open network computing environment, a workstation cannot be trusted to identify its users correctly to network services.  Kerberos provides an alternative approach whereby a trusted third-party authentication service is used to verify users’ identities. This paper gives an overview of the Kerberos authentication model as implemented for MIT’s Project Athena. It describes the protocols used by clients, servers, and Kerberos to achieve authentication. It also describes the management and replication of the database required.   The views of Kerberos as seen by the user, programmer, and administrator are described.  Finally, the role of Kerberos in the larger Athena picture is given, along with a list of applications that presently use Kerberos for user authentication.  We describe the addition of Kerberos authentication to the Sun Network File System as a case study for integrating Kerberos with an existing application.
582|A Security Architecture for Computational Grids|State-of-the-art and emerging scientific applications require fast access to large quantities of data and commensurately fast computational resources. Both resources and data are often distributed in a wide-area network with components administered locally and independently. Computations may involve hundreds of processes that must be able to acquire resources dynamically and communicate e#ciently. This paper analyzes the unique security requirements of large-scale distributed (grid) computing and develops a security policy and a corresponding security architecture. An implementation of the architecture within the Globus metacomputing toolkit is discussed.  
583|A Resource Management Architecture for Metacomputing Systems|Metacomputing systems are intended to support remote and/or  concurrent use of geographically distributed computational resources.  Resource management in such systems is complicated by five concerns  that do not typically arise in other situations: site autonomy and heterogeneous  substrates at the resources, and application requirements for policy  extensibility, co-allocation, and online control. We describe a resource  management architecture that addresses these concerns. This architecture  distributes the resource management problem among distinct local  manager, resource broker, and resource co-allocator components and defines  an extensible resource specification language to exchange information  about requirements. We describe how these techniques have been  implemented in the context of the Globus metacomputing toolkit and  used to implement a variety of different resource management strategies.  We report on our experiences applying our techniques in a large testbed,  GUSTO, incorporating 15 sites, 330 computers, and 3600 processors.  
584|Matchmaking: Distributed Resource Management for High Throughput Computing|Conventional resource management systems use a system model to describe resources and a centralized scheduler to control their allocation. We argue that this paradigm does not adapt well to distributed systems, particularly those built to support high-throughput computing. Obstacles include heterogeneity of resources, which make uniform allocation algorithms difficult to formulate, and distributed ownership, leading to widely varying allocation policies. Faced with these problems, we developed and implemented the classified advertisement (classad) matchmaking framework, a flexible and general approach to resource management in distributed environment with decentralized ownership of resources. Novel aspects of the framework include a semi-structured data model that combines schema, data, and query in a simple but powerful specification language, and a clean separation of the matching and claiming phases of resource allocation. The representation and protocols result in a robust, scalabl...
585|BEOWULF: A Parallel Workstation For Scientific Computation|Network-of-Workstations technology is applied to the challenge of implementing very high performance workstations for Earth and space science applications. The Beowulf parallel workstation employs 16 PCbased processing modules integrated with multiple Ethernet networks. Large disk capacity and high disk to memory bandwidth is achieved through the use of a hard disk and controller for each processing module supporting up to 16 way concurrent accesses. The paper presents results from a series of experiments that measure the scaling characteristics of Beowulf in terms of communication bandwidth, file transfer rates, and processing performance. The evaluation includes a computational fluid dynamics code and an N-body gravitational simulation program. It is shown that the Beowulf architecture provides a new operating point in performance to cost for high performance workstations, especially for file transfers under favorable conditions.  1 INTRODUCTION  Networks Of Workstations, or NOW [?] ...
586|Dealing with disaster: Surviving misbehaved kernel extensions|Today’s extensible operating systems allow applications to modify kernel behavior by providing mechanisms for application code to run in the kernel address space. The advantage of this approach is that it provides improved application flexibility and performance; the disadvantage is that buggy or malicious code can jeopardize the integrity of the kernel. It has been demonstrated that it is feasible to use safe languages, software fault isolation, or virtual memory protection to safeguard the main kernel. However, such protection mechanisms do not address the full range of problems, such as resource hoarding, that can arise when application code is introduced into the kernel. In this paper, we present an analysis of extension mechanisms in the VINO kernel. VINO uses software fault isolation as its safety mechanism and a lightweight transaction system to cope with resource-hoarding. We explain how these two mechanisms are sufficient to protect against a large class of errant or malicious extensions, and we quantify the overhead that this protection introduces. We find that while the overhead of these techniques is high relative to the cost of the extensions themselves, it is low relative to the benefits that extensibility brings.
587|The locus distributed operating system|LOCUS Is a distributed operating system which supports transparent access to data through a network wide fllesystem, permits automatic replication of storaget supports transparent distributed process execution, supplies a number of high reliability functions such as nested transactions, and is upward compatible with Unix. Partitioned operation of subnetl and their dynamic merge is also supported. The system has been operational for about two years at UCLA and extensive experience In its use has been obtained. The complete system architecture is outlined in this paper, and that experience is summarized. 1
588|Core algorithms of the Maui scheduler|Abstract. The Maui scheduler has received wide acceptance in the HPC community as a highly configurable and effective batch scheduler. It is currently in use on hundreds of SP, O2K, and Linux cluster systems throughout the world including a high percentage of the largest and most cutting edge research sites. While the algorithms used within Maui have proven themselves effective, nothing has been published to date documenting these algorithms nor the configurable aspects they support. This paper focuses on three areas of Maui scheduling, specifically, backfill, job prioritization, and fairshare. It briefly discusses the goals of each component, the issues and corresponding design decisions, and the algorithms enabling the Maui policies. It also covers the configurable aspects of each algorithm and the impact of various parameter selections. 1
589|A worldwide flock of Condors: load sharing among workstation clusters|Condor is a distributed batch system for sharing the workload of compute-intensive
590|Stork: Making Data Placement a First Class Citizen in the Grid|Todays scientific applications have huge data requirements which continue to increase drastically every year. These data are generally accessed by many users from all across the the globe. This implies a major necessity to move huge amounts of data around wide area networks to complete the computation cycle, which brings with it the problem of efficient and reliable data placement. The current approach to solve this problem of data placement is either doing it manually, or employing simple scripts which do not have any automation or fault tolerance capabilities. Our goal is to make data placement activities first class citizens in the Grid just like the computational jobs. They will be queued, scheduled, monitored, managed, and even check-pointed. More importantly, it will be made sure that they complete successfully and without any human interaction. We also believe that data placement jobs should be treated differently from computational jobs, since they may have different semantics and different characteristics. For this purpose, we have developed Stork, a scheduler for data placement activities in the Grid.
591|Condor Technical Summary|Condor is a software package for executing long running &#034;batch&#034; type jobs on workstations which would otherwise be idle. Major features of Condor are automatic location and allocation of idle machines, and checkpointing and migration of processes. All of these features are achieved without any modifications to the UNIX kernel whatsoever. Also, users of Condor do not need to change their source programs to run with Condor, although such programs must be specially linked. The features of Condor for both users and workstation owners along with the limitations on the kinds of jobs which may be executed by Condor are described. The mechanisms behind our implementations of checkpointing and process migration are discussed in detail. Finally, the software which detects idle machines and allocates those machines to Condor users is described along with the techniques used to configure that software to meet the demands of a particular computing site or workstation owner.  1. Introduction to the ...
592|Solving Large Quadratic Assignment Problems on Computational Grids|The quadratic assignment problem (QAP) is among the hardest combinatorial optimization problems. Some instances of size n = 30 have remained unsolved for decades. The solution of these problems requires both improvements in mathematical programming algorithms and the utilization of powerful computational platforms. In this article we describe a novel approach to solve QAPs using a state-of-the-art branch-and-bound algorithm running on a federation of geographically distributed resources known as a computational grid. Solution of QAPs of unprecedented complexity, including the nug30, kra30b, and tho30 instances, is reported.
593|Resource Management through Multilateral Matchmaking|Federated distributed systems present new challenges to resource management, which cannot be met by conventional systems that employ relatively static resource models and centralized allocators. We previously argued that Matchmaking provides an elegant and robust resource management solution for these highly dynamic environments [5]. Although powerful and flexible, multiparty policies (e.g., co-allocation) cannot be accomodated by Matchmaking. In this paper we present Gang-Matching, a multilateral matchmaking formalism to address this deficiency.
594|Interfacing Condor and PVM to harness the cycles of workstation clusters|A continuing challenge to the scientific research and engineering communities is how to fully utilize computational hardware. In particular, the proliferation of clusters of high performance workstations has become an increasingly attractive source of compute power. Developments to take advantage of this environment have previously focused primarily on managing the resources, or on providing interfaces so that a number of machines can be used in parallel to solve large problems. Both approaches are desirable, and indeed should be complementary. Unfortunately, the resource management and parallel processing systems are usually developed by independent groups, and they usually do not interact well together. To bridge this gap, we have developed a framework for interfacing these two sorts of systems. Using this framework, we have interfaced PVM, a popular system for parallel programming with Condor, a powerful resource management system. This combined system is operational, and we have ma...
595|Parrot: Transparent User-Level Middleware for Data Intensive Computing|Distributed computing continues to be an alphabet-soup of services and protocols for managing computation and storage. To live in this environment, applications require middleware that can transparently adapt standard interfaces to new distributed systems; such software is known as an interposition agent. In this paper, we present several lessons learned about interposition agents via a progressive study of design possibilities. Although performance is an important concern, we pay special attention to less tangible issues such as portability, reliability, and compatibility. We begin with a comparison of seven methods of interposition, focusing on one method, the debugger trap, that requires special techniques to achieve acceptable performance on popular operating systems. Using this method, we implement a complete interposition agent, Parrot, that splices existing remote I/O systems into the namespace of standard applications. The primary design problem of Parrot is the mapping of fixed application semantics into the semantics of the available I/O systems. We offer a detailed discussion of how errors and other unexpected conditions must be carefully managed in order to keep this mapping intact. We conclude with a evaluation of the performance of the I/O protocols employed by Parrot, and use an Andrew-like benchmark to demonstrate that semantic differences have consequences in performance. 1. 
596|Protocols and services for distributed data-intensive science|Abstract. We describe work being performed in the Globus project to develop enabling protocols and services for distributed data-intensive science. These services include: * High-performance, secure data transfer protocols based on FTP, plus a range of libraries and tools that use these protocols * Replica catalog services supporting the creation and location of file replicas in distributed systems These components leverage the substantial body of &#034;Grid &#034; services and protocols developed within the Globus project and by its collaborators, and are being used in a number of data-intensive application projects.
597|Matchmaking Frameworks for Distributed Resource Management|Federated distributed systems present new challenges to resource management. Conventional resource managers are based on a relatively static resource model and a centralized allocator that assigns resources to customers. Distributed envi-ronments, particularly those built to support high-throughput computing (HTC), are often characterized by distributed management and distributed ownership. Distributed management introduces resource heterogeneity: Not only the set of available resources, but even the set of resource types is constantly changing. Distributed ownership introduces policy heterogeneity: Each resource may have its own idiosyncratic allocation policy. We propose a resource management framework based on a matchmaking paradigm to address these shortcomings. Matchmaking services enable discov-ery and exchange of goods and services in marketplaces. Agents that provide or require services advertise their presence by publishing constraints and pref-erences on the entities they would like to be matched with, as well as their own
598|Cheap cycles from the desktop to the dedicated cluster: combining opportunistic and dedicated scheduling with Condor|Clusters of commodity PC hardware running Linux are becoming widely used as computational resources. Most software for controlling clusters relies on dedicated scheduling algorithms. These algorithms assume the constant availability of resources to compute fixed schedules. Unfortunately, due to hardware and software failures, dedicated resources are not always available over the long-term. Moreover, these dedicated scheduling solutions are only applicable to certain classes of jobs, and they can only manage clusters or large SMP machines. The Condor High Throughput Computing System overcomes these limitations by combining aspects of dedicated and opportunistic scheduling into a single system. Both parallel and serial jobs are managed at the same time, allowing a simpler interface for the user and better resource utilization. This paper describes the Condor system, defines opportunistic scheduling, explains how Condor supports MPI jobs with a combination of dedicated and opportunistic scheduling, and shows the advantages gained by such an approach. An exploration of future work in these areas concludes the paper. By using both desktop workstations and dedicated clusters, Condor harnesses all available computational power to enable the best possible science at a low cost.  1 
599|Providing Resource Management Services to Parallel Applications|Because resource management (RM) services are vital to the performance of parallel applications, it is essential that parallel programming environments (PPEs) and RM systems work together. We believe that no single RM system is always the best choice for every application and every computing environment. Therefore, the interface between the PPE and the resource manager must be flexible enough to allow for customization and extension based on the environment. We present a framework for interfacing general PPEs and RM systems. This framework is based on clearly defining the responsibilities of these two components of the system. This framework has been applied to PVM, and two separate instances of RM systems have been implemented. One behaves exactly as PVM always has, while the second uses Condor to extend the set of RM services available to PVM applications. 1 Introduction To fulfill the promises of high performance computing, parallel applications must be provided with effective reso...
600|Gathering at the well: Creating communities for grid I/O|Grid applications have demanding I/O needs. Schedulers must bring jobs and data in close proximity in order to satisfy throughput, scalability, and policy requirements. Most systems accomplish this by making either jobs or data mobile. We propose a system that allows jobs and data to meet by binding execution and storage sites together into I/O communities which then participate in the wide-area system. The relationships between participants in a community may be expressed by the ClassAd framework. Extensions to the framework allow community members to express indirect relations. We demonstrate our implementation of I/O communities by improving the performance of a key high-energy physics simulation on an international distributed system. 1.
601|Bypass: A Tool for Building Split Execution Systems|Split execution is a common model for providing a friendly environment on a foreign machine. In this model, a remotely executing process sends some or all of its system calls back to a home environment for execution. Unfortunately, hand-coding split execution systems for experimentation and research is difficult and error-prone. We have built a tool, Bypass, for quickly producing portable and correct split execution systems for unmodified legacy applications. We demonstrate Bypass by using it to transparently connect a POSIX application to a simple data staging system based on the Globus toolkit. 1. Introduction The split execution model allows a process running on a foreign machine to behave as if it were running on its home machine. Split execution generally involves three software components: an application, an agent, and a shadow. Figure 1 shows these components. Kernel Agent Application Local System Calls Calls System Trapped Kernel Shadow Local System Calls Other...
602|Utilizing Widely Distributed Computational Resources Efficiently with Execution Domains|Wide-area computational grids have the potential to provide large amounts of computing capacity to the scientific community. Realizing this potential requires intelligent data management, enabling applications to harness remote computing resources with minimal remote data access overhead. We define execution domains, a framework which defines an affinity between CPU and data resources in the grid, so applications are scheduled to run on CPUs which have the needed access to datasets and storage devices. The framework also includes domain managers, agents which dynamically adjust the execution domain configuration to support the efficient execution of grid applications. In this paper, we present the execution domain framework and show how we apply it in the Condor resource management system.
603|Error Scope on a Computational Grid: Theory and Practice|Error propagation is a central problem in grid computing. We re-learned this while adding a Java feature to the Condor computational grid. Our initial experience with the system was negative, due to the large number of new ways in which the system could fail. To reason about this problem, we developed a theory of error propagation. Central to our theory is the concept of an error&#039;s scope, defined as the portion of a system that it invalidates. With this theory in hand, we recognized that the expanded system did not properly consider the scope of errors it discovered. We modified the system according to our theory, and succeeded in making it a more robust platform for distributed computing.
604|The DBC: Processing Scientific Data Over the Internet|We present the Distributed Batch Controller (DBC), a system built to support batch processing of large scientific datasets. The DBC implements a federation of autonomous workstation pools, which may be widely-distributed. Individual batch jobs are executed using idle workstations in these pools. Input data are staged to the pool before processing begins. We describe the architecture and implementation of the DBC, and present the results of experiments in which it is used to perform image compression. 1 Introduction  In this paper we present the DBC (Distributed Batch Controller), a system that processes data using widely-distributed computational resources. The DBC was built as a tool for enriching scientific data stored in two mass storage systems at NASA&#039;s Goddard Space Flight Center (GSFC). Enriching data means processing it to make it more useful. For example, satellite images may be classified according to some domain-specific criteria. These classifications can then be stored as ...
605|Toward the next generation of recommender systems: A survey of the state-of-the-art and possible extensions|This paper presents an overview of the field of recommender systems and describes the current generation of recommendation methods that are usually classified into the following three main categories: content-based, collaborative, and hybrid recommendation approaches. This paper also describes various limitations of current recommendation methods and discusses possible extensions that can improve recommendation capabilities and make recommender systems applicable to an even broader range of applications. These extensions include, among others, an improvement of understanding of users and items, incorporation of the contextual information into the recommendation process, support for multcriteria ratings, and a provision of more flexible and less intrusive types of recommendations. 
608|Grouplens: Applying collaborative filtering to usenet news|... a collaborative filtering system for Usenet news—a high-volume, high-turnover discussion list service on the Internet. Usenet newsgroups—the individual discussion lists—may carry hundreds of messages each day. While in theory the newsgroup organization allows readers to select the content that most interests them, in practice most newsgroups carry a wide enough spread of messages to make most individuals consider Usenet news to be a high noise information resource. Furthermore, each user values a different set of messages. Both taste and prior knowledge are major factors in evaluating news articles. For example, readers of the rec.humor newsgroup, a group designed for jokes and other humorous postings, value articles based on whether they perceive them to be funny. Readers of technical groups, such as comp.lang.c? ? value articles based
609|Probabilistic Latent Semantic Analysis|Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two--mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments.
610|Active Learning with Statistical Models|For manytypes of learners one can compute the statistically &#034;optimal&#034; way to select data. We review how  these techniques have been used with feedforward neural networks [MacKay, 1992# Cohn, 1994]. We then  showhow the same principles may be used to select data for two alternative, statistically-based learning  architectures: mixtures of Gaussians and locally weighted regression. While the techniques for neural  networks are expensive and approximate, the techniques for mixtures of Gaussians and locally weighted  regression are both efficient and accurate.
611|NewsWeeder: Learning to Filter Netnews|A significant problem in many information filtering systems is the dependence on the user for the creation and maintenance of a user profile, which describes the user&#039;s interests. NewsWeeder is a netnews-filtering system that addresses this problem by letting the user rate his or her interest level for each article being read (1-5), and then learning a user profile based on these ratings. This paper describes how NewsWeeder accomplishes this task, and examines the alternative learning methods used. The results show that a learning algorithm based on the Minimum Description Length (MDL) principle was able to raise the percentage of interesting articles to be shown to users from 14% to 52% on average. Further, this performance significantly outperformed (by 21%) one of the most successful techniques in Information Retrieval (IR), termfrequency /inverse-document-frequency (tf-idf) weighting. 1
612|Improving generalization with active learning|Abstract. Active learning differs from &amp;quot;learning from examples &amp;quot; in that the learning algorithm assumes at least some control over what part of the input domain it receives information about. In some situations, active learning is provably more powerful than learning from examples alone, giving better generalization for a fixed number of training examples. In this article, we consider the problem of learning a binary concept in the absence of noise. We describe a formalism for active concept learning called selective sampling and show how it may be approximately implemented by a neural network. In selective sampling, a learner receives distribution information from the environment and queries an oracle on parts of the domain it considers &amp;quot;useful. &amp;quot; We test our implementation, called an SGnetwork, on three domains and observe significant improvement in generalization.
613|Selective sampling using the Query by Committee algorithm|We analyze the &#034;query by committee&#034; algorithm, a method for filtering informative queries from a random stream of inputs. We show that if the two-member committee algorithm achieves information gain with positive lower bound, then the prediction error decreases exponentially with the number of queries. We show that, in particular, this exponential decrease holds for query learning of perceptrons.
614|Learning and Revising User Profiles: The Identification of Interesting Web Sites|. We discuss algorithms for learning and revising user profiles that can determine which World Wide Web sites on a given topic would be interesting to a user. We describe the use of a naive Bayesian classifier for this task, and demonstrate that it can incrementally learn profiles from user feedback on the interestingness of Web sites. Furthermore, the Bayesian classifier may easily be extended to revise user provided profiles. In an experimental evaluation we compare the Bayesian classifier to computationally more intensive alternatives, and show that it performs at least as well as these approaches throughout a range of different domains. In addition, we empirically analyze the effects of providing the classifier with background knowledge in form of user defined profiles and examine the use of lexical knowledge for feature selection. We find that both approaches can substantially increase the prediction accuracy.  Keywords: Information filtering, intelligent agents, multistrategy lea...
615|Eigentaste: A Constant Time Collaborative Filtering Algorithm|Eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real-valued user ratings on a common set of items and applies principal component analysis (PCA) to the resulting dense subset of the ratings matrix. PCA facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations. For a database of n users, standard nearest-neighbor techniques require O(n) processing time to compute recommendations, whereas Eigentaste requires O(1) (constant) time. We compare Eigentaste to alternative algorithms using data from Jester, an online joke recommending system. Jester has collected approximately 2,500,000 ratings from 57,000 users. We use the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms. In the Appendix we use Uniform and Normal distribution models to derive analytic estimates of NMAE when predictions are random. On the Jester dataset, Eigentaste computes recommendations two ...
616|Learning Collaborative Information Filters|Predicting items a user would like on the basis of other users’ ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algo-rithms proposed thus far do not draw on results from the ma-chine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another&#039;s preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly out-performs current collaborative filtering algorithms.
617|Recommendation as Classification: Using Social and Content-Based Information in Recommendation|Recommendation systems make suggestions about artifacts to a user. For instance, they may predict whether a user would be interested in seeing a particular movie. Social recomendation methods collect ratings of artifacts from many individuals and use nearest-neighbor techniques to make recommendations to a user concerning new artifacts. However, these methods do not use the significant amount of other information that is often available about the nature of each artifact --- such as cast lists or movie reviews, for example. This paper presents an inductive learning approach to recommendation that is able to use both ratings information and other forms of information about each artifact in predicting user preferences. We show that our method outperforms an existing social-filtering method in the domain of movie recommendations on a dataset of more than 45,000 movie ratings collected from a community of over 250 users.  Introduction  Recommendations are a part of everyday life. We usually...
618|Latent Semantic Models for Collaborative filtering |Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, that is, a database of available user preferences. In this article, we describe a new family of model-based algorithms designed for this task. These algorithms rely on a statistical modelling technique that introduces latent class variables in a mixture model setting to discover user communities and prototypical interest profiles. We investigate several variations to deal with discrete and continuous response variables as well as with different objective functions. The main advantages of this technique over standard memory-based methods are higher accuracy, constant time prediction, and an explicit and compact model representation. The latter can also be used to mine for user communitites. The experimental evaluation shows that substantial improvements in accucracy over existing methods and published results can be obtained.
619|Content-Boosted Collaborative Filtering for Improved Recommendations|Most recommender systems use Collaborative Filtering or Content-based methods to predict new items of interest for a user. While both methods have their own advantages, individually they fail to provide good recommendations in many situations. Incorporating components from both methods, a hybrid recommender system can overcome these shortcomings.
620|Content-Based Book Recommending Using Learning for Text Categorization|Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user&#039;s likes and dislikes. Most existing recommender systems use collaborative filtering methods that base recommendations on other users&#039; preferences. By contrast, content-based methods use information about an item itself to make suggestions. This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization. Initial experimental results demonstrate that this approach can produce accurate recommendations.
621|Multicriteria Optimization|n Using some real-world examples I illustrate the important role of multiobjective optimization in decision making and its interface with preference handling. I explain what optimization in the presence of multiple objectives means and discuss some of the most common methods of solving multiobjective optimization problems using transformations to single-objective optimization problems. Finally, I address linear and combinatorial optimization problems with multiple objectives and summarize techniques for solving them. Throughout the article I
622|Heterogeneous uncertainty sampling for supervised learning|Uncertainty sampling methods iteratively request class labels for training instances whose classes are uncertain despite the previous labeled instances. These methods can greatly reduce the number of instances that an expert need label. One problem with this approach is that the classifier best suited for an application may be too expensive to train or use during the selection of instances. We test the use of one classifier (a highly efficient probabilistic one) to select examples for training another (the C4.5 rule induction program). Despite being chosen by this heterogeneous approach, the uncertainty samples yielded classifiers with lower error rates than random samples ten times larger. 1
623|Item-Based Top-N Recommendation Algorithms|... In this paper we present one such class of model-based recommendation algorithms that first determines the similarities between the various items and then uses them to identify the set of items to be recommended. The key steps in this class of algorithms are (i) the method used to compute the similarity between the items, and (ii) the method used to combine these similarities in order to compute the similarity between a basket of items and a candidate recommender item. Our experimental evaluation on eight real datasets shows that these item-based algorithms are up to two orders of magnitude faster than the traditional user-neighborhood based recommender systems and provide recommendations with comparable or better quality
624|Combining collaborative filtering with personal agents for better recommendations|Information filtering agents and collaborative filtering both attempt to alleviate information overload by identifying which items a user will find worthwhile. Information filtering (IF) focuses on the analysis of item content and the development of a personal user interest profile. Collaborative filtering (CF) focuses on identification of other users with similar tastes and the use of their opinions to recommend items. Each technique has advantages and limitations that suggest that the two could be beneficially combined. This paper shows that a CF framework can be used to combine personal IF agents and the opinions of a community of users to produce better recommendations than either agents or users can produce alone. It also shows that using CF to create a personal combination of a set of agents produces better results than either individual agents or other combination mechanisms. One key implication of these results is that users can avoid having to select among agents; they can use them all and let the CF framework select the best ones for them.
625|Incorporating Contextual Information in Recommender Systems Using a Multidimensional Approach|The paper presents a multidimensional (MD) approach to recommender systems that can provide recommendations based on additional contextual information besides the typical information on users and items used in most of the current recommender systems. This approach supports multiple dimensions, extensive profiling, and hierarchical aggregation of recommendations. The paper also presents a multidimensional rating estimation method capable of selecting two-dimensional segments of ratings pertinent to the recommendation context and applying standard collaborative filtering or other traditional two-dimensional rating estimation techniques to these segments. A comparison of the multidimensional and two-dimensional rating estimation approaches is made, and the tradeoffs between the two are studied. Moreover, the paper introduces a combined rating estimation method that identifies the situations where the MD approach outperforms the standard two-dimensional approach and uses the MD approach in those situations and the standard two-dimensional approach elsewhere. Finally, the paper presents a pilot empirical study of the combined approach, using a multidimensional movie recommender system that was developed for implementing this approach and testing its performance. 1 1.
626|Collaborative Filtering by Personality Diagnosis: A Hybrid Memory- and Model-Based Approach|The growth of Internet commerce has stimulated  the use of collaborative filtering (CF) algorithms  as recommender systems. Such systems leverage  knowledge about the known preferences of  multiple users to recommend items of interest to  other users. CF methods have been harnessed  to make recommendations about such items as  web pages, movies, books, and toys. Researchers  have proposed and evaluated many approaches  for generating recommendations. We describe  and evaluate a new method called personality  diagnosis (PD). Given a user&#039;s preferences for  some items, we compute the probability that he  or she is of the same &#034;personality type&#034; as other  users, and, in turn, the probability that he or she  will like new items. PD retains some of the advantages  of traditional similarity-weighting techniques  in that all data is brought to bear on each  prediction and new data can be added easily and  incrementally. Additionally, PD has a meaningful  probabilistic interpretation, which ma...
627|Clustering Methods for Collaborative Filtering|Grouping people into clusters based on the items they have purchased allows accurate recommendations of new items for purchase: if you and I have liked many of the same movies, then I will probably enjoy other movies that you like. Recommending items based on similarity of interest (a.k.a. collaborative filtering) is attractive for many domains: books, CDs, movies, etc., but does not always work well. Because data are always sparse -- any given person has seen only a small fraction of all movies -- much more accurate predictions can be made by grouping people into clusters with similar movies and grouping movies into clusters which tend to be liked by the same people. Finding optimal clusters is tricky because the movie groups should be used to help determine the people groups and visa versa. We present a formal statistical model of collaborative filtering, and compare different algorithms for estimating the model parameters including variations of K-means clustering and Gibbs Sampling. This...
628|E-Commerce Recommendation Applications|Recommender systems are being used by an ever-increasing number of E-commerce sites to help consumers find products to purchase. What started as a novelty has turned into a serious business tool. Recommender systems use product knowledge -- either hand-coded knowledge provided by experts or &#034;mined&#034; knowledge learned from the behavior of consumers -- to guide consumers through the often-overwhelming task of locating products they will like. In this article we present an explanation of how recommender systems are related to some traditional database analysis techniques. We examine how recommender systems help E-commerce sites increase sales and analyze the recommender systems at six market-leading sites. Based on these examples, we create a taxonomy of recommender systems, including the inputs required from the consumers, the additional knowledge required from the database, the ways the recommendations are presented to consumers, the technologies used to create the recommendations, and t...
629|The digitization of word of mouth: Promise and challenges of online feedback mechanisms|Online feedback mechanisms harness the bidirectional communication capabilities of the Internet to engineer large-scale, word-of-mouth networks. Best known so far as a technology for building trust and fostering cooperation in online marketplaces, such as eBay, these mechanisms are poised to have a much wider impact on organizations. Their growing popularity has potentially important implications for a wide range of management activities such as brand building, customer acquisition and retention, product development, and quality assurance. This paper surveys our progress in understanding the new possibilities and challenges that these mechanisms represent. It discusses some important dimensions in which Internet-based feedback mechanisms differ from traditional word-of-mouth networks and surveys the most important issues related to their design, evaluation, and use. It provides an overview of relevant work in game theory and economics on the topic of reputation. It discusses how this body of work is being extended and combined with insights from computer science, management science, sociology, and psychology to take into consideration the special properties of online environments. Finally, it identifies opportunities that this new area presents for operations research/management science (OR/MS) research.
630|Horting Hatches an Egg: A New Graph-Theoretic Approach to Collaborative Filtering|This paper introduces a new and novel approach to ratingbased collaborative filtering. The new technique is most appropriate for e-commerce merchants offering one or more groups of relatively homogeneous items such as compact disks, videos, books, software and the like. In contrast with other known collaborative filtering techniques, the new algorithm is graph-theoretic, based on the twin new concepts of horting and predictability. As is demonstrated in this paper, the technique is fast, scalable, accurate, and requires only a modest learning curve. It makes use of a hierarchical classification scheme in order to introduce context into the rating process, and uses so-called creative links in order to find surprising and atypical items to recommend, perhaps even items which cross the group boundaries. The new technique is one of the key engines of the Intelligent Recommendation Algorithm (IRA) project, now being developed at IBM Research. In addition to several other recommendation engines, IRA contains a situation analyzer to determine the most appropriate mix of engines for a particular e-commerce merchant, as well as an engine for optimizing the placement of advertisements.
631|Discovery and Evaluation of Aggregate Usage Profiles for Web Personalization|Web usage mining, possibly used in conjunction with standard approaches to personalization such as collaborative filtering, can help address some of the shortcomings of these techniques, including reliance on subjective user ratings, lack of scalability, and poor performance in the face of high-dimensional and sparse data. However, the discovery of patterns from usage data by itself is not sufficient for performing the personalization tasks. The critical step is the effective derivation of good quality and useful (i.e., actionable) &#034;aggregate usage profiles&#034; from these patterns. In this paper we present and experimentally evaluate two techniques, based on clustering of user transactions and clustering of pageviews, in order to discover overlapping aggregate profiles that can be effectively used by recommender systems for real-time Web personalization. We evaluate these techniques both in terms of the quality of the individual profiles generated, as well as in the context of providing recommendations as an integrated part of a personalization engine. In particular, our results indicate that using the generated aggregate profiles, we can achieve effective personalization at early stages of users&#039; visits to a site, based only on anonymous clickstream data and without the benefit of explicit input by these users or deeper knowledge about them.
632|Applying Associative Retrieval Techniques to Alleviate the Sparsity Problem in Collaborative Filtering|this article, we propose to deal with this sparsity problem by applying an associative retrieval framework and related spreading activation algorithms to explore transitive associations among consumers through their past transactions and feedback. Such transitive associations are a valuable source of information to help infer consumer interests and can be explored to deal with the sparsity problem. To evaluate the effectiveness of our approach, we have conducted an experimental study using a data set from an online bookstore. We experimented with three spreading activation algorithms including a constrained Leaky Capacitor algorithm, a branch-and-bound serial symbolic search algorithm, and a Hopfield net parallel relaxation search algorithm. These algorithms were compared with several collaborative filtering approaches that do not consider the transitive associations: a simple graph search approach, two variations of the user-based approach, and an item-based approach. Our experimental results indicate that spreading activation-based approaches significantly outperformed the other collaborative filtering methods as measured by recommendation precision, recall, the F-measure, and the rank score. We also observed the over-activation effect of the spreading activation approach, that is, incorporating transitive associations with past transactional data that is not sparse may &#034;dilute&#034; the data used to infer user preferences and lead to degradation in recommendation performance
633|MovieLens Unplugged: Experiences with an Occasionally Connected Recommender System|Recommender systems have changed the way people shop online. Recommender systems on wireless mobile devices may have the same impact on the way people shop in stores. We present our experience with implementing a recommender system on a PDA that is occasionally connected to the network. This interface helps users of the MovieLens movie recommendation service select movies to rent, buy, or see while away from their computer. The results of a nine month field study show that although there are several challenges to overcome, mobile recommender systems have the potential to provide value to their users today.
634|Implicit Feedback for Recommender System|Can implicit feedback substitute for explicit ratings in recommender systems? If so, we could avoid the difficulties associated with gathering explicit ratings from users. How, then, can we capture useful information unobtrusively, and how might we use that information to make recommendations? In this paper we identify three types of implicit feedback and suggest two strategies for using implicit feedback to make recommendations.
635|Collaborative Filtering via Gaussian Probabilistic Latent Semantic Analysis|Collaborative filtering aims at learning predictive models of user preferences, interests or behavior from community data, i.e. a database of available user preferences. In this paper, we describe a new model-based algorithm designed for this task, which is based on a generalization of probabilistic latent semantic analysis to continuous-valued response variables. More specifically, we assume that the observed user ratings can be modeled as a mixture of user communities or interest groups, where users may participate probabilistically in one or more groups. Each community is characterized by a Gaussian distribution on the normalized ratings for each item. The normalization of ratings is performed in a user-specific manner to account for variations in absolute shift and variance of ratings. Experiments on the EachMovie data set show that the proposed approach compares favorably with other collaborative filtering techniques.
636|Flexible Mixture Model for Collaborative Filtering|This paper presents a flexible mixture model  (FMM) for collaborative filtering. FMM extends  existing partitioning/clustering algorithms for  collaborative filtering by clustering both users  and items together simultaneously without  assuming that each user and item should only  belong to a single cluster. Furthermore, with the  introduction of `preference&#039; nodes, the proposed  framework is able to explicitly model how users  rate items, which can vary dramatically, even  among the users with similar tastes on items.
638|Combining Content and Collaboration in Text Filtering|We describe a technique for combining collaborative input and document content for text filtering. This technique uses latent semantic indexing to create a collaborative view of a collection of user profiles. The profiles themselves are term vectors constructed from documents deemed relevant to the user&#039;s information need. Using standard text collections, this approach performs quite favorably compared to other content-based approaches.  1 Introduction  Filtering is a process of comparing an incoming document stream to a profile of a user&#039;s interests and recommending the documents according to that profile [ Belkin and Croft, 1992 ] . A simple approach for filtering textual content might be to look at each document&#039;s similarity to an average of known relevant documents. Collaborative filtering takes into account the similarities and differences among the profiles of several users in determining how to recommend a document. Typically, collaborative filtering is done by correlating users...
639|Recommendation Systems: A Probabilistic Analysis|A recommendation system tracks past actions of a group of users to make recommendations to individual members of the group. The growth of computer-mediated marketing and commerce has led to increased interest in such systems. We introduce a simple analytical framework for recommendation systems, including a basis for defining the utility of such a system. We perform probabilistic analyses of algorithmic methods within this framework. These analyses yield insights into how much utility can be derived from the memory of past actions and on how this memory can be exploited. 1. Introduction  Collaborative filtering (sometimes known as a recommendation  system) is a process by which information on the preferences and actions of a group of users is tracked by a system which then, based on the patterns it observes, tries to make useful recommendations to individual users [10, 12, 18, 19, 20, 22, 23]. For instance, a book recommendation system might recommend Jules Verne to someone interested ...
640|Bayesian Mixed-Effects Models for Recommender Systems|We propose a Bayesian methodology for recommender systems that incorporates user ratings, user features, and item features in a single unified framework. In principle our approach should address the cold-start issue and can address both scalability issues as well as sparse ratings. However, our early experiments have shown mixed results.  1 Introduction  Recommender systems have emerged as an important application area and have been the focus of considerable recent academic and commercial interest. The 1997 special issue of the Communications of the ACM [14] contains some key papers. Other important contributions include [2], [4], [8], [13], [16], [9], [1], [12], and [15]. In addition, many online retailers are using this technology to recommend new items to their customers, based on what they have bought in the past. Currently, most recommender systems are either  content-based or collaborative, depending on the type of information that the system uses to recommend items to a user. Co...
641|The TREC-6 Filtering Track: Description and Analysis|This article details the experiments conducted in the TREC-6 filtering track. The filtering track is an extension of the routing track which adds time sequencing of the document stream and set-based evaluation strategies which simulate immediate distribution of the retrieved documents. It also introduces an adaptive filtering subtrack which is designed to simulate on-line or sequential filtering of documents. In addition to motivating the task and describing the practical details of participating in the track, this document includes a detailed graphical presentation of the experimental results and attempts to analyze and explain the observed patterns. The final section suggests some ways to extend the current research in future experiments. 1 Introduction  There is increasing evidence that text filtering will become a critical tool in searching and managing the flow of data in the information age. New companies are appearing daily which offer push services or intelligent agents centere...
642|Expert-driven validation of rule-based user models in personalization 9 |Abstract. In many e-commerce applications, ranging from dynamic Web content presentation, to personalized ad targeting, to individual recommendations to the customers, it is important to build personalized profiles of individual users from their transactional histories. These profiles constitute models of individual user behavior and can be specified with sets of rules learned from user transactional histories using various data mining techniques. Since many discovered rules can be spurious, irrelevant, or trivial, one of the main problems is how to perform post-analysis of the discovered rules, i.e., how to validate user profiles by separating “good ” rules from the “bad.” This validation process should be done with an explicit participation of the human expert. However, complications may arise because there can be very large numbers of rules discovered in the applications that deal with many users, and the expert cannot perform the validation on a rule-by-rule basis in a reasonable period of time. This paper presents a framework for building behavioral profiles of individual users. It also introduces a new approach to expert-driven validation of a very large number of rules pertaining to these users. In particular, it presents several types of validation operators, including rule grouping, filtering, browsing, and redundant rule elimination operators, that allow a human expert validate many individual rules at a time. By iteratively applying such operators, the human expert can validate a significant part of all the initially discovered rules in an acceptable time period. These validation operators were implemented as a part of a one-to-one profiling system. The paper also presents
643|Privacy risks in recommender systems|this article, we can state some general guidelines. Like most problems in computer security, the ideal deterrents are better awareness of the issues and more openness in how systems operate in the marketplace. In particular, individual sites should clearly state the policies and methodologies they employ with recommender systems, including the role played by straddlers in their data sets and system designs. This is especially true for sites with multiple homogeneous networks (as in Figures 6c and 6d).   By conveying benefits and risks to users intuitively, recommender systems could achieve greater acceptance. We envisage three general ways to highlight the implications of our analyses. # Present plots of benefit and risk versus usermodifiable parameters such as ratings, w, and l (if the algorithm allows their direct specification) to allow users to make informed choices about their levels of involvement
644|Combining Usage, Content, and Structure Data to Improve Web Site Recommendation|Web recommender systems anticipate the needs of web users  and provide them with recommendations to personalize their navigation.
645|A Bayesian Model for Collaborative Filtering|Consider the general setup where a set of items have been partially rated by a set of judges, in the sense that not every item has been rated by every judge. For this setup, we propose a Bayesian approach for the problem of predicting the missing ratings from the observed ratings. This approach incorporates similarity by assuming the set of judges can be partitioned into groups which share the same ratings probability distribution. This leads to a predictive distribution of missing ratings based on the posterior distribution of the groupings and associated ratings probabilities. Markov chain Monte Carlo methods and a hybrid search algorithm are then used to obtain predictions of the missing ratings. 1
646|A maximum entropy approach to collaborative filtering in dynamic, sparse, high-dimensional domains|We develop a maximum entropy (maxent) approach to generating recommendations in the context of a user’s current navigation stream, suitable for environments where data is sparse, highdimensional, and dynamic—conditions typical of many recommendation applications. We address sparsity and dimensionality reduction by first clustering items based on user access patterns so as to attempt to minimize the apriori probability that recommendations will cross cluster boundaries and then recommending only within clusters. We address the inherent dynamic nature of the problem by explicitly modeling the data as a time series; we show how this representational expressivity fits naturally into a maxent framework. We conduct experiments on data from ResearchIndex, a popular online repository of over 470,000 computer science documents. We show that our maxent formulation outperforms several competing algorithms in offline tests simulating the recommendation of documents to ResearchIndex users. 1
647|Using Probabilistic Relational Models for Collaborative Filtering|Recent projects in collaborative filtering and information filtering address the task of inferring user preference relationships for products or information. The data on which these inferences are based typically consists of pairs of people and items. The items may be information sources (such as web pages or newspaper articles) or products (such as books, software, movies or CDs). We are interested in making recommendations or predictions. Traditional approaches to the problem derive from classical algorithms in statistical pattern recognition and machine learning. The majority of these approaches assume a &#034;flat&#034; data representation for each object, and focus on a single dyadic relationship between the objects. In this paper, we examine a richer model that allows us to reason about many different relations at the same time. We build on the recent work on probabilistic relational models (PRMs), and describe how PRMs can be applied to the task of collaborative filtering. PRMs allow us t...
648|Characterization and construction of radial basis functions|We review characterizations of (conditional) positive deniteness and show how they apply to the theory of radial basis functions. We then give complete proofs for the (conditional) positive deniteness of all practically relevant basis functions. Furthermore, we show how some of these characterizations may lead to construction tools for positive denite functions. Finally, we give new construction techniques based on discrete methods which lead to non-radial, even nontranslation invariant, local basis functions.  
649|Collaborative Learning for Recommender Systems|Recommender systems use ratings from users on  items such as movies and music for the purpose  of predicting the user preferences on items that  have not been rated. Predictions are normally  done by using the ratings of other users of the  system, by learning the user preference as a function  of the features of the items or by a combination  of both these methods.
650|Book Recommending using Text Categorization with Extracted Information|Content-based recommender systems suggest documents,  items, and services to users based on learning  a profile of the user from rated examples containing  information about the given items. Text categorization  methods are very useful for this task but generally  rely on unstructured text. We have developed a bookrecommending  system that utilizes semi-structured information  about items gathered from the web using  simple information extraction techniques. Initial experimental  results demonstrate that this approach can  produce fairly accurate recommendations.
651|Memory-Based Weighted-Majority Prediction For Recommender Systems|Recommender Systems are learning systems that make use of data representing multi-user preferences over items (e.g. Vote [user, item] matrix), to try to predict the preference towards new items or products regarding a particular user. User preferences are in fact the learning target functions. The main objective of the system is to filter items according to the predicted preferences and present to the user the options that are most attractive to him; i.e. he would probably like the most. We study Recommender Systems viewed as a pool of independent prediction algorithms, one per every user, in situations in which each learner faces a sequence of trials, with a prediction to make in each step. The goal is to make as few mistakes as possible. We are interested in the case that each learner has reasons to believe that there exists some other target functions in the pool that consistently behaves similar, neutral or opposite to the target function it is trying to learn. The learner doesn&#039;t ...
652|Customer Lifetime Value Modeling and Its Use for Customer Retention Planning|We present and discuss the important business problem of estimating the effect of retention efforts on the Lifetime Value of a customer in the Telecommunications industry. We discuss the components of this problem, in particular customer value and length of service (or tenure) modeling, and present a novel segment-based approach, motivated by the segment-level view marketing analysts usually employ. We then describe how we build on this approach to estimate the effects of retention on Lifetime Value. Our solution has been successfully implemented in Amdocs&#039; Business Insight (BI) platform, and we illustrate its usefulness in real-world scenarios.
653|Preference-based Graphic Models for Collaborative Filtering|Collaborative filtering is a very useful general technique for exploiting the preference patterns of a group of users to predict the utility of items to a particular user. Previous research has studied several probabilistic graphic models for collaborative filtering with promising results. However, while these models have succeeded in capturing the similarity among users and items, none of them has considered the fact that users with similar interests in items can have very different rating patterns; some users tend to assign a higher rating to all items than other users. In this paper, we propose and study two new graphic models that address the distinction between user preferences and ratings. In one model, called the decoupled model, we introduce two different variables to decouple a user’s preferences from his/her ratings. In the other, called the preference model, we model the orderings of items preferred by a user, rather than the user’s numerical ratings of items. Empirical study over two datasets of movie ratings shows that, due to its appropriate modeling of the distinction between user preferences and ratings, the proposed decoupled model significantly outperforms all the five existing approaches that we compared with. The preference model, however, performs much worse than the decoupled model, suggesting that while explicit modeling of the underlying user preferences is very important for collaborative filtering, we can not afford ignoring the rating information completely. 1.
654|Adaptive lightweight text filtering|Abstract. We present a lightweight text filtering algorithm intended for use with personal Web information agents. Fast response and low resource usage were the key design criteria, in order to allow the algorithm to run on the client side. The algorithm learns adaptive queries and dissemination thresholds for each topic of interest in its user profile. We describe a factorial experiment used to test the robustness of the algorithm under different learning parameters and more importantly, under limited training feedback. The experiment borrows from standard practice in TREC by using TREC-5 data to simulate a user reading and categorizing documents. Results indicate that the algorithm is capable of achieving good filtering performance, even with little user feedback. 1
655|Image Quilting for Texture Synthesis and Transfer|We present a simple image-based method of generating novel visual appearance in which a new image is synthesized by stitching together small patches of existing images. We call this process image  quilting. First, we use quilting as a fast and very simple texture synthesis algorithm which produces surprisingly good results for a wide range of textures. Second, we extend the algorithm to perform texture transfer -- rendering an object with a texture taken from a different object. More generally, we demonstrate how an image can be re-rendered in the style of a different image. The method works directly on the images and does not require 3D information.
656|Texture Synthesis by Non-parametric Sampling|A non-parametric method for texture synthesis is proposed. The texture synthesis process grows a new image outward from an initial seed, one pixel at a time. A Markov random field model is assumed, and the conditional distribution of a pixel given all its neighbors synthesized so far is estimated by querying the sample image and finding all similar neighborhoods. The degree of randomness is controlled by a single perceptually intuitive parameter. The method aims at preserving as much local structure as possible and produces good results for a wide variety of synthetic and real-world textures. 1. Introduction Texture synthesis has been an active research topic in computer vision both as a way to verify texture analysis methods, as well as in its own right. Potential applications of a successful texture synthesis algorithm are broad, including occlusion fill-in, lossy image and video compression, foreground removal, etc. The problem of texture synthesis can be formulated as follows: let...
657|Fast texture synthesis using tree-structured vector quantization|Figure 1: Our texture generation process takes an example texture patch (left) and a random noise (middle) as input, and modifies this random noise to make it look like the given example texture. The synthesized texture (right) can be of arbitrary size, and is perceived as very similar to the given example. Using our algorithm, textures can be generated within seconds, and the synthesized results are always tileable. Texture synthesis is important for many applications in computer graphics, vision, and image processing. However, it remains difficult to design an algorithm that is both efficient and capable of generating high quality results. In this paper, we present an efficient algorithm for realistic texture synthesis. The algorithm is easy to use and requires only a sample texture as input. It generates textures with perceived quality equal to or better than those produced by previous techniques, but runs two orders of magnitude faster. This permits us to apply texture synthesis to problems where it has traditionally been considered impractical. In particular, we have applied it to constrained synthesis for image editing and temporal texture generation. Our algorithm is derived from Markov Random Field texture models and generates textures through a deterministic searching process. We accelerate this synthesis process using tree-structured vector quantization.
658|Pyramid-Based Texture Analysis/Synthesis|This paper describes a method for synthesizing images that match the texture appearanceof a given digitized sample. This synthesis is completely automatic and requires only the &#034;target&#034; texture as input. It allows generation of as much texture as desired so that any object can be covered. It can be used to produce solid textures for creating textured 3-d objects without the distortions inherent in texture mapping. It can also be used to synthesize texture mixtures, images that look a bit like each of several digitized samples. The approach is based on a model of human texture perception, and has potential to be a practically useful tool for graphics applications. 1 Introduction  Computer renderings of objects with surface texture are more interesting and realistic than those without texture. Texture mapping [15] is a technique for adding the appearance of surface detail by wrapping or projecting a digitized texture image ontoa surface. Digitized textures can be obtained from a variety ...
659|Image analogies|Figure 1 An image analogy. Our problem is to compute a new “analogous ” image B ' that relates to B in “the same way ” as A ' relates to A. Here, A, A ' , and B are inputs to our algorithm, and B ' is the output. The full-size images are shown in Figures 10 and 11. This paper describes a new framework for processing images by example, called “image analogies. ” The framework involves two stages: a design phase, in which a pair of images, with one image purported to be a “filtered ” version of the other, is presented as “training data”; and an application phase, in which the learned filter is applied to some new target image in order to create an “analogous” filtered result. Image analogies are based on a simple multiscale autoregression, inspired primarily by recent results in texture synthesis. By choosing different types of source image pairs as input, the framework supports a wide variety of “image filter ” effects, including traditional image filters, such as blurring or embossing; improved texture synthesis, in which some textures are synthesized with higher quality than by previous approaches; super-resolution, in which a higher-resolution image is inferred from a low-resolution source; texture transfer, in which images are “texturized ” with some arbitrary source texture; artistic filters, in which various drawing and painting styles are synthesized based on scanned real-world examples; and texture-by-numbers, in which realistic scenes, composed of a variety of textures, are created using a simple painting interface.
660|A Parametric Texture Model based on Joint Statistics of Complex Wavelet Coefficients|We present a universal statistical model for texture images in the context of an overcomplete complex wavelet transform. The model is parameterized by a set of statistics computed on pairs of coefficients corresponding to basis functions at adjacent spatial locations, orientations, and scales. We develop an efficient algorithm for synthesizing random images subject to these constraints, by iteratively projecting onto the set of images satisfying each constraint, and we use this to test the perceptual validity of the model. In particular, we demonstrate the necessity of subgroups of the parameter set by showing examples of texture synthesis that fail when those parameters are removed from the set. We also demonstrate the power of our model by successfully synthesizing examples drawn from a diverse collection of artificial and natural textures.
661|Synthesizing Natural Textures|We present a simple texture synthesis algorithm that is well-suited for a specific class of naturally occurring textures. This class includes quasi-repeating patterns consisting of small objects of familiar but irregular size, such as flower fields, pebbles, forest undergrowth, bushes and tree branches. The algorithm starts from a sample image and generates a new image of arbitrary size the appearance of which is similar to that of the original image. This new image does not change the basic spatial frequencies the original image; instead it creates an image that is a visually similar, and is of a size set by the user. This method is fast and its implementation is straightforward. We extend the algorithm to allow direct user input for interactive control over the texture synthesis process. This allows the user to indicate large-scale properties of the texture appearance using a standard painting-style interface, and to choose among various candidate textures the algorithm can create by performing different number of iterations.
662|Multiresolution Sampling Procedure for Analysis and Synthesis of Texture Images|This paper outlines a technique for treating input texture images as probability density estimators from which new textures, with similar appearance and structural properties, can be sampled. In a two-phase process, the input texture is first analyzed by measuring the joint occurrence of texture discrimination features at multiple resolutions. In the second phase, a new texture is synthesized by sampling successive spatial frequency bands from the input texture, conditioned on the similar joint occurrence of features at lower spatial frequencies. Textures synthesized with this method more successfully capture the characteristics of input textures than do previous techniques.  1 Introduction  Synthetic texture generation has been an increasingly active research area in computer graphics. The primary approach has been to develop specialized procedural models which emulate the generative process of the texture they are trying to mimic. For example, models based on reaction-diffusion inter...
663|Lapped Textures |Figure 1: Four different textures pasted on the bunny model. The last picture illustrates changing local orientation and scale on the body. We present a method for creating texture over an arbitrary surface mesh using an example 2D texture. The approach is to identify interesting regions (texture patches) in the 2D example, and to repeatedly paste them onto the surface until it is completely covered. We call such a collection of overlapping patches a lapped texture. It is rendered using compositing operations, either into a traditional global texture map during a preprocess, or directly with the surface at runtime. The runtime compositing approach avoids resampling artifacts and drastically reduces texture memory requirements. Through a simple interface, the user specifies a tangential vector field over the surface, providing local control over the texture scale, and for anisotropic textures, the orientation. To paste a texture patch onto the surface, a surface patch is grown and parametrized over texture space. Specifically, we optimize the parametrization of each surface patch such that the tangential vector field aligns everywhere with the standard frame of the texture patch. We show that this optimization is solved efficiently as a sparse linear system.
664|Orientable Textures for Image-Based Pen-and-Ink Illustration  |We present an interactive system for creating pen-and-ink-style line drawings from greyscale images in which the strokes of the renderedillustrationfollowthefeaturesoftheoriginalimage. The user, via new interaction techniquesforeditingadirectionfield, specifies anorientationforeachregionoftheimage; thecomputerdrawsorientedstrokes, basedonauser-specifiedsetofexamplestrokes,that achieve the same tone as the image via a new algorithm that compares anadaptively-blurred version ofthecurrent illustrationtothe targettoneimage. By aligning the direction field with surface orientations of the objects in the image, the user can create textures that appear attached to those objects instead of merely conveying their darkness. The result is a more compelling pen-and-ink illustration thanwas previously possible from 2D reference imagery.  
666|Mosaics of Scenes with Moving Objects|Image mosaics are useful for a variety of tasks in vision and computer graphics. A particularly convenient way to generate mosaics is by `stitching&#039; together many ordinary photographs. Existing algorithms focus on capturing static scenes. This paper presents a complete system for creating visually pleasing mosaics in the presence of moving objects. There are three primary contributions. The first component of our system is a registration method that remains unbiased by movement--- the Mellin transform is extended to register images related by a projective transform. Second, an efficient method for finding a globally consistent registration of all images is developed. By solving a linear system of equations, derived from many pairwise registration matrices, we find an optimal global registration. Lastly, a new method of compositing images is presented. Blurred areas due to moving objects are avoided by segmenting the mosaic into disjoint regions and sampling pixels in each region from a...
667|Novel Cluster-Based Probability Model for Texture Synthesis, Classification, and Compression|We present a new probabilistic modeling technique for high-dimensional vector sources, and consider its application to the problems of texture synthesis, classification, and compression. Our model combines kernel estimation with clustering, to obtain a semiparametric probability mass function estimate which summarizes --- rather than contains --- the training data. Because the model is cluster based, it is inferable from a limited set of training data, despite the model&#039;s high dimensionality. Moreover, its functional form allows recursive implementation that avoids exponential growth in required memory as the number of dimensions increases. Experimental results are presented for each of the three applications considered.  1. INTRODUCTION  In many information processing tasks individual data samples exhibit a great deal of statistical interdependence, and should be treated jointly (e.g., in vectors) rather than separately. For some tasks this requires modeling vectors probabilistically....
668|A Non-Hierarchical Procedure for Re-Synthesis of Complex Textures|A procedure is described for synthesizing an image with the same texture as a given input image.
669|Multi-color and artistic dithering|A multi-color dithering algorithm is proposed, which converts a barycentric combination of color intensities into a multi-color nonoverlapping surface coverage. Multi-color dithering is a generalization of standard bi-level dithering. Combined with tetrahedral color separation, multi-color dithering makes it possible to print images made of a set of non-standard inks. In contrast to most previous color halftoning methods, multi-color dithering ensures by construction that the different selected basic colors are printed side by side. Multi-color dithering is applied to generate color images whose screen dots are made of artistic shapes (letters, symbols, ornaments, etc.). Two dither matrix postprocessing techniques are developed, one for enhancing the visibility of screen motives and one for the local equilibration of large dither matrices. The dither matrix equilibration process corrects disturbing local intensity variations by taking dot gain and the human visual system transfer function into account. Thanks to the combination of the presented techniques, high quality images can be produced, which incorporate at the micro level the desired artistic screens and at the macro level the full color image. Applications include designs for advertisements and posters as well as security printing. Multi-color dithering also offers new perspectives for printing with special inks, such as fluorescent and metallic inks.
670|Efficient Variants of the ICP Algorithm|The ICP (Iterative Closest Point) algorithm is widely used for geometric alignment of three-dimensional models when an initial estimate of the relative pose is known. Many variants of ICP have been proposed, affecting all phases of the algorithm from the selection and matching of points to the minimization strategy. We enumerate and classify many of these variants, and evaluate their effect on the speed with which the correct alignment is reached. In order to improve convergence for nearly-flat meshes with small features, such as inscribed surfaces, we introduce a new variant based on uniform sampling of the space of normals. We conclude by proposing a combination of ICP variants optimized for high speed. We demonstrate an implementation that is able to align two range images in a few tens of milliseconds, assuming a good initial guess. This capability has potential application to real-time 3D model acquisition and model-based tracking.
671|Closed-form solution of absolute orientation using unit quaternions|Finding the relationship between two coordinate systems using pairs of measurements of the coordinates of a number of points in both systems is a classic photogrammetric task. It finds applications in stereophotogrammetry and in robotics. I present here a closed-form solution to the least-squares problem for three or more points. Currently various empirical, graphical, and numerical iterative methods are in use. Derivation of the solution is simplified by use of unit quaternions to represent rotation. I emphasize a symmetry property that a solution to this problem ought to possess. The best translational offset is the difference between the centroid of the coordinates in one system and the rotated and scaled centroid of the coordinates in the other system. The best scale is equal to the ratio of the root-mean-square deviations of the coordinates in the two systems from their respective centroids. These exact results are to be preferred to approximate methods based on measurements of a few selected points. The unit quaternion representing the best rotation is the eigenvector associated with the most positive eigenvalue of a symmetric 4 X 4 matrix. The elements of this matrix are combinations of sums of products of corresponding coordinates of the points. 1.
672|Registering Multiview Range Data to Create 3D Computer Objects|This research deals with the problem of range image registration for the purpose of building surface models of three-dimensional objects. The registration task involves finding the translation and rotation parameters which properly align overlapping views of the object so as to reconstruct from these partial surfaces, an integrated surface representation of the object. The approach taken is to express the registration task as an optimization problem. We define a function which measures the quality of the alignment between the partial surfaces contained in two range images as produced by a set of motion parameters . This function computes a sum of Euclidean distances between a set of control points on one of the surfaces to corresponding points on the other. The strength of this approach resides in the method used to determine point correspondences across range images. It is based on reversing the rangefinder calibration process, resulting in a set of equations which can be used to dire...
673|RANSAC-based DARCES: A New Approach for Fast Automatic Registration of Partially Overlapping Range Images |Registration of two partially-overlapping range images taken from different views is an important task in 3D computer vision. In general, if there is no initial knowledge about the poses of these two views, the information used for solving the 3D registration problem is mainly the 3D shape of the common parts of the two partially-overlapping data sets.
674|Fast global registration of 3D sampled surfaces using a multi-Z-buffer technique|We present a new method for the global registration of several overlapping 3D surfaces sampled on an object. The method is based on the ICP algorithm and on a segmentation of the sampled points in an optimized set of z-buffers. This multi-z-buffer technique provides a 3D space partitioning which greatly accelerates the search of the nearest neighbours in the establishment of the point-to-point correspondence between overlapping surfaces. Then a randomized iterative registration is processed on the surface set. We have tested an implementation of this technique on real sampled surfaces. It appears to be rapid, accurate and robust, especially in the case of highly curved objects. 1.
675|Surface Registration by Matching Oriented Points|For registration of 3-D free-form surfaces we have developed a representation which requires no knowledge of the transformation between views. The representation comprises descriptive images associated with oriented points on the surface of an object. Constructed using single point bases, these images are data level shape descriptions that are used for efficient matching of oriented points. Correlation of images is used to establish point correspondences between two views; from these correspondences a rigid transformation that aligns the views is calculated. The transformation is then refined and verified using a modified iterative closest point algorithm. To demonstrate the generality of our approach, we present results from multiple sensing domains.  1. Introduction  Surface registration is the process that aligns 3-D data sets acquired from different view points or at different times. A common application of surface registration is to spatially reconcile multiple views of a scene in...
676|Optimal Registration of Object Views Using Range Data|This paper deals with robust registration of object views in the presence of uncertainties and noise in depth data. Errors in registration of multiple views of a 3D object severely affect view integration during automatic construction of object models. We derive a minimum variance estimator (MVE) for computing the view transformation parameters accurately from range data of two views of a 3D object. The results of our experiments show that view transformation estimates obtained using MVE are significantly more accurate than those computed with an unweighted error criterion for registration.  Key words: Image registration, view transformation estimation, view integration, automatic object modeling, 3D free-form objects, range data. 1 Introduction  An important issue in the design of 3D object recognition systems is building models of physical objects. Object models are extensively used for synthesizing and predicting object appearances from desired viewpoints and also for recognizing th...
677|Registration and Integration of Multiple Object Views for 3D Model Construction|Automatic 3D object model construction is important in  applications ranging from manufacturing to entertainment, since CAD models of existing objects may be either unavailable or unusable. We describe a prototype system for automatically registering and  integrating multiple views of objects from range data. The results can  then be used to construct geometric models of the objects. New  techniques for handling key problems such as robust estimation of  transformations relating multiple views and seamless integration of  registered data to form an unbroken surface have been proposed and implemented in the system. Experimental results on real surface data  acquired using a digital interferometric sensor as well as a laser range scanner demonstrate the good performance of our system.
678|Registration of 3-D Partial Surface Models Using Luminance and Depth Information|Textured surface models of three-dimensional objects are gaining importance in computer graphics applications. These models often have to be merged from several overlapping partial models which have to be registered (i.e. the relative transformation between the partial models has to be determined) prior to the merging process. In this paper a method is presented that makes use of both camera-based depth information (e.g. from stereo) and the luminance image. The luminance information is exploited to determine corresponding point sets on the partial surfaces using an optical flow approach. Quaternions are then employed to determine the transformation between the partial models which minimizes the sum of the 3-D Euclidian distances between the corresponding point sets. In order to find corresponding points on the partial surfaces luminance information is linearized. The procedure is iterated until convergence is reached. In contrast to only using depth information, employing luminance sp...
679|Geometrical Cloning 3D Objects via Simultaneous Registration of Multiview Range Images|In this paper, we present a method for the reconstruction of real world objects from multiple range images. One ma-jor contribution of our approach is the simultaneous reg-istration of all range images acquired from different scan-ner views. Thus, registration errors are not accumulated, and it is even possible to reconstruct large objects from an arbitrary number of small range images. The registration process is based on a least-squares approach where a dis-tance metric between the overlapping range images is min-imized. A resolution hierarchy accelerates the registration substantially. After registration, a volumetric model of the object is carved out. This step is based on the idea that no part of the object can lie between the measured surface and the camera of the scanner. With the marching cube algo-rithm a polygonal representation is generated. The accu-racy of this polygonal mesh is improved by moving the ver-tices of the mesh onto the surface implicitly defined by the registered range images. 1.
681|Querying Heterogeneous Information Sources Using Source Descriptions|We witness a rapid increase in the number of structured information sources that are available online, especially on the WWW. These sources include commercial databases on product information, stock market information, real estate, automobiles, and entertainment. We would like to use the data stored in these databases to answer complex queries that go beyond keyword searches. We face the following challenges: (1) Several information sources store interrelated data, and any query-answering system must understand the relationships between their contents. (2) Many sources are not full-featured database systems and can answer only a small set of queries over their data (for example, forms on the WWW restrict the set of queries one can ask). (3) Since the number of sources is very large, effective techniques are needed to prune the set of information sources accessed to answer a query. (4) The details of interacting with each source vary greatly. We describe the Information Manifold, an imp...
682|Answering queries using views|We consider the problem of computing answers to queries by using materialized views. Aside from its potential in optimizing query evaluation, the problem also arises in applications such as Global Information Systems, Mobile Computing and maintaining physical data independence. We consider the problem of nding a rewriting of a query that uses the materialized views, the problem of nding minimal rewritings, and nding complete rewritings (i.e., rewritings that use only the views). We show that all the possible rewritings can be obtained by considering containment mappings from the views to the query, and that the problems we consider are NP-complete when both the query and the views are conjunctive and don&#039;t involve built-in comparison predicates. We show that the problem has two independent sources of complexity (the number of possible containment mappings, and the complexity of deciding which literals from the original query can be deleted). We describe a polynomial time algorithm for nding rewritings, and show that under certain conditions, it will nd the minimal rewriting. Finally, we analyze the complexity of the problems when the queries and views may be disjunctive and involve built-in comparison predicates. 1
683|Retrieving And Integrating Datafrom Multiple Information Sources|With the current explosion of data, retrieving and integrating information  from various sources is a critical problem. Work in multidatabase systems  has begun to address this problem, but it has primarily focused on methods  for communicating between databases and requires significant effort for each  new database added to the system. This paper describes a more general  approach that exploits a semantic model of a problem domain to integrate  the information from various information sources. The information sources  handled include both databases and knowledge bases, and other information  sources (e.g., programs) could potentially be incorporated into the system.  This paper describes how both the domain and the information sources are  modeled, shows how a query at the domain level is mapped into a set of  queries to individual information sources, and presents algorithms for automatically  improving the efficiency of queries using knowledge about both the  domain and the informat...
684|A Softbot-Based Interface to the Internet|this article, we focus on the ideas underlying the softbot-based interface.
685|Optimizing Queries with Materialized Views|While much work has addressed the problem of maintaining materialized views, the important question of optimizing queries in the presence of materialized views has not been resolved. In this paper, we analyze the optimization question and provide a comprehensive and efficient solution. Our solution has the desirable property that it is a simple generalization of the traditional query optimization algorithm. 1 Introduction  The idea of using materialized views for the benefit of improved query processing has been proposed in the literature more than a decade ago. In this context, problems such as definition of views, composition of views, maintenance of views [BC79, KP81, SI84, BLT86, CW91, Rou91, GMS93] have been researched but one topic has been conspicuous by its absence. This concerns the problem of the judicious use of materialized views in answering a query. It may seem that materialized views should be used to evaluate a query whenever they are applicable. In fact, blind applicat...
686|Data Model and Query Evaluation in Global Information Systems|. Global information systems involve a large number of information sources distributed over computer networks. The variety of information sources and disparity of interfaces makes the task of easily locating and efficiently accessing information over the network very cumbersome. We describe an architecture for global information systems that is especially tailored to address the challenges raised in such an environment, and distinguish our architecture from architectures of multidatabase and distributed database systems. Our architecture is based on presenting a conceptually unified view of the information space to a user, specifying rich descriptions of the contents of the information sources, and using these descriptions for optimizing queries posed in the unified view. The contributions of this paper include: (1) we identify aspects of site descriptions that are useful in query optimization; (2) we describe query optimization techniques that minimize the number of information source...
687|Answering queries using templates with binding patterns|When integrating heterogeneous information re-sources, it is often the case that the source is rather limited in the kinds of queries it can answer. If a query is asked of the entire system, we have a new kind of optimization problem, in which we must try to express the given query in terms of the lim-ited query templates that this source can answer. For the case of conjunctive queries, we show how to decide with a nondeterministic polynomial-time al-gorithm whether the given query can be answered. We then extend our results to allow arithmetic comparisons in the given query and in the tem-plates.
688|Query Caching and Optimization in Distributed Mediator Systems|Query processing and optimization in mediator systems that access distributed non-proprietary sources pose many novel problems. Cost-based query optimization is hard because the mediator does not have access to source statistics information and furthermore it may not be easy to model the source&#039;s performance. At the same time, querying remote sources may be very expensive because of high connection overhead, long computation time, financial charges, and temporary unavailability. We propose a costbased optimization technique that caches statistics of actual calls to the sources and consequently estimates the cost of the possible execution plans based on the statistics cache. We investigate issues pertaining to the design of the statistics cache and experimentally analyze various tradeoffs. We also present a query result caching mechanism that allows us to effectively use results of prior queries when the source is not readily available. We employ the novel invariants  mechanism, which s...
689|A Query Translation Scheme for Rapid Implementation of Wrappers|Wrappers provide access to heterogeneous information sources by converting application queries into source specific queries or commands. In this paper we present a wrapper implementation toolkit that facilitates rapid development of wrappers. We focus on the query translation component of the toolkit, called the converter. The converter takes as input a Query Description and Translation Language (QDTL) description of the queries that can be processed by the underlying source. Based on this description the converter decides if an application query is (a) directly supported, i.e., it can be translated to a query of the underlying system following instructions in the QDTL description; (b) logically supported, i.e., logically equivalent to a directly supported query; (c) indirectly supported, i.e., it can be computed by applying a filter,  automatically generated by the converter, to the result of a directly supported query. 1 Introduction  A wrapper or translator [C  +  94, PGMW95] is a s...
690|Distributed Active Catalogs and Meta-Data Caching in Descriptive Name Services|Today&#039;s global internetworks challenge the ability of name services and other information services to locate data quickly. We introduce a distributed active catalog and meta-data caching for optimizing queries in this environment. Our active catalog constrains the search space for a query by returning a list of data repositories where the answer to the query is likely to be found. Meta-data caching improves performance by keeping frequently used characterizations of the search space close to the user, and eliminating active catalog communication and processing costs. When searching for query responses, our techniques contact only the small percentage of the data repositories with actual responses, resulting in search times of a few seconds. We implemented a distributed active catalog and meta-data caching in a prototype descriptive name service called &#034;Nomenclator. &#034; We present performance results for Nomenclator in a search space of 1000 data repositories. 1. Introduction  Users canno...
691|Using Heterogeneous Equivalences for Query Rewriting in Multidatabase Systems|In order to have significant practical impact on future information systems, multidatabase management systems (MDBMS) must be both flexible and efficient. We consider a MDBMS with a common object-oriented model, based on the ODMG standard, and local databases that may be relational or object-oriented. In this context, query rewriting (for optimization) is made difficult by schematic discrepancy, and the need to model mapping information between the multidatabase and local schemas. We address the flexibility issue by representing the mappings from a local schema to the multidatabase schema, as a set of heterogeneous object equivalences, in a declarative language. Efficiency is obtained by exploiting these equivalences to rewrite multidatabase OQL queries into equivalent, simplified queries on the local schemas. 1 Introduction  The advent of open systems is increasingly stimulating the development of information systems which can provide high-level integration of heterogeneous informatio...
692|The Identification and Resolution of Semantic Heterogeneity in Multidatabase Systems|This paper describes several aspects of the  Remote--Exchange project at USC, which focuses on the controlled sharing and exchange of information among autonomous, heterogeneous database systems. The spectrum of heterogeneity which may exist among the components in a federation of database systems is examined, and an approach to accommodating such heterogeneity is described. An overview of the Remote--Exchange experimental system is provided. 1 Introduction  Consider an environment consisting of a collection of data/knowledge bases and their supporting systems, and in which it is desired to accommodate the controlled sharing and exchange of information among the collection. We shall refer to this as the (interconnected) autonomous heterogeneous database environment. Such environments are extremely common in various application domains, including office information systems, computer-integrated manufacturing systems (with computer-aided design as a subset), personal computing, business a...
693|Probabilistic Roadmaps for Path Planning in High-Dimensional Configuration Spaces|A new motion planning method for robots in static workspaces is presented. This method proceeds in two phases: a learning phase and a query phase. In the learning phase, a probabilistic roadmap is constructed and stored as a graph whose nodes correspond to collision-free configurations and whose edges correspond to feasible paths between these configurations. These paths are computed using a simple and fast local planner. In the query phase, any given start and goal configurations of the robot are connected to two nodes of the roadmap; the roadmap is then searched for a path joining these two nodes. The method is general and easy to implement. It can be applied to virtually any type of holonomic robot. It requires selecting certain parameters (e.g., the duration of the learning phase) whose values depend on the scene, that is the robot and its workspace. But these values turn out to be relatively easy to choose, Increased efficiency can also be achieved by tailoring some components of the method (e.g., the local planner) to the considered robots. In this paper the method is applied to planar articulated robots with many degrees of freedom. Experimental results show that path planning can be done in a fraction of a second on a contemporary workstation (=150 MIPS), after learning for relatively short periods of time (a few dozen seconds)
694|Planning Motions with Intentions|We apply manipulation planning to computer animation. A new path planner is presented that automatically computes the collision-free trajectories for several cooperating arms to manipulate a movable object between two configurations. This implemented planner is capable of dealing with complicated tasks where regrasping is involved. In addition, we present a new inverse kinematics algorithm for the human arms. This algorithm is utilized by the planner for the generation  of realistic human arm motions as they manipulate objects. We view our system as a tool for facilitating the production of animation.
695|A Probabilistic Learning Approach to Motion Planning|In this paper a new paradigm for robot motion planning is proposed.  We split the motion planning process into two phases: the learning phase  and the query phase. In the learning phase we construct a probabilistic  roadmap in configuration space. This roadmap is a graph where nodes  correspond to randomly chosen configurations in free space and edges correspond  to simple collision-free motions between the nodes. These simple  motions are computed using a fast local method. The longer we learn, the  denser the roadmap becomes and the better it is for motion planning. In  the query phase we can use this roadmap to find paths between different  pairs of configurations. If a possible path is not found one can always extend  the roadmap by learning further. This gives a very flexible scheme in  which learning time and success for queries can be balanced.  We will demonstrate the power of the paradigm by applying it to various  instances of motion planning : free flying planar robots, plan...
696|Real-Time Robot Motion Planning Using Rasterizing Computer Graphics Hardware|We present a real-time robot motion planner that is fast andcomplete to a resolution. The technique is guaranteed to find a path if one exists at the resolution, and all paths returned are safe. The planner can handle any polyhedral geometry of robot and obstacles, including disjoint and highly concave unions of polyhedra. The planner uses standard graphics hardware to rasterize configuration space obstacles into a series of bitmap slices, and then uses dynamic programming to create a navigation function (a discrete vector-valued function) and to calculate paths in this rasterized space. The motion paths which the planner produces are minimal with respect to an L 1 (Manhattan) distance metric that includes rotation as well as translation. Several examples are shown illustrating the competence of the planner at generating planar rotational and translational plans for complex two and three dimensional robots. Dynamic motion sequences, including complicated and non-obvious backtracking so...
697|Randomized Query Processing in Robot Path Planning (Extended Abstract)  (1995) |The subject of this paper is the analysis of a randomized preprocessing scheme that has been used for query processing in robot path planning. The attractiveness of the scheme stems from its general applicability to virtually any path-planning problem, and its empirically observed success. In this paper we initiate a theoretical basis for explaining this empirical success. Under a simple assumption about the configuration space, we show that it is possible to perform preprocessing following which queries can be answered quickly. En route, we consider related problems on graph connectivity in the evasiveness model, and art-gallery theorems.
698|A Random Approach to Motion Planning|The motion planning problem asks for determining a collision-free path  for a robot amidst a set of obstacles. In this paper we present a new approach  for solving this problem, based on the construction of a random network of  possible motions, connecting the source and goal configuration of the robot.
699|Motion planning with six degrees of freedom by multistrategic bidirectional heuristic free-space enumeration|Abstract-This paper presents a general and efficient method that uses a configuration space for planning a collision-free path among known stationary obstacles for an arbitrarily moving object with six degrees of freedom. The basic approach taken in this method is to restrict the free space concerning path planning and to avoid executing unnecessary collision detections. The six-dimensional configuration space is equally quantized into cells by placing a regular grid, and the cells concerning path planning are enumerated by simultaneously executing multiple search strategies. Search strategies of different characteristics are defined by assigning different values to the coefficients of heuristic functions. The efficiency of each search strategy is evaluated during free-space enumeration, and a more promising one is automatically selected and is preferentially executed. The total number of necessary collision detections for free-space enumeration mainly depends on the most efficient search strategy among the evaluated strategies. Therefore, the free-space cells are efficiently enumerated for an arbitrary moving object in all kinds of working environments. This method has been implemented and has been applied to several examples that have different characteristics. I.
700|Randomized Preprocessing of Configuration Space for Path Planning: Articulated Robots|This paper presents a new approach to path planning for robots with many degrees of freedom (dof) operating in known static environments. The approach consists of a preprocessing and a planning stage. Preprocessing, which is done only once for a given environment, generates a network of randomly, but properly selected, collision-free configurations (nodes). Planning then connects any given initial and final configurations of the robot to two nodes of the network and computes a path through the network between these two nodes. Experiments show that after paying the preprocessing cost (on the order of hundreds of seconds), planning is extremely fast (on the order of a fraction of a second for many difficult examples involving a 10-dof robot). The approach is particularly attractive for many-dof robots which have to perform many successive point-to-point motions in the same environment.
701|Computation of Configuration-Space Obstacles Using the Fast Fourier Transform|This paper presents a new method for computing the configuration-space  map of obstacles that is used in motion-planning algorithms. The method de-  rives from the observation that, when the robot is a rigid object that can only  translate, the configuration space is a convolution of the workspace and the  robot. This convolution is computed with the use of the Fast Fourier Trans-  form (FFT) algorithm. The method is particularly promising for workspaces  with many and/or complicated obstacles, or when the shape of the robot is  not simple. It is an inherently parallel method that can significantly benefit  from existing experience and hardware on the FFT.
702|The particel swarm: Explosion, stability, and convergence in a multi-dimensional complex space  |The particle swarm is an algorithm for finding optimal regions of complex search spaces through interaction of individuals in a population of particles. Though the algorithm, which is based on a metaphor of social interaction, has been shown to perform well, researchers have not adequately explained how it works. Further, traditional versions of the algorithm have had some dynamical properties that were not considered to be desirable, notably the particles’ velocities needed to be limited in order to control their trajectories. The present paper analyzes the particle’s trajectory as it moves in discrete time (the algebraic view), then progresses to the view of it in continuous time (the analytical view). A 5-dimensional depiction is developed, which completely describes the system. These analyses lead to a generalized model of the algorithm, containing a set of coefficients to control the system’s convergence tendencies. Some results of the particle swarm optimizer, implementing modifications derived from the analysis, suggest methods for altering the original algorithm in ways that eliminate problems and increase the optimization power of the particle swarm
703|Particle swarm optimization| A concept for the optimization of nonlinear functions using particle swarm methodology is introduced. The evolution of several paradigms is outlined, and an implementation of one of the paradigms is discussed. Benchmark testing of the paradigm is described, and applications, including nonlinear function optimization and neural network training, are proposed. The relationships between particle swarm optimization and both artificial life and genetic algorithms are described.
704|Comparing inertia weights and constriction factors in particle swarm optimization|The performance of particle swarm optimization using an inertia weight is compared with performance using a constriction factor. Five benchmark functions are used for the comparison. It is concluded that the best approach is to use the constriction factor while limiting the maximum velocity Vmax to the dynamic range of the variable Xmax on each dimension. This approach provides performance on the benchmark functions superior to any other published results known by the authors.
705|Focused crawling: a new approach to topic-specific Web resource discovery|The rapid growth of the World-Wide Web poses unprecedented scaling challenges for general-purpose crawlers and search engines. In this paper we describe a new hypertext resource discovery system called a Focused Crawler. The goal of a focused crawler is to selectively seek out pages that are relevant to a pre-defined set of topics. The topics are specified not using keywords, but using exemplary documents. Rather than collecting and indexing all accessible Web documents to be able to answer all possible ad-hoc queries, a focused crawler analyzes its crawl boundary to find the links that are likely to be most relevant for the crawl, and avoids irrelevant regions of the Web. This leads to significant savings in hardware and network resources, and helps keep the crawl more up-to-date.  To achieve such goal-directed crawling, we designed two hypertext mining programs that guide our crawler: a classifier that evaluates the relevance of a hypertext document with respect to the focus topics, ...
706|The Anatomy of a Large-Scale Hypertextual Web Search Engine|In this paper, we present Google, a prototype of a large-scale search engine which makes heavy use of the structure present in hypertext. Google is designed to crawl and index the Web efficiently and produce much more satisfying search results than existing systems. The prototype with a full text and hyperlink database of at least 24 million pages is available at http://google.stanford.edu/

To engineer a search engine is a challenging task. Search engines index tens to hundreds of millions of web pages involving a comparable number of distinct terms. They answer tens of millions of queries every day. Despite the importance of large-scale search engines on the web, very little academic research has been done on them. Furthermore, due to rapid advance in technology and web proliferation, creating a web search engine today is very different from three years ago. This paper provides an in-depth description of our large-scale web search engine -- the first such detailed public description we know of to date.

Apart from the problems of scaling traditional search techniques to data of this magnitude, there are new technical challenges involved with using the additional information present in hypertext to produce better search results. This paper addresses this question of how to build a practical large-scale system which can exploit the additional information present in hypertext. Also we look at the problem of how to effectively deal with uncontrolled hypertext collections where anyone can publish anything they want.
707|Authoritative Sources in a Hyperlinked Environment|The network structure of a hyperlinked environment can be a rich source of information about the content of the environment, provided we have effective means for understanding it. We develop a set of algorithmic tools for extracting information from the link structures of such environments, and report on experiments that demonstrate their effectiveness in a variety of contexts on the World Wide Web. The central issue we address within our framework is the distillation of broad search topics, through the discovery of “authoritative ” information sources on such topics. We propose and test an algorithmic formulation of the notion of authority, based on the relationship between a set of relevant authoritative pages and the set of “hub pages ” that join them together in the link structure. Our formulation has connections to the eigenvectors of certain matrices associated with the link graph; these connections in turn motivate additional heuristics for link-based analysis.
708|Text Classification from Labeled and Unlabeled Documents using EM|  This paper shows that the accuracy of learned text classifiers can be improved by augmenting a small number of labeled training documents with a large pool of unlabeled documents. This is important because in many text classification problems obtaining training labels is expensive, while large quantities of unlabeled documents are readily available. We introduce an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation-Maximization (EM) and a naive Bayes classifier. The algorithm first trains a classifier using the available labeled documents, and probabilistically labels the unlabeled documents. It then trains a new classifier using the labels for all the documents, and iterates to convergence. This basic EM procedure works well when the data conform to the generative assumptions of the model. However these assumptions are often violated in practice, and poor performance can result. We present two extensions to the algorithm that improve ...
709|Improved algorithms for topic distillation in a hyperlinked environment|Abstract This paper addresses the problem of topic distillation on the World Wide Web, namely, given a typical user query to find quality documents related to the query topic. Connectivity analysis has been shown to be useful in identifying high quality pages within a topic specific graph of hyperlinked documents. The essence of our approach is to augment a previous connectivity analysis based algorithm with content analysis. We identify three problems with the existing approach and devise algorithms to tackle them. The results of a user evaluation are reported that show an improvement of precision at 10 documents by at least 45 % over pure connectivity analysis. 1
710|Enhanced Hypertext Categorization Using Hyperlinks|A major challenge in indexing unstructured hypertext databases is to automatically extract meta-data that enables structured search using topic taxonomies, circumvents keyword ambiguity, and improves the quality of search and profile-based routing and filtering. Therefore, an accurate classifier is an essential component of a hypertext database. Hyperlinks pose new problems not addressed in the extensive text classification literature. Links clearly contain highquality semantic clues that are lost upon a purely termbased classifier, but exploiting link information is non-trivial because it is noisy. Naive use of terms in the link neighborhood of a document can even degrade accuracy. Our contribution is to propose robust statistical models and a  relaxation labeling technique for better classification by exploiting link information in a small neighborhood around documents. Our technique also adapts gracefully to the fraction of neighboring documents having known topics. We experimented ...
711|Efficient Crawling Through URL Ordering|In this paper we study in what order a crawler should visit the URLs it has seen, in order to obtain more “important” pages first. Obtaining important pages rapidly can be very useful when a crawler cannot visit the entire Web in a reasonable amount of time. We define several importance metrics, ordering schemes, and performance evaluation measures for this problem. We also experimentally evaluate the ordering schemes on the Stanford University Web. Our results show that a crawler with a good ordering scheme can obtain important pages significantly faster than one without.
712|Automatic Resource Compilation by Analyzing Hyperlink Structure and Associated Text|We describe the design, prototyping and evaluation of ARC, a system for automatically compiling a list of authoritative web resources on any (sufficiently broad) topic. The goal of ARC is to compile resource lists similar to those provided by Yahoo! or Infoseek. The fundamental difference is that these services construct lists either manually or through a combination of human and automated effort, while ARC operates fully automatically. We describe the evaluation of ARC, Yahoo!, and Infoseek resource lists by a panel of human users. This evaluation suggests that the resources found by ARC frequently fare almost as well as, and sometimes better than, lists of resources that are manually compiled or classified into a topic. We also provide examples of ARC resource lists for the reader to examine.
713|Analysis of a very large AltaVista query log|In this paper we present an analysis of a 280 GB AltaVista Search Engine query log consisting of approximately 1 billion entries for search requests over a period of six weeks. This represents approximately 285 million user sessions, each an attempt to fill a single information need. We present an analysis of individual queries, query duplication, and query sessions. Furthermore we present results of a correlation analysis of the log entries, studying the interaction of terms within queries. Our data supports the conjecture that web users differ significantly from the user assumed in the standard information retrieval literature. Specifically, we show that web users type in short queries, mostly look at the first 10 results only, and seldom modify the query. This suggests that traditional information retrieval techniques might not work well for answering web search requests. The correlation analysis showed that the most highly correlated items are constituents of phrases. This result indicates it may be useful for search engines to consider search terms as parts of phrases even if the user did not explicitly specify them as such. 1
714|Searching the World Wide Web|Permissions: Requests for permissions to reproduce figures, tables, or portions of articles originally published in Circulation can be obtained via RightsLink, a service of the Copyright Clearance Center, not the Editorial Office. Once the online version of the published article for which permission is being requested is located, click Request Permissions in the middle column of the Web page under Services. Further information about this process is available in the Permissions and Rights Question and Answer document. Reprints: Information about reprints can be found online at:
715|Finding related pages in the World Wide Web|When using traditional search engines, users have to formulate queries to describe their information need. This paper discusses a different approach toweb searching where the input to the search process is not a set of query terms, but instead is the URL of a page, and the output is a set of related web pages. A related web page is one that addresses the same topic as the original page. For example, www.washingtonpost.com is a page related to www.nytimes.com, since both are online newspapers. We describe two algorithms to identify related web pages. These algorithms use only the connectivity information in the web (i.e., the links between pages) and not the content of pages or usage information. We haveimplemented both algorithms and measured their runtime performance. To evaluate the e ectiveness of our algorithms, we performed a user study comparing our algorithms with Netscape&#039;s \What&#039;s Related &#034; service [12]. Our study showed that the precision at 10 for our two algorithms are 73 % better and 51 % better than that of Netscape, despite the fact that Netscape uses both content and usage pattern information in addition to connectivity information.
716|Strong regularities in World Wide Web surfing|One of the most common modes of accessing information in the World Wide Web (WWW) is surfing from one document to another along hyperlinks. Several large empirical studies have revealed common patterns of surfing behavior. A model which assumes that users make a sequence of decisions to proceed to another page, continuing as long as the value of the current page exceeds some threshold, yields the probability distribution for the number of pages, or depth, that a user visits within a Web site. This model was verified by comparing its predictions with detailed measurements of surfing patterns. It also explains the observed Zipf-like distributions in page hits observed at WWW sites. Huberman et al 1The exponential growth of World Wide Web (WWW) is making it the standard information system for an increasing segment of the world&#039;s population. From electronic commerce and information resource to entertainment, the Web allows inexpensive and fast access to unique and novel services provided by individuals and institutions scattered throughout the world (1).
717|Scalable Feature Selection, Classification and Signature Generation for Organizing Large Text Databases Into Hierarchical Topic Taxonomies|We explore how to organize large text databases hierarchically by topic to aid better searching, browsing and filtering. Many corpora, such as internet directories, digital libraries, and patent databases are manually organized into topic hierarchies, also called taxonomies. Similar to indices for relational data, taxonomies make search and access more efficient. However, the exponential growth in the volume of on-line textual information makes it nearly impossible to maintain such taxonomic organization for large, fast-changing corpora by hand. We describe an automatic system that starts with a small sample of the corpus in which topics have been assigned by hand, and then updates the database with new documents as the corpus grows, assigning topics to these new documents with high speed and accuracy. To do this, we use techniques from statistical pattern recognition to efficiently separate the feature words, or...
718|Web Search Using Automatic Classification|We study the automatic classification of Web documents into pre-specified categories, with the objective of increasing the precision of Web search. We describe experiments in which we classify documents into high-level categories of the Yahoo! taxonomy, and a simple search architecture and implementation using this classification. The validation of our classification experiments offers interesting insights into the power of such automatic classification, as well as into the nature of Web content. Our research indicates that Web classification and search tools must compensate for artifices such as Web spamming that have resulted from the very existence of such tools. Keywords: Automatic classification, Web search tools, Web spamming, Yahoo! categories.
719|Learning from hotlists and coldlists: Towards a WWW information filtering and seeking agent|We describe a software agent that learns to find information on the World Wide Web (WWW), deciding what new pages might interest a user. The agent maintains a separate hotlist (for links that were interesting) and coldlist (for links that were not interesting) for each topic. By analyzing the information immediately accessible from each link, the agent learns the types of information the user is interested in. This can be used to inform the user when a new interesting page becomes available or to order the user&#039;s exploration of unseen existing links so that the more promising ones are investigated first. We compare four different learning algorithms on this task. We describe an experiment in which a simple Bayesian classifier acquires a user profile that agrees with a user&#039;s judgment over 90% of the time.
720|Surfing the Web Backwards|From a user’s perspective, hypertext links on the web form a directed graph between distinct information sources. We investigate the effects of discovering “backlinks ” from web resources, namely links pointing to the resource. We describe tools for backlink navigation on both the client and server side, using an applet for the client and a module for the Apache web server. We also discuss possible extensions to the HTTP protocol to facilitate the collection and navigation of backlink information in the world wide web. 1
721|Learning Probabilistic User Profiles: Applications to Finding Interesting Web Sites, Notifying Users of Relevant Changes to Web Pages, and Locating Grant Opportunities.|this article are:
722|Automatic Acquisition of Hyponyms from Large Text Corpora|We describe a method for the automatic acquisition of the hyponymy lexical relation from unrestricted text. Two goals motivate the approach: (i) avoidante of the need for pre-encoded knowledge and (ii) applicability across a wide range of text. We identify a set of lexico-syntactic patterns that are easily recognizable, that occur frequently and across text genre boundaries, and that indisputably indicate the lexical relation of interest. We describe a method for discovering these patterns and suggest that other lexical relations will also he acquirable iu this way. A subset of the acquisitiou algorithm is implemented and the results are used to augment and critique the structure of a large hand-built thesaurus. Extensions and applications to areas such as information retrieval are suggested.
723|WordNet: An on-line lexical database|WordNet is an on-line lexical reference system whose design is inspired by current
724|A practical part-of-speech tagger|We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
725|Noun Classification From Predicate.argument Structures|A method of determining the similarity of nouns on the basis of a metric derived from the distribution  of subject, verb and object in a large text corpus is described. The resulting quasi-semantic classification of nouns demonstrates the plausibility of the distributional hypothesis, and has potential application to a variety of tasks, including automatic indexing, resolving nominal compounds, and determining the scope of modification.
726|Automatic Acquisition Of Subcategorization Frames From Untagged Text|that takes a raw, untagged text corpus as its  only input (no open-class dictionary) and generates  a partial list of verbs occurring in the text  and the subcategorization frames (SFs) in which  they occur. Verbs are detected by a novel technique  based on the Case Filter of Rouvret and  Vergnaud (1980). The completeness of the output  list increases monotonically with the total number  of occurrences of each verb in the corpus. Fakse  positive rates are one to three percent of observations.
727|Noun Homograph Disambiguation Using Local Context in Large Text Corpora|This paper describes an accurate, relatively inexpensive method for the disambiguation of noun homographs using large text corpora. The algorithm checks the context surrounding the target noun against that of previously observed instances and chooses the sense for which the most evidence is found, where evidence consists of a set of orthographic, syntactic, and lexical features. Because the sense distinctions made are coarse, the disambiguation can be accomplished without the expense of knowledge bases or inference mechanisms. An implementation of the algorithm is described which, starting with a small set of hand-labeled instances, improves its results automatically via unsupervised training. The approach is compared to other attempts at homograph disambiguation using both machine readable dictionaries and unrestricted text and the use of training instances is determined to be a crucial difference. 1 Introduction  Large text corpora and the computational resources to handle them have ...
728|Automatically Extracting And Representing Collocations For Language Generation|... this paper, focusing on the acquisition problem. We describe a program, Xtract, that automatically acquires a range of collocations from large textual corpora and we describe how they can be repreSented in a flexible lexicon using a unification bed formalism
729|Disambiguating Prepositional Phrase Attachments by Using On-Line Dictionary Definitions|... This paper sustains that claim by describing a set of computational tools and techniques used to disambiguate prepositional phrase attachments in English sentences, by accessing on-line dictionary definitions. Such techniques offer hope for eliminating the time-consuming, and often incomplete, hand coding of semantic information that has been conventional in natural language understanding systems.
730|Semantically significant patterns in dictionary definitions|Natural language processing systems need large lexicons containing explicit information about lexical-semantlc relationships, selection restrictions, and verb categories. Because the labor involved in constructing such lexicons by hand is overwhelming, we have been trying to construct lexical entries automatically from information available in the machine-readable version of
731|Extraction of Semantic Information from an Ordinary English Dictionary and its Evaluation|U Collective + &#039;O&#039; V P&#039; + W &#039;T&#039; + T X &#039;T&#039; + Y &#039;T&#039; + &#039;Q&#039; Z UNMARKED  total 957 836  26 15  359 181  27 21  257 187  453 314  111 79  3457 2426  42 26  5794 3927  2 2  631 464  875 603  2144 1436  69 42  758 593  23 14  4 3  1291 887  16577 9668  789 398  20 15  103 61  97 108  4t 18  415 199  43560 249(16 p&#039;lan  life  mothe an. developerl under the influence  the nous related to &#039;Anima;;e&#039;,  Nouns rduted to the concept animate have a relatively simple structure bt the thesaurus, s aimale is ofteu us am an xample of ;t theqmrtts.like system. ]qxmnple of tim words mark as %rfimte (Q)&#039; m*d related mmm, especially marked ms &#039;plant +  The, prodaced thesgnras contains laol than 60% of the words ;g&#039;t-Nd i simple concepts, such &#039;pD, nt&#039; (table 10), &#039;mtinml&#039;, anal ;}l..lgo (pers0u ht definitions)&#039;, in correct positions. As shown in t,ble 10 for example 845 words ure traversed kom  &#039;lLble 10: Nouns Related to (Living) Thing aml Plant  (hving) thing - plu; (P)  A 2  1) 2 P 370  Q othe 270  total 645  i, la&#039;,ai in the prodhoed &#039;thesaurus; 370 words (62.4%) of these winds are n&#039;.arked t,.s &#039;Plmtt?  , &#039;owever, the produc thesaurus does not captm dionactive co*ceFs sa&lt;h as %nimal or plant (V)&#039; corrtl. In the definition of c&#039;roasbreed (tgble 9), the produced thanrus only u lant as :, key noun aud ingor animal. This h a typical problem ht the current produced hmmas. Note thirst the distinction between mte (Q)&#039; and &#039;animal m. pl.n (&#039;V)&#039; (animate wiChoat human) .nm o be dimcult for ht leico;;rphek-s: bed is marked as Q; crossbreed, however, is z,m&#039;kcd as V lot exple.
732|Acquiring Lexical Knowledge from Text: A Case Study|Language acquisition addresses two important text processing issues. The immediate problem is understanding a text in spite of the existence of lexical gaps. The long term issue is that the understander must incorporate new words into its lexicon for future use. This paper describes an approach to constructing new lexical entries in a gradual process by analyzing a sequence of example texts. This approach permits the graceful tolerance of new words while enabling the automated extension of the lexicon. Each new acquired lexeme starts as a set of assumptions derived from the analysis of each word in a textual context. A variety of knowledge sources, including
733|Processing Dictionary Definitions With Phrasal Pattern Hierarchies|This paper shows how dictionary word sense definitions can be analysed by applying a hierarchy of  phrasal patterns. An experimental system embodying this mechanism has been implemented for  processing definitions from the Longman Dictionary of Contemporary English. A property of this  dictionary, exploited by the system, is that it uses a restricted vocabulary in its word sense definitions
734|Where the REALLY Hard Problems Are|It is well known that for many NP-complete problems, such as K-Sat, etc., typical cases are easy to solve; so that computationally hard cases must be rare (assuming P != NP). This paper shows that NP-complete problems can be summarized by at least one &#034;order parameter&#034;, and that the hard problems occur at a critical value of such a parameter. This critical value separates two regions of characteristically different properties. For example, for K-colorability, the critical value separates overconstrained from underconstrained random graphs, and it marks the value at which the probability of a solution changes abruptly from near 0 to near 1. It is the high density of well-separated almost solutions (local minima) at this boundary that cause search algorithms to &#034;thrash&#034;. This boundary is a type of phase transition and we show that it is preserved under mappings between problems. We show that for some P problems either there is no phase transition or it occurs for bounded N (and so bound...
735|Almost all k-colorable graphs are easy to color|We describe a simple and e	cient heuristic algorithm for the graph coloring problem and show that for all k    it nds an optimal coloring for almost all kcolorable graphs  We also show that an algorithm proposed by Brelaz and justi ed on experimental grounds optimally colors almost all kcolorable graphs  E	cient implementations of both algorithms are given  The rst one runs in O
nm log k  time where n is the number of vertices and m the number of edges  The new implementation of Brelazs algorithm runs in O
m logn time  We observe that the popular greedy heuristic works poorly on kcolorable graphs
736|T.Hogg: Phase Transition in|From birth, children live in a sexual world, and the ways they are touched and treated send messages about their worth and about being loved. Voice and body language convey feelings about intimacy and relationship. Words and actions impart values about sexuality, sexual orientation, responsibility, and gender roles. Parents, television and films, religious leaders, musicians and actors, politicians, peers, and advertisers send messages about sexuality. We want our children to have healthy, rewarding lives, to like themselves, and to develop loving, mutually supportive relationships. We want them to act responsibly and to make choices that arise from the values they hold. Unfortunately, in many homes, across many cultures, adults are embarrassed about sexuality and fail to let their children know that sexual expression is integral to loving, committed, mutually supportive, intimate relationships. Research shows that when parents approach their role as sex educators in positive, affirming ways, young people are better able to make sexually healthy decisions and to build loving relationships. Parents who respond honestly to questions, provide resources, express their feelings and values, and portray sexuality and the need for intimacy as integral elements of life rear youth who respect themselves and behave responsibly. In 2001, Advocates for Youth launched the Rights.
737|Photo tourism: Exploring photo collections in 3D|Figure 1: Our system takes unstructured collections of photographs such as those from online image searches (a) and reconstructs 3D points and viewpoints (b) to enable novel ways of browsing the photos (c). We present a system for interactively browsing and exploring large unstructured collections of photographs of a scene using a novel 3D interface. Our system consists of an image-based modeling front end that automatically computes the viewpoint of each photograph as well as a sparse 3D model of the scene and image to model correspondences. Our photo explorer uses image-based rendering techniques to smoothly transition between photographs, while also enabling full 3D navigation and exploration of the set of images and world geometry, along with auxiliary information such as overhead maps. Our system also makes it easy to construct photo tours of scenic or historic locations, and to annotate image details, which are automatically transferred to other relevant images. We demonstrate our system on several large personal photo collections as well as images gathered from Internet photo sharing sites.
738|Distinctive Image Features from Scale-Invariant Keypoints|This paper presents a method for extracting distinctive invariant features from  images, which can be used to perform reliable matching between different images  of an object or scene. The features are invariant to image scale and rotation,  and are shown to provide robust matching across a a substantial range  of affine distortion, addition of noise, change in 3D viewpoint, and change in  illumination. The features are highly distinctive, in the sense that a single feature  can be correctly matched with high probability against a large database of  features from many images. This paper also describes an approach to using  these features for object recognition. The recognition proceeds by matching  individual features to a database of features from known objects using a fast  nearest-neighbor algorithm, followed by a Hough transform to identify clusters  belonging to a single object, and finally performing verification through leastsquares  solution for consistent pose parameters. This approach to recognition  can robustly identify objects among clutter and occlusion while achieving near  real-time performance.
740|Video google: A text retrieval approach to object matching in videos|We describe an approach to object and scene retrieval which searches for and localizes all the occurrences of a user outlined object in a video. The object is represented by a set of viewpoint invariant region descriptors so that recognition can proceed successfully despite changes in viewpoint, illumination and partial occlusion. The temporal continuity of the video within a shot is used to track the regions in order to reject unstable regions and reduce the effects of noise in the descriptors. The analogy with text retrieval is in the implementation where matches on descriptors are pre-computed (using vector quantization), and inverted file systems and document rankings are used. The result is that retrieval is immediate, returning a ranked list of key frames/shots in the manner of Google. The method is illustrated for matching on two full length feature films. 1.
741|Light Field Rendering|A number of techniques have been proposed for flying through scenes by redisplaying previously rendered or digitized views. Techniques have also been proposed for interpolating between views by warping input images, using depth information or correspondences between multiple images. In this paper, we describe a simple and robust method for generating new views from arbitrary camera positions without depth information or feature matching, simply by combining and resampling the available images. The key to this technique lies in interpreting the input images as 2D slices of a 4D function - the light field. This function completely characterizes the flow of light through unobstructed space in a static scene with fixed illumination. We describe a 
743|The Lumigraph|This paper discusses a new method for capturing the complete appearanceof both synthetic and real world objects and scenes, representing this information, and then using this representation to render images of the object from new camera positions. Unlike the shape capture process traditionally used in computer vision and the rendering process traditionally used in computer graphics, our approach does not rely on geometric representations. Instead we sample and reconstruct a 4D function, which we call a Lumigraph. The Lumigraph is a subset of the complete plenoptic function that describes the flow of light at all positions in all directions. With the Lumigraph, new images of the object can be generated very quickly, independent of the geometric or illumination complexity of the scene or object. The paper discusses a complete working system including the capture of samples, the construction of the Lumigraph, and the subsequent rendering of images from this new representation. 1
744|An Optimal Algorithm for Approximate Nearest Neighbor Searching in Fixed Dimensions|Consider a set S of n data points in real d-dimensional space, R d , where distances are measured using any Minkowski metric. In nearest neighbor searching we preprocess S into a data structure, so that given any query point q 2 R d , the closest point of S to q can be reported quickly. Given any positive real ffl, a data point p is a (1 + ffl)-approximate nearest neighbor of q if its distance from q is within a factor of (1 + ffl) of the distance to the true nearest neighbor. We show that it is possible to preprocess a set of n points in R d in O(dn log n) time and O(dn) space, so that given a query point q 2 R d , and ffl ? 0, a (1 + ffl)-approximate nearest neighbor of q can be computed in O(c d;ffl log n) time, where c d;ffl d d1 + 6d=ffle d is a factor depending only on dimension and ffl. In general, we show that given an integer k 1, (1 + ffl)-approximations to the k nearest neighbors of q can be computed in additional O(kd log n) time.
745|Plenoptic Modeling: An Image-Based Rendering System|Image-based rendering is a powerful new approach for generating real-time photorealistic computer graphics. It can provide convincing animations without an explicit geometric representation. We use the “plenoptic function” of Adelson and Bergen to provide a concise problem statement for image-based rendering paradigms, such as morphing and view interpolation. The plenoptic function is a parameterized function for describing everything that is visible from a given point in space. We present an image-based rendering system based on sampling, reconstructing, and resampling the plenoptic function. In addition, we introduce a novel visible surface algorithm and a geometric invariant for cylindrical projections that is equivalent to the epipolar constraint defined for planar projections.
746|Labeling Images with a Computer Game|We introduce a new interactive system: a game that is fun and can be used to create valuable output. When people  play the game they help determine the contents of images  by providing meaningful labels for them. If the game is  played as much as popular online games, we estimate that most images on the Web can be labeled in a few months. Having proper labels associated with each image on the  Web would allow for more accurate image search, improve the accessibility of sites (by providing descriptions of images to visually impaired individuals), and help users  block inappropriate images. Our system makes a significant contribution because of its valuable output and because of the way it addresses the image-labeling problem. Rather than using computer vision techniques, which don&#039;t work well enough, we encourage people to do the work by taking advantage of their desire to be entertained.
747|LabelMe: A Database and Web-Based Tool for Image Annotation|  We seek to build a large collection of images with ground truth labels to be used for object detection and recognition research. Such data is useful for supervised learning and quantitative evaluation. To achieve this, we developed a web-based tool that allows easy image annotation and instant sharing of such annotations. Using this annotation tool, we have collected a large dataset that spans many object categories, often containing multiple instances over a wide variety of images. We quantify the contents of the dataset and compare against existing state of the art datasets used for object recognition and detection. Also, we show how to extend the dataset to automatically enhance object labels with WordNet, discover object parts, recover a depth ordering of objects in a scene, and increase the number of labels using minimal user supervision and images from the web.
748| View Interpolation for Image Synthesis |Image-space simplifications have been used to accelerate the calculation of computer graphic images since the dawn of visual simulation. Texture mapping has been used to provide a means by which images may themselves be used as display primitives. The work reported by this paper endeavors to carry this concept to its logical extreme by using interpolated images to portray three-dimensional scenes. The special-effects technique of morphing, which combines interpolation of texture maps and their shape, is applied to computing arbitrary intermediate frames from an array of prestored images. If the images are a structured set of views of a 3D object or scene, intermediate frames derived by morphing can be used to approximate intermediate 3D transformations of the object or scene. Using the view interpolation approach to synthesize 3D scenes has two main advantages. First, the 3D representation of the scene may be replaced with images. Second, the image synthesis time is independent of the scene complexity. The correspondence between images, required for the morphing method, can be predetermined automatically using the range data associated with the images. The method is further accelerated by a quadtree decomposition and a view-independent visible priority. Our experiments have shown that the morphing can be performed at interactive rates on today’s high-end personal computers. Potential applications of the method include virtual holograms, a walkthrough in a virtual environment, image-based primitives and incremental rendering. The method also can be used to greatly accelerate the computation of motion blur and soft shadows cast by area light sources.
749|A Touring Machine: Prototyping 3D Mobile Augmented Reality Systems for Exploring the Urban Environment|We describe a prototype system that combines together the overlaid 3D graphics of augmented reality with the untethered freedom of mobile computing. The goal is to explore how these two technologies might together make possible wearable computer systems that can support users in their everyday interactions with the world. We introduce an application that presents information about our university’s campus, using a head-tracked, see-through, headworn, 3D display, and an untracked, opaque, handheld, 2D display with stylus and trackpad. We provide an illustrated explanation of how our prototype is used, and describe our rationale behind designing its software infrastructure and selecting the hardware on which it runs.
750|A comparison of affine region detectors|The paper gives a snapshot of the state of the art in affine covariant region detectors, and compares their performance on a set of test images under varying imaging conditions. Six types of detectors are included: detectors based on affine normalization around Harris [24, 34] and Hessian points [24], as proposed by Mikolajczyk and Schmid and by Schaffalitzky and Zisserman; a detector of ‘maximally stable extremal regions’, proposed by Matas et al. [21]; an edge-based region detector [45] and a detector based on intensity extrema [47], proposed by Tuytelaars and Van Gool; and a detector of ‘salient regions’, proposed by Kadir, Zisserman and Brady [12]. The performance is measured against changes in viewpoint, scale, illumination, defocus and image compression. The objective of this paper is also to establish a reference test set of images and performance software, so that future detectors can be evaluated in the same framework. 1
751|Unstructured lumigraph rendering|We describe an image based rendering approach that generalizes many image based rendering algorithms currently in use including light field rendering and view-dependent texture mapping. In particular it allows for lumigraph style rendering from a set of input cameras that are not restricted to a plane or to any specific manifold. In the case of regular and planar input camera positions, our algorithm reduces to a typical lumigraph approach. In the case of fewer cameras and good approximate geometry, our algorithm behaves like view-dependent texture mapping. Our algorithm achieves this flexibility because it is designed to meet a set of desirable goals that we describe. We demonstrate this flexibility with a variety of examples. Keyword Image-Based Rendering 1
752|View morphing|Image morphing techniques can generate compelling 2D transitions between images. However, differences in object pose or viewpoint often cause unnatural distortions in image morphs that are difficult to correct manually. Using basic principles of projective geometry, this paper introduces a simple extension to image morphing that correctly handles 3D projective camera and scene transformations. The technique, called view morphing, works by prewarping two images prior to computing a morph and then postwarping the interpolated images. Because no knowledge of 3D shape is required, the technique may be applied to photographs and drawings, as well as rendered scenes. The ability to synthesize changes both in viewpoint and image structure affords a wide variety of interesting 3D effects via simple image transformations.
753|Constrained Delaunay triangulations|Given a set of n vertices in the plane together with a set of noncrossing edges, the constrained Delaunay triangulation (CDT) is the triangulation of the vertices with the following properties: (1) the prespecified edges are included in the triangulation, and (2) it is as close as possible to the Delaunay triangulation. We show that the CDT can be built in optimal O(n log n) time using a divide-and-conquer technique. This matches the time required to build an arbitrary (unconstrained) Delaunay triangulation and the time required to build an arbitrary constrained (nonDelaunay) triangulation. CDTs, because of their relationship with Delaunay triangulations, have a number of properties that should make them useful for the finite-element method. Applications also include motion planning in the presence of polygonal obstacles in the plane and constrained Euclidean minimum spanning trees, spanning trees subject to the restriction that some edges are prespecified. I’wnishi0tt to copy without tix all or part of thk material is granlcd provided thal IIIC wpics arc not nude or distributed li)r direct commercial advanlagc, the ACM copyright wficc and the title of lhc publication and its date appear. and notice is given that copying is hy permission ol the Association Car Computing Machinery. ‘To copy otherwise. or to republish. requires a fee and/or specific permission.
754|How Do People Manage Their Digital Photographs|In this paper we present and discuss the findings of a study that investigated how people manage their collections of digital photographs. The six-month, 13-participant study included interviews, questionnaires, and analysis of usage statistics gathered from an instrumented digital photograph management tool called Shoebox. Alongside simple browsing features such as folders, thumbnails and timelines, Shoebox has some advanced multimedia features: content-based image retrieval and speech recognition applied to voice annotations. Our results suggest that participants found their digital photos much easier to manage than their non-digital ones, but that this advantage was almost entirely due to the simple browsing features. The advanced features were not used very often and their perceived utility was low. These results should help to inform the design of improved tools for managing personal digital photographs.
755|Video Indexing Based on Mosaic Representations|Video is a rich source of information. It provides visual information about scenes. However, this information is implicitly buried inside the raw video data, and is provided with the cost of very high temporal redundancy. While the standard sequential form of video storage is adequate for viewing in a &#034;movie mode&#034;, it fails to support rapid access to information of interest that is required in many of the emerging applications of video. This paper presents an approach for efficient access, use and manipulation of video data. The video data is first transformed from its sequential and redundant frame-based representation in which the information about the scene is distributed over many frames, to an explicit and compact scene-based representation, to which each frame can be directly related. This compact reorganization of the video data supports  non-linear browsing and efficient indexing to provide rapid access directly to information of interest. The paper describes a new set of metho...
756|Modelling and interpretation of architecture from several images |The modelling of 3-dimensional (3D) environments has become a requirement for many applications in engineering design, virtual reality, visualisation and entertainment. However the scale and complexity demanded from such models has risen to the point where the acquisition of 3D models can require a vast amount of specialist time and equipment. Because of this much research has been undertaken in the computer vision community into automating all or part of the process of acquiring a 3D model from a sequence of images. This thesis focuses specifically on the automatic acquisition of architectural models from short image sequences. An architectural model is defined as a set of planes corresponding to walls which contain a variety of labelled primitives such as doors and windows. As well as a label defining its type, each primitive contains parameters defining its shape and texture. The key advantage of this representation is that the model defines not only geometry and texture, but also an interpretation of the scene. This is crucial as it enables reasoning about the scene; for instance, structure and texture can be inferred in areas of the model which are unseen in any
757|Image alignment and stitching: a tutorial|This tutorial reviews image alignment and image stitching algorithms. Image alignment algorithms can discover the correspondence relationships among images with varying degrees of overlap. They are ideally suited for applications such as video stabilization, summarization, and the creation of panoramic mosaics. Image stitching algorithms take the alignment estimates produced by such registration algorithms and blend the images in a seamless manner, taking care to deal with potential problems such as blurring or ghosting caused by parallax and scene movement as well as varying image exposures. This tutorial reviews the basic motion models underlying alignment and stitching algorithms, describes effective direct (pixel-based) and feature-based alignment algorithms, and describes blending algorithms used to produce
758|The design and implementation of a generic sparse bundle adjustment software package based on the levenberg-marquardt algorithm|The most recent revision of this document will always be found at
759|Automatic Line matching across views|HAL is a multi-disciplinary open access archive for the deposit and dissemination of sci-entific research documents, whether they are pub-lished or not. The documents may come from teaching and research institutions in France or abroad, or from public or private research centers. L’archive ouverte pluridisciplinaire HAL, est destine´e au de´po^t et a ` la diffusion de documents scientifiques de niveau recherche, publie´s ou non, e´manant des e´tablissements d’enseignement et de recherche franc¸ais ou e´trangers, des laboratoires publics ou prive´s.
760|Temporal event clustering for digital photo collections |Organizing digital photograph collections according to events such as holiday gatherings or vacations is a common practice among photographers. To support photographers in this task, we present similarity-based methods to cluster digital photos by time and image content. The approach is general and unsupervised, and makes minimal assumptions regarding the structure or statistics of the photo collection. We present several variants of an automatic unsupervised algorithm to partition a collection of digital photographs based either on temporal similarity alone, or on temporal and content-based similarity. First, interphoto similarity is quantified at multiple temporal scales to identify likely event clusters. Second, the final clusters are determined according to one of three clustering goodness criteria. The clustering criteria trade off computational complexity and performance. We also describe a supervised clustering method based on learning vector quantization. Finally, we review the results of an experimental evaluation of the proposed algorithms and existing approaches on two test collections.
761|Automatic organization for digital photographs with geographic coordinates|We describe PhotoCompas, a system that utilizes the time and location information embedded in digital photographs to automatically organize a personal photo collection. PhotoCompas produces browseable location and event hierarchies for the collection. These hierarchies are created using algorithms that interleave time and location to produce an organization that mimics the way people think about their photo collections. In addition, the algorithm annotates the generated hierarchy with geographical names. We tested our approach in case studies of three real-world collections and verified that the results are meaningful and useful for the collection owners.
762|Calibrated, Registered Images of an Extended Urban Area|We describe a dataset of several thousand calibrated, time-stamped, geo-referenced, high  dynamic range color images, acquired under uncontrolled, variable illumination conditions in  an outdoor region spanning several hundred meters. The image data is grouped into several  regions which have little mutual inter-visibility. For each group, the calibration data is globally  consistent on average to roughly five centimeters and 0.1 # , or about four pixels of epipolar  registration. All image, feature and calibration data is available for interactive inspection and  downloading at http://city.lcs.mit.edu/data.
763|From where to what: Metadata sharing for digital photographs with geographic coordinates|Abstract. We describe LOCALE, a system that allows cooperating information systems to share labels for photographs. Participating photographs are enhanced with a geographic location stamp – the latitude and longitude where the photograph was taken. For a photograph with no label, LOCALE can use the shared information to assign a label based on other photographs that were taken in the same area. LOCALE thus allows (i) text search over unlabeled sets of photos, and (ii) automated label suggestions for unlabeled photos. We have implemented a LOCALE prototype where users cooperate in submitting labels and locations, enhancing search quality for all users in the system. We ran an experiment to test the system in centralized and distributed settings. The results show that the system performs search tasks with surprising accuracy, even when searching for specific landmarks. 1
764|Interactive Design of Multi-Perspective Images For Visualizing Urban Landscapes|Multi-perspective images are a useful way to visualize extended, roughly planar scenes such as landscapes or city blocks. However, constructing effective multi-perspective images is something of an art. In this paper, we describe an interactive system for creating multi-perspective images composed of serially blended cross-slits images. Beginning with a sideways-looking video of the scene as might be captured from a moving vehicle, we allow the user to interactively specify a set of cross-slits cameras, possibly with gaps between them. In each camera, one of the slits is defined to be the camera path, which is typically horizontal, and the user is left to choose the second slit, which is typically vertical. The system then generates intermediate views between these cameras using a novel interpolation scheme, thereby producing a multi-perspective image with no seams. The user can also choose the picture surface in space onto which viewing rays are projected, thereby establishing a parameterization for the image. We show how the choice of this surface can be used to create interesting visual effects. We demonstrate our system by constructing multi-perspective images that summarize city blocks, including corners, blocks with deep plazas and other challenging urban situations.
765|A System Architecture for Ubiquitous Video |Realityflythrough is a telepresence/tele-reality system that works in the dynamic, uncalibrated environments typically associated with ubiquitous computing. By harnessing networked mobile video cameras, it allows a user to remotely and immersively explore a physical space. RealityFlythrough creates the illusion of complete live camera coverage in a physical environment. This paper describes the architecture of RealityFlythrough, and evaluates it along three dimensions: (1) its support of the abstractions for infinite camera coverage, (2) its scalability, and (3) its robustness to changing user requirements. 1
766|A System for Automatic Pose-Estimation from a Single Image in a City Scene|We describe an automatic system for pose-estimation from a single image in a city scene. Each building has a model consisting of a number of parallel planes associated with it. The homographies for the best match of the planes to the image is estimated automatically for each of the possible buildings. We show how the estimation of homographies can be done effectively by reducing the search space and using fast convolution. The model having the best match is then used to determine the position and orientation of the camera. The results
767|Sea of Images|A long-standing research problem in computer graphics is to reproduce the visual experience of walking through a large photorealistic environment interactively. On one hand, traditional geometry-based rendering systems fall short of simulating the visual realism of a complex environment. On the other hand, image-based rendering systems have to date been unable to capture and store a sampled representation of a large environment with complex lighting and visibility effects.
768|Spectral Partitioning for Structure from Motion |We propose a spectral partitioning approach for large-scale optimization problems, specifically structure from motion. In structure from motion, partitioning methods reduce the problem into smaller and better conditioned subproblems which can be efficiently optimized. Our partitioning method uses only the Hessian of the reprojection error and its eigenvectors. We show that partitioned systems that preserve the eigenvectors corresponding to small eigenvalues result in lower residual error when optimized. We create partitions by clustering the entries of the eigenvectors of the Hessian corresponding to small eigenvalues. This is a more general technique than relying on domain knowledge and heuristics such as bottom-up structure from motion approaches. Simultaneously, it takes advantage of more information than generic matrix partitioning algorithms.
769|Interactive Image-Based Rendering Using Feature Globalization|Image-based rendering (IBR) systems enable virtual walkthroughs of photorealistic environments by warping and combining reference images to novel viewpoints under interactive user control. A significant challenge in such systems is to automatically compute image correspondences that enable accurate image warping.
